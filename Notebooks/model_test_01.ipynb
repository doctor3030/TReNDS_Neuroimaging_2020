{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "\n",
    "import scipy.stats as sts\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing as prep\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn import discriminant_analysis as disan\n",
    "from sklearn import calibration as calib\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn import svm\n",
    "from sklearn import gaussian_process as gaup\n",
    "from sklearn import mixture as mix\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble as ens\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# from keras import models as kermdls\n",
    "# from keras import layers as kerlrs\n",
    "# from keras import metrics as kmetrics\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "import pickle\n",
    "\n",
    "import nilearn as nl\n",
    "from nilearn import plotting, image\n",
    "from nilearn import datasets\n",
    "import nibabel as nb\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>age</th>\n",
       "      <th>domain1_var1</th>\n",
       "      <th>domain1_var2</th>\n",
       "      <th>domain2_var1</th>\n",
       "      <th>domain2_var2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>57.436077</td>\n",
       "      <td>30.571975</td>\n",
       "      <td>62.553736</td>\n",
       "      <td>53.325130</td>\n",
       "      <td>51.427998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>59.580851</td>\n",
       "      <td>50.969456</td>\n",
       "      <td>67.470628</td>\n",
       "      <td>60.651856</td>\n",
       "      <td>58.311361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10004</td>\n",
       "      <td>71.413018</td>\n",
       "      <td>53.152498</td>\n",
       "      <td>58.012103</td>\n",
       "      <td>52.418389</td>\n",
       "      <td>62.536641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10005</td>\n",
       "      <td>66.532630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.108977</td>\n",
       "      <td>69.993075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10007</td>\n",
       "      <td>38.617381</td>\n",
       "      <td>49.197021</td>\n",
       "      <td>65.674285</td>\n",
       "      <td>40.151376</td>\n",
       "      <td>34.096421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>21746</td>\n",
       "      <td>14.257265</td>\n",
       "      <td>21.358872</td>\n",
       "      <td>61.165998</td>\n",
       "      <td>51.778483</td>\n",
       "      <td>54.640179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>21747</td>\n",
       "      <td>55.456978</td>\n",
       "      <td>68.169675</td>\n",
       "      <td>29.907995</td>\n",
       "      <td>55.349257</td>\n",
       "      <td>54.019517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>21750</td>\n",
       "      <td>48.948756</td>\n",
       "      <td>55.114811</td>\n",
       "      <td>60.878271</td>\n",
       "      <td>38.617246</td>\n",
       "      <td>50.679885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>21752</td>\n",
       "      <td>66.532630</td>\n",
       "      <td>59.844808</td>\n",
       "      <td>72.303110</td>\n",
       "      <td>55.458281</td>\n",
       "      <td>46.870235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>21754</td>\n",
       "      <td>68.820928</td>\n",
       "      <td>56.594193</td>\n",
       "      <td>34.605868</td>\n",
       "      <td>49.922535</td>\n",
       "      <td>50.383078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5877 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id        age  domain1_var1  domain1_var2  domain2_var1  domain2_var2\n",
       "0     10001  57.436077     30.571975     62.553736     53.325130     51.427998\n",
       "1     10002  59.580851     50.969456     67.470628     60.651856     58.311361\n",
       "2     10004  71.413018     53.152498     58.012103     52.418389     62.536641\n",
       "3     10005  66.532630           NaN           NaN     52.108977     69.993075\n",
       "4     10007  38.617381     49.197021     65.674285     40.151376     34.096421\n",
       "...     ...        ...           ...           ...           ...           ...\n",
       "5872  21746  14.257265     21.358872     61.165998     51.778483     54.640179\n",
       "5873  21747  55.456978     68.169675     29.907995     55.349257     54.019517\n",
       "5874  21750  48.948756     55.114811     60.878271     38.617246     50.679885\n",
       "5875  21752  66.532630     59.844808     72.303110     55.458281     46.870235\n",
       "5876  21754  68.820928     56.594193     34.605868     49.922535     50.383078\n",
       "\n",
       "[5877 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('00_Data/train_scores.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                0\n",
       "age               0\n",
       "domain1_var1    438\n",
       "domain1_var2    438\n",
       "domain2_var1     39\n",
       "domain2_var2     39\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_individual_maps_img(ids, icn_idx, ntwk_idx):\n",
    "#     maps = {}\n",
    "#     for patient_id in ids:\n",
    "#         patient_SM = h5py.File('00_Data/fMRI_train/{0}.mat'.format(patient_id), mode='r')\n",
    "#         patient_SM = np.array(patient_SM.get('SM_feature'))\n",
    "    \n",
    "#         patient_maps = {}\n",
    "#         for i in icn_idx.keys():\n",
    "#             component_maps = {}\n",
    "#             for j, _ in enumerate(icn_idx[i]):\n",
    "#                 jj = ntwk_idx[i][j]\n",
    "#                 component_maps[jj] = patient_SM[icn_idx[i]][j].transpose([2, 1, 0])\n",
    "#             patient_maps[i] = component_maps\n",
    "#         maps[patient_id] = patient_maps\n",
    "#     return maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('00_Data/train_scores.csv')\n",
    "\n",
    "# Nifti MASK image\n",
    "brain_mask = nb.load('00_Data/fMRI_mask.nii')\n",
    "\n",
    "# load functional componets correlation matrix\n",
    "fnc_10 = pd.read_csv('00_Data/fnc.csv')\n",
    "\n",
    "# retrive unique network names with corresponding components\n",
    "fnc10_cols = fnc_10.columns.to_list()[1:]\n",
    "fnc10_cols_filtered = []\n",
    "for i in fnc10_cols:\n",
    "    fnc10_cols_filtered.append(i.split('_')[0])\n",
    "    fnc10_cols_filtered.append(i.split('_')[2])\n",
    "    \n",
    "# Network index:\n",
    "ntw_components = {}\n",
    "ntw_labs = np.unique([i[:3] for i in fnc10_cols_filtered])\n",
    "for ii in ntw_labs:\n",
    "    ntw_components[ii] = np.unique([np.int(i.split('(')[-1].split(')')[0]) for i in fnc10_cols_filtered if ii in i])\n",
    "    \n",
    "# Look up matrix index\n",
    "icn_number = pd.read_csv('00_Data/ICN_numbers.csv')\n",
    "\n",
    "# create dictionary of networks with corresponding components\n",
    "icn_idx = {}\n",
    "for jj in ntw_components.keys():\n",
    "    icn_idx[jj] = np.array(icn_number.index[icn_number['ICN_number'].isin(ntw_components[jj])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADN': array([21, 56]),\n",
       " 'CBN': array([ 4,  7, 13, 18]),\n",
       " 'CON': array([33, 37, 38, 43, 48, 55, 61, 63, 67, 68, 70, 79, 81, 83, 84, 88, 96]),\n",
       " 'DMN': array([17, 23, 32, 40, 51, 71, 94]),\n",
       " 'SCN': array([45, 53, 69, 98, 99]),\n",
       " 'SMN': array([ 2,  3,  9, 11, 27, 54, 66, 72, 80]),\n",
       " 'VSN': array([ 5,  8, 12, 15, 16, 20, 62, 77, 93])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntw_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 5\n",
      "2 6\n",
      "3 7\n"
     ]
    }
   ],
   "source": [
    "for i, ID in enumerate([1,5,6,7]):\n",
    "    print(i,ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'age'\n",
    "\n",
    "train, test = model_selection.train_test_split(data, test_size=0.1)\n",
    "train, val = model_selection.train_test_split(train, test_size=0.2)\n",
    "X_train = train['Id']\n",
    "X_val = val['Id']\n",
    "X_test = test['Id']\n",
    "Y_train = train[target]\n",
    "Y_val = val[target]\n",
    "Y_test = test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_individual_maps(patient_id, icn_idx, ntwk_idx):\n",
    "    patient_SM = h5py.File('00_Data/fMRI_train/{0}.mat'.format(patient_id), mode='r')\n",
    "    patient_SM = np.array(patient_SM.get('SM_feature'))\n",
    "    \n",
    "    patient_maps = {}\n",
    "    for i in icn_idx.keys():\n",
    "        component_maps = {}\n",
    "        for j, _ in enumerate(icn_idx[i]):\n",
    "            jj = ntwk_idx[i][j]\n",
    "            component_maps[jj] = patient_SM[icn_idx[i]][j].transpose([2, 1, 0])\n",
    "        patient_maps[i] = component_maps\n",
    "    return patient_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ntwk_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-991e5eacb73d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mn_blocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mntwk_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mn_blocks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ntwk_idx' is not defined"
     ]
    }
   ],
   "source": [
    "batch_input = ['10005', '10007', '10008']\n",
    "\n",
    "n_blocks = 0\n",
    "for i in ntwk_idx.values():\n",
    "    n_blocks += len(i)\n",
    "\n",
    "sample_shape0 = (53,63,52) # MRI map shape\n",
    "\n",
    "k = 2 # define region split factor\n",
    "channel_size = n_blocks * k**3 # corresponding channel size\n",
    "# print(n_blocks, channel_size)\n",
    "\n",
    "inputs_ = []\n",
    "for i in batch_input:\n",
    "    sample_maps = load_individual_maps(i, icn_idx, ntwk_idx)\n",
    "    \n",
    "    arr_inputs = []\n",
    "    for ntw in network_names:\n",
    "        for c in ntwk_idx[ntw]:\n",
    "            sample_map = sample_maps[ntw][c]\n",
    "            \n",
    "            # padding MRI map\n",
    "            shape_pad = ((sample_shape0[0]//k + 1)*k - sample_shape0[0],\n",
    "                         (sample_shape0[1]//k + 1)*k - sample_shape0[1],\n",
    "                         (sample_shape0[2]//k + 1)*k - sample_shape0[2])\n",
    "\n",
    "            npad = ((shape_pad[0]//2, (shape_pad[0]//2 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "                    (shape_pad[1]//2, (shape_pad[1]//2 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "                    (shape_pad[2]//2, (shape_pad[2]//2 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "            sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "            \n",
    "            sx = sample_map_padded.shape[0] / k\n",
    "            sy = sample_map_padded.shape[1] / k\n",
    "            sz = sample_map_padded.shape[2] / k\n",
    "            for kz in range(k):\n",
    "                for ky in range(k):\n",
    "                    for kx in range(k):\n",
    "                        ki_slice = sample_map_padded[int(kx*sx): int(kx*sx + sx - 1), \n",
    "                                                     int(ky*sy): int(ky*sy + sy - 1), \n",
    "                                                     int(kz*sz): int(kz*sz + sz - 1)]\n",
    "                        arr_inputs.append(ki_slice)\n",
    "    arr_inputs = np.stack(arr_inputs, axis=3)\n",
    "    \n",
    "    inputs_.append(arr_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntwk_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.shuffle(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data, ntw_labs, ntw_components, icn_idx, mri_map_shape=(53,63,52), k=2, batch_size=32, shuffle=True):\n",
    "        'Initialization'\n",
    "# #         self.dim = dim\n",
    "        self.X = list(data['Id'])\n",
    "        self.y = list(data['age'])\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "#         self.labels = labels\n",
    "#         self.list_IDs = list_IDs\n",
    "#         self.n_channels = n_channels\n",
    "#         self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "#         self.data = pd.read_csv('00_Data/train_scores.csv')\n",
    "#         self.brain_mask = nb.load('00_Data/fMRI_mask.nii')\n",
    "#         self.fnc_10 = pd.read_csv('00_Data/fnc.csv')\n",
    "#         self.icn_number = pd.read_csv('00_Data/ICN_numbers.csv')\n",
    "        self.ntw_labs = ntw_labs\n",
    "        self.ntw_components = ntw_components\n",
    "        self.icn_idx = icn_idx\n",
    "        self.sample_shape0 = mri_map_shape\n",
    "        self.k = k\n",
    "        \n",
    "    def __load_individual_maps(self, patient_id, icn_idx, ntw_components):\n",
    "        patient_SM = h5py.File('00_Data/fMRI_train/{0}.mat'.format(patient_id), mode='r')\n",
    "        patient_SM = np.array(patient_SM.get('SM_feature'))\n",
    "\n",
    "        patient_maps = {}\n",
    "        for i in icn_idx.keys():\n",
    "            component_maps = {}\n",
    "            for j, _ in enumerate(icn_idx[i]):\n",
    "                jj = ntw_components[i][j]\n",
    "                component_maps[jj] = patient_SM[icn_idx[i]][j].transpose([2, 1, 0])\n",
    "            patient_maps[i] = component_maps\n",
    "        return patient_maps\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "#         return int(np.floor(len(self.X) / self.batch_size))\n",
    "        return int(np.floor(len(self.data) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        X_batch = [self.X[i] for i in indexes]\n",
    "        y_batch = [self.y[i] for i in indexes]\n",
    "        \n",
    "#         print(X_batch)\n",
    "\n",
    "        # Generate data\n",
    "        X_batch = self.__data_generation(X_batch)\n",
    "\n",
    "        return X_batch, np.array(y_batch)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.data))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, batch_input):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # iterate through subjects in the batch\n",
    "        X = []\n",
    "        for i in batch_input:\n",
    "            sample_maps = self.__load_individual_maps(i, self.icn_idx, self.ntw_components)\n",
    "\n",
    "            # iterate through mri maps of the subject\n",
    "            # split each map by k**3 regions and concatenate these regions along channel dimension\n",
    "            arr_regions = []\n",
    "            for ntw in self.ntw_labs:\n",
    "                for c in self.ntw_components[ntw]:\n",
    "                    sample_map = sample_maps[ntw][c]\n",
    "\n",
    "                    # padding MRI map\n",
    "                    shape_pad = ((self.sample_shape0[0]//k + 1)*k - self.sample_shape0[0],\n",
    "                                 (self.sample_shape0[1]//k + 1)*k - self.sample_shape0[1],\n",
    "                                 (self.sample_shape0[2]//k + 1)*k - self.sample_shape0[2])\n",
    "\n",
    "                    npad = ((shape_pad[0]//2, (shape_pad[0]//2 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "                            (shape_pad[1]//2, (shape_pad[1]//2 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "                            (shape_pad[2]//2, (shape_pad[2]//2 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "                    sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "                    sx = sample_map_padded.shape[0] / k\n",
    "                    sy = sample_map_padded.shape[1] / k\n",
    "                    sz = sample_map_padded.shape[2] / k\n",
    "                    for kz in range(k):\n",
    "                        for ky in range(k):\n",
    "                            for kx in range(k):\n",
    "                                ki_region = sample_map_padded[int(kx*sx): int(kx*sx + sx - 1), \n",
    "                                                             int(ky*sy): int(ky*sy + sy - 1), \n",
    "                                                             int(kz*sz): int(kz*sz + sz - 1)]\n",
    "                                arr_regions.append(ki_region)\n",
    "            arr_regions = np.stack(arr_regions, axis=3)\n",
    "            X.append(arr_regions)\n",
    "        return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# convolution block #1\n",
    "model.add(keras.layers.Conv3D(64, kernel_size=(3, 3, 3), strides=(1,1,1), activation='elu', input_shape=(26, 31, 26, 424), data_format=\"channels_last\"))\n",
    "# model.add(keras.layers.Activation(keras.activations.elu(alpha=1.0)))\n",
    "\n",
    "model.add(keras.layers.Conv3D(64, kernel_size=(3, 3, 3), strides=(1,1,1), activation='elu'))\n",
    "# model.add(keras.layers.Activation(keras.activations.elu(alpha=1.0)))\n",
    "\n",
    "model.add(keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(1,1,1)))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                          beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                          moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                          beta_constraint=None, gamma_constraint=None))\n",
    "\n",
    "# convolution block #2\n",
    "model.add(keras.layers.Conv3D(32, kernel_size=(3, 3, 3), strides=(1,1,1), activation='elu'))\n",
    "# model.add(keras.layers.Activation(keras.activations.elu(alpha=1.0)))\n",
    "\n",
    "model.add(keras.layers.Conv3D(32, kernel_size=(3, 3, 3), strides=(1,1,1), activation='elu'))\n",
    "# model.add(keras.layers.Activation(keras.activations.elu(alpha=1.0)))\n",
    "\n",
    "model.add(keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(1,1,1)))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                          beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                          moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                          beta_constraint=None, gamma_constraint=None))\n",
    "\n",
    "# convolution block #3\n",
    "model.add(keras.layers.Conv3D(16, kernel_size=(3, 3, 3), strides=(1,1,1), activation='elu'))\n",
    "# model.add(keras.layers.Activation(keras.activations.elu(alpha=1.0)))\n",
    "\n",
    "model.add(keras.layers.Conv3D(16, kernel_size=(3, 3, 3), strides=(1,1,1), activation='elu'))\n",
    "# model.add(keras.layers.Activation(keras.activations.elu(alpha=1.0)))\n",
    "\n",
    "model.add(keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(1,1,1)))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                          beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                          moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                          beta_constraint=None, gamma_constraint=None))\n",
    "\n",
    "# convolution block #4\n",
    "model.add(keras.layers.Conv3D(8, kernel_size=(3, 3, 3), strides=(1,1,1), activation='elu'))\n",
    "# model.add(keras.layers.Activation(keras.activations.elu(alpha=1.0)))\n",
    "\n",
    "model.add(keras.layers.Conv3D(8, kernel_size=(3, 3, 3), strides=(1,1,1), activation='elu'))\n",
    "# model.add(keras.layers.Activation(keras.activations.elu(alpha=1.0)))\n",
    "\n",
    "model.add(keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(1,1,1)))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                          beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                          moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                          beta_constraint=None, gamma_constraint=None))\n",
    "\n",
    "\n",
    "# hidden layer\n",
    "model.add(keras.layers.Dense(256, activation='elu'))\n",
    "# model.add(keras.layers.Activation(keras.activations.elu(alpha=1.0)))\n",
    "\n",
    "# output\n",
    "model.add(keras.layers.Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = keras.optimizers.Adam(lr=0.0001,\n",
    "                                 beta_1=0.99,\n",
    "                                 beta_2=0.999,\n",
    "                                 amsgrad=False)\n",
    "        \n",
    "METRICS = [keras.metrics.RootMeanSquaredError(name='rmse'),\n",
    "           keras.metrics.MeanSquaredError(name='mse'),\n",
    "           keras.metrics.MeanAbsoluteError(name='mae')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', metrics=METRICS, optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.1)\n",
    "train, val = model_selection.train_test_split(train, test_size=0.2)\n",
    "\n",
    "# data, ntw_labs, ntw_components, icn_idx, mri_map_shape=(53,63,52), k=2, batch_size=32, shuffle=True\n",
    "\n",
    "params = {'ntw_labs': ntw_labs,\n",
    "          'ntw_components': ntw_components,\n",
    "          'icn_idx': icn_idx,\n",
    "          'mri_map_shape': (53,63,52),\n",
    "          'k': 2,\n",
    "          'batch_size': 5,\n",
    "          'shuffle': False}\n",
    "\n",
    "training_generator = DataGenerator(train, **params)\n",
    "validation_generator = DataGenerator(val, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = training_generator.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 26, 31, 26, 424)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Incompatible shapes: [5,6,11,6,1] vs. [5,1]\n\t [[node sub (defined at <ipython-input-101-1b74f4d93c71>:1) ]] [Op:__inference_train_function_11676]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-1b74f4d93c71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m hist = model.fit(training_generator,\n\u001b[0m\u001b[0;32m      2\u001b[0m                  \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                  \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                  verbose=1)\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m     \"\"\"\n\u001b[1;32m-> 1661\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    591\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Incompatible shapes: [5,6,11,6,1] vs. [5,1]\n\t [[node sub (defined at <ipython-input-101-1b74f4d93c71>:1) ]] [Op:__inference_train_function_11676]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(training_generator,\n",
    "                 validation_data=validation_generator,\n",
    "                 epochs=10,\n",
    "                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, Y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(16,16)\n",
    "\n",
    "    ax=fig.add_subplot(3,2,1)\n",
    "    ax.plot(hist.history['AUC'])\n",
    "    ax.plot(hist.history['CategoricalCrossentropy'])\n",
    "    ax.legend(['Metric', 'Loss'])\n",
    "    ax.set_title('Train')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,2)\n",
    "    ax.plot(hist.history['val_AUC'])\n",
    "    ax.plot(hist.history['val_CategoricalCrossentropy'])\n",
    "    ax.legend(['Metric', 'Loss'])\n",
    "    ax.set_title('Test')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,3)\n",
    "    ax.plot(hist.history['CategoricalCrossentropy'])\n",
    "    ax.plot(hist.history['val_CategoricalCrossentropy'])\n",
    "    ax.legend(['Train', 'Test'])\n",
    "    ax.set_title('Loss')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,4)\n",
    "    ax.plot(hist.history['AUC'])\n",
    "    ax.plot(hist.history['val_AUC'])\n",
    "    ax.legend(['Train', 'Test'])\n",
    "    ax.set_title('Metric')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
