{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing as prep\n",
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your own project id here\n",
    "# PROJECT_ID = 'your-google-cloud-project'\n",
    "# from google.cloud import storage\n",
    "# storage_client = storage.Client(project=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_test'))]\n",
    "TRAIN_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_train'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decode(serialized_example):\n",
    "#     feats = tf.io.parse_single_example(serialized_example, features={'x1': tf.io.FixedLenFeature([], tf.string),\n",
    "#                                                                      'x2': tf.io.FixedLenFeature([], tf.string),\n",
    "#                                                                      'x3': tf.io.FixedLenFeature([], tf.string),\n",
    "#                                                                      'y': tf.io.FixedLenFeature([], tf.string)})\n",
    "\n",
    "#     x1 = tf.io.decode_raw(feats['x1'], tf.float64)\n",
    "#     x1 = tf.reshape(x1, [32,37,32,53])\n",
    "\n",
    "#     x2 = tf.io.decode_raw(feats['x2'], tf.float64)\n",
    "#     x2.set_shape((1378))\n",
    "\n",
    "#     x3 = tf.io.decode_raw(feats['x3'], tf.float64)\n",
    "#     x3.set_shape((26))\n",
    "\n",
    "#     y = tf.io.decode_raw(feats['y'], tf.float64)\n",
    "#     y.set_shape((5))\n",
    "\n",
    "#     X = (x1, x2, x3)\n",
    "#     return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_dataset(tfr_data, batch_size):\n",
    "#     ignore_order = tf.data.Options()\n",
    "#     ignore_order.experimental_deterministic = False\n",
    "\n",
    "#     ds = tf.data.TFRecordDataset(tfr_data, compression_type=\"GZIP\")\n",
    "#     ds = ds.with_options(ignore_order)\n",
    "#     ds = ds.map(decode, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# #     ds = ds.with_options(ignore_order)\n",
    "# #     ds = ds.shuffle(buffer_size=4000, seed=30, reshuffle_each_iteration=True)\n",
    "#     ds = ds.batch(batch_size, drop_remainder=False)\n",
    "#     ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#     return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 32\n",
    "\n",
    "# tfr_train = tf.io.gfile.glob(\"00_Data/tfr_train/*.tfrecord\")\n",
    "# tfr_val = tf.io.gfile.glob(\"00_Data/tfr_val/*.tfrecord\")\n",
    "# tfr_test = tf.io.gfile.glob(\"00_Data/tfr_test/*.tfrecord\")\n",
    "\n",
    "# # tfr_train = tf.io.gfile.glob(\"gs://trends_2020_tfr/train/*.tfrecord\")\n",
    "# # tfr_val = tf.io.gfile.glob(\"gs://trends_2020_tfr/val/*.tfrecord\")\n",
    "# # tfr_test = tf.io.gfile.glob(\"gs://trends_2020_tfr/test/*.tfrecord\")\n",
    "\n",
    "# ds_train = get_dataset(tfr_train, batch_size)\n",
    "# ds_val = get_dataset(tfr_val, batch_size)\n",
    "# ds_test = get_dataset(tfr_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>age</th>\n",
       "      <th>domain1_var1</th>\n",
       "      <th>domain1_var2</th>\n",
       "      <th>domain2_var1</th>\n",
       "      <th>domain2_var2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>57.436077</td>\n",
       "      <td>30.571975</td>\n",
       "      <td>62.553736</td>\n",
       "      <td>53.325130</td>\n",
       "      <td>51.427998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>59.580851</td>\n",
       "      <td>50.969456</td>\n",
       "      <td>67.470628</td>\n",
       "      <td>60.651856</td>\n",
       "      <td>58.311361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10004</td>\n",
       "      <td>71.413018</td>\n",
       "      <td>53.152498</td>\n",
       "      <td>58.012103</td>\n",
       "      <td>52.418389</td>\n",
       "      <td>62.536641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10007</td>\n",
       "      <td>38.617381</td>\n",
       "      <td>49.197021</td>\n",
       "      <td>65.674285</td>\n",
       "      <td>40.151376</td>\n",
       "      <td>34.096421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10008</td>\n",
       "      <td>35.326582</td>\n",
       "      <td>15.769168</td>\n",
       "      <td>65.782269</td>\n",
       "      <td>44.643805</td>\n",
       "      <td>50.448485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>21654</td>\n",
       "      <td>53.103634</td>\n",
       "      <td>50.951656</td>\n",
       "      <td>62.168022</td>\n",
       "      <td>49.389400</td>\n",
       "      <td>53.020847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>21665</td>\n",
       "      <td>38.246437</td>\n",
       "      <td>48.018227</td>\n",
       "      <td>59.522285</td>\n",
       "      <td>45.697098</td>\n",
       "      <td>53.208160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>21674</td>\n",
       "      <td>69.414169</td>\n",
       "      <td>58.593918</td>\n",
       "      <td>60.298779</td>\n",
       "      <td>49.865669</td>\n",
       "      <td>47.863167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>21693</td>\n",
       "      <td>62.009209</td>\n",
       "      <td>54.272484</td>\n",
       "      <td>60.474388</td>\n",
       "      <td>52.325031</td>\n",
       "      <td>52.989803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>21734</td>\n",
       "      <td>36.072495</td>\n",
       "      <td>46.474880</td>\n",
       "      <td>61.304012</td>\n",
       "      <td>42.742592</td>\n",
       "      <td>53.425110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5877 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id        age  domain1_var1  domain1_var2  domain2_var1  domain2_var2\n",
       "0     10001  57.436077     30.571975     62.553736     53.325130     51.427998\n",
       "1     10002  59.580851     50.969456     67.470628     60.651856     58.311361\n",
       "2     10004  71.413018     53.152498     58.012103     52.418389     62.536641\n",
       "3     10007  38.617381     49.197021     65.674285     40.151376     34.096421\n",
       "4     10008  35.326582     15.769168     65.782269     44.643805     50.448485\n",
       "...     ...        ...           ...           ...           ...           ...\n",
       "5872  21654  53.103634     50.951656     62.168022     49.389400     53.020847\n",
       "5873  21665  38.246437     48.018227     59.522285     45.697098     53.208160\n",
       "5874  21674  69.414169     58.593918     60.298779     49.865669     47.863167\n",
       "5875  21693  62.009209     54.272484     60.474388     52.325031     52.989803\n",
       "5876  21734  36.072495     46.474880     61.304012     42.742592     53.425110\n",
       "\n",
       "[5877 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('00_Data/train_scores_full.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(idx, labels):\n",
    "    # FNC inputs\n",
    "    df_fnc = pd.read_csv('00_Data/fnc_csv/{0}.csv'.format(idx), index_col=0)\n",
    "    X_fnc = np.array(df_fnc.values).reshape(-1)\n",
    "    \n",
    "    # Loading inputs\n",
    "    df_loading = pd.read_csv('00_Data/loading_csv/{0}.csv'.format(idx), index_col=0)\n",
    "    X_loading = np.array(df_loading.values).reshape(-1)\n",
    "    \n",
    "    #MRI inputs\n",
    "    patient_SM = h5py.File('00_Data/fMRI_train/{0}.mat'.format(idx), mode='r')\n",
    "    patient_SM = np.array(patient_SM.get('SM_feature'))\n",
    "#     patient_SM = mat.transpose([1,2,3,0])\n",
    "\n",
    "#     print('patient: {idx}')\n",
    "    \n",
    "    k = 1\n",
    "    ki_padding = 3\n",
    "    \n",
    "    arr_regions = []\n",
    "    for i in range(patient_SM.shape[0]):\n",
    "        sample_map = patient_SM[i,:,:,:]\n",
    "#         print(len(arr_regions))\n",
    "        # padding MRI map\n",
    "        if k > 1:\n",
    "            map_shape = sample_map.shape\n",
    "            shape_pad = ((map_shape[0]//k + 1)*k - map_shape[0],\n",
    "                         (map_shape[1]//k + 1)*k - map_shape[1],\n",
    "                         (map_shape[2]//k + 1)*k - map_shape[2])\n",
    "\n",
    "            npad = ((shape_pad[0]//2, (shape_pad[0]//2 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "                    (shape_pad[1]//2, (shape_pad[1]//2 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "                    (shape_pad[2]//2, (shape_pad[2]//2 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "            sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "            sx = sample_map_padded.shape[0] / k\n",
    "            sy = sample_map_padded.shape[1] / k\n",
    "            sz = sample_map_padded.shape[2] / k\n",
    "            for kz in range(k):\n",
    "                for ky in range(k):\n",
    "                    for kx in range(k):\n",
    "                        ki_region = sample_map_padded[int(kx*sx): int(kx*sx + sx - 1), \n",
    "                                                     int(ky*sy): int(ky*sy + sy - 1), \n",
    "                                                     int(kz*sz): int(kz*sz + sz - 1)]\n",
    "                        #padding i-th region by 3 pixels\n",
    "                        ki_region_padded = np.pad(ki_region, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "                        arr_regions.append(ki_region_padded)\n",
    "        else:\n",
    "            map_padded = np.pad(sample_map, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "            arr_regions.append(map_padded)\n",
    "#             print(map_padded.shape)\n",
    "    X_mri = np.stack(arr_regions, axis=3)\n",
    "    \n",
    "    X = (X_mri, X_fnc, X_loading)\n",
    "    return X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_py_function(func, inp, Tout, name=None):\n",
    "    \n",
    "    def wrapped_func(*flat_inp):\n",
    "        reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp,\n",
    "                                                     expand_composites=True)\n",
    "        out = func(*reconstructed_inp)\n",
    "        return tf.nest.flatten(out, expand_composites=True)\n",
    "    \n",
    "    flat_Tout = tf.nest.flatten(Tout, expand_composites=True)\n",
    "    flat_out = tf.py_function(func=wrapped_func, \n",
    "                              inp=tf.nest.flatten(inp, expand_composites=True),\n",
    "                              Tout=[_tensor_spec_to_dtype(v) for v in flat_Tout],\n",
    "                              name=name)\n",
    "    spec_out = tf.nest.map_structure(_dtype_to_tensor_spec, Tout, expand_composites=True)\n",
    "    out = tf.nest.pack_sequence_as(spec_out, flat_out, expand_composites=True)\n",
    "    return out\n",
    "\n",
    "def _dtype_to_tensor_spec(v):\n",
    "    return tf.TensorSpec(None, v) if isinstance(v, tf.dtypes.DType) else v\n",
    "\n",
    "def _tensor_spec_to_dtype(v):\n",
    "    return v.dtype if isinstance(v, tf.TensorSpec) else v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data, batch_size):\n",
    "    data = tf.data.Dataset.from_tensor_slices((data['Id'].values, \n",
    "                                               data[['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']].values))\n",
    "    data = data.shuffle(buffer_size=4000, seed=30, reshuffle_each_iteration=True)\n",
    "    \n",
    "    data = data.map(lambda idx, lbl:new_py_function(get_inputs, inp=(idx, lbl), \n",
    "                                                    Tout=((tf.TensorSpec(shape=(None, 58, 69, 59, 53), dtype=tf.dtypes.float64, name='input_mri'),\n",
    "                                                           tf.TensorSpec(shape=(None, 1378), dtype=tf.dtypes.float64, name='input_fnc'), \n",
    "                                                           tf.TensorSpec(shape=(None, 26), dtype=tf.dtypes.float64, name='input_loading')), \n",
    "                                                          tf.TensorSpec(shape=(None, 5), dtype=tf.dtypes.float64, name='labels')), name=None), \n",
    "                     num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "                     deterministic=True)\n",
    "    data = data.batch(batch_size, drop_remainder=False)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.2, shuffle=True, random_state=30)\n",
    "train, val = model_selection.train_test_split(train, test_size=0.2, shuffle=True, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "ds_train = get_dataset(train, batch_size)\n",
    "ds_val = get_dataset(val, batch_size)\n",
    "ds_test = get_dataset(test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ds_train.take(1):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE_mri = (58, 69, 59, 53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE_fnc = (1378,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE_loading = (26,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(INPUT_SHAPE_mri, INPUT_SHAPE_fnc, INPUT_SHAPE_loading):\n",
    "    #============================================================================\n",
    "    # CNN for MRI images processing\n",
    "    #============================================================================\n",
    "    inputs_mri = keras.layers.Input(shape=INPUT_SHAPE_mri, name='inpupt_mri')\n",
    "\n",
    "    # convolution block #1\n",
    "    block_1 = keras.layers.Conv3D(64, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(inputs_mri)\n",
    "    block_1 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(block_1)\n",
    "    block_1 = keras.layers.Conv3D(64, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(block_1)\n",
    "    block_1 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(block_1)\n",
    "    block_1 = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(block_1)\n",
    "    block_1 = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(block_1)\n",
    "\n",
    "    # convolution block #2\n",
    "    block_2 = keras.layers.Conv3D(32, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(block_1)\n",
    "    block_2 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(block_2)\n",
    "    block_2 = keras.layers.Conv3D(32, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(block_2)\n",
    "    block_2 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(block_2)\n",
    "    block_2 = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(block_2)\n",
    "    block_2 = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(block_2)\n",
    "\n",
    "    # convolution block #3\n",
    "    block_3 = keras.layers.Conv3D(16, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(block_2)\n",
    "    block_3 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(block_3)\n",
    "    block_3 = keras.layers.Conv3D(16, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(block_3)\n",
    "    block_3 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(block_3)\n",
    "    block_3 = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(block_3)\n",
    "    block_3 = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(block_3)\n",
    "\n",
    "    # convolution block #4\n",
    "    block_4 = keras.layers.Conv3D(8, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(block_3)\n",
    "    block_4 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(block_4)\n",
    "    block_4 = keras.layers.Conv3D(8, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(block_4)\n",
    "    block_4 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(block_4)\n",
    "    block_4 = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(block_4)\n",
    "    block_4 = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(block_4)\n",
    "\n",
    "    flatten = keras.layers.Flatten(data_format='channels_last')(block_4)\n",
    "\n",
    "    # hidden layer\n",
    "    x = keras.layers.Dense(256,\n",
    "                               kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                               bias_initializer=keras.initializers.Constant(5.))(flatten)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(x)\n",
    "    x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "    # output\n",
    "    x = keras.Model(inputs=inputs_mri, outputs=x, name='model_mri')\n",
    "\n",
    "    #============================================================================\n",
    "    # MLP for FNC correlation features processing\n",
    "    #============================================================================\n",
    "    inputs_fnc = keras.layers.Input(shape=INPUT_SHAPE_fnc, name='input_fnc')\n",
    "\n",
    "    y = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(inputs_fnc)\n",
    "    y = keras.layers.Dense(2048,\n",
    "                               kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                               bias_initializer=keras.initializers.Constant(5.))(y)\n",
    "    y = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(y)\n",
    "\n",
    "    y1 = keras.layers.Dense(512,\n",
    "                               kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                               bias_initializer=keras.initializers.Constant(5.))(y)\n",
    "    y1 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(y1)\n",
    "    y1 = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(y1)\n",
    "\n",
    "    y2 = keras.layers.Dense(512,\n",
    "                               kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                               bias_initializer=keras.initializers.Constant(5.))(y)\n",
    "    y2 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(y2)\n",
    "    y2 = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(y2)\n",
    "\n",
    "    y = keras.layers.concatenate([y1, y2])\n",
    "\n",
    "    y = keras.layers.Dense(256,\n",
    "                               kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                               bias_initializer=keras.initializers.Constant(5.))(y)\n",
    "    y = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(y)\n",
    "    y = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(y)\n",
    "\n",
    "    # output\n",
    "    y = keras.Model(inputs=inputs_fnc, outputs=y, name='model_fnc')\n",
    "\n",
    "    #============================================================================\n",
    "    # MLP for sMRI SBM loadings processing\n",
    "    #============================================================================\n",
    "    inputs_loading = keras.layers.Input(shape=INPUT_SHAPE_loading, name='input_load')\n",
    "\n",
    "    z = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(inputs_loading)\n",
    "\n",
    "    z = keras.layers.Dense(256,\n",
    "                               kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                               bias_initializer=keras.initializers.Constant(5.))(z)\n",
    "    z = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(z)\n",
    "\n",
    "    z1 = keras.layers.Dense(128,\n",
    "                               kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                               bias_initializer=keras.initializers.Constant(5.))(z)\n",
    "    z1 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(z1)\n",
    "    z1 = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(z1)\n",
    "\n",
    "    z2 = keras.layers.Dense(128,\n",
    "                               kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                               bias_initializer=keras.initializers.Constant(5.))(z)\n",
    "    z2 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(z2)\n",
    "    z2 = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(z2)\n",
    "\n",
    "    z = keras.layers.concatenate([z1, z2])\n",
    "\n",
    "    z = keras.layers.Dense(512,\n",
    "                               kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                               bias_initializer=keras.initializers.Constant(5.))(z)\n",
    "    z = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(z)\n",
    "    z = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(z)\n",
    "\n",
    "    # output\n",
    "    z = keras.Model(inputs=inputs_loading, outputs=z, name='model_loading')\n",
    "\n",
    "    \n",
    "    concat = keras.layers.concatenate([x.output, y.output, z.output])\n",
    "\n",
    "\n",
    "    concat1 = keras.layers.Dense(256, \n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(concat)\n",
    "    concat1 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(concat1)\n",
    "    concat1 = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(concat1)\n",
    "    \n",
    "    concat2 = keras.layers.Dense(256, \n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(concat)\n",
    "    concat2 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(concat2)\n",
    "    concat2 = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(concat2)\n",
    "    \n",
    "    concat = keras.layers.concatenate([concat1, concat2])\n",
    "\n",
    "    concat = keras.layers.Dense(512, \n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(concat)\n",
    "    concat = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(concat)\n",
    "\n",
    "    outputs = keras.layers.Dense(5, activation='linear')(concat)\n",
    "\n",
    "    model = keras.Model(inputs=[x.input, y.input, z.input], outputs=outputs, name='model_combined')\n",
    "\n",
    "    optim = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95)\n",
    "\n",
    "    METRICS = [keras.metrics.RootMeanSquaredError(name='rmse'),\n",
    "               keras.metrics.MeanSquaredError(name='mse')]\n",
    "\n",
    "    model.compile(loss='mae', metrics=METRICS, optimizer=optim)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(INPUT_SHAPE_mri, INPUT_SHAPE_fnc, INPUT_SHAPE_loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_combined\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inpupt_mri (InputLayer)         [(None, 58, 69, 59,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d (Conv3D)                 (None, 58, 69, 59, 6 91648       inpupt_mri[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu (PReLU)                 (None, 58, 69, 59, 6 15111552    conv3d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_1 (Conv3D)               (None, 58, 69, 59, 6 110656      p_re_lu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 58, 69, 59, 6 15111552    conv3d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D)    (None, 29, 34, 29, 6 0           p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 29, 34, 29, 6 256         max_pooling3d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_2 (Conv3D)               (None, 29, 34, 29, 3 55328       batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 29, 34, 29, 3 915008      conv3d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_3 (Conv3D)               (None, 29, 34, 29, 3 27680       p_re_lu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_3 (PReLU)               (None, 29, 34, 29, 3 915008      conv3d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3D)  (None, 14, 17, 14, 3 0           p_re_lu_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 14, 17, 14, 3 128         max_pooling3d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_4 (Conv3D)               (None, 14, 17, 14, 1 13840       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_4 (PReLU)               (None, 14, 17, 14, 1 53312       conv3d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_5 (Conv3D)               (None, 14, 17, 14, 1 6928        p_re_lu_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_5 (PReLU)               (None, 14, 17, 14, 1 53312       conv3d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3D)  (None, 7, 8, 7, 16)  0           p_re_lu_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 7, 8, 7, 16)  64          max_pooling3d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_fnc (InputLayer)          [(None, 1378)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_load (InputLayer)         [(None, 26)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_6 (Conv3D)               (None, 7, 8, 7, 8)   3464        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1378)         5512        input_fnc[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 26)           104         input_load[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_6 (PReLU)               (None, 7, 8, 7, 8)   3136        conv3d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2048)         2824192     batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          6912        batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_7 (Conv3D)               (None, 7, 8, 7, 8)   1736        p_re_lu_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_9 (PReLU)               (None, 2048)         2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_13 (PReLU)              (None, 256)          256         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_7 (PReLU)               (None, 7, 8, 7, 8)   3136        conv3d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          1049088     p_re_lu_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          1049088     p_re_lu_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          32896       p_re_lu_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 128)          32896       p_re_lu_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3D)  (None, 3, 4, 3, 8)   0           p_re_lu_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_10 (PReLU)              (None, 512)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_11 (PReLU)              (None, 512)          512         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_14 (PReLU)              (None, 128)          128         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_15 (PReLU)              (None, 128)          128         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 3, 4, 3, 8)   32          max_pooling3d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 512)          2048        p_re_lu_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 512)          2048        p_re_lu_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 128)          512         p_re_lu_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 128)          512         p_re_lu_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 288)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1024)         0           batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           batch_normalization_10[0][0]     \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          73984       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          262400      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 512)          131584      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_8 (PReLU)               (None, 256)          256         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_12 (PReLU)              (None, 256)          256         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_16 (PReLU)              (None, 512)          512         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 256)          1024        p_re_lu_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 256)          1024        p_re_lu_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 512)          2048        p_re_lu_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1024)         0           batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          262400      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 256)          262400      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_17 (PReLU)              (None, 256)          256         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_18 (PReLU)              (None, 256)          256         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 256)          1024        p_re_lu_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 256)          1024        p_re_lu_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 512)          262656      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_19 (PReLU)              (None, 512)          512         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 5)            2565        p_re_lu_19[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 38,753,349\n",
      "Trainable params: 38,744,669\n",
      "Non-trainable params: 8,680\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import plot_model\n",
    "# plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './99_Training_checkpoints/mri-fnc-loading'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x227d8a88f70>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                               min_delta=0.001, \n",
    "#                                               patience=10, \n",
    "#                                               verbose=1, \n",
    "#                                               mode='min',\n",
    "#                                               baseline=None, \n",
    "#                                               restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_prefix = os.path.join('./99_Training_checkpoints/mri-fnc-loading', \"ckpt_{epoch}\")\n",
    "\n",
    "callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./99_Logs/mri-fnc-loading'),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                                save_weights_only=True),\n",
    "             tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                              min_delta=0.001, \n",
    "                                              patience=5, \n",
    "                                              verbose=1, \n",
    "                                              mode='min',\n",
    "                                              baseline=None, \n",
    "                                              restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "470/470 [==============================] - 1969s 4s/step - loss: 52.7655 - rmse: 54.9866 - mse: 3023.5239 - val_loss: 51.0152 - val_rmse: 53.2143 - val_mse: 2831.7598\n",
      "Epoch 2/400\n",
      "470/470 [==============================] - 1993s 4s/step - loss: 50.1769 - rmse: 52.5114 - mse: 2757.4429 - val_loss: 48.2894 - val_rmse: 50.6562 - val_mse: 2566.0464\n",
      "Epoch 3/400\n",
      "470/470 [==============================] - 1925s 4s/step - loss: 46.9892 - rmse: 49.4688 - mse: 2447.1675 - val_loss: 44.8396 - val_rmse: 47.3759 - val_mse: 2244.4758\n",
      "Epoch 4/400\n",
      "470/470 [==============================] - 1924s 4s/step - loss: 43.3097 - rmse: 45.9785 - mse: 2114.0271 - val_loss: 40.9185 - val_rmse: 43.6553 - val_mse: 1905.7866\n",
      "Epoch 5/400\n",
      "470/470 [==============================] - 1928s 4s/step - loss: 39.2130 - rmse: 42.1169 - mse: 1773.8353 - val_loss: 36.6465 - val_rmse: 39.6396 - val_mse: 1571.2964\n",
      "Epoch 6/400\n",
      "470/470 [==============================] - 1927s 4s/step - loss: 34.7881 - rmse: 37.9662 - mse: 1441.4343 - val_loss: 32.1072 - val_rmse: 35.3716 - val_mse: 1251.1504\n",
      "Epoch 7/400\n",
      "470/470 [==============================] - 1923s 4s/step - loss: 30.1660 - rmse: 33.6143 - mse: 1129.9236 - val_loss: 27.4771 - val_rmse: 30.9772 - val_mse: 959.5847\n",
      "Epoch 8/400\n",
      "470/470 [==============================] - 1925s 4s/step - loss: 25.5698 - rmse: 29.2189 - mse: 853.7429 - val_loss: 23.0524 - val_rmse: 26.6547 - val_mse: 710.4722\n",
      "Epoch 9/400\n",
      "470/470 [==============================] - 1930s 4s/step - loss: 21.2680 - rmse: 24.9453 - mse: 622.2684 - val_loss: 19.0083 - val_rmse: 22.4960 - val_mse: 506.0698\n",
      "Epoch 10/400\n",
      "470/470 [==============================] - 1927s 4s/step - loss: 17.5188 - rmse: 21.0187 - mse: 441.7838 - val_loss: 15.6398 - val_rmse: 18.8428 - val_mse: 355.0514\n",
      "Epoch 11/400\n",
      "470/470 [==============================] - 1926s 4s/step - loss: 14.4767 - rmse: 17.6488 - mse: 311.4802 - val_loss: 13.0375 - val_rmse: 15.8902 - val_mse: 252.4984\n",
      "Epoch 12/400\n",
      "470/470 [==============================] - 1933s 4s/step - loss: 12.1730 - rmse: 15.0171 - mse: 225.5119 - val_loss: 11.1230 - val_rmse: 13.6828 - val_mse: 187.2181\n",
      "Epoch 13/400\n",
      "470/470 [==============================] - 1932s 4s/step - loss: 10.5844 - rmse: 13.1678 - mse: 173.3909 - val_loss: 9.8865 - val_rmse: 12.2832 - val_mse: 150.8779\n",
      "Epoch 14/400\n",
      "470/470 [==============================] - 1929s 4s/step - loss: 9.5791 - rmse: 12.0217 - mse: 144.5223 - val_loss: 9.1364 - val_rmse: 11.4671 - val_mse: 131.4950\n",
      "Epoch 15/400\n",
      "470/470 [==============================] - 1927s 4s/step - loss: 8.9817 - rmse: 11.3655 - mse: 129.1737 - val_loss: 8.7138 - val_rmse: 11.0343 - val_mse: 121.7552\n",
      "Epoch 16/400\n",
      "470/470 [==============================] - 1927s 4s/step - loss: 8.6315 - rmse: 10.9912 - mse: 120.8069 - val_loss: 8.4349 - val_rmse: 10.7668 - val_mse: 115.9240\n",
      "Epoch 17/400\n",
      "470/470 [==============================] - 1927s 4s/step - loss: 8.3910 - rmse: 10.7379 - mse: 115.3017 - val_loss: 8.2567 - val_rmse: 10.6043 - val_mse: 112.4503\n",
      "Epoch 18/400\n",
      "470/470 [==============================] - 1929s 4s/step - loss: 8.2306 - rmse: 10.5847 - mse: 112.0356 - val_loss: 8.1424 - val_rmse: 10.5104 - val_mse: 110.4691\n",
      "Epoch 19/400\n",
      "470/470 [==============================] - 1929s 4s/step - loss: 8.1106 - rmse: 10.4697 - mse: 109.6136 - val_loss: 8.0656 - val_rmse: 10.4503 - val_mse: 109.2081\n",
      "Epoch 20/400\n",
      "470/470 [==============================] - 1929s 4s/step - loss: 8.0617 - rmse: 10.4235 - mse: 108.6498 - val_loss: 8.0228 - val_rmse: 10.4202 - val_mse: 108.5806\n",
      "Epoch 21/400\n",
      "470/470 [==============================] - 1934s 4s/step - loss: 8.0100 - rmse: 10.3733 - mse: 107.6060 - val_loss: 7.9875 - val_rmse: 10.3940 - val_mse: 108.0357\n",
      "Epoch 22/400\n",
      "470/470 [==============================] - 1931s 4s/step - loss: 7.9366 - rmse: 10.3039 - mse: 106.1703 - val_loss: 7.9580 - val_rmse: 10.3704 - val_mse: 107.5453\n",
      "Epoch 23/400\n",
      "470/470 [==============================] - 1932s 4s/step - loss: 7.8807 - rmse: 10.2590 - mse: 105.2467 - val_loss: 7.9435 - val_rmse: 10.3645 - val_mse: 107.4234\n",
      "Epoch 24/400\n",
      "470/470 [==============================] - 1938s 4s/step - loss: 7.8739 - rmse: 10.2650 - mse: 105.3698 - val_loss: 7.9309 - val_rmse: 10.3514 - val_mse: 107.1514\n",
      "Epoch 25/400\n",
      "470/470 [==============================] - 1945s 4s/step - loss: 7.8583 - rmse: 10.2477 - mse: 105.0163 - val_loss: 7.9276 - val_rmse: 10.3492 - val_mse: 107.1059\n",
      "Epoch 26/400\n",
      "470/470 [==============================] - 1936s 4s/step - loss: 7.8022 - rmse: 10.1915 - mse: 103.8675 - val_loss: 7.9215 - val_rmse: 10.3400 - val_mse: 106.9162\n",
      "Epoch 27/400\n",
      "470/470 [==============================] - 1933s 4s/step - loss: 7.7918 - rmse: 10.1799 - mse: 103.6299 - val_loss: 7.9158 - val_rmse: 10.3397 - val_mse: 106.9088\n",
      "Epoch 28/400\n",
      "470/470 [==============================] - 1925s 4s/step - loss: 7.7592 - rmse: 10.1584 - mse: 103.1928 - val_loss: 7.9111 - val_rmse: 10.3383 - val_mse: 106.8811\n",
      "Epoch 29/400\n",
      "470/470 [==============================] - 1911s 4s/step - loss: 7.7632 - rmse: 10.1517 - mse: 103.0563 - val_loss: 7.9029 - val_rmse: 10.3296 - val_mse: 106.6998\n",
      "Epoch 30/400\n",
      "470/470 [==============================] - 1932s 4s/step - loss: 7.7121 - rmse: 10.1135 - mse: 102.2831 - val_loss: 7.9038 - val_rmse: 10.3282 - val_mse: 106.6707\n",
      "Epoch 31/400\n",
      "470/470 [==============================] - 1926s 4s/step - loss: 7.6745 - rmse: 10.0727 - mse: 101.4596 - val_loss: 7.9029 - val_rmse: 10.3242 - val_mse: 106.5893\n",
      "Epoch 32/400\n",
      "470/470 [==============================] - 1917s 4s/step - loss: 7.6702 - rmse: 10.0942 - mse: 101.8924 - val_loss: 7.9075 - val_rmse: 10.3263 - val_mse: 106.6315\n",
      "Epoch 33/400\n",
      "470/470 [==============================] - 1926s 4s/step - loss: 7.6426 - rmse: 10.0525 - mse: 101.0518 - val_loss: 7.9039 - val_rmse: 10.3194 - val_mse: 106.4902\n",
      "Epoch 34/400\n",
      "470/470 [==============================] - 1928s 4s/step - loss: 7.6435 - rmse: 10.0605 - mse: 101.2142 - val_loss: 7.9049 - val_rmse: 10.3242 - val_mse: 106.5893\n",
      "Epoch 35/400\n",
      "470/470 [==============================] - 1932s 4s/step - loss: 7.5915 - rmse: 10.0117 - mse: 100.2346 - val_loss: 7.9091 - val_rmse: 10.3247 - val_mse: 106.5993\n",
      "Epoch 36/400\n",
      "470/470 [==============================] - 1921s 4s/step - loss: 7.5747 - rmse: 9.9939 - mse: 99.8775 - val_loss: 7.9068 - val_rmse: 10.3206 - val_mse: 106.5154\n",
      "Epoch 37/400\n",
      " 39/470 [=>............................] - ETA: 23:59 - loss: 7.3937 - rmse: 9.6959 - mse: 94.0105"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[8,64,58,69,59] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/model_combined/max_pooling3d/MaxPool3D/MaxPool3DGrad (defined at <ipython-input-24-556e1565734a>:3) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_9743]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-556e1565734a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/GPU:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# with tf.device('/CPU:0'):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     hist = model.fit(ds_train,\n\u001b[0m\u001b[0;32m      4\u001b[0m                      \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                      \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m     \"\"\"\n\u001b[1;32m-> 1661\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    591\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[8,64,58,69,59] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/model_combined/max_pooling3d/MaxPool3D/MaxPool3DGrad (defined at <ipython-input-24-556e1565734a>:3) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_9743]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "# with tf.device('/CPU:0'):\n",
    "    hist = model.fit(ds_train,\n",
    "                     validation_data=ds_val,\n",
    "                     callbacks=callbacks,\n",
    "                     epochs=400,\n",
    "                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 432s 3s/step - loss: 8.0011 - rmse: 10.5004 - mse: 110.2587TA: 5:53 - loss: 8.1505 - rmse: 10.7224 - mse: 114.970 - ETA: 5:54 - loss: 8.2938 - rmse: 10.828 - ETA: 5:40 - loss: 8.2050 - rmse: 10.6929 - mse: 114.3 - ETA: 5:24 - loss: 8.2535 - rmse: 10.7792 - mse - ETA: 4:56 - loss: 8.1050  - ETA: 3:26 - loss: 7.8890 - rmse: 10.3689 - mse: 107.51 - ETA: 3:19 - loss: 7.9023 - rmse: 1 - ETA: 2:16 -  - ETA: 32s - loss: 7.9700 - rmse: 10.4486 - mse: 109.17 - ETA: 29s - loss: 7.9624 - rmse: 10.44\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    results = model.evaluate(ds_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-28efc6a72ce7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rmse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Metric'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(16,16)\n",
    "\n",
    "    ax=fig.add_subplot(3,2,1)\n",
    "    ax.plot(hist.history['rmse'])\n",
    "    ax.plot(hist.history['mse'])\n",
    "    ax.legend(['Metric', 'Loss'])\n",
    "    ax.set_title('Train')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,2)\n",
    "    ax.plot(hist.history['val_rmse'])\n",
    "    ax.plot(hist.history['val_mse'])\n",
    "    ax.legend(['Metric', 'Loss'])\n",
    "    ax.set_title('Test')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,3)\n",
    "    ax.plot(hist.history['loss'])\n",
    "    ax.plot(hist.history['val_loss'])\n",
    "    ax.legend(['Train', 'Test'])\n",
    "    ax.set_title('Loss')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,4)\n",
    "    ax.plot(hist.history['mse'])\n",
    "    ax.plot(hist.history['val_mse'])\n",
    "    ax.legend(['Train', 'Test'])\n",
    "    ax.set_title('Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_subm(idx):\n",
    "    # FNC inputs\n",
    "    df_fnc = pd.read_csv('00_Data/fnc_csv/{0}.csv'.format(idx), index_col=0)\n",
    "    X_fnc = np.array(df_fnc.values).reshape(-1)\n",
    "    \n",
    "    # Loading inputs\n",
    "    df_loading = pd.read_csv('00_Data/loading_csv/{0}.csv'.format(idx), index_col=0)\n",
    "    X_loading = np.array(df_loading.values).reshape(-1)\n",
    "    \n",
    "    #MRI inputs\n",
    "    patient_SM = h5py.File('00_Data/fMRI_test/{0}.mat'.format(idx), mode='r')\n",
    "    patient_SM = np.array(patient_SM.get('SM_feature'))\n",
    "#     patient_SM = mat.transpose([1,2,3,0])\n",
    "\n",
    "#     print('patient: {idx}')\n",
    "    \n",
    "    k = 1\n",
    "    ki_padding = 3\n",
    "    \n",
    "    arr_regions = []\n",
    "    for i in range(patient_SM.shape[0]):\n",
    "        sample_map = patient_SM[i,:,:,:]\n",
    "#         print(len(arr_regions))\n",
    "        # padding MRI map\n",
    "        if k > 1:\n",
    "            map_shape = sample_map.shape\n",
    "            shape_pad = ((map_shape[0]//k + 1)*k - map_shape[0],\n",
    "                         (map_shape[1]//k + 1)*k - map_shape[1],\n",
    "                         (map_shape[2]//k + 1)*k - map_shape[2])\n",
    "\n",
    "            npad = ((shape_pad[0]//2, (shape_pad[0]//2 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "                    (shape_pad[1]//2, (shape_pad[1]//2 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "                    (shape_pad[2]//2, (shape_pad[2]//2 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "            sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "            sx = sample_map_padded.shape[0] / k\n",
    "            sy = sample_map_padded.shape[1] / k\n",
    "            sz = sample_map_padded.shape[2] / k\n",
    "            for kz in range(k):\n",
    "                for ky in range(k):\n",
    "                    for kx in range(k):\n",
    "                        ki_region = sample_map_padded[int(kx*sx): int(kx*sx + sx - 1), \n",
    "                                                     int(ky*sy): int(ky*sy + sy - 1), \n",
    "                                                     int(kz*sz): int(kz*sz + sz - 1)]\n",
    "                        #padding i-th region by 3 pixels\n",
    "                        ki_region_padded = np.pad(ki_region, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "                        arr_regions.append(ki_region_padded)\n",
    "        else:\n",
    "            map_padded = np.pad(sample_map, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "            arr_regions.append(map_padded)\n",
    "#             print(map_padded.shape)\n",
    "    X_mri = np.stack(arr_regions, axis=3)\n",
    "    \n",
    "#     X = (X_mri, X_fnc, X_loading)\n",
    "    return X_mri, X_fnc, X_loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "for i in TEST_IDS:\n",
    "    X_mri, X_fnc, X_loading = get_inputs_subm(i)\n",
    "    X_mri = X_mri.reshape(1,58, 69, 59, 53)\n",
    "    X_fnc = X_fnc.reshape(1,1378)\n",
    "    X_loading = X_loading.reshape(1,26)\n",
    "    preds = model.predict([X_mri, X_fnc, X_loading], batch_size=1)\n",
    "    y_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5877"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.array(y_preds).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = []\n",
    "i = 0\n",
    "for idx in TEST_IDS:\n",
    "    df_submission.append(['{0}_age'.format(idx), y_preds[i]])\n",
    "    df_submission.append(['{0}_domain1_var1'.format(idx), y_preds[i+1]])\n",
    "    df_submission.append(['{0}_domain1_var2'.format(idx), y_preds[i+2]])\n",
    "    df_submission.append(['{0}_domain2_var1'.format(idx), y_preds[i+3]])\n",
    "    df_submission.append(['{0}_domain2_var2'.format(idx), y_preds[i+4]])\n",
    "    i += 5\n",
    "\n",
    "df_submission = pd.DataFrame(df_submission, columns=['Id', 'Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('submission_mri-fnc-load_mae_08.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
