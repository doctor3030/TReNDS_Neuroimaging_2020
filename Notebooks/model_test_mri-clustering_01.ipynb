{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing as prep\n",
    "import sklearn.model_selection as model_selection\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_test'))]\n",
    "TRAIN_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_train'))]\n",
    "ALL_IDS = TRAIN_IDS + TEST_IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>age</th>\n",
       "      <th>domain1_var1</th>\n",
       "      <th>domain1_var2</th>\n",
       "      <th>domain2_var1</th>\n",
       "      <th>domain2_var2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>57.436077</td>\n",
       "      <td>30.571975</td>\n",
       "      <td>62.553736</td>\n",
       "      <td>53.325130</td>\n",
       "      <td>51.427998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>59.580851</td>\n",
       "      <td>50.969456</td>\n",
       "      <td>67.470628</td>\n",
       "      <td>60.651856</td>\n",
       "      <td>58.311361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10004</td>\n",
       "      <td>71.413018</td>\n",
       "      <td>53.152498</td>\n",
       "      <td>58.012103</td>\n",
       "      <td>52.418389</td>\n",
       "      <td>62.536641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10007</td>\n",
       "      <td>38.617381</td>\n",
       "      <td>49.197021</td>\n",
       "      <td>65.674285</td>\n",
       "      <td>40.151376</td>\n",
       "      <td>34.096421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10008</td>\n",
       "      <td>35.326582</td>\n",
       "      <td>15.769168</td>\n",
       "      <td>65.782269</td>\n",
       "      <td>44.643805</td>\n",
       "      <td>50.448485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>21654</td>\n",
       "      <td>53.103634</td>\n",
       "      <td>50.951656</td>\n",
       "      <td>62.168022</td>\n",
       "      <td>49.389400</td>\n",
       "      <td>53.020847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>21665</td>\n",
       "      <td>38.246437</td>\n",
       "      <td>48.018227</td>\n",
       "      <td>59.522285</td>\n",
       "      <td>45.697098</td>\n",
       "      <td>53.208160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>21674</td>\n",
       "      <td>69.414169</td>\n",
       "      <td>58.593918</td>\n",
       "      <td>60.298779</td>\n",
       "      <td>49.865669</td>\n",
       "      <td>47.863167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>21693</td>\n",
       "      <td>62.009209</td>\n",
       "      <td>54.272484</td>\n",
       "      <td>60.474388</td>\n",
       "      <td>52.325031</td>\n",
       "      <td>52.989803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>21734</td>\n",
       "      <td>36.072495</td>\n",
       "      <td>46.474880</td>\n",
       "      <td>61.304012</td>\n",
       "      <td>42.742592</td>\n",
       "      <td>53.425110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5877 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id        age  domain1_var1  domain1_var2  domain2_var1  domain2_var2\n",
       "0     10001  57.436077     30.571975     62.553736     53.325130     51.427998\n",
       "1     10002  59.580851     50.969456     67.470628     60.651856     58.311361\n",
       "2     10004  71.413018     53.152498     58.012103     52.418389     62.536641\n",
       "3     10007  38.617381     49.197021     65.674285     40.151376     34.096421\n",
       "4     10008  35.326582     15.769168     65.782269     44.643805     50.448485\n",
       "...     ...        ...           ...           ...           ...           ...\n",
       "5872  21654  53.103634     50.951656     62.168022     49.389400     53.020847\n",
       "5873  21665  38.246437     48.018227     59.522285     45.697098     53.208160\n",
       "5874  21674  69.414169     58.593918     60.298779     49.865669     47.863167\n",
       "5875  21693  62.009209     54.272484     60.474388     52.325031     52.989803\n",
       "5876  21734  36.072495     46.474880     61.304012     42.742592     53.425110\n",
       "\n",
       "[5877 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('00_Data/train_scores_full.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(idx):\n",
    "    #MRI inputs\n",
    "    patient_SM = h5py.File('00_Data/fMRI_train/{0}.mat'.format(idx), mode='r')\n",
    "    patient_SM = np.array(patient_SM.get('SM_feature'))\n",
    "#     print(patient_SM.shape)\n",
    "    k = 1\n",
    "    ki_padding = 2\n",
    "    \n",
    "    arr_regions = []\n",
    "    for i in range(patient_SM.shape[0]):\n",
    "        sample_map = patient_SM[i,:,:,:]\n",
    "        if k > 1:\n",
    "            map_shape = sample_map.shape\n",
    "            shape_pad = ((map_shape[0]//k + 1)*k - map_shape[0],\n",
    "                         (map_shape[1]//k + 1)*k - map_shape[1],\n",
    "                         (map_shape[2]//k + 1)*k - map_shape[2])\n",
    "\n",
    "            npad = (((0 if shape_pad[0]%2==0 else shape_pad[0]//2), (shape_pad[0]//2 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "                    ((0 if shape_pad[1]%2==0 else shape_pad[0]//2), (shape_pad[1]//2 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "                    ((0 if shape_pad[2]%2==0 else shape_pad[0]//2), (shape_pad[2]//2 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "            sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "            sx = sample_map_padded.shape[0] / k\n",
    "            sy = sample_map_padded.shape[1] / k\n",
    "            sz = sample_map_padded.shape[2] / k\n",
    "            for kz in range(k):\n",
    "                for ky in range(k):\n",
    "                    for kx in range(k):\n",
    "                        ki_region = sample_map_padded[int(kx*sx): int(kx*sx + sx - 1), \n",
    "                                                     int(ky*sy): int(ky*sy + sy - 1), \n",
    "                                                     int(kz*sz): int(kz*sz + sz - 1)]\n",
    "                        #padding i-th region by 3 pixels\n",
    "                        ki_region_padded = np.pad(ki_region, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "                        arr_regions.append(ki_region_padded)\n",
    "        else:\n",
    "            map_shape = sample_map.shape\n",
    "            shape_pad = ((map_shape[0]//2 + 1)*2 - map_shape[0],\n",
    "                         (map_shape[1]//2 + 1)*2 - map_shape[1],\n",
    "                         (map_shape[2]//2 + 1)*2 - map_shape[2])\n",
    "\n",
    "            npad = (((0 if shape_pad[0]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "                    ((0 if shape_pad[1]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "                    ((0 if shape_pad[2]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "            sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "            \n",
    "#             map_padded = np.pad(sample_map_padded, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "            arr_regions.append(sample_map_padded)\n",
    "            \n",
    "    X_mri = np.stack(arr_regions, axis=3)\n",
    "#     print(X_mri.shape)\n",
    "    return X_mri, X_mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_inputs('10002').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_py_function(func, inp, Tout, name=None):\n",
    "    \n",
    "    def wrapped_func(*flat_inp):\n",
    "        reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp,\n",
    "                                                     expand_composites=True)\n",
    "        out = func(*reconstructed_inp)\n",
    "        return tf.nest.flatten(out, expand_composites=True)\n",
    "    \n",
    "    flat_Tout = tf.nest.flatten(Tout, expand_composites=True)\n",
    "    flat_out = tf.py_function(func=wrapped_func, \n",
    "                              inp=tf.nest.flatten(inp, expand_composites=True),\n",
    "                              Tout=[_tensor_spec_to_dtype(v) for v in flat_Tout],\n",
    "                              name=name)\n",
    "    spec_out = tf.nest.map_structure(_dtype_to_tensor_spec, Tout, expand_composites=True)\n",
    "    out = tf.nest.pack_sequence_as(spec_out, flat_out, expand_composites=True)\n",
    "    return out\n",
    "\n",
    "def _dtype_to_tensor_spec(v):\n",
    "    return tf.TensorSpec(None, v) if isinstance(v, tf.dtypes.DType) else v\n",
    "\n",
    "def _tensor_spec_to_dtype(v):\n",
    "    return v.dtype if isinstance(v, tf.TensorSpec) else v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data, batch_size):\n",
    "    data = np.array([int(i) for i in data])\n",
    "    data = tf.data.Dataset.from_tensor_slices(data)\n",
    "    data = data.shuffle(buffer_size=4000, seed=30, reshuffle_each_iteration=True)\n",
    "    \n",
    "    data = data.map(lambda idx: new_py_function(get_inputs, inp=[idx], \n",
    "                                                    Tout=(tf.TensorSpec(shape=(None, 52, 66, 56, 53), dtype=tf.dtypes.float64),\n",
    "                                                          tf.TensorSpec(shape=(None, 52, 66, 56, 53), dtype=tf.dtypes.float64)), name=None), \n",
    "                     num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "                     deterministic=False)\n",
    "#     data = data.map(lambda idx: tf.py_function(get_inputs, inp=[idx], \n",
    "#                                                     Tout=tf.float64, name=None), \n",
    "#                      num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "#                      deterministic=False)\n",
    "    data = data.batch(batch_size, drop_remainder=False)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10001', '10002', '10004', ..., '21749', '21751', '21753'],\n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(ALL_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10001, 10002, 10004, ..., 21674, 21693, 21734], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10001, 10002, 10004, ..., 21749, 21751, 21753])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([int(i) for i in ALL_IDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = model_selection.train_test_split(data, test_size=0.2, shuffle=True, random_state=30)\n",
    "# train, val = model_selection.train_test_split(train, test_size=0.2, shuffle=True, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "ds_train = get_dataset(ALL_IDS, batch_size)\n",
    "# ds_val = get_dataset(val, batch_size)\n",
    "# ds_test = get_dataset(test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ds_train.take(1):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE_mri = (52, 66, 56, 53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = keras.layers.InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.        \n",
    "                 q_ij = 1/(1+dist(x_i, Âµ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, filters=[64, 32, 16, 8, 2]):\n",
    "#     if input_shape[0] % 8 == 0:\n",
    "#         pad3 = 'same'\n",
    "#     else:\n",
    "#         pad3 = 'valid'\n",
    "    \n",
    "    #============================================================================\n",
    "    # CNN for MRI images processing\n",
    "    #============================================================================\n",
    "    inputs_mri = keras.layers.Input(shape=INPUT_SHAPE_mri, name='inpupt_mri')\n",
    "\n",
    "    # convolution block #1\n",
    "    x = keras.layers.Conv3D(filters[0], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(inputs_mri)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.Conv3D(filters[0], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "#     x, p1_idx = tf.nn.max_pool_with_argmax(x, ksize=[2], strides=[2], padding='SAME', name=\"p1\")\n",
    "    x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "    # convolution block #2\n",
    "    x = keras.layers.Conv3D(filters[1], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.Conv3D(filters[1], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "    x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "    # convolution block #3\n",
    "    x = keras.layers.Conv3D(filters[2], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.Conv3D(filters[2], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "    x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "    # convolution block #4\n",
    "    x = keras.layers.Conv3D(filters[3], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.Conv3D(filters[3], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "    x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(x)\n",
    "    \n",
    "    shape_before_flattening = K.int_shape(x)\n",
    "\n",
    "    flatten = keras.layers.Flatten(data_format='channels_last')(x)\n",
    "\n",
    "    encoded = keras.layers.Dense(2,\n",
    "                               kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                               bias_initializer=keras.initializers.Constant(5.))(flatten)\n",
    "    encoded = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(encoded)\n",
    "    \n",
    "\n",
    "#     filters=[64, 32, 16, 8, 2]\n",
    "#     INPUT_SHAPE_mri = (52, 66, 56, 53)\n",
    "    \n",
    "    # Decoder\n",
    "    x = keras.layers.Dense(filters[3]*int(input_shape[0]/8)*int(input_shape[0]/8),\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(encoded)\n",
    "    \n",
    "    x = keras.layers.Reshape((int(input_shape[0]/16), int(input_shape[1]/16), int(input_shape[2]/16), filters[3]))(x)\n",
    "    \n",
    "    # convolution block #4\n",
    "    x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "#     x = tf.keras.layers.Conv3DTranspose(filters[2], kernel_size=(3, 1, 4), strides=(2,2,2), padding='valid',\n",
    "#                                         kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                         bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[2], kernel_size=(1, 1, 2), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    # convolution block #3\n",
    "    x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[1], kernel_size=(2, 1, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    # convolution block #2\n",
    "    x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[0], kernel_size=(1, 2, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    # convolution block #1\n",
    "    x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(input_shape[3], kernel_size=(1, 1, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "#     x = keras.layers.Dense(np.prod(shape_before_flattening[1:]),\n",
    "#                                kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                bias_initializer=keras.initializers.Constant(5.))(encoded)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(x)\n",
    "                           \n",
    "#     # Reshape into an image of the same shape as before our last `Flatten` layer\n",
    "#     x = keras.layers.Reshape(shape_before_flattening[1:])(x)\n",
    "                       \n",
    "#     # convolution block #4\n",
    "#     tf.keras.layers.Conv3DTranspose(8, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same', output_padding=None,\n",
    "#     data_format=None, activation=None, use_bias=True,\n",
    "#     kernel_initializer='glorot_uniform', bias_initializer='zeros',\n",
    "#     kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
    "#     kernel_constraint=None, bias_constraint=None, **kwargs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     x = keras.layers.Conv3D(8, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "#                                   kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                   bias_initializer=keras.initializers.Constant(0.01), name='aaa')(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = keras.layers.Conv3D(8, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "#                                   kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                   bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)                       \n",
    "#     x = keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "#     x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "#                                               beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "#                                               moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "#                                               beta_constraint=None, gamma_constraint=None)(x)\n",
    "                           \n",
    "#     # convolution block #3\n",
    "#     x = keras.layers.Conv3D(16, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "#                                   kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                   bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = keras.layers.Conv3D(16, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "#                                   kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                   bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "#     x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "#                                               beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "#                                               moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "#                                               beta_constraint=None, gamma_constraint=None)(x)\n",
    "                           \n",
    "#     # convolution block #2\n",
    "#     x = keras.layers.Conv3D(32, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "#                                   kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                   bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = keras.layers.Conv3D(32, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "#                                   kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                   bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "#     x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "#                                               beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "#                                               moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "#                                               beta_constraint=None, gamma_constraint=None)(x)\n",
    "                           \n",
    "#     # convolution block #1\n",
    "#     x = keras.layers.Conv3D(64, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "#                                   kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                   bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = keras.layers.Conv3D(64, kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "#                                   kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                   bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "#     x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "#                                               beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "#                                               moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "#                                               beta_constraint=None, gamma_constraint=None)(x)\n",
    "    \n",
    "    # Output\n",
    "#     decoded = keras.layers.Conv3D(53, kernel_size=(3, 3, 3), activation='sigmoid', padding='same')(x)\n",
    "    decoded = x\n",
    "\n",
    "    autoencoder = keras.Model(inputs=inputs_mri, outputs=decoded, name='autoencoder')\n",
    "    encoder = keras.Model(inputs=inputs_mri, outputs=encoded, name='encoder')\n",
    "\n",
    "    optim = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95)\n",
    "\n",
    "    METRICS = [keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    "\n",
    "    autoencoder.compile(loss='mse', metrics=METRICS, optimizer=optim)\n",
    "#     encoder.compile(loss='mse', metrics=METRICS, optimizer=optim)\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters=[64, 32, 16, 8, 2]\n",
    "# input_shape = (52, 66, 56, 53)\n",
    "# # filters[3]*int(input_shape[0]/8)*int(input_shape[0]/8)\n",
    "# (int(input_shape[0]/16), int(input_shape[1]/16), int(input_shape[2]/16), filters[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, encoder = create_model(INPUT_SHAPE_mri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inpupt_mri (InputLayer)      [(None, 52, 66, 56, 53)]  0         \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 52, 66, 56, 64)    91648     \n",
      "_________________________________________________________________\n",
      "p_re_lu (PReLU)              (None, 52, 66, 56, 64)    12300288  \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 52, 66, 56, 64)    110656    \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (None, 52, 66, 56, 64)    12300288  \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 26, 33, 28, 64)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 26, 33, 28, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 26, 33, 28, 32)    55328     \n",
      "_________________________________________________________________\n",
      "p_re_lu_2 (PReLU)            (None, 26, 33, 28, 32)    768768    \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 26, 33, 28, 32)    27680     \n",
      "_________________________________________________________________\n",
      "p_re_lu_3 (PReLU)            (None, 26, 33, 28, 32)    768768    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 13, 16, 14, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 13, 16, 14, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 13, 16, 14, 16)    13840     \n",
      "_________________________________________________________________\n",
      "p_re_lu_4 (PReLU)            (None, 13, 16, 14, 16)    46592     \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 13, 16, 14, 16)    6928      \n",
      "_________________________________________________________________\n",
      "p_re_lu_5 (PReLU)            (None, 13, 16, 14, 16)    46592     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 6, 8, 7, 16)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 8, 7, 16)       64        \n",
      "_________________________________________________________________\n",
      "conv3d_6 (Conv3D)            (None, 6, 8, 7, 8)        3464      \n",
      "_________________________________________________________________\n",
      "p_re_lu_6 (PReLU)            (None, 6, 8, 7, 8)        2688      \n",
      "_________________________________________________________________\n",
      "conv3d_7 (Conv3D)            (None, 6, 8, 7, 8)        1736      \n",
      "_________________________________________________________________\n",
      "p_re_lu_7 (PReLU)            (None, 6, 8, 7, 8)        2688      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_3 (MaxPooling3 (None, 3, 4, 3, 8)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 4, 3, 8)        32        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 578       \n",
      "_________________________________________________________________\n",
      "p_re_lu_8 (PReLU)            (None, 2)                 2         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 288)               864       \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 3, 4, 3, 8)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling3d (UpSampling3D) (None, 6, 8, 6, 8)        0         \n",
      "_________________________________________________________________\n",
      "conv3d_transpose (Conv3DTran (None, 6, 8, 7, 16)       272       \n",
      "_________________________________________________________________\n",
      "p_re_lu_9 (PReLU)            (None, 6, 8, 7, 16)       5376      \n",
      "_________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3 (None, 12, 16, 14, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_1 (Conv3DTr (None, 13, 16, 14, 32)    1056      \n",
      "_________________________________________________________________\n",
      "p_re_lu_10 (PReLU)           (None, 13, 16, 14, 32)    93184     \n",
      "_________________________________________________________________\n",
      "up_sampling3d_2 (UpSampling3 (None, 26, 32, 28, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_2 (Conv3DTr (None, 26, 33, 28, 64)    4160      \n",
      "_________________________________________________________________\n",
      "p_re_lu_11 (PReLU)           (None, 26, 33, 28, 64)    1537536   \n",
      "_________________________________________________________________\n",
      "up_sampling3d_3 (UpSampling3 (None, 52, 66, 56, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_3 (Conv3DTr (None, 52, 66, 56, 53)    3445      \n",
      "_________________________________________________________________\n",
      "p_re_lu_12 (PReLU)           (None, 52, 66, 56, 53)    10186176  \n",
      "=================================================================\n",
      "Total params: 38,381,081\n",
      "Trainable params: 38,380,841\n",
      "Non-trainable params: 240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### checkpoint_dir = './99_Training_checkpoints/mri-fnc-loading'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                               min_delta=0.001, \n",
    "#                                               patience=10, \n",
    "#                                               verbose=1, \n",
    "#                                               mode='min',\n",
    "#                                               baseline=None, \n",
    "#                                               restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_prefix = os.path.join('./99_Training_checkpoints/mri_clustering', \"ckpt_{epoch}\")\n",
    "\n",
    "callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./99_Logs/mri_clustering'),\n",
    "             tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                                save_weights_only=True),\n",
    "             tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                              min_delta=0.001, \n",
    "                                              patience=5, \n",
    "                                              verbose=1, \n",
    "                                              mode='min',\n",
    "                                              baseline=None, \n",
    "                                              restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "   3/1470 [..............................] - ETA: 41:39 - loss: 4.3239 - rmse: 2.0794"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[8,52,66,56,53] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node mean_squared_error/SquaredDifference (defined at <ipython-input-23-f130ff41a64a>:3) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_7655]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-f130ff41a64a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/GPU:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# with tf.device('/CPU:0'):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     hist = autoencoder.fit(ds_train,\n\u001b[0m\u001b[0;32m      4\u001b[0m                      \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                      \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m     \"\"\"\n\u001b[1;32m-> 1661\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    591\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[8,52,66,56,53] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node mean_squared_error/SquaredDifference (defined at <ipython-input-23-f130ff41a64a>:3) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_7655]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "# with tf.device('/CPU:0'):\n",
    "    hist = autoencoder.fit(ds_train,\n",
    "                     validation_data=ds_train,\n",
    "                     callbacks=callbacks,\n",
    "                     epochs=400,\n",
    "                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147/147 [==============================] - 432s 3s/step - loss: 8.0011 - rmse: 10.5004 - mse: 110.2587TA: 5:53 - loss: 8.1505 - rmse: 10.7224 - mse: 114.970 - ETA: 5:54 - loss: 8.2938 - rmse: 10.828 - ETA: 5:40 - loss: 8.2050 - rmse: 10.6929 - mse: 114.3 - ETA: 5:24 - loss: 8.2535 - rmse: 10.7792 - mse - ETA: 4:56 - loss: 8.1050  - ETA: 3:26 - loss: 7.8890 - rmse: 10.3689 - mse: 107.51 - ETA: 3:19 - loss: 7.9023 - rmse: 1 - ETA: 2:16 -  - ETA: 32s - loss: 7.9700 - rmse: 10.4486 - mse: 109.17 - ETA: 29s - loss: 7.9624 - rmse: 10.44\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    results = model.evaluate(ds_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-28efc6a72ce7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rmse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Metric'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(16,16)\n",
    "\n",
    "    ax=fig.add_subplot(3,2,1)\n",
    "    ax.plot(hist.history['rmse'])\n",
    "    ax.plot(hist.history['mse'])\n",
    "    ax.legend(['Metric', 'Loss'])\n",
    "    ax.set_title('Train')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,2)\n",
    "    ax.plot(hist.history['val_rmse'])\n",
    "    ax.plot(hist.history['val_mse'])\n",
    "    ax.legend(['Metric', 'Loss'])\n",
    "    ax.set_title('Test')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,3)\n",
    "    ax.plot(hist.history['loss'])\n",
    "    ax.plot(hist.history['val_loss'])\n",
    "    ax.legend(['Train', 'Test'])\n",
    "    ax.set_title('Loss')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,4)\n",
    "    ax.plot(hist.history['mse'])\n",
    "    ax.plot(hist.history['val_mse'])\n",
    "    ax.legend(['Train', 'Test'])\n",
    "    ax.set_title('Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs_subm(idx):\n",
    "    # FNC inputs\n",
    "    df_fnc = pd.read_csv('00_Data/fnc_csv/{0}.csv'.format(idx), index_col=0)\n",
    "    X_fnc = np.array(df_fnc.values).reshape(-1)\n",
    "    \n",
    "    # Loading inputs\n",
    "    df_loading = pd.read_csv('00_Data/loading_csv/{0}.csv'.format(idx), index_col=0)\n",
    "    X_loading = np.array(df_loading.values).reshape(-1)\n",
    "    \n",
    "    #MRI inputs\n",
    "    patient_SM = h5py.File('00_Data/fMRI_test/{0}.mat'.format(idx), mode='r')\n",
    "    patient_SM = np.array(patient_SM.get('SM_feature'))\n",
    "#     patient_SM = mat.transpose([1,2,3,0])\n",
    "\n",
    "#     print('patient: {idx}')\n",
    "    \n",
    "    k = 1\n",
    "    ki_padding = 3\n",
    "    \n",
    "    arr_regions = []\n",
    "    for i in range(patient_SM.shape[0]):\n",
    "        sample_map = patient_SM[i,:,:,:]\n",
    "#         print(len(arr_regions))\n",
    "        # padding MRI map\n",
    "        if k > 1:\n",
    "            map_shape = sample_map.shape\n",
    "            shape_pad = ((map_shape[0]//k + 1)*k - map_shape[0],\n",
    "                         (map_shape[1]//k + 1)*k - map_shape[1],\n",
    "                         (map_shape[2]//k + 1)*k - map_shape[2])\n",
    "\n",
    "            npad = ((shape_pad[0]//2, (shape_pad[0]//2 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "                    (shape_pad[1]//2, (shape_pad[1]//2 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "                    (shape_pad[2]//2, (shape_pad[2]//2 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "            sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "            sx = sample_map_padded.shape[0] / k\n",
    "            sy = sample_map_padded.shape[1] / k\n",
    "            sz = sample_map_padded.shape[2] / k\n",
    "            for kz in range(k):\n",
    "                for ky in range(k):\n",
    "                    for kx in range(k):\n",
    "                        ki_region = sample_map_padded[int(kx*sx): int(kx*sx + sx - 1), \n",
    "                                                     int(ky*sy): int(ky*sy + sy - 1), \n",
    "                                                     int(kz*sz): int(kz*sz + sz - 1)]\n",
    "                        #padding i-th region by 3 pixels\n",
    "                        ki_region_padded = np.pad(ki_region, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "                        arr_regions.append(ki_region_padded)\n",
    "        else:\n",
    "            map_padded = np.pad(sample_map, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "            arr_regions.append(map_padded)\n",
    "#             print(map_padded.shape)\n",
    "    X_mri = np.stack(arr_regions, axis=3)\n",
    "    \n",
    "#     X = (X_mri, X_fnc, X_loading)\n",
    "    return X_mri, X_fnc, X_loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = []\n",
    "for i in TEST_IDS:\n",
    "    X_mri, X_fnc, X_loading = get_inputs_subm(i)\n",
    "    X_mri = X_mri.reshape(1,58, 69, 59, 53)\n",
    "    X_fnc = X_fnc.reshape(1,1378)\n",
    "    X_loading = X_loading.reshape(1,26)\n",
    "    preds = model.predict([X_mri, X_fnc, X_loading], batch_size=1)\n",
    "    y_preds.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5877"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.array(y_preds).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = []\n",
    "i = 0\n",
    "for idx in TEST_IDS:\n",
    "    df_submission.append(['{0}_age'.format(idx), y_preds[i]])\n",
    "    df_submission.append(['{0}_domain1_var1'.format(idx), y_preds[i+1]])\n",
    "    df_submission.append(['{0}_domain1_var2'.format(idx), y_preds[i+2]])\n",
    "    df_submission.append(['{0}_domain2_var1'.format(idx), y_preds[i+3]])\n",
    "    df_submission.append(['{0}_domain2_var2'.format(idx), y_preds[i+4]])\n",
    "    i += 5\n",
    "\n",
    "df_submission = pd.DataFrame(df_submission, columns=['Id', 'Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('submission_mri-fnc-load_mae_08.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
