{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "\n",
    "import scipy.stats as sts\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing as prep\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn import discriminant_analysis as disan\n",
    "from sklearn import calibration as calib\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn import svm\n",
    "from sklearn import gaussian_process as gaup\n",
    "from sklearn import mixture as mix\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble as ens\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# from keras import models as kermdls\n",
    "# from keras import layers as kerlrs\n",
    "# from keras import metrics as kmetrics\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "import pickle\n",
    "\n",
    "import nilearn as nl\n",
    "from nilearn import plotting, image\n",
    "from nilearn import datasets\n",
    "import nibabel as nb\n",
    "import h5py\n",
    "\n",
    "import os\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10774185867225038904\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2672954675553854554\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6589725830\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11543069491715066944\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 5035152554555290209\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fnc_10 = pd.read_csv('00_Data/fnc.csv')\n",
    "# fnc_10 = fnc_10.head(5)\n",
    "# for row in fnc_10.iterrows():\n",
    "#     idx = int(row[1][0])\n",
    "#     row = row[1][1:]\n",
    "#     print(row)\n",
    "#     row.to_csv('00_Data/fnc_csv_norm/{0}.csv'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_test'))]\n",
    "TRAIN_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_train'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>age</th>\n",
       "      <th>domain1_var1</th>\n",
       "      <th>domain1_var2</th>\n",
       "      <th>domain2_var1</th>\n",
       "      <th>domain2_var2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>57.436077</td>\n",
       "      <td>30.571975</td>\n",
       "      <td>62.553736</td>\n",
       "      <td>53.325130</td>\n",
       "      <td>51.427998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>59.580851</td>\n",
       "      <td>50.969456</td>\n",
       "      <td>67.470628</td>\n",
       "      <td>60.651856</td>\n",
       "      <td>58.311361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10004</td>\n",
       "      <td>71.413018</td>\n",
       "      <td>53.152498</td>\n",
       "      <td>58.012103</td>\n",
       "      <td>52.418389</td>\n",
       "      <td>62.536641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10005</td>\n",
       "      <td>66.532630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.108977</td>\n",
       "      <td>69.993075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10007</td>\n",
       "      <td>38.617381</td>\n",
       "      <td>49.197021</td>\n",
       "      <td>65.674285</td>\n",
       "      <td>40.151376</td>\n",
       "      <td>34.096421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>21746</td>\n",
       "      <td>14.257265</td>\n",
       "      <td>21.358872</td>\n",
       "      <td>61.165998</td>\n",
       "      <td>51.778483</td>\n",
       "      <td>54.640179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>21747</td>\n",
       "      <td>55.456978</td>\n",
       "      <td>68.169675</td>\n",
       "      <td>29.907995</td>\n",
       "      <td>55.349257</td>\n",
       "      <td>54.019517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>21750</td>\n",
       "      <td>48.948756</td>\n",
       "      <td>55.114811</td>\n",
       "      <td>60.878271</td>\n",
       "      <td>38.617246</td>\n",
       "      <td>50.679885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>21752</td>\n",
       "      <td>66.532630</td>\n",
       "      <td>59.844808</td>\n",
       "      <td>72.303110</td>\n",
       "      <td>55.458281</td>\n",
       "      <td>46.870235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>21754</td>\n",
       "      <td>68.820928</td>\n",
       "      <td>56.594193</td>\n",
       "      <td>34.605868</td>\n",
       "      <td>49.922535</td>\n",
       "      <td>50.383078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5877 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id        age  domain1_var1  domain1_var2  domain2_var1  domain2_var2\n",
       "0     10001  57.436077     30.571975     62.553736     53.325130     51.427998\n",
       "1     10002  59.580851     50.969456     67.470628     60.651856     58.311361\n",
       "2     10004  71.413018     53.152498     58.012103     52.418389     62.536641\n",
       "3     10005  66.532630           NaN           NaN     52.108977     69.993075\n",
       "4     10007  38.617381     49.197021     65.674285     40.151376     34.096421\n",
       "...     ...        ...           ...           ...           ...           ...\n",
       "5872  21746  14.257265     21.358872     61.165998     51.778483     54.640179\n",
       "5873  21747  55.456978     68.169675     29.907995     55.349257     54.019517\n",
       "5874  21750  48.948756     55.114811     60.878271     38.617246     50.679885\n",
       "5875  21752  66.532630     59.844808     72.303110     55.458281     46.870235\n",
       "5876  21754  68.820928     56.594193     34.605868     49.922535     50.383078\n",
       "\n",
       "[5877 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('00_Data/train_scores.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                0\n",
       "age               0\n",
       "domain1_var1    438\n",
       "domain1_var2    438\n",
       "domain2_var1     39\n",
       "domain2_var2     39\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nulls = data.isnull().sum()\n",
    "# l = len(data.index)\n",
    "\n",
    "# nulls['domain1_var1'] / l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length:  5434\n"
     ]
    }
   ],
   "source": [
    "print('Dataset length: ', len(data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_inputs_fnc(idx, labels):\n",
    "#     df = pd.read_csv('00_Data/fnc_csv/{0}.csv'.format(idx), index_col=0)\n",
    "#     X = np.array(df.values).reshape(-1)\n",
    "#     return X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_inputs_loading(idx, labels):\n",
    "#     df = pd.read_csv('00_Data/loading_csv/{0}.csv'.format(idx), index_col=0)\n",
    "#     X = np.array(df.values).reshape(-1)\n",
    "#     return X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(idx, labels):\n",
    "    df_fnc = pd.read_csv('00_Data/fnc_csv/{0}.csv'.format(idx), index_col=0)\n",
    "    X_fnc = np.array(df_fnc.values).reshape(-1)\n",
    "    \n",
    "    df_loading = pd.read_csv('00_Data/loading_csv/{0}.csv'.format(idx), index_col=0)\n",
    "    X_loading = np.array(df_loading.values).reshape(-1)\n",
    "#     print(X_fnc[0])\n",
    "#     print(X_loading[0])\n",
    "#     print(labels[0])\n",
    "#     print(X_fnc.shape)\n",
    "#     print(X_loading.shape)\n",
    "#     print(labels.shape)\n",
    "\n",
    "#     X_fnc = tf.convert_to_tensor(X_fnc, dtype=tf.float64)\n",
    "#     X_loading = tf.convert_to_tensor(X_loading, dtype=tf.float64)\n",
    "#     labels = tf.convert_to_tensor(labels, dtype=tf.float64)\n",
    "#     X = tf.tuple([X_fnc, X_loading])\n",
    "\n",
    "#     X = dict()\n",
    "#     X['input_1'] = X_fnc\n",
    "#     X['input_2'] = X_loading\n",
    "    X = (X_fnc, X_loading)\n",
    "    return X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_decorator(func):\n",
    "    def wrapper(idx, labels):\n",
    "        # Use a tf.py_function to prevent auto-graph from compiling the method\n",
    "        return tf.py_function(func,\n",
    "                              inp=(idx, labels),\n",
    "                              Tout=tf.float64)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_py_function(func, inp, Tout, name=None):\n",
    "    \n",
    "    def wrapped_func(*flat_inp):\n",
    "        reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp,\n",
    "                                                     expand_composites=True)\n",
    "        out = func(*reconstructed_inp)\n",
    "        return tf.nest.flatten(out, expand_composites=True)\n",
    "    \n",
    "    flat_Tout = tf.nest.flatten(Tout, expand_composites=True)\n",
    "    flat_out = tf.py_function(func=wrapped_func, \n",
    "                              inp=tf.nest.flatten(inp, expand_composites=True),\n",
    "                              Tout=[_tensor_spec_to_dtype(v) for v in flat_Tout],\n",
    "                              name=name)\n",
    "    spec_out = tf.nest.map_structure(_dtype_to_tensor_spec, Tout, expand_composites=True)\n",
    "    out = tf.nest.pack_sequence_as(spec_out, flat_out, expand_composites=True)\n",
    "    return out\n",
    "\n",
    "def _dtype_to_tensor_spec(v):\n",
    "    return tf.TensorSpec(None, v) if isinstance(v, tf.dtypes.DType) else v\n",
    "\n",
    "def _tensor_spec_to_dtype(v):\n",
    "    return v.dtype if isinstance(v, tf.TensorSpec) else v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_dataset(data, batch_size):\n",
    "#     data = tf.data.Dataset.from_tensor_slices((data['Id'].values, \n",
    "#                                                data[['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']].values))\n",
    "#     data = data.shuffle(buffer_size=5500, seed=30, reshuffle_each_iteration=True)\n",
    "    \n",
    "#     data_fnc = data.map(map_decorator(get_inputs_fnc), \n",
    "#                      num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "#                      deterministic=True)\n",
    "#     data_loading = data.map(map_decorator(get_inputs_loading), \n",
    "#                      num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "#                      deterministic=True)\n",
    "\n",
    "#     data_fnc = data_fnc.batch(batch_size, drop_remainder=True)\n",
    "#     data_fnc = data_fnc.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "#     data_loading = data_loading.batch(batch_size, drop_remainder=True)\n",
    "#     data_loading = data_loading.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#     return (data_fnc, data_loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data, batch_size):\n",
    "    data = tf.data.Dataset.from_tensor_slices((data['Id'].values, \n",
    "                                               data[['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']].values))\n",
    "    data = data.shuffle(buffer_size=5500, seed=30, reshuffle_each_iteration=True)\n",
    "    \n",
    "    data = data.map(lambda idx, lbl:new_py_function(get_inputs, inp=(idx, lbl), Tout=((tf.float64, tf.float64), tf.float64), name=None), \n",
    "                     num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "                     deterministic=True)\n",
    "    data = data.batch(batch_size, drop_remainder=True)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.2, shuffle=True, random_state=30)\n",
    "train, val = model_selection.train_test_split(train, test_size=0.2, shuffle=True, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "\n",
    "# ds_train_fnc, ds_train_loading = get_dataset(train, batch_size)\n",
    "# ds_val_fnc, ds_val_loading = get_dataset(val, batch_size)\n",
    "# ds_test_fnc, ds_test_loading = get_dataset(test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "ds_train = get_dataset(train, batch_size)\n",
    "ds_val = get_dataset(val, batch_size)\n",
    "ds_test = get_dataset(test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.perf_counter()\n",
    "# for f in ds_train.take(1):\n",
    "#     pass\n",
    "# tf.print(\"Execution time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE_fnc = (1378,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE_loading = (26,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_fnc = keras.layers.Input(shape=INPUT_SHAPE_fnc, name='inp_fnc')\n",
    "\n",
    "x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                          beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                          moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                          beta_constraint=None, gamma_constraint=None)(inputs_fnc)\n",
    "x = keras.layers.Dense(2048,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(x)\n",
    "x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(x)\n",
    "# x = keras.layers.Dropout(rate=0.2, seed=30)(x)\n",
    "\n",
    "x1 = keras.layers.Dense(512,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(x)\n",
    "x1 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(x1)\n",
    "# x1 = keras.layers.Dropout(rate=0.2, seed=30)(x1)\n",
    "\n",
    "x2 = keras.layers.Dense(512,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(x)\n",
    "x2 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(x2)\n",
    "# x2 = keras.layers.Dropout(rate=0.2, seed=30)(x2)\n",
    "\n",
    "x = keras.layers.concatenate([x1, x2])\n",
    "\n",
    "x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                          beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                          moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                          beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "x = keras.layers.Dense(256,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(x)\n",
    "x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(x)\n",
    "# x = keras.layers.Dropout(rate=0.2, seed=30)(x)\n",
    "\n",
    "# output\n",
    "x = keras.Model(inputs=inputs_fnc, outputs=x, name='model_fnc')\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_loading = keras.layers.Input(shape=INPUT_SHAPE_loading, name='inp_load')\n",
    "\n",
    "y = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                          beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                          moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                          beta_constraint=None, gamma_constraint=None)(inputs_loading)\n",
    "\n",
    "y = keras.layers.Dense(256,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(y)\n",
    "y = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(y)\n",
    "# y = keras.layers.Dropout(rate=0.2, seed=30)(y)\n",
    "\n",
    "y1 = keras.layers.Dense(128,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(y)\n",
    "y1 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(y1)\n",
    "# x1 = keras.layers.Dropout(rate=0.2, seed=30)(x1)\n",
    "\n",
    "y2 = keras.layers.Dense(128,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(y)\n",
    "y2 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(y2)\n",
    "# x2 = keras.layers.Dropout(rate=0.2, seed=30)(x2)\n",
    "\n",
    "y = keras.layers.concatenate([y1, y2])\n",
    "\n",
    "y = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                          beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                          moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                          beta_constraint=None, gamma_constraint=None)(y)\n",
    "\n",
    "y = keras.layers.Dense(256,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(y)\n",
    "y = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(y)\n",
    "# x = keras.layers.Dropout(rate=0.2, seed=30)(x)\n",
    "\n",
    "# output\n",
    "y = keras.Model(inputs=inputs_loading, outputs=y, name='model_loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = keras.layers.concatenate([x.output, y.output])\n",
    "\n",
    "z = keras.layers.Dense(512, \n",
    "                       kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                       bias_initializer=keras.initializers.Constant(5.))(concat)\n",
    "z = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(z)\n",
    "# z = keras.layers.Dropout(rate=0.2, seed=30)(z)\n",
    "\n",
    "z = keras.layers.Dense(512, \n",
    "                       kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                       bias_initializer=keras.initializers.Constant(5.))(z)\n",
    "z = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(z)\n",
    "# z = keras.layers.Dropout(rate=0.2, seed=30)(z)\n",
    "\n",
    "outputs = keras.layers.Dense(5, activation='linear')(z)\n",
    "\n",
    "model = keras.Model(inputs=[x.input, y.input], outputs=outputs, name='model_combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_combined\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inp_fnc (InputLayer)            [(None, 1378)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inp_load (InputLayer)           [(None, 26)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1378)         5512        inp_fnc[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 26)           104         inp_load[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2048)         2824192     batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          6912        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu (PReLU)                 (None, 2048)         2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_4 (PReLU)               (None, 256)          256         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1049088     p_re_lu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          1049088     p_re_lu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          32896       p_re_lu_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 128)          32896       p_re_lu_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 512)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 512)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_5 (PReLU)               (None, 128)          128         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_6 (PReLU)               (None, 128)          128         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1024)         0           p_re_lu_1[0][0]                  \n",
      "                                                                 p_re_lu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           p_re_lu_5[0][0]                  \n",
      "                                                                 p_re_lu_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1024)         4096        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 256)          1024        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          262400      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          65792       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_3 (PReLU)               (None, 256)          256         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_7 (PReLU)               (None, 256)          256         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           p_re_lu_3[0][0]                  \n",
      "                                                                 p_re_lu_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 512)          262656      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_8 (PReLU)               (None, 512)          512         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 512)          262656      p_re_lu_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_9 (PReLU)               (None, 512)          512         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 5)            2565        p_re_lu_9[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,866,997\n",
      "Trainable params: 5,861,629\n",
      "Non-trainable params: 5,368\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim = keras.optimizers.Adam(lr=0.000001,\n",
    "#                                  beta_1=0.99,\n",
    "#                                  beta_2=0.999,\n",
    "#                                  amsgrad=False)\n",
    "\n",
    "optim = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95)\n",
    "        \n",
    "METRICS = [keras.metrics.RootMeanSquaredError(name='rmse'),\n",
    "           keras.metrics.MeanSquaredError(name='mse'),\n",
    "           keras.metrics.MeanAbsoluteError(name='mae')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weighted_mae(y_true, y_pred):\n",
    "# #     tf.print(y_true)\n",
    "#     W = tf.constant([[0.2, 0.2, 0.2, 0.2, 0.2]])\n",
    "# #     tf.print(W / tf.math.reduce_mean(y_true, axis=0))\n",
    "#     return tf.math.reduce_mean(tf.linalg.matmul(tf.math.abs(y_pred - y_true), tf.transpose(W / tf.math.reduce_mean(y_true, axis=0))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mae', metrics=METRICS, optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint directory to store the checkpoints\n",
    "# Name of the checkpoint files\n",
    "# checkpoint_prefix = os.path.join('./99_Training_checkpoints/fnc-loading', \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./99_Logs/fnc-loading'),\n",
    "#              tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "#                                                 save_weights_only=False),\n",
    "#              tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "#                                                   factor=0.7, \n",
    "#                                                   patience=2, \n",
    "#                                                   verbose=1, \n",
    "#                                                   mode='min',\n",
    "#                                                   min_delta=0.01, \n",
    "#                                                   cooldown=5, \n",
    "#                                                   min_lr=0.00000001),\n",
    "#              tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                               min_delta=0.001, \n",
    "#                                               patience=10, \n",
    "#                                               verbose=1, \n",
    "#                                               mode='min',\n",
    "#                                               baseline=None, \n",
    "#                                               restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./99_Logs/fnc-loading'),\n",
    "#              tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "#                                                   factor=0.7, \n",
    "#                                                   patience=2, \n",
    "#                                                   verbose=1, \n",
    "#                                                   mode='min',\n",
    "#                                                   min_delta=0.01, \n",
    "#                                                   cooldown=5, \n",
    "#                                                   min_lr=0.00000001),\n",
    "#              tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                               min_delta=0.001, \n",
    "#                                               patience=10, \n",
    "#                                               verbose=1, \n",
    "#                                               mode='min',\n",
    "#                                               baseline=None, \n",
    "#                                               restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./99_Logs/fnc-loading'),\n",
    "             tf.keras.callbacks.EarlyStopping(monitor='loss', \n",
    "                                              min_delta=0.001, \n",
    "                                              patience=10, \n",
    "                                              verbose=1, \n",
    "                                              mode='min',\n",
    "                                              baseline=None, \n",
    "                                              restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decay(epoch):\n",
    "#     if epoch < 2:\n",
    "#         return 0.01\n",
    "#     elif epoch >= 2 and epoch < 10:\n",
    "#         return 0.005\n",
    "#     else:\n",
    "#         return 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [tf.keras.callbacks.LearningRateScheduler(decay)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "54/54 [==============================] - 15s 271ms/step - loss: 66.2726 - rmse: 72.4280 - mse: 5245.8159 - mae: 66.2726 - val_loss: 54.8515 - val_rmse: 61.5766 - val_mse: 3791.6838 - val_mae: 54.8515\n",
      "Epoch 2/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 60.2610 - rmse: 66.6709 - mse: 4445.0054 - mae: 60.2610 - val_loss: 49.7743 - val_rmse: 56.0183 - val_mse: 3138.0457 - val_mae: 49.7743\n",
      "Epoch 3/200\n",
      "54/54 [==============================] - 14s 265ms/step - loss: 53.9625 - rmse: 60.5729 - mse: 3669.0811 - mae: 53.9625 - val_loss: 44.7136 - val_rmse: 50.8208 - val_mse: 2582.7510 - val_mae: 44.7136\n",
      "Epoch 4/200\n",
      "54/54 [==============================] - 14s 265ms/step - loss: 47.3381 - rmse: 54.1102 - mse: 2927.9124 - mae: 47.3381 - val_loss: 39.4069 - val_rmse: 45.4233 - val_mse: 2063.2791 - val_mae: 39.4069\n",
      "Epoch 5/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 40.4330 - rmse: 47.1924 - mse: 2227.1201 - mae: 40.4330 - val_loss: 33.8488 - val_rmse: 39.7317 - val_mse: 1578.6060 - val_mae: 33.8488\n",
      "Epoch 6/200\n",
      "54/54 [==============================] - 14s 268ms/step - loss: 33.3030 - rmse: 39.7147 - mse: 1577.2594 - mae: 33.3030 - val_loss: 27.6806 - val_rmse: 33.0648 - val_mse: 1093.2820 - val_mae: 27.6806\n",
      "Epoch 7/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 25.9217 - rmse: 31.5309 - mse: 994.1972 - mae: 25.9217 - val_loss: 21.0157 - val_rmse: 25.5492 - val_mse: 652.7621 - val_mae: 21.0157\n",
      "Epoch 8/200\n",
      "54/54 [==============================] - 14s 268ms/step - loss: 18.4526 - rmse: 22.8703 - mse: 523.0500 - mae: 18.4526 - val_loss: 14.5534 - val_rmse: 17.9390 - val_mse: 321.8087 - val_mae: 14.5534\n",
      "Epoch 9/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 12.3304 - rmse: 15.3927 - mse: 236.9358 - mae: 12.3304 - val_loss: 10.3740 - val_rmse: 12.8826 - val_mse: 165.9614 - val_mae: 10.3740\n",
      "Epoch 10/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 9.5599 - rmse: 11.9714 - mse: 143.3148 - mae: 9.5599 - val_loss: 9.3352 - val_rmse: 11.7993 - val_mse: 139.2235 - val_mae: 9.3352\n",
      "Epoch 11/200\n",
      "54/54 [==============================] - 15s 273ms/step - loss: 9.0721 - rmse: 11.4562 - mse: 131.2451 - mae: 9.0721 - val_loss: 9.1621 - val_rmse: 11.5890 - val_mse: 134.3056 - val_mae: 9.1621\n",
      "Epoch 12/200\n",
      "54/54 [==============================] - 14s 265ms/step - loss: 8.9782 - rmse: 11.3628 - mse: 129.1139 - mae: 8.9782 - val_loss: 9.1464 - val_rmse: 11.5840 - val_mse: 134.1885 - val_mae: 9.1464\n",
      "Epoch 13/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 8.9079 - rmse: 11.3041 - mse: 127.7832 - mae: 8.9079 - val_loss: 9.0398 - val_rmse: 11.4770 - val_mse: 131.7220 - val_mae: 9.0398\n",
      "Epoch 14/200\n",
      "54/54 [==============================] - 14s 264ms/step - loss: 8.8458 - rmse: 11.2372 - mse: 126.2747 - mae: 8.8458 - val_loss: 9.0110 - val_rmse: 11.4430 - val_mse: 130.9414 - val_mae: 9.0110\n",
      "Epoch 15/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 8.7997 - rmse: 11.1892 - mse: 125.1976 - mae: 8.7997 - val_loss: 8.9210 - val_rmse: 11.3318 - val_mse: 128.4102 - val_mae: 8.9210\n",
      "Epoch 16/200\n",
      "54/54 [==============================] - 15s 270ms/step - loss: 8.7606 - rmse: 11.1449 - mse: 124.2089 - mae: 8.7606 - val_loss: 8.9180 - val_rmse: 11.3196 - val_mse: 128.1333 - val_mae: 8.9180\n",
      "Epoch 17/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 8.7351 - rmse: 11.1160 - mse: 123.5648 - mae: 8.7351 - val_loss: 8.8655 - val_rmse: 11.2950 - val_mse: 127.5763 - val_mae: 8.8655\n",
      "Epoch 18/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 8.6898 - rmse: 11.0657 - mse: 122.4507 - mae: 8.6898 - val_loss: 8.8344 - val_rmse: 11.2513 - val_mse: 126.5920 - val_mae: 8.8344\n",
      "Epoch 19/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 8.6555 - rmse: 11.0306 - mse: 121.6735 - mae: 8.6555 - val_loss: 8.8398 - val_rmse: 11.2172 - val_mse: 125.8253 - val_mae: 8.8398\n",
      "Epoch 20/200\n",
      "54/54 [==============================] - 14s 268ms/step - loss: 8.6210 - rmse: 11.0009 - mse: 121.0199 - mae: 8.6210 - val_loss: 8.8171 - val_rmse: 11.2079 - val_mse: 125.6170 - val_mae: 8.8171\n",
      "Epoch 21/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 8.5859 - rmse: 10.9507 - mse: 119.9178 - mae: 8.5859 - val_loss: 8.7869 - val_rmse: 11.1974 - val_mse: 125.3825 - val_mae: 8.7869\n",
      "Epoch 22/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 8.5646 - rmse: 10.9281 - mse: 119.4227 - mae: 8.5646 - val_loss: 8.7992 - val_rmse: 11.2153 - val_mse: 125.7819 - val_mae: 8.7992\n",
      "Epoch 23/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 8.5337 - rmse: 10.9118 - mse: 119.0677 - mae: 8.5337 - val_loss: 8.7631 - val_rmse: 11.1464 - val_mse: 124.2425 - val_mae: 8.7631\n",
      "Epoch 24/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 8.5107 - rmse: 10.8841 - mse: 118.4628 - mae: 8.5107 - val_loss: 8.7286 - val_rmse: 11.1059 - val_mse: 123.3405 - val_mae: 8.7286\n",
      "Epoch 25/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 8.4927 - rmse: 10.8702 - mse: 118.1621 - mae: 8.4927 - val_loss: 8.7597 - val_rmse: 11.1347 - val_mse: 123.9809 - val_mae: 8.7597\n",
      "Epoch 26/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.4819 - rmse: 10.8609 - mse: 117.9593 - mae: 8.4819 - val_loss: 8.6783 - val_rmse: 11.0502 - val_mse: 122.1062 - val_mae: 8.6783\n",
      "Epoch 27/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.4517 - rmse: 10.8223 - mse: 117.1229 - mae: 8.4517 - val_loss: 8.6946 - val_rmse: 11.0807 - val_mse: 122.7820 - val_mae: 8.6946\n",
      "Epoch 28/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 8.4339 - rmse: 10.8037 - mse: 116.7206 - mae: 8.4339 - val_loss: 8.6993 - val_rmse: 11.0694 - val_mse: 122.5312 - val_mae: 8.6993\n",
      "Epoch 29/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.4254 - rmse: 10.7918 - mse: 116.4624 - mae: 8.4254 - val_loss: 8.6693 - val_rmse: 11.0230 - val_mse: 121.5066 - val_mae: 8.6693\n",
      "Epoch 30/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 8.4094 - rmse: 10.7783 - mse: 116.1713 - mae: 8.4094 - val_loss: 8.7045 - val_rmse: 11.0799 - val_mse: 122.7634 - val_mae: 8.7045\n",
      "Epoch 31/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 8.3912 - rmse: 10.7600 - mse: 115.7767 - mae: 8.3912 - val_loss: 8.6398 - val_rmse: 10.9990 - val_mse: 120.9780 - val_mae: 8.6398\n",
      "Epoch 32/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 8.3682 - rmse: 10.7370 - mse: 115.2840 - mae: 8.3682 - val_loss: 8.6923 - val_rmse: 11.0785 - val_mse: 122.7328 - val_mae: 8.6923rmse\n",
      "Epoch 33/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.3539 - rmse: 10.7234 - mse: 114.9919 - mae: 8.3539 - val_loss: 8.6155 - val_rmse: 10.9703 - val_mse: 120.3486 - val_mae: 8.6155\n",
      "Epoch 34/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 8.3382 - rmse: 10.7070 - mse: 114.6393 - mae: 8.3382 - val_loss: 8.6203 - val_rmse: 10.9881 - val_mse: 120.7384 - val_mae: 8.6203\n",
      "Epoch 35/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 8.3047 - rmse: 10.6706 - mse: 113.8612 - mae: 8.3047 - val_loss: 8.6398 - val_rmse: 11.0028 - val_mse: 121.0607 - val_mae: 8.6398\n",
      "Epoch 36/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 8.2997 - rmse: 10.6656 - mse: 113.7547 - mae: 8.2997 - val_loss: 8.6651 - val_rmse: 11.0214 - val_mse: 121.4718 - val_mae: 8.6651\n",
      "Epoch 37/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.3017 - rmse: 10.6680 - mse: 113.8056 - mae: 8.3017 - val_loss: 8.6219 - val_rmse: 10.9890 - val_mse: 120.7575 - val_mae: 8.6219\n",
      "Epoch 38/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.2784 - rmse: 10.6465 - mse: 113.3477 - mae: 8.2784 - val_loss: 8.5971 - val_rmse: 10.9325 - val_mse: 119.5205 - val_mae: 8.5971\n",
      "Epoch 39/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.2752 - rmse: 10.6424 - mse: 113.2611 - mae: 8.2752 - val_loss: 8.6393 - val_rmse: 10.9934 - val_mse: 120.8548 - val_mae: 8.6393\n",
      "Epoch 40/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 8.2646 - rmse: 10.6338 - mse: 113.0778 - mae: 8.2646 - val_loss: 8.6318 - val_rmse: 10.9658 - val_mse: 120.2498 - val_mae: 8.6318\n",
      "Epoch 41/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 8.2662 - rmse: 10.6292 - mse: 112.9809 - mae: 8.2662 - val_loss: 8.6389 - val_rmse: 10.9915 - val_mse: 120.8134 - val_mae: 8.6389ss: 8.2862 - rmse: 10.6445 - mse: 113.3057 - ma - ETA: 0s - loss: 8.2731 - rmse: 10.6337 - mse: 113.0758 - mae: 8.27\n",
      "Epoch 42/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 8.2362 - rmse: 10.6066 - mse: 112.5008 - mae: 8.2362 - val_loss: 8.5916 - val_rmse: 10.9274 - val_mse: 119.4076 - val_mae: 8.5916\n",
      "Epoch 43/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 8.2299 - rmse: 10.5977 - mse: 112.3103 - mae: 8.2299 - val_loss: 8.6076 - val_rmse: 10.9665 - val_mse: 120.2642 - val_mae: 8.6076\n",
      "Epoch 44/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.2298 - rmse: 10.6047 - mse: 112.4589 - mae: 8.2298 - val_loss: 8.5813 - val_rmse: 10.9243 - val_mse: 119.3407 - val_mae: 8.5813\n",
      "Epoch 45/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 8.2065 - rmse: 10.5737 - mse: 111.8038 - mae: 8.2065 - val_loss: 8.6288 - val_rmse: 10.9768 - val_mse: 120.4895 - val_mae: 8.6288\n",
      "Epoch 46/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.2054 - rmse: 10.5665 - mse: 111.6508 - mae: 8.2054 - val_loss: 8.6039 - val_rmse: 10.9469 - val_mse: 119.8342 - val_mae: 8.6039\n",
      "Epoch 47/200\n",
      "54/54 [==============================] - 14s 252ms/step - loss: 8.2064 - rmse: 10.5737 - mse: 111.8021 - mae: 8.2064 - val_loss: 8.5853 - val_rmse: 10.9337 - val_mse: 119.5451 - val_mae: 8.5853\n",
      "Epoch 48/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.1966 - rmse: 10.5568 - mse: 111.4466 - mae: 8.1966 - val_loss: 8.5480 - val_rmse: 10.8831 - val_mse: 118.4421 - val_mae: 8.5480\n",
      "Epoch 49/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 8.1853 - rmse: 10.5477 - mse: 111.2536 - mae: 8.1853 - val_loss: 8.5467 - val_rmse: 10.8988 - val_mse: 118.7836 - val_mae: 8.5467 10.492\n",
      "Epoch 50/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.1828 - rmse: 10.5527 - mse: 111.3604 - mae: 8.1828 - val_loss: 8.6006 - val_rmse: 10.9379 - val_mse: 119.6379 - val_mae: 8.6006\n",
      "Epoch 51/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.1693 - rmse: 10.5412 - mse: 111.1164 - mae: 8.1693 - val_loss: 8.5615 - val_rmse: 10.8976 - val_mse: 118.7580 - val_mae: 8.5615\n",
      "Epoch 52/200\n",
      "54/54 [==============================] - 14s 252ms/step - loss: 8.1504 - rmse: 10.5197 - mse: 110.6636 - mae: 8.1504 - val_loss: 8.5414 - val_rmse: 10.8716 - val_mse: 118.1920 - val_mae: 8.5414\n",
      "Epoch 53/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.1486 - rmse: 10.5157 - mse: 110.5792 - mae: 8.1486 - val_loss: 8.5584 - val_rmse: 10.8956 - val_mse: 118.7149 - val_mae: 8.5584\n",
      "Epoch 54/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.1353 - rmse: 10.5073 - mse: 110.4037 - mae: 8.1353 - val_loss: 8.5998 - val_rmse: 10.9491 - val_mse: 119.8819 - val_mae: 8.5998\n",
      "Epoch 55/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.1339 - rmse: 10.5032 - mse: 110.3180 - mae: 8.1339 - val_loss: 8.6009 - val_rmse: 10.9363 - val_mse: 119.6031 - val_mae: 8.6009\n",
      "Epoch 56/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 8.1243 - rmse: 10.4904 - mse: 110.0475 - mae: 8.1243 - val_loss: 8.5880 - val_rmse: 10.9299 - val_mse: 119.4618 - val_mae: 8.5880\n",
      "Epoch 57/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 8.1232 - rmse: 10.4871 - mse: 109.9782 - mae: 8.1232 - val_loss: 8.5587 - val_rmse: 10.9196 - val_mse: 119.2377 - val_mae: 8.5587\n",
      "Epoch 58/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 8.1061 - rmse: 10.4820 - mse: 109.8713 - mae: 8.1061 - val_loss: 8.5711 - val_rmse: 10.9195 - val_mse: 119.2359 - val_mae: 8.5711\n",
      "Epoch 59/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 8.1009 - rmse: 10.4753 - mse: 109.7309 - mae: 8.1009 - val_loss: 8.6131 - val_rmse: 10.9570 - val_mse: 120.0556 - val_mae: 8.6131\n",
      "Epoch 60/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 8.1110 - rmse: 10.4810 - mse: 109.8514 - mae: 8.1110 - val_loss: 8.5537 - val_rmse: 10.9170 - val_mse: 119.1816 - val_mae: 8.5537\n",
      "Epoch 61/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 8.0967 - rmse: 10.4674 - mse: 109.5673 - mae: 8.0967 - val_loss: 8.5317 - val_rmse: 10.8761 - val_mse: 118.2892 - val_mae: 8.5317\n",
      "Epoch 62/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 8.0905 - rmse: 10.4622 - mse: 109.4584 - mae: 8.0905 - val_loss: 8.5227 - val_rmse: 10.8627 - val_mse: 117.9984 - val_mae: 8.5227\n",
      "Epoch 63/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.0827 - rmse: 10.4525 - mse: 109.2548 - mae: 8.0827 - val_loss: 8.5755 - val_rmse: 10.9369 - val_mse: 119.6161 - val_mae: 8.5755\n",
      "Epoch 64/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.0638 - rmse: 10.4393 - mse: 108.9799 - mae: 8.0638 - val_loss: 8.5199 - val_rmse: 10.8573 - val_mse: 117.8801 - val_mae: 8.5199s: 8.0657 - rmse: 10.4344 - mse: 108.8768 - mae: 8\n",
      "Epoch 65/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 8.0618 - rmse: 10.4328 - mse: 108.8443 - mae: 8.0618 - val_loss: 8.5619 - val_rmse: 10.9200 - val_mse: 119.2462 - val_mae: 8.5619\n",
      "Epoch 66/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.0601 - rmse: 10.4353 - mse: 108.8951 - mae: 8.0601 - val_loss: 8.5466 - val_rmse: 10.8886 - val_mse: 118.5623 - val_mae: 8.5466\n",
      "Epoch 67/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 8.0565 - rmse: 10.4335 - mse: 108.8572 - mae: 8.0565 - val_loss: 8.5140 - val_rmse: 10.8612 - val_mse: 117.9659 - val_mae: 8.5140\n",
      "Epoch 68/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.0570 - rmse: 10.4297 - mse: 108.7791 - mae: 8.0570 - val_loss: 8.5187 - val_rmse: 10.8544 - val_mse: 117.8172 - val_mae: 8.5187\n",
      "Epoch 69/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.0404 - rmse: 10.4148 - mse: 108.4676 - mae: 8.0404 - val_loss: 8.5541 - val_rmse: 10.9152 - val_mse: 119.1411 - val_mae: 8.5541\n",
      "Epoch 70/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.0355 - rmse: 10.4139 - mse: 108.4502 - mae: 8.0355 - val_loss: 8.5292 - val_rmse: 10.8620 - val_mse: 117.9828 - val_mae: 8.5292\n",
      "Epoch 71/200\n",
      "54/54 [==============================] - 15s 272ms/step - loss: 8.0438 - rmse: 10.4192 - mse: 108.5587 - mae: 8.0438 - val_loss: 8.5422 - val_rmse: 10.8898 - val_mse: 118.5879 - val_mae: 8.5422\n",
      "Epoch 72/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 8.0142 - rmse: 10.3929 - mse: 108.0123 - mae: 8.0142 - val_loss: 8.5547 - val_rmse: 10.9167 - val_mse: 119.1745 - val_mae: 8.5547\n",
      "Epoch 73/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 8.0092 - rmse: 10.3828 - mse: 107.8031 - mae: 8.0092 - val_loss: 8.5677 - val_rmse: 10.9259 - val_mse: 119.3754 - val_mae: 8.5677\n",
      "Epoch 74/200\n",
      "54/54 [==============================] - 15s 270ms/step - loss: 8.0119 - rmse: 10.3917 - mse: 107.9868 - mae: 8.0119 - val_loss: 8.5520 - val_rmse: 10.9006 - val_mse: 118.8239 - val_mae: 8.5520\n",
      "Epoch 75/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 8.0114 - rmse: 10.3903 - mse: 107.9583 - mae: 8.0114 - val_loss: 8.5106 - val_rmse: 10.8686 - val_mse: 118.1272 - val_mae: 8.5106\n",
      "Epoch 76/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 8.0188 - rmse: 10.3998 - mse: 108.1552 - mae: 8.0188 - val_loss: 8.5815 - val_rmse: 10.9372 - val_mse: 119.6225 - val_mae: 8.5815\n",
      "Epoch 77/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 7.9931 - rmse: 10.3735 - mse: 107.6096 - mae: 7.9931 - val_loss: 8.5270 - val_rmse: 10.8795 - val_mse: 118.3640 - val_mae: 8.5270\n",
      "Epoch 78/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 7.9974 - rmse: 10.3722 - mse: 107.5817 - mae: 7.9974 - val_loss: 8.5636 - val_rmse: 10.9162 - val_mse: 119.1624 - val_mae: 8.5636\n",
      "Epoch 79/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 7.9883 - rmse: 10.3719 - mse: 107.5765 - mae: 7.9883 - val_loss: 8.5419 - val_rmse: 10.8834 - val_mse: 118.4485 - val_mae: 8.5419\n",
      "Epoch 80/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 7.9929 - rmse: 10.3658 - mse: 107.4500 - mae: 7.9929 - val_loss: 8.5099 - val_rmse: 10.8579 - val_mse: 117.8940 - val_mae: 8.5099\n",
      "Epoch 81/200\n",
      "54/54 [==============================] - 15s 271ms/step - loss: 7.9659 - rmse: 10.3458 - mse: 107.0348 - mae: 7.9659 - val_loss: 8.5346 - val_rmse: 10.8871 - val_mse: 118.5282 - val_mae: 8.5346\n",
      "Epoch 82/200\n",
      "54/54 [==============================] - 15s 275ms/step - loss: 7.9554 - rmse: 10.3377 - mse: 106.8680 - mae: 7.9554 - val_loss: 8.5174 - val_rmse: 10.8456 - val_mse: 117.6275 - val_mae: 8.5174\n",
      "Epoch 83/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 7.9638 - rmse: 10.3449 - mse: 107.0163 - mae: 7.9638 - val_loss: 8.5330 - val_rmse: 10.8762 - val_mse: 118.2911 - val_mae: 8.5330\n",
      "Epoch 84/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 7.9676 - rmse: 10.3459 - mse: 107.0385 - mae: 7.9676 - val_loss: 8.5488 - val_rmse: 10.9037 - val_mse: 118.8913 - val_mae: 8.5488\n",
      "Epoch 85/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 7.9654 - rmse: 10.3451 - mse: 107.0217 - mae: 7.9654 - val_loss: 8.4932 - val_rmse: 10.8267 - val_mse: 117.2166 - val_mae: 8.4932\n",
      "Epoch 86/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 7.9475 - rmse: 10.3313 - mse: 106.7360 - mae: 7.9475 - val_loss: 8.5750 - val_rmse: 10.9243 - val_mse: 119.3406 - val_mae: 8.5750\n",
      "Epoch 87/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 7.9414 - rmse: 10.3251 - mse: 106.6079 - mae: 7.9414 - val_loss: 8.5407 - val_rmse: 10.8914 - val_mse: 118.6227 - val_mae: 8.5407\n",
      "Epoch 88/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 7.9470 - rmse: 10.3287 - mse: 106.6828 - mae: 7.9470 - val_loss: 8.5191 - val_rmse: 10.8722 - val_mse: 118.2041 - val_mae: 8.5191\n",
      "Epoch 89/200\n",
      "54/54 [==============================] - 14s 268ms/step - loss: 7.9476 - rmse: 10.3295 - mse: 106.6984 - mae: 7.9476 - val_loss: 8.5593 - val_rmse: 10.9283 - val_mse: 119.4277 - val_mae: 8.5593\n",
      "Epoch 90/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 7.9388 - rmse: 10.3229 - mse: 106.5621 - mae: 7.9388 - val_loss: 8.5435 - val_rmse: 10.8858 - val_mse: 118.5009 - val_mae: 8.5435\n",
      "Epoch 91/200\n",
      "54/54 [==============================] - 14s 265ms/step - loss: 7.9421 - rmse: 10.3145 - mse: 106.3894 - mae: 7.9421 - val_loss: 8.5673 - val_rmse: 10.9164 - val_mse: 119.1687 - val_mae: 8.5673\n",
      "Epoch 92/200\n",
      "54/54 [==============================] - 14s 264ms/step - loss: 7.9322 - rmse: 10.3162 - mse: 106.4231 - mae: 7.9322 - val_loss: 8.5207 - val_rmse: 10.8676 - val_mse: 118.1058 - val_mae: 8.5207\n",
      "Epoch 93/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 7.9178 - rmse: 10.3075 - mse: 106.2447 - mae: 7.9178 - val_loss: 8.5105 - val_rmse: 10.8356 - val_mse: 117.4112 - val_mae: 8.5105\n",
      "Epoch 94/200\n",
      "54/54 [==============================] - 15s 273ms/step - loss: 7.9188 - rmse: 10.3062 - mse: 106.2187 - mae: 7.9188 - val_loss: 8.5674 - val_rmse: 10.9246 - val_mse: 119.3475 - val_mae: 8.5674\n",
      "Epoch 95/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 7.9163 - rmse: 10.3133 - mse: 106.3636 - mae: 7.9163 - val_loss: 8.5336 - val_rmse: 10.8880 - val_mse: 118.5495 - val_mae: 8.5336\n",
      "Epoch 96/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 7.8979 - rmse: 10.2825 - mse: 105.7288 - mae: 7.8979 - val_loss: 8.5247 - val_rmse: 10.8531 - val_mse: 117.7889 - val_mae: 8.5247\n",
      "Epoch 97/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 7.8876 - rmse: 10.2782 - mse: 105.6411 - mae: 7.8876 - val_loss: 8.5706 - val_rmse: 10.9338 - val_mse: 119.5473 - val_mae: 8.5706\n",
      "Epoch 98/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 7.8942 - rmse: 10.2832 - mse: 105.7444 - mae: 7.8942 - val_loss: 8.5715 - val_rmse: 10.9231 - val_mse: 119.3133 - val_mae: 8.5715\n",
      "Epoch 99/200\n",
      "54/54 [==============================] - 15s 271ms/step - loss: 7.8960 - rmse: 10.2799 - mse: 105.6762 - mae: 7.8960 - val_loss: 8.5608 - val_rmse: 10.9193 - val_mse: 119.2317 - val_mae: 8.5608\n",
      "Epoch 100/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 7.8947 - rmse: 10.2797 - mse: 105.6723 - mae: 7.8947 - val_loss: 8.5481 - val_rmse: 10.8841 - val_mse: 118.4643 - val_mae: 8.5481\n",
      "Epoch 101/200\n",
      "54/54 [==============================] - 14s 268ms/step - loss: 7.8819 - rmse: 10.2732 - mse: 105.5392 - mae: 7.8819 - val_loss: 8.5574 - val_rmse: 10.8932 - val_mse: 118.6618 - val_mae: 8.5574\n",
      "Epoch 102/200\n",
      "54/54 [==============================] - 15s 272ms/step - loss: 7.8820 - rmse: 10.2708 - mse: 105.4901 - mae: 7.8820 - val_loss: 8.5704 - val_rmse: 10.9160 - val_mse: 119.1587 - val_mae: 8.5704\n",
      "Epoch 103/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 7.8797 - rmse: 10.2720 - mse: 105.5138 - mae: 7.8797 - val_loss: 8.5225 - val_rmse: 10.8645 - val_mse: 118.0380 - val_mae: 8.5225\n",
      "Epoch 104/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 7.8862 - rmse: 10.2808 - mse: 105.6956 - mae: 7.8862 - val_loss: 8.5396 - val_rmse: 10.8749 - val_mse: 118.2645 - val_mae: 8.5396\n",
      "Epoch 105/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 7.8669 - rmse: 10.2591 - mse: 105.2490 - mae: 7.8669 - val_loss: 8.5480 - val_rmse: 10.8718 - val_mse: 118.1953 - val_mae: 8.5480\n",
      "Epoch 106/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 7.8569 - rmse: 10.2564 - mse: 105.1928 - mae: 7.8569 - val_loss: 8.5209 - val_rmse: 10.8511 - val_mse: 117.7468 - val_mae: 8.5209\n",
      "Epoch 107/200\n",
      "54/54 [==============================] - 15s 273ms/step - loss: 7.8675 - rmse: 10.2666 - mse: 105.4025 - mae: 7.8675 - val_loss: 8.5253 - val_rmse: 10.8703 - val_mse: 118.1637 - val_mae: 8.5253\n",
      "Epoch 108/200\n",
      "54/54 [==============================] - 15s 272ms/step - loss: 7.8628 - rmse: 10.2514 - mse: 105.0917 - mae: 7.8628 - val_loss: 8.4934 - val_rmse: 10.8472 - val_mse: 117.6615 - val_mae: 8.4934\n",
      "Epoch 109/200\n",
      "54/54 [==============================] - 15s 276ms/step - loss: 7.8574 - rmse: 10.2498 - mse: 105.0582 - mae: 7.8574 - val_loss: 8.5091 - val_rmse: 10.8585 - val_mse: 117.9078 - val_mae: 8.5091\n",
      "Epoch 110/200\n",
      "54/54 [==============================] - 15s 274ms/step - loss: 7.8490 - rmse: 10.2437 - mse: 104.9335 - mae: 7.8490 - val_loss: 8.5421 - val_rmse: 10.8779 - val_mse: 118.3276 - val_mae: 8.5421\n",
      "Epoch 111/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 7.8261 - rmse: 10.2233 - mse: 104.5167 - mae: 7.8261 - val_loss: 8.5518 - val_rmse: 10.8908 - val_mse: 118.6085 - val_mae: 8.5518\n",
      "Epoch 112/200\n",
      "54/54 [==============================] - 14s 268ms/step - loss: 7.8445 - rmse: 10.2361 - mse: 104.7781 - mae: 7.8445 - val_loss: 8.5185 - val_rmse: 10.8773 - val_mse: 118.3160 - val_mae: 8.5185\n",
      "Epoch 113/200\n",
      "54/54 [==============================] - 15s 273ms/step - loss: 7.8359 - rmse: 10.2322 - mse: 104.6976 - mae: 7.8359 - val_loss: 8.5337 - val_rmse: 10.8803 - val_mse: 118.3815 - val_mae: 8.5337\n",
      "Epoch 114/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 7.8332 - rmse: 10.2304 - mse: 104.6613 - mae: 7.8332 - val_loss: 8.5120 - val_rmse: 10.8350 - val_mse: 117.3980 - val_mae: 8.5120\n",
      "Epoch 115/200\n",
      "54/54 [==============================] - 15s 273ms/step - loss: 7.8341 - rmse: 10.2325 - mse: 104.7044 - mae: 7.8341 - val_loss: 8.5306 - val_rmse: 10.8637 - val_mse: 118.0203 - val_mae: 8.5306\n",
      "Epoch 116/200\n",
      "54/54 [==============================] - 14s 264ms/step - loss: 7.8096 - rmse: 10.2141 - mse: 104.3288 - mae: 7.8096 - val_loss: 8.5301 - val_rmse: 10.8655 - val_mse: 118.0592 - val_mae: 8.5301\n",
      "Epoch 117/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 7.8269 - rmse: 10.2290 - mse: 104.6317 - mae: 7.8269 - val_loss: 8.5191 - val_rmse: 10.8571 - val_mse: 117.8762 - val_mae: 8.5191\n",
      "Epoch 118/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 7.8164 - rmse: 10.2175 - mse: 104.3969 - mae: 7.8164 - val_loss: 8.5455 - val_rmse: 10.8914 - val_mse: 118.6237 - val_mae: 8.5455\n",
      "Epoch 119/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 7.8118 - rmse: 10.2106 - mse: 104.2566 - mae: 7.8118 - val_loss: 8.5561 - val_rmse: 10.9099 - val_mse: 119.0251 - val_mae: 8.5561\n",
      "Epoch 120/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 7.8013 - rmse: 10.1992 - mse: 104.0229 - mae: 7.8013 - val_loss: 8.5097 - val_rmse: 10.8489 - val_mse: 117.6990 - val_mae: 8.5097\n",
      "Epoch 121/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 7.8041 - rmse: 10.2162 - mse: 104.3708 - mae: 7.8041 - val_loss: 8.5337 - val_rmse: 10.8767 - val_mse: 118.3027 - val_mae: 8.5337\n",
      "Epoch 122/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 7.7925 - rmse: 10.1806 - mse: 103.6452 - mae: 7.7925 - val_loss: 8.5079 - val_rmse: 10.8540 - val_mse: 117.8088 - val_mae: 8.5079\n",
      "Epoch 123/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 7.7974 - rmse: 10.2008 - mse: 104.0566 - mae: 7.7974 - val_loss: 8.5256 - val_rmse: 10.8825 - val_mse: 118.4285 - val_mae: 8.5256\n",
      "Epoch 124/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 7.7953 - rmse: 10.1979 - mse: 103.9964 - mae: 7.7953 - val_loss: 8.4900 - val_rmse: 10.8322 - val_mse: 117.3372 - val_mae: 8.4900\n",
      "Epoch 125/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 7.7802 - rmse: 10.1822 - mse: 103.6778 - mae: 7.7802 - val_loss: 8.5504 - val_rmse: 10.9031 - val_mse: 118.8781 - val_mae: 8.5504\n",
      "Epoch 126/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 7.7856 - rmse: 10.1826 - mse: 103.6855 - mae: 7.7856 - val_loss: 8.5350 - val_rmse: 10.8834 - val_mse: 118.4481 - val_mae: 8.5350\n",
      "Epoch 127/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 7.7775 - rmse: 10.1714 - mse: 103.4572 - mae: 7.7775 - val_loss: 8.5603 - val_rmse: 10.8954 - val_mse: 118.7101 - val_mae: 8.5603\n",
      "Epoch 128/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 7.7680 - rmse: 10.1722 - mse: 103.4736 - mae: 7.7680 - val_loss: 8.5316 - val_rmse: 10.8818 - val_mse: 118.4134 - val_mae: 8.5316\n",
      "Epoch 129/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 7.7745 - rmse: 10.1850 - mse: 103.7346 - mae: 7.7745 - val_loss: 8.4769 - val_rmse: 10.8309 - val_mse: 117.3092 - val_mae: 8.4769\n",
      "Epoch 130/200\n",
      "54/54 [==============================] - 15s 274ms/step - loss: 7.7550 - rmse: 10.1630 - mse: 103.2869 - mae: 7.7550 - val_loss: 8.5529 - val_rmse: 10.9050 - val_mse: 118.9195 - val_mae: 8.5529\n",
      "Epoch 131/200\n",
      "54/54 [==============================] - 15s 271ms/step - loss: 7.7780 - rmse: 10.1846 - mse: 103.7257 - mae: 7.7780 - val_loss: 8.5714 - val_rmse: 10.9308 - val_mse: 119.4814 - val_mae: 8.5714\n",
      "Epoch 132/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 7.7461 - rmse: 10.1544 - mse: 103.1124 - mae: 7.7461 - val_loss: 8.5174 - val_rmse: 10.8625 - val_mse: 117.9943 - val_mae: 8.5174\n",
      "Epoch 133/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 7.7608 - rmse: 10.1693 - mse: 103.4145 - mae: 7.7608 - val_loss: 8.5118 - val_rmse: 10.8518 - val_mse: 117.7609 - val_mae: 8.5118\n",
      "Epoch 134/200\n",
      "54/54 [==============================] - 15s 274ms/step - loss: 7.7836 - rmse: 10.1866 - mse: 103.7666 - mae: 7.7836 - val_loss: 8.5361 - val_rmse: 10.8776 - val_mse: 118.3226 - val_mae: 8.5361\n",
      "Epoch 135/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 7.7504 - rmse: 10.1596 - mse: 103.2179 - mae: 7.7504 - val_loss: 8.5188 - val_rmse: 10.8790 - val_mse: 118.3523 - val_mae: 8.5188\n",
      "Epoch 136/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 7.7604 - rmse: 10.1674 - mse: 103.3767 - mae: 7.7604 - val_loss: 8.5091 - val_rmse: 10.8596 - val_mse: 117.9299 - val_mae: 8.5091\n",
      "Epoch 137/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 7.7414 - rmse: 10.1579 - mse: 103.1825 - mae: 7.7414 - val_loss: 8.5668 - val_rmse: 10.9186 - val_mse: 119.2150 - val_mae: 8.5668\n",
      "Epoch 138/200\n",
      "54/54 [==============================] - 15s 270ms/step - loss: 7.7273 - rmse: 10.1456 - mse: 102.9332 - mae: 7.7273 - val_loss: 8.5298 - val_rmse: 10.8764 - val_mse: 118.2968 - val_mae: 8.5298\n",
      "Epoch 139/200\n",
      "54/54 [==============================] - 14s 265ms/step - loss: 7.7392 - rmse: 10.1515 - mse: 103.0538 - mae: 7.7392 - val_loss: 8.5174 - val_rmse: 10.8600 - val_mse: 117.9401 - val_mae: 8.5174\n",
      "Epoch 140/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 7.7361 - rmse: 10.1445 - mse: 102.9104 - mae: 7.7361 - val_loss: 8.5710 - val_rmse: 10.9448 - val_mse: 119.7882 - val_mae: 8.5710\n",
      "Epoch 141/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 7.7443 - rmse: 10.1548 - mse: 103.1197 - mae: 7.7443 - val_loss: 8.5707 - val_rmse: 10.9303 - val_mse: 119.4725 - val_mae: 8.5707\n",
      "Epoch 142/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 7.7340 - rmse: 10.1504 - mse: 103.0307 - mae: 7.7340 - val_loss: 8.5743 - val_rmse: 10.9372 - val_mse: 119.6220 - val_mae: 8.5743\n",
      "Epoch 143/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 7.7218 - rmse: 10.1293 - mse: 102.6030 - mae: 7.7218 - val_loss: 8.5564 - val_rmse: 10.8872 - val_mse: 118.5305 - val_mae: 8.5564\n",
      "Epoch 144/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 7.7349 - rmse: 10.1434 - mse: 102.8896 - mae: 7.7349 - val_loss: 8.5515 - val_rmse: 10.9067 - val_mse: 118.9557 - val_mae: 8.5515\n",
      "Epoch 145/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 7.7348 - rmse: 10.1459 - mse: 102.9401 - mae: 7.7348 - val_loss: 8.5099 - val_rmse: 10.8608 - val_mse: 117.9561 - val_mae: 8.5099\n",
      "Epoch 146/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 7.7298 - rmse: 10.1512 - mse: 103.0472 - mae: 7.7298 - val_loss: 8.5598 - val_rmse: 10.9175 - val_mse: 119.1927 - val_mae: 8.5598\n",
      "Epoch 147/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 7.7096 - rmse: 10.1232 - mse: 102.4796 - mae: 7.7096 - val_loss: 8.5526 - val_rmse: 10.9153 - val_mse: 119.1428 - val_mae: 8.5526\n",
      "Epoch 148/200\n",
      "54/54 [==============================] - 14s 264ms/step - loss: 7.7119 - rmse: 10.1231 - mse: 102.4771 - mae: 7.7119 - val_loss: 8.5201 - val_rmse: 10.8724 - val_mse: 118.2098 - val_mae: 8.5201\n",
      "Epoch 149/200\n",
      "54/54 [==============================] - 15s 276ms/step - loss: 7.7179 - rmse: 10.1373 - mse: 102.7650 - mae: 7.7179 - val_loss: 8.5632 - val_rmse: 10.9049 - val_mse: 118.9174 - val_mae: 8.5632\n",
      "Epoch 150/200\n",
      "54/54 [==============================] - 15s 275ms/step - loss: 7.7126 - rmse: 10.1280 - mse: 102.5770 - mae: 7.7126 - val_loss: 8.5708 - val_rmse: 10.9262 - val_mse: 119.3816 - val_mae: 8.5708\n",
      "Epoch 151/200\n",
      "54/54 [==============================] - 15s 275ms/step - loss: 7.6936 - rmse: 10.1033 - mse: 102.0776 - mae: 7.6936 - val_loss: 8.5234 - val_rmse: 10.8710 - val_mse: 118.1796 - val_mae: 8.5234\n",
      "Epoch 152/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 7.7014 - rmse: 10.1179 - mse: 102.3728 - mae: 7.7014 - val_loss: 8.5456 - val_rmse: 10.8812 - val_mse: 118.3997 - val_mae: 8.5456\n",
      "Epoch 153/200\n",
      "54/54 [==============================] - 14s 268ms/step - loss: 7.6993 - rmse: 10.1203 - mse: 102.4214 - mae: 7.6993 - val_loss: 8.5086 - val_rmse: 10.8369 - val_mse: 117.4374 - val_mae: 8.5086\n",
      "Epoch 154/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 7.6936 - rmse: 10.1077 - mse: 102.1658 - mae: 7.6936 - val_loss: 8.5694 - val_rmse: 10.9139 - val_mse: 119.1125 - val_mae: 8.5694\n",
      "Epoch 155/200\n",
      "54/54 [==============================] - 14s 264ms/step - loss: 7.6881 - rmse: 10.1162 - mse: 102.3376 - mae: 7.6881 - val_loss: 8.5650 - val_rmse: 10.9055 - val_mse: 118.9298 - val_mae: 8.5650\n",
      "Epoch 156/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 7.6780 - rmse: 10.0988 - mse: 101.9866 - mae: 7.6780 - val_loss: 8.5198 - val_rmse: 10.8551 - val_mse: 117.8339 - val_mae: 8.5198\n",
      "Epoch 157/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 7.6761 - rmse: 10.0962 - mse: 101.9341 - mae: 7.6761 - val_loss: 8.5461 - val_rmse: 10.8989 - val_mse: 118.7859 - val_mae: 8.5461\n",
      "Epoch 158/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 7.6820 - rmse: 10.0974 - mse: 101.9565 - mae: 7.6820 - val_loss: 8.4956 - val_rmse: 10.8637 - val_mse: 118.0210 - val_mae: 8.4956\n",
      "Epoch 159/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 7.6803 - rmse: 10.1050 - mse: 102.1112 - mae: 7.6803 - val_loss: 8.5416 - val_rmse: 10.9074 - val_mse: 118.9710 - val_mae: 8.5416\n",
      "Epoch 160/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 7.6594 - rmse: 10.0914 - mse: 101.8365 - mae: 7.6594 - val_loss: 8.5016 - val_rmse: 10.8427 - val_mse: 117.5640 - val_mae: 8.5016\n",
      "Epoch 161/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 7.6690 - rmse: 10.0944 - mse: 101.8964 - mae: 7.6690 - val_loss: 8.5365 - val_rmse: 10.8876 - val_mse: 118.5393 - val_mae: 8.5365\n",
      "Epoch 162/200\n",
      "54/54 [==============================] - 14s 265ms/step - loss: 7.6595 - rmse: 10.0770 - mse: 101.5467 - mae: 7.6595 - val_loss: 8.5417 - val_rmse: 10.8980 - val_mse: 118.7667 - val_mae: 8.5417\n",
      "Epoch 163/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 7.6718 - rmse: 10.0912 - mse: 101.8322 - mae: 7.6718 - val_loss: 8.5260 - val_rmse: 10.8559 - val_mse: 117.8503 - val_mae: 8.5260\n",
      "Epoch 164/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 7.6478 - rmse: 10.0767 - mse: 101.5389 - mae: 7.6478 - val_loss: 8.5106 - val_rmse: 10.8384 - val_mse: 117.4716 - val_mae: 8.5106\n",
      "Epoch 165/200\n",
      "54/54 [==============================] - 15s 280ms/step - loss: 7.6660 - rmse: 10.0896 - mse: 101.8002 - mae: 7.6660 - val_loss: 8.5474 - val_rmse: 10.8899 - val_mse: 118.5897 - val_mae: 8.5474\n",
      "Epoch 166/200\n",
      "54/54 [==============================] - 14s 268ms/step - loss: 7.6621 - rmse: 10.0855 - mse: 101.7166 - mae: 7.6621 - val_loss: 8.5210 - val_rmse: 10.8430 - val_mse: 117.5715 - val_mae: 8.5210\n",
      "Epoch 167/200\n",
      "54/54 [==============================] - 15s 274ms/step - loss: 7.6540 - rmse: 10.0774 - mse: 101.5546 - mae: 7.6540 - val_loss: 8.5533 - val_rmse: 10.8796 - val_mse: 118.3663 - val_mae: 8.5533\n",
      "Epoch 168/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 7.6423 - rmse: 10.0762 - mse: 101.5291 - mae: 7.6423 - val_loss: 8.5889 - val_rmse: 10.9440 - val_mse: 119.7720 - val_mae: 8.5889\n",
      "Epoch 169/200\n",
      "54/54 [==============================] - 15s 272ms/step - loss: 7.6405 - rmse: 10.0625 - mse: 101.2541 - mae: 7.6405 - val_loss: 8.5441 - val_rmse: 10.8910 - val_mse: 118.6128 - val_mae: 8.5441\n",
      "Epoch 170/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 7.6410 - rmse: 10.0608 - mse: 101.2205 - mae: 7.6410 - val_loss: 8.5514 - val_rmse: 10.9001 - val_mse: 118.8115 - val_mae: 8.5514\n",
      "Epoch 171/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 7.6428 - rmse: 10.0609 - mse: 101.2216 - mae: 7.6428 - val_loss: 8.5123 - val_rmse: 10.8472 - val_mse: 117.6625 - val_mae: 8.5123\n",
      "Epoch 172/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 7.6431 - rmse: 10.0619 - mse: 101.2414 - mae: 7.6431 - val_loss: 8.5455 - val_rmse: 10.8959 - val_mse: 118.7199 - val_mae: 8.5455\n",
      "Epoch 173/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 7.6211 - rmse: 10.0614 - mse: 101.2311 - mae: 7.6211 - val_loss: 8.5897 - val_rmse: 10.9457 - val_mse: 119.8089 - val_mae: 8.5897\n",
      "Epoch 174/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 7.6304 - rmse: 10.0590 - mse: 101.1833 - mae: 7.6304 - val_loss: 8.5640 - val_rmse: 10.9054 - val_mse: 118.9270 - val_mae: 8.5640\n",
      "Epoch 175/200\n",
      "54/54 [==============================] - 14s 268ms/step - loss: 7.6431 - rmse: 10.0738 - mse: 101.4822 - mae: 7.6431 - val_loss: 8.5602 - val_rmse: 10.9035 - val_mse: 118.8869 - val_mae: 8.5602\n",
      "Epoch 176/200\n",
      "54/54 [==============================] - 14s 268ms/step - loss: 7.6343 - rmse: 10.0584 - mse: 101.1708 - mae: 7.6343 - val_loss: 8.5413 - val_rmse: 10.8846 - val_mse: 118.4735 - val_mae: 8.5413\n",
      "Epoch 177/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 7.6245 - rmse: 10.0538 - mse: 101.0789 - mae: 7.6245 - val_loss: 8.5585 - val_rmse: 10.9070 - val_mse: 118.9627 - val_mae: 8.5585\n",
      "Epoch 178/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 7.6190 - rmse: 10.0513 - mse: 101.0279 - mae: 7.6190 - val_loss: 8.5710 - val_rmse: 10.9175 - val_mse: 119.1920 - val_mae: 8.5710\n",
      "Epoch 179/200\n",
      "54/54 [==============================] - 14s 265ms/step - loss: 7.6105 - rmse: 10.0400 - mse: 100.8014 - mae: 7.6105 - val_loss: 8.5874 - val_rmse: 10.9549 - val_mse: 120.0100 - val_mae: 8.5874\n",
      "Epoch 180/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 7.6097 - rmse: 10.0377 - mse: 100.7553 - mae: 7.6097 - val_loss: 8.5886 - val_rmse: 10.9360 - val_mse: 119.5971 - val_mae: 8.5886\n",
      "Epoch 181/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 7.6239 - rmse: 10.0554 - mse: 101.1104 - mae: 7.6239 - val_loss: 8.5310 - val_rmse: 10.8855 - val_mse: 118.4934 - val_mae: 8.5310\n",
      "Epoch 182/200\n",
      "54/54 [==============================] - 16s 302ms/step - loss: 7.5923 - rmse: 10.0185 - mse: 100.3705 - mae: 7.5923 - val_loss: 8.5425 - val_rmse: 10.9016 - val_mse: 118.8451 - val_mae: 8.5425\n",
      "Epoch 183/200\n",
      "54/54 [==============================] - 15s 270ms/step - loss: 7.6009 - rmse: 10.0316 - mse: 100.6330 - mae: 7.6009 - val_loss: 8.5822 - val_rmse: 10.9389 - val_mse: 119.6590 - val_mae: 8.5822\n",
      "Epoch 184/200\n",
      "54/54 [==============================] - 15s 277ms/step - loss: 7.5934 - rmse: 10.0252 - mse: 100.5050 - mae: 7.5934 - val_loss: 8.5249 - val_rmse: 10.8647 - val_mse: 118.0419 - val_mae: 8.5249\n",
      "Epoch 185/200\n",
      "54/54 [==============================] - 15s 279ms/step - loss: 7.5899 - rmse: 10.0272 - mse: 100.5452 - mae: 7.5899 - val_loss: 8.5336 - val_rmse: 10.8726 - val_mse: 118.2128 - val_mae: 8.5336\n",
      "Epoch 186/200\n",
      "54/54 [==============================] - 15s 284ms/step - loss: 7.5979 - rmse: 10.0319 - mse: 100.6396 - mae: 7.5979 - val_loss: 8.5642 - val_rmse: 10.9188 - val_mse: 119.2196 - val_mae: 8.5642\n",
      "Epoch 187/200\n",
      "54/54 [==============================] - 15s 285ms/step - loss: 7.5979 - rmse: 10.0291 - mse: 100.5836 - mae: 7.5979 - val_loss: 8.5560 - val_rmse: 10.9081 - val_mse: 118.9863 - val_mae: 8.5560\n",
      "Epoch 188/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 7.5973 - rmse: 10.0376 - mse: 100.7528 - mae: 7.5973 - val_loss: 8.5577 - val_rmse: 10.9267 - val_mse: 119.3937 - val_mae: 8.5577\n",
      "Epoch 189/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 7.5959 - rmse: 10.0331 - mse: 100.6634 - mae: 7.5959 - val_loss: 8.5385 - val_rmse: 10.8738 - val_mse: 118.2404 - val_mae: 8.5385\n",
      "Epoch 190/200\n",
      "54/54 [==============================] - 15s 275ms/step - loss: 7.5706 - rmse: 10.0128 - mse: 100.2561 - mae: 7.5706 - val_loss: 8.5080 - val_rmse: 10.8509 - val_mse: 117.7419 - val_mae: 8.5080\n",
      "Epoch 191/200\n",
      "54/54 [==============================] - 15s 271ms/step - loss: 7.5832 - rmse: 10.0159 - mse: 100.3188 - mae: 7.5832 - val_loss: 8.5847 - val_rmse: 10.9527 - val_mse: 119.9616 - val_mae: 8.5847\n",
      "Epoch 192/200\n",
      "54/54 [==============================] - 15s 273ms/step - loss: 7.5714 - rmse: 10.0144 - mse: 100.2883 - mae: 7.5714 - val_loss: 8.5408 - val_rmse: 10.8669 - val_mse: 118.0889 - val_mae: 8.5408\n",
      "Epoch 193/200\n",
      "54/54 [==============================] - 15s 279ms/step - loss: 7.5709 - rmse: 10.0070 - mse: 100.1393 - mae: 7.5709 - val_loss: 8.6058 - val_rmse: 10.9550 - val_mse: 120.0112 - val_mae: 8.6058\n",
      "Epoch 194/200\n",
      "54/54 [==============================] - 15s 279ms/step - loss: 7.5848 - rmse: 10.0223 - mse: 100.4467 - mae: 7.5848 - val_loss: 8.5579 - val_rmse: 10.8912 - val_mse: 118.6182 - val_mae: 8.5579\n",
      "Epoch 195/200\n",
      "54/54 [==============================] - 15s 276ms/step - loss: 7.5642 - rmse: 9.9961 - mse: 99.9227 - mae: 7.5642 - val_loss: 8.5712 - val_rmse: 10.9184 - val_mse: 119.2104 - val_mae: 8.5712\n",
      "Epoch 196/200\n",
      "54/54 [==============================] - 15s 273ms/step - loss: 7.5700 - rmse: 10.0129 - mse: 100.2578 - mae: 7.5700 - val_loss: 8.5234 - val_rmse: 10.8810 - val_mse: 118.3965 - val_mae: 8.5234\n",
      "Epoch 197/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 7.5716 - rmse: 10.0155 - mse: 100.3094 - mae: 7.5716 - val_loss: 8.5665 - val_rmse: 10.9278 - val_mse: 119.4173 - val_mae: 8.5665\n",
      "Epoch 198/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 7.5646 - rmse: 10.0074 - mse: 100.1473 - mae: 7.5646 - val_loss: 8.5555 - val_rmse: 10.9127 - val_mse: 119.0874 - val_mae: 8.5555\n",
      "Epoch 199/200\n",
      "54/54 [==============================] - 15s 276ms/step - loss: 7.5617 - rmse: 10.0058 - mse: 100.1158 - mae: 7.5617 - val_loss: 8.5717 - val_rmse: 10.9015 - val_mse: 118.8432 - val_mae: 8.5717\n",
      "Epoch 200/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 7.5400 - rmse: 9.9828 - mse: 99.6569 - mae: 7.5400 - val_loss: 8.5455 - val_rmse: 10.9271 - val_mse: 119.4018 - val_mae: 8.5455\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    hist = model.fit(ds_train,\n",
    "                     validation_data=ds_val,\n",
    "                     callbacks=callbacks,\n",
    "                     epochs=200,\n",
    "                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 3s 203ms/step - loss: 8.4823 - rmse: 10.8302 - mse: 117.2942 - mae: 8.4823\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    results = model.evaluate(ds_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Metric')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(16,16)\n",
    "\n",
    "    ax=fig.add_subplot(3,2,1)\n",
    "    ax.plot(hist.history['rmse'])\n",
    "    ax.plot(hist.history['mse'])\n",
    "    ax.legend(['Metric', 'Loss'])\n",
    "    ax.set_title('Train')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,2)\n",
    "    ax.plot(hist.history['val_rmse'])\n",
    "    ax.plot(hist.history['val_mse'])\n",
    "    ax.legend(['Metric', 'Loss'])\n",
    "    ax.set_title('Test')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,3)\n",
    "    ax.plot(hist.history['loss'])\n",
    "    ax.plot(hist.history['val_loss'])\n",
    "    ax.legend(['Train', 'Test'])\n",
    "    ax.set_title('Loss')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,4)\n",
    "    ax.plot(hist.history['mse'])\n",
    "    ax.plot(hist.history['val_mse'])\n",
    "    ax.legend(['Train', 'Test'])\n",
    "    ax.set_title('Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test = pd.read_csv('00_Data/fnc.csv')\n",
    "X1_test = X1_test[X1_test['Id'].isin(TEST_IDS)]\n",
    "X1_test = X1_test.to_numpy()\n",
    "X1_test = X1_test[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_test = pd.read_csv('00_Data/loading.csv')\n",
    "X2_test = X2_test[X2_test['Id'].isin(TEST_IDS)]\n",
    "X2_test = X2_test.to_numpy()\n",
    "X2_test = X2_test[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict([X1_test, X2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = y_preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = []\n",
    "i = 0\n",
    "for idx in TEST_IDS:\n",
    "    df_submission.append(['{0}_age'.format(idx), y_preds[i]])\n",
    "    df_submission.append(['{0}_domain1_var1'.format(idx), y_preds[i+1]])\n",
    "    df_submission.append(['{0}_domain1_var2'.format(idx), y_preds[i+2]])\n",
    "    df_submission.append(['{0}_domain2_var1'.format(idx), y_preds[i+3]])\n",
    "    df_submission.append(['{0}_domain2_var2'.format(idx), y_preds[i+4]])\n",
    "    i += 5\n",
    "\n",
    "df_submission = pd.DataFrame(df_submission, columns=['Id', 'Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('submission_fnc-load_mae_05.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>age</th>\n",
       "      <th>domain1_var1</th>\n",
       "      <th>domain1_var2</th>\n",
       "      <th>domain2_var1</th>\n",
       "      <th>domain2_var2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>57.436077</td>\n",
       "      <td>30.571975</td>\n",
       "      <td>62.553736</td>\n",
       "      <td>53.325130</td>\n",
       "      <td>51.427998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>59.580851</td>\n",
       "      <td>50.969456</td>\n",
       "      <td>67.470628</td>\n",
       "      <td>60.651856</td>\n",
       "      <td>58.311361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10004</td>\n",
       "      <td>71.413018</td>\n",
       "      <td>53.152498</td>\n",
       "      <td>58.012103</td>\n",
       "      <td>52.418389</td>\n",
       "      <td>62.536641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10005</td>\n",
       "      <td>66.532630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.108977</td>\n",
       "      <td>69.993075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10007</td>\n",
       "      <td>38.617381</td>\n",
       "      <td>49.197021</td>\n",
       "      <td>65.674285</td>\n",
       "      <td>40.151376</td>\n",
       "      <td>34.096421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>21746</td>\n",
       "      <td>14.257265</td>\n",
       "      <td>21.358872</td>\n",
       "      <td>61.165998</td>\n",
       "      <td>51.778483</td>\n",
       "      <td>54.640179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>21747</td>\n",
       "      <td>55.456978</td>\n",
       "      <td>68.169675</td>\n",
       "      <td>29.907995</td>\n",
       "      <td>55.349257</td>\n",
       "      <td>54.019517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>21750</td>\n",
       "      <td>48.948756</td>\n",
       "      <td>55.114811</td>\n",
       "      <td>60.878271</td>\n",
       "      <td>38.617246</td>\n",
       "      <td>50.679885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>21752</td>\n",
       "      <td>66.532630</td>\n",
       "      <td>59.844808</td>\n",
       "      <td>72.303110</td>\n",
       "      <td>55.458281</td>\n",
       "      <td>46.870235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>21754</td>\n",
       "      <td>68.820928</td>\n",
       "      <td>56.594193</td>\n",
       "      <td>34.605868</td>\n",
       "      <td>49.922535</td>\n",
       "      <td>50.383078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5877 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id        age  domain1_var1  domain1_var2  domain2_var1  domain2_var2\n",
       "0     10001  57.436077     30.571975     62.553736     53.325130     51.427998\n",
       "1     10002  59.580851     50.969456     67.470628     60.651856     58.311361\n",
       "2     10004  71.413018     53.152498     58.012103     52.418389     62.536641\n",
       "3     10005  66.532630           NaN           NaN     52.108977     69.993075\n",
       "4     10007  38.617381     49.197021     65.674285     40.151376     34.096421\n",
       "...     ...        ...           ...           ...           ...           ...\n",
       "5872  21746  14.257265     21.358872     61.165998     51.778483     54.640179\n",
       "5873  21747  55.456978     68.169675     29.907995     55.349257     54.019517\n",
       "5874  21750  48.948756     55.114811     60.878271     38.617246     50.679885\n",
       "5875  21752  66.532630     59.844808     72.303110     55.458281     46.870235\n",
       "5876  21754  68.820928     56.594193     34.605868     49.922535     50.383078\n",
       "\n",
       "[5877 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('00_Data/train_scores.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nulls = data[data.isnull().any(axis=1)]\n",
    "NULL_IDS = list(data_nulls['Id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
