{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing as prep\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn import cluster\n",
    "from sklearn import mixture\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import h5py\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11754 510 11754\n"
     ]
    }
   ],
   "source": [
    "print(len(ALL_IDS), len(REVEAL_IDS_S2), len(NOREVEAL_IDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-10ffc12f77d3>:8: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  NOREVEAL_IDS = [i for i in ALL_IDS if i not in REVEAL_IDS_S2]\n"
     ]
    }
   ],
   "source": [
    "# TEST_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_test'))]\n",
    "# TRAIN_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_train'))]\n",
    "# ALL_IDS = TRAIN_IDS + TEST_IDS\n",
    "# # REVEAL_IDS_S2 = list(pd.read_csv('00_Data/reveal_ID_site2.csv', header=0, squeeze=True).values)\n",
    "# REVEAL_IDS_S2 = np.genfromtxt('00_Data/reveal_ID_site2.csv', delimiter='\\n')\n",
    "# REVEAL_IDS_S2 = REVEAL_IDS_S2[1:]\n",
    "# REVEAL_IDS_S2 = REVEAL_IDS_S2.astype(int)\n",
    "# NOREVEAL_IDS = [i for i in ALL_IDS if i not in REVEAL_IDS_S2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_test'))]\n",
    "TRAIN_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_train'))]\n",
    "ALL_IDS = TRAIN_IDS + TEST_IDS\n",
    "REVEAL_IDS_S2 = pd.read_csv('00_Data/reveal_ID_site2.csv', header=None, squeeze=True, dtype=str)\n",
    "# REVEAL_IDS_S2 = pd.read_csv('00_Data/reveal_ID_site2.csv', header=None, squeeze=True, dtype=np.int64)\n",
    "NOREVEAL_IDS = [i for i in ALL_IDS if i not in REVEAL_IDS_S2.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11244"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NOREVEAL_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('00_Data/train_scores_full.csv')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(idx):\n",
    "    #MRI inputs\n",
    "    patient_SM = h5py.File('00_Data/fMRI_all/{0}.mat'.format(idx), mode='r')\n",
    "    patient_SM = np.array(patient_SM.get('SM_feature'))\n",
    "#     print(patient_SM.shape)\n",
    "    k = 1\n",
    "    ki_padding = 3\n",
    "    \n",
    "    arr_regions = []\n",
    "    for i in range(patient_SM.shape[0]):\n",
    "        sample_map = patient_SM[i,:,:,:]\n",
    "        if k > 1:\n",
    "            map_shape = sample_map.shape\n",
    "            shape_pad = ((map_shape[0]//k + 1)*k - map_shape[0],\n",
    "                         (map_shape[1]//k + 1)*k - map_shape[1],\n",
    "                         (map_shape[2]//k + 1)*k - map_shape[2])\n",
    "\n",
    "            npad = (((0 if shape_pad[0]%2==0 else shape_pad[0]//2), (shape_pad[0]//2 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "                    ((0 if shape_pad[1]%2==0 else shape_pad[0]//2), (shape_pad[1]//2 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "                    ((0 if shape_pad[2]%2==0 else shape_pad[0]//2), (shape_pad[2]//2 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "            sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "            sx = sample_map_padded.shape[0] / k\n",
    "            sy = sample_map_padded.shape[1] / k\n",
    "            sz = sample_map_padded.shape[2] / k\n",
    "            for kz in range(k):\n",
    "                for ky in range(k):\n",
    "                    for kx in range(k):\n",
    "                        ki_region = sample_map_padded[int(kx*sx): int(kx*sx + sx - 1), \n",
    "                                                     int(ky*sy): int(ky*sy + sy - 1), \n",
    "                                                     int(kz*sz): int(kz*sz + sz - 1)]\n",
    "                        #padding i-th region by 3 pixels\n",
    "                        ki_region_padded = np.pad(ki_region, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "                        arr_regions.append(ki_region_padded)\n",
    "        else:\n",
    "            map_shape = sample_map.shape\n",
    "            shape_pad = ((map_shape[0]//2 + 1)*2 - map_shape[0],\n",
    "                         (map_shape[1]//2 + 1)*2 - map_shape[1],\n",
    "                         (map_shape[2]//2 + 1)*2 - map_shape[2])\n",
    "\n",
    "            npad = (((0 if shape_pad[0]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "                    ((0 if shape_pad[1]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "                    ((0 if shape_pad[2]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "            sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "            \n",
    "#             sample_map_padded = np.pad(sample_map, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "            arr_regions.append(sample_map_padded)\n",
    "            \n",
    "    X_mri = np.stack(arr_regions, axis=3)\n",
    "#     print(X_mri.shape)\n",
    "    return X_mri, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_inputs('10002')[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_py_function(func, inp, Tout, name=None):\n",
    "    \n",
    "    def wrapped_func(*flat_inp):\n",
    "        reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp,\n",
    "                                                     expand_composites=True)\n",
    "        out = func(*reconstructed_inp)\n",
    "        return tf.nest.flatten(out, expand_composites=True)\n",
    "    \n",
    "    flat_Tout = tf.nest.flatten(Tout, expand_composites=True)\n",
    "    flat_out = tf.py_function(func=wrapped_func, \n",
    "                              inp=tf.nest.flatten(inp, expand_composites=True),\n",
    "                              Tout=[_tensor_spec_to_dtype(v) for v in flat_Tout],\n",
    "                              name=name)\n",
    "    spec_out = tf.nest.map_structure(_dtype_to_tensor_spec, Tout, expand_composites=True)\n",
    "    out = tf.nest.pack_sequence_as(spec_out, flat_out, expand_composites=True)\n",
    "    return out\n",
    "\n",
    "def _dtype_to_tensor_spec(v):\n",
    "    return tf.TensorSpec(None, v) if isinstance(v, tf.dtypes.DType) else v\n",
    "\n",
    "def _tensor_spec_to_dtype(v):\n",
    "    return v.dtype if isinstance(v, tf.TensorSpec) else v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data, batch_size):\n",
    "    data = np.array([int(i) for i in data])\n",
    "    data = tf.data.Dataset.from_tensor_slices(data)\n",
    "    data = data.shuffle(buffer_size=12000, seed=30, reshuffle_each_iteration=True)\n",
    "    \n",
    "    data = data.map(lambda idx: new_py_function(get_inputs, inp=[idx], \n",
    "                                                    Tout=(tf.TensorSpec(shape=(None, 52, 66, 56, 53), dtype=tf.dtypes.float64),\n",
    "                                                          tf.int32), \n",
    "                                                name=None), \n",
    "                     num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "                     deterministic=False)\n",
    "    data = data.batch(batch_size, drop_remainder=False)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "ds_train = get_dataset(ALL_IDS, batch_size)\n",
    "ds_reveal_s2 = get_dataset(REVEAL_IDS_S2, batch_size)\n",
    "ds_noreveal = get_dataset(NOREVEAL_IDS, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ds_train.take(1):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE_mri = (52, 66, 56, 53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, filters=[32, 16, 8, 2]):\n",
    "    \n",
    "    #============================================================================\n",
    "    # ENCODER\n",
    "    #============================================================================\n",
    "    inputs_mri = keras.layers.Input(shape=INPUT_SHAPE_mri, name='inpupt_mri')\n",
    "\n",
    "    # convolution block #1\n",
    "    x = keras.layers.Conv3D(filters[0], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(inputs_mri)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.Conv3D(filters[0], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "#     x, p1_idx = tf.nn.max_pool_with_argmax(x, ksize=[2], strides=[2], padding='SAME', name=\"p1\")\n",
    "    x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "    # convolution block #2\n",
    "    x = keras.layers.Conv3D(filters[1], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.Conv3D(filters[1], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "    x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "    # convolution block #3\n",
    "    x = keras.layers.Conv3D(filters[2], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.Conv3D(filters[2], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "    x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "    # convolution block #4\n",
    "#     x = keras.layers.Conv3D(filters[3], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "#                                   kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                   bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = keras.layers.Conv3D(filters[3], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "#                                   kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                   bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "#     x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "#                                               beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "#                                               moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "#                                               beta_constraint=None, gamma_constraint=None)(x)\n",
    "    \n",
    "\n",
    "    flatten = keras.layers.Flatten(data_format='channels_last')(x)\n",
    "\n",
    "    encoded = keras.layers.Dense(2,\n",
    "                               kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                               bias_initializer=keras.initializers.Constant(5.))(flatten)\n",
    "    encoded = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(encoded)\n",
    "\n",
    "    \n",
    "    #============================================================================\n",
    "    # DECODER\n",
    "    #============================================================================\n",
    "    x = keras.layers.Dense(filters[2]*int(input_shape[0]/8)*int(input_shape[1]/8)*int(input_shape[2]/8),\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(encoded)\n",
    "    \n",
    "    x = keras.layers.Reshape((int(input_shape[0]/8), int(input_shape[1]/8), int(input_shape[2]/8), filters[2]))(x)\n",
    "    \n",
    "    # convolution block #4\n",
    "#     x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "#     x = tf.keras.layers.Conv3DTranspose(filters[2], kernel_size=(1, 1, 2), strides=(1,1,1), padding='valid',\n",
    "#                                         kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                         bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    # convolution block #3\n",
    "    x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[2], kernel_size=(2, 1, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[2], kernel_size=(2, 1, 1), strides=(1,1,1), padding='same',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    # convolution block #2\n",
    "    x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[1], kernel_size=(1, 2, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[1], kernel_size=(1, 2, 1), strides=(1,1,1), padding='same',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    # convolution block #1\n",
    "    x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[0], kernel_size=(1, 1, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[0], kernel_size=(1, 1, 1), strides=(1,1,1), padding='same',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(input_shape[3], kernel_size=(1, 1, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    decoded = x\n",
    "    \n",
    "    #============================================================================\n",
    "    # COMPILE\n",
    "    #============================================================================\n",
    "    autoencoder = keras.Model(inputs=inputs_mri, outputs=decoded, name='autoencoder')\n",
    "    encoder = keras.Model(inputs=inputs_mri, outputs=encoded, name='encoder')\n",
    "\n",
    "    optim = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95)\n",
    "\n",
    "#     METRICS = [keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    "\n",
    "#     autoencoder.compile(loss='mse', metrics=METRICS, optimizer=optim)\n",
    "    autoencoder.compile(loss='mse', optimizer=optim)\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, encoder = create_model(INPUT_SHAPE_mri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inpupt_mri (InputLayer)      [(None, 52, 66, 56, 53)]  0         \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 52, 66, 56, 32)    45824     \n",
      "_________________________________________________________________\n",
      "p_re_lu (PReLU)              (None, 52, 66, 56, 32)    6150144   \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 52, 66, 56, 32)    27680     \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (None, 52, 66, 56, 32)    6150144   \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 26, 33, 28, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 26, 33, 28, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 26, 33, 28, 16)    13840     \n",
      "_________________________________________________________________\n",
      "p_re_lu_2 (PReLU)            (None, 26, 33, 28, 16)    384384    \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 26, 33, 28, 16)    6928      \n",
      "_________________________________________________________________\n",
      "p_re_lu_3 (PReLU)            (None, 26, 33, 28, 16)    384384    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 13, 16, 14, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 13, 16, 14, 16)    64        \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 13, 16, 14, 8)     3464      \n",
      "_________________________________________________________________\n",
      "p_re_lu_4 (PReLU)            (None, 13, 16, 14, 8)     23296     \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 13, 16, 14, 8)     1736      \n",
      "_________________________________________________________________\n",
      "p_re_lu_5 (PReLU)            (None, 13, 16, 14, 8)     23296     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 6, 8, 7, 8)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 8, 7, 8)        32        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 5378      \n",
      "_________________________________________________________________\n",
      "p_re_lu_6 (PReLU)            (None, 2)                 2         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2688)              8064      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 6, 8, 7, 8)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling3d (UpSampling3D) (None, 12, 16, 14, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_transpose (Conv3DTran (None, 13, 16, 14, 8)     136       \n",
      "_________________________________________________________________\n",
      "p_re_lu_7 (PReLU)            (None, 13, 16, 14, 8)     23296     \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_1 (Conv3DTr (None, 13, 16, 14, 8)     136       \n",
      "_________________________________________________________________\n",
      "p_re_lu_8 (PReLU)            (None, 13, 16, 14, 8)     23296     \n",
      "_________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3 (None, 26, 32, 28, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_2 (Conv3DTr (None, 26, 33, 28, 16)    272       \n",
      "_________________________________________________________________\n",
      "p_re_lu_9 (PReLU)            (None, 26, 33, 28, 16)    384384    \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_3 (Conv3DTr (None, 26, 33, 28, 16)    528       \n",
      "_________________________________________________________________\n",
      "p_re_lu_10 (PReLU)           (None, 26, 33, 28, 16)    384384    \n",
      "_________________________________________________________________\n",
      "up_sampling3d_2 (UpSampling3 (None, 52, 66, 56, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_4 (Conv3DTr (None, 52, 66, 56, 32)    544       \n",
      "_________________________________________________________________\n",
      "p_re_lu_11 (PReLU)           (None, 52, 66, 56, 32)    6150144   \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_5 (Conv3DTr (None, 52, 66, 56, 32)    1056      \n",
      "_________________________________________________________________\n",
      "p_re_lu_12 (PReLU)           (None, 52, 66, 56, 32)    6150144   \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_6 (Conv3DTr (None, 52, 66, 56, 53)    1749      \n",
      "_________________________________________________________________\n",
      "p_re_lu_13 (PReLU)           (None, 52, 66, 56, 53)    10186176  \n",
      "=================================================================\n",
      "Total params: 36,535,033\n",
      "Trainable params: 36,534,921\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights('./99_Training_checkpoints/mri_clustering/run_02/model_weights_02.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11754/11754 [1:41:46<00:00,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "enc_preds = {}\n",
    "with tqdm(total=len(ALL_IDS)) as pbar:\n",
    "    for i in ALL_IDS:\n",
    "        x, _ = get_inputs(i)\n",
    "        x = x.reshape(1,52, 66, 56, 53)\n",
    "        preds = encoder.predict(x, batch_size=1, verbose=0)\n",
    "        enc_preds[i] = preds[0]\n",
    "        pbar.update(1)\n",
    "enc_preds = pd.DataFrame(enc_preds).T\n",
    "enc_preds.to_csv('00_Data/encoder_preds.csv', index=True, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>0.240295</td>\n",
       "      <td>-0.394146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>-0.022644</td>\n",
       "      <td>-0.465305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>-0.099391</td>\n",
       "      <td>-0.320526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>0.452530</td>\n",
       "      <td>0.073141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>-0.091789</td>\n",
       "      <td>-0.036786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "10001  0.240295 -0.394146\n",
       "10002 -0.022644 -0.465305\n",
       "10004 -0.099391 -0.320526\n",
       "10005  0.452530  0.073141\n",
       "10007 -0.091789 -0.036786"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_preds_s2 = enc_preds.loc[REVEAL_IDS_S2]\n",
    "enc_preds_na = enc_preds.loc[NOREVEAL_IDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reveal_s2_enc = enc_preds_s2.values\n",
    "y_noreveal_enc = enc_preds_na.values\n",
    "\n",
    "y_reveal_s2_enc_mean = np.mean(y_reveal_s2_enc, axis=0)\n",
    "y_noreveal_enc_mean = np.mean(y_noreveal_enc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_reveal_s2_enc = np.genfromtxt('y_reveal_s2_enc.csv', delimiter=',')\n",
    "# y_noreveal_enc = np.genfromtxt('y_noreveal_enc.csv', delimiter=',')\n",
    "\n",
    "# y_reveal_s2_enc_mean = np.genfromtxt('y_reveal_s2_enc_mean.csv', delimiter=',')\n",
    "# y_noreveal_enc_mean = np.genfromtxt('y_noreveal_enc_mean.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14742053, -0.07687217], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_reveal_s2_enc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10948468, -0.10004541], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_noreveal_enc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 2) (11244, 2)\n"
     ]
    }
   ],
   "source": [
    "print(y_reveal_s2_enc.shape, y_noreveal_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = np.append(y_reveal_s2_enc, y_noreveal_enc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11754, 2)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = keras.layers.InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "#         print(input_dim)\n",
    "        self.input_spec = keras.layers.InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.        \n",
    "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "#         q = 1 / (1 + tf.norm(inputs - self.clusters) / self.alpha) ** (self.alpha + 1) / 2\n",
    "#         q = q / tf.math.reduce_sum(q, axis=)\n",
    "#         print(self.clusters)\n",
    "#         print(inputs)\n",
    "#         print(K.expand_dims(inputs, axis=1))\n",
    "#         print(K.expand_dims(inputs, axis=1) - self.clusters)\n",
    "#         print(K.square(K.expand_dims(inputs, axis=1) - self.clusters))\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "# model = keras.Model(inputs=encoder.input, outputs=clustering_layer)\n",
    "model = keras.Model(inputs=encoder.input, outputs=[clustering_layer, autoencoder.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.14742053, -0.07687217], dtype=float32),\n",
       " array([-0.10948468, -0.10004541], dtype=float32)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[y_reveal_s2_enc_mean, y_noreveal_enc_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\00_data\\python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1008: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return self.fit(X, sample_weight=sample_weight).labels_\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.2764053 , -0.22242014],\n",
       "       [ 0.192379  ,  0.12753388]], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=n_clusters, init=np.array([y_reveal_s2_enc_mean, y_noreveal_enc_mean]))\n",
    "# kmeans = cluster.KMeans(n_clusters=n_clusters)\n",
    "y_pred_km = kmeans.fit_predict(y_all)\n",
    "kmeans.cluster_centers_\n",
    "# kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array([[-0.27640564, -0.22242   ],\n",
    "#        [ 0.19237857,  0.12753383]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization 0\n",
      "  Iteration 10\t time lapse 0.04987s\t ll change 0.00101\n",
      "  Iteration 20\t time lapse 0.02493s\t ll change 0.00020\n",
      "Initialization converged: True\t time lapse 0.08876s\t ll -0.59734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.20951488, -0.20186344],\n",
       "       [ 0.13619564,  0.15944615]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm = mixture.GaussianMixture(n_components=2, \n",
    "                             covariance_type='full', \n",
    "                             tol=0.0001, \n",
    "                             reg_covar=1e-06, \n",
    "                             max_iter=2000, \n",
    "                             n_init=1, \n",
    "                             init_params='kmeans', \n",
    "                             weights_init=[1-len(y_reveal_s2_enc)/len(y_all),\n",
    "                                           1-len(y_noreveal_enc)/len(y_all)], \n",
    "                             means_init=[y_reveal_s2_enc_mean, y_noreveal_enc_mean], \n",
    "                             precisions_init=None, \n",
    "                             random_state=30, \n",
    "                             verbose=2, \n",
    "                             verbose_interval=10)\n",
    "y_pred_gm = gm.fit_predict(y_all)\n",
    "gm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array([[ 0.04507672,  0.05673662],\n",
    "#        [-0.22917223, -0.21675597]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8903"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_gm[np.where(y_pred_gm==0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2851"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_gm[np.where(y_pred_gm==1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_centers_km = kmeans.cluster_centers_\n",
    "init_centers_gm = gm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,) (126,)\n"
     ]
    }
   ],
   "source": [
    "y_pred_s2 = gm.predict(y_reveal_s2_enc)\n",
    "y0 = y_pred_s2[np.where(y_pred_s2==0)]\n",
    "y1 = y_pred_s2[np.where(y_pred_s2==1)]\n",
    "\n",
    "print(y0.shape,y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339,) (171,)\n"
     ]
    }
   ],
   "source": [
    "y_pred_s2 = kmeans.predict(y_reveal_s2_enc)\n",
    "y0 = y_pred_s2[np.where(y_pred_s2==0)]\n",
    "y1 = y_pred_s2[np.where(y_pred_s2==1)]\n",
    "\n",
    "print(y0.shape,y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(name='clustering').set_weights([init_centers_gm])\n",
    "# model.get_layer(name='clustering').set_weights([init_centers_km])\n",
    "optim = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95)\n",
    "# optim = tf.keras.optimizers.SGD(learning_rate=0.00001, momentum=0.9)\n",
    "# model.compile(loss='kld', optimizer=optim)\n",
    "model.compile(loss=['kld', 'mse'], loss_weights=[0.1, 1], optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_last = np.copy(y_pred_gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "#     print(weight.shape)\n",
    "    return (weight.T / weight.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "index = 0\n",
    "# maxiter = 1470\n",
    "update_interval = 489\n",
    "# index_array = np.arange(x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.001 # tolerance threshold to stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = pd.read_csv('00_Data/q_distrib/q_1_1467.csv', index_col=0, header=None)\n",
    "# y_pred = np.argmax(q.values, axis=1)\n",
    "# #             print(q.values)\n",
    "# #             print(q.loc[REVEAL_IDS_S2[2]])\n",
    "# y_pred_s2 = np.argmax(q.loc[REVEAL_IDS_S2].values, axis=1)\n",
    "# pos_rate = len(y_pred_s2[np.where(y_pred_s2==1)]) / len(y_pred_s2)\n",
    "# neg_rate = len(y_pred_s2[np.where(y_pred_s2==0)])/len(y_pred_s2)\n",
    "# print(pos_rate,neg_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDS_S2 = pd.read_csv('00_Data/reveal_ID_site2.csv', header=None, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDS_S2.to_csv('00_Data/reveal_ID_site2_int.csv', index=True, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Pos. rate:0.5941176470588235 Neg. rate:0.40588235294117647\n",
      "delta_label  0.9646928705121661\n",
      "Step 0, loss = [0.3113613724708557, 0.05623418837785721, 0.3057379424571991]\n",
      "Step 1, loss = [0.30847546458244324, 0.048053279519081116, 0.30367013812065125]\n",
      "Step 2, loss = [0.2746799886226654, 0.032676372677087784, 0.27141234278678894]\n",
      "Step 3, loss = [0.30908530950546265, 0.04608887806534767, 0.30447641015052795]\n",
      "Step 4, loss = [0.3121620714664459, 0.03367875516414642, 0.30879420042037964]\n",
      "Step 5, loss = [0.30222317576408386, 0.040087565779685974, 0.2982144057750702]\n",
      "Step 6, loss = [0.30569857358932495, 0.03683668375015259, 0.30201491713523865]\n",
      "Step 7, loss = [0.3000527322292328, 0.031579673290252686, 0.29689475893974304]\n",
      "Step 8, loss = [0.3159763813018799, 0.03973408043384552, 0.3120029866695404]\n",
      "Step 9, loss = [0.30248501896858215, 0.05552027374505997, 0.29693299531936646]\n",
      "Step 10, loss = [0.3048131763935089, 0.035948753356933594, 0.30121830105781555]\n",
      "Step 11, loss = [0.31088510155677795, 0.04430687054991722, 0.3064544200897217]\n",
      "Step 12, loss = [0.2925356328487396, 0.03394155204296112, 0.2891414761543274]\n",
      "Step 13, loss = [0.2941196858882904, 0.0529213473200798, 0.2888275384902954]\n",
      "Step 14, loss = [0.3129766285419464, 0.06550464034080505, 0.30642616748809814]\n",
      "Step 15, loss = [0.3177792429924011, 0.05263732746243477, 0.3125154972076416]\n",
      "Step 16, loss = [0.2988683581352234, 0.037188977003097534, 0.29514944553375244]\n",
      "Step 17, loss = [0.29720476269721985, 0.0477878600358963, 0.2924259901046753]\n",
      "Step 18, loss = [0.31061798334121704, 0.023747626692056656, 0.3082432150840759]\n",
      "Step 19, loss = [0.31660979986190796, 0.048259563744068146, 0.3117838501930237]\n",
      "Step 20, loss = [0.29986244440078735, 0.05377881973981857, 0.29448455572128296]\n",
      "Step 21, loss = [0.30059245228767395, 0.072965607047081, 0.29329589009284973]\n",
      "Step 22, loss = [0.3178156316280365, 0.0641816258430481, 0.3113974630832672]\n",
      "Step 23, loss = [0.30936166644096375, 0.03214950114488602, 0.3061467111110687]\n",
      "Step 24, loss = [0.2961677312850952, 0.05392630025744438, 0.2907750904560089]\n",
      "Step 25, loss = [0.31957876682281494, 0.08110643923282623, 0.31146812438964844]\n",
      "Step 26, loss = [0.3029051125049591, 0.026768449693918228, 0.3002282679080963]\n",
      "Step 27, loss = [0.3111378252506256, 0.05852828174829483, 0.3052850067615509]\n",
      "Step 28, loss = [0.298923134803772, 0.04973581060767174, 0.29394954442977905]\n",
      "Step 29, loss = [0.297518789768219, 0.05083687603473663, 0.2924351096153259]\n",
      "Step 30, loss = [0.30362144112586975, 0.042380012571811676, 0.2993834316730499]\n",
      "Step 31, loss = [0.3040310740470886, 0.037461698055267334, 0.30028489232063293]\n",
      "Step 32, loss = [0.31057384610176086, 0.034566547721624374, 0.30711719393730164]\n",
      "Step 33, loss = [0.31129732728004456, 0.039158307015895844, 0.3073815107345581]\n",
      "Step 34, loss = [0.3096230924129486, 0.0846499353647232, 0.3011581003665924]\n",
      "Step 35, loss = [0.2967684268951416, 0.030612193048000336, 0.2937072217464447]\n",
      "Step 36, loss = [0.29520145058631897, 0.0505966916680336, 0.2901417911052704]\n",
      "Step 37, loss = [0.3062426745891571, 0.0538109689950943, 0.30086156725883484]\n",
      "Step 38, loss = [0.3105282783508301, 0.05916794389486313, 0.304611474275589]\n",
      "Step 39, loss = [0.3123387396335602, 0.020082255825400352, 0.31033051013946533]\n",
      "Step 40, loss = [0.308467298746109, 0.0406927652657032, 0.3043980300426483]\n",
      "Step 41, loss = [0.2933060824871063, 0.05018463730812073, 0.28828760981559753]\n",
      "Step 42, loss = [0.3113870322704315, 0.031080171465873718, 0.30827900767326355]\n",
      "Step 43, loss = [0.31044232845306396, 0.05758695304393768, 0.3046836256980896]\n",
      "Step 44, loss = [0.31753820180892944, 0.03637631982564926, 0.31390056014060974]\n",
      "Step 45, loss = [0.3148820996284485, 0.05224107578396797, 0.3096579909324646]\n",
      "Step 46, loss = [0.3101901412010193, 0.08812259137630463, 0.30137789249420166]\n",
      "Step 47, loss = [0.2809068262577057, 0.09815147519111633, 0.27109166979789734]\n",
      "Step 48, loss = [0.3137384057044983, 0.05357789248228073, 0.3083806037902832]\n",
      "Step 49, loss = [0.30052900314331055, 0.0543980598449707, 0.2950891852378845]\n",
      "Step 50, loss = [0.3077005445957184, 0.04033840075135231, 0.30366671085357666]\n",
      "Step 51, loss = [0.303053081035614, 0.048601824790239334, 0.2981928884983063]\n",
      "Step 52, loss = [0.29451972246170044, 0.023566793650388718, 0.29216304421424866]\n",
      "Step 53, loss = [0.3044498562812805, 0.052639856934547424, 0.2991858720779419]\n",
      "Step 54, loss = [0.30633544921875, 0.017372343689203262, 0.30459821224212646]\n",
      "Step 55, loss = [0.30327945947647095, 0.02352421171963215, 0.3009270429611206]\n",
      "Step 56, loss = [0.30671316385269165, 0.04173421859741211, 0.30253973603248596]\n",
      "Step 57, loss = [0.30501872301101685, 0.02684703841805458, 0.3023340106010437]\n",
      "Step 58, loss = [0.3105262219905853, 0.048349712044000626, 0.3056912422180176]\n",
      "Step 59, loss = [0.31825071573257446, 0.047548435628414154, 0.3134958744049072]\n",
      "Step 60, loss = [0.30414894223213196, 0.017401082441210747, 0.3024088442325592]\n",
      "Step 61, loss = [0.30464568734169006, 0.02398187667131424, 0.3022474944591522]\n",
      "Step 62, loss = [0.2980378270149231, 0.037552252411842346, 0.29428261518478394]\n",
      "Step 63, loss = [0.2975621819496155, 0.057346973568201065, 0.2918274700641632]\n",
      "Step 64, loss = [0.3011558949947357, 0.060222577303647995, 0.29513365030288696]\n",
      "Step 65, loss = [0.3041248917579651, 0.06331194937229156, 0.2977936863899231]\n",
      "Step 66, loss = [0.2970157563686371, 0.03081696853041649, 0.29393404722213745]\n",
      "Step 67, loss = [0.3218517601490021, 0.04499804973602295, 0.31735196709632874]\n",
      "Step 68, loss = [0.3017965257167816, 0.07047286629676819, 0.2947492301464081]\n",
      "Step 69, loss = [0.30521905422210693, 0.027272837236523628, 0.30249178409576416]\n",
      "Step 70, loss = [0.27892494201660156, 0.042892761528491974, 0.2746356725692749]\n",
      "Step 71, loss = [0.3085017800331116, 0.051034025847911835, 0.30339837074279785]\n",
      "Step 72, loss = [0.3125883936882019, 0.048400066792964935, 0.30774837732315063]\n",
      "Step 73, loss = [0.29602646827697754, 0.05111877620220184, 0.2909145951271057]\n",
      "Step 74, loss = [0.3149700462818146, 0.06992874294519424, 0.30797716975212097]\n",
      "Step 75, loss = [0.3087499141693115, 0.047566529363393784, 0.30399325489997864]\n",
      "Step 76, loss = [0.3023490905761719, 0.0527329295873642, 0.2970758080482483]\n",
      "Step 77, loss = [0.3188944458961487, 0.029142796993255615, 0.3159801661968231]\n",
      "Step 78, loss = [0.3153351843357086, 0.08366984128952026, 0.30696821212768555]\n",
      "Step 79, loss = [0.2880838215351105, 0.05648167431354523, 0.28243565559387207]\n",
      "Step 80, loss = [0.3109833002090454, 0.04691291227936745, 0.30629199743270874]\n",
      "Step 81, loss = [0.3091471493244171, 0.041013866662979126, 0.3050457537174225]\n",
      "Step 82, loss = [0.28921884298324585, 0.04633574187755585, 0.28458526730537415]\n",
      "Step 83, loss = [0.31567737460136414, 0.08226074278354645, 0.3074513077735901]\n",
      "Step 84, loss = [0.3093014061450958, 0.04953143745660782, 0.30434826016426086]\n",
      "Step 85, loss = [0.3132092356681824, 0.025955917313694954, 0.31061363220214844]\n",
      "Step 86, loss = [0.2878227233886719, 0.017908528447151184, 0.2860318720340729]\n",
      "Step 87, loss = [0.31285715103149414, 0.04567335173487663, 0.3082898259162903]\n",
      "Step 88, loss = [0.29769015312194824, 0.042544953525066376, 0.293435662984848]\n",
      "Step 89, loss = [0.30616167187690735, 0.05075787007808685, 0.301085889339447]\n",
      "Step 90, loss = [0.2976621091365814, 0.036008261144161224, 0.2940612733364105]\n",
      "Step 91, loss = [0.30588245391845703, 0.04647228494286537, 0.30123522877693176]\n",
      "Step 92, loss = [0.3157604932785034, 0.0341012142598629, 0.3123503625392914]\n",
      "Step 93, loss = [0.29581302404403687, 0.037525713443756104, 0.2920604646205902]\n",
      "Step 94, loss = [0.3066418766975403, 0.07106892764568329, 0.29953497648239136]\n",
      "Step 95, loss = [0.2981526255607605, 0.030260225757956505, 0.29512661695480347]\n",
      "Step 96, loss = [0.3084145486354828, 0.04159108176827431, 0.3042554259300232]\n",
      "Step 97, loss = [0.2992880046367645, 0.04937088489532471, 0.29435092210769653]\n",
      "Step 98, loss = [0.2925846576690674, 0.04436689615249634, 0.2881479561328888]\n",
      "Step 99, loss = [0.2968246638774872, 0.05416306480765343, 0.29140836000442505]\n",
      "Step 100, loss = [0.2965710759162903, 0.029167845845222473, 0.29365429282188416]\n",
      "Step 101, loss = [0.3034736216068268, 0.04679416865110397, 0.2987942099571228]\n",
      "Step 102, loss = [0.3043675422668457, 0.05075916647911072, 0.29929161071777344]\n",
      "Step 103, loss = [0.3044866621494293, 0.037734419107437134, 0.3007132112979889]\n",
      "Step 104, loss = [0.3056020438671112, 0.01769464835524559, 0.3038325905799866]\n",
      "Step 105, loss = [0.2955379784107208, 0.05686791613698006, 0.28985118865966797]\n",
      "Step 106, loss = [0.3080832362174988, 0.053330883383750916, 0.3027501404285431]\n",
      "Step 107, loss = [0.31121647357940674, 0.052542995661497116, 0.3059621751308441]\n",
      "Step 108, loss = [0.29482969641685486, 0.014028511941432953, 0.29342684149742126]\n",
      "Step 109, loss = [0.2908572554588318, 0.027492240071296692, 0.2881080210208893]\n",
      "Step 110, loss = [0.30063942074775696, 0.0677843987941742, 0.2938609719276428]\n",
      "Step 111, loss = [0.3108644485473633, 0.06208326295018196, 0.30465611815452576]\n",
      "Step 112, loss = [0.29970160126686096, 0.06154874339699745, 0.29354673624038696]\n",
      "Step 113, loss = [0.32143017649650574, 0.047187600284814835, 0.31671142578125]\n",
      "Step 114, loss = [0.3098392188549042, 0.026761863380670547, 0.3071630299091339]\n",
      "Step 115, loss = [0.31833624839782715, 0.06996588408946991, 0.3113396465778351]\n",
      "Step 116, loss = [0.3011408746242523, 0.048586659133434296, 0.29628220200538635]\n",
      "Step 117, loss = [0.29848310351371765, 0.05860738456249237, 0.2926223576068878]\n",
      "Step 118, loss = [0.31369179487228394, 0.04034167900681496, 0.30965763330459595]\n",
      "Step 119, loss = [0.28359878063201904, 0.04837306961417198, 0.27876147627830505]\n",
      "Step 120, loss = [0.2903791069984436, 0.03501184284687042, 0.28687793016433716]\n",
      "Step 121, loss = [0.2977346181869507, 0.038959063589572906, 0.2938387095928192]\n",
      "Step 122, loss = [0.307187557220459, 0.03651108965277672, 0.30353644490242004]\n",
      "Step 123, loss = [0.3002273440361023, 0.052385978400707245, 0.294988751411438]\n",
      "Step 124, loss = [0.3039107620716095, 0.03585369139909744, 0.3003253936767578]\n",
      "Step 125, loss = [0.30389299988746643, 0.0665423572063446, 0.2972387671470642]\n",
      "Step 126, loss = [0.3070791959762573, 0.04365963861346245, 0.3027132451534271]\n",
      "Step 127, loss = [0.3120441734790802, 0.043359991163015366, 0.30770817399024963]\n",
      "Step 128, loss = [0.31698936223983765, 0.038206733763217926, 0.3131686747074127]\n",
      "Step 129, loss = [0.3060344457626343, 0.027907339856028557, 0.3032437264919281]\n",
      "Step 130, loss = [0.3089829087257385, 0.028559155762195587, 0.30612698197364807]\n",
      "Step 131, loss = [0.29323825240135193, 0.025176087394356728, 0.2907206416130066]\n",
      "Step 132, loss = [0.31007054448127747, 0.039677537977695465, 0.30610278248786926]\n",
      "Step 133, loss = [0.3148927390575409, 0.06487618386745453, 0.3084051311016083]\n",
      "Step 134, loss = [0.30329981446266174, 0.04428441449999809, 0.29887136816978455]\n",
      "Step 135, loss = [0.3107188045978546, 0.050344351679086685, 0.305684357881546]\n",
      "Step 136, loss = [0.31503841280937195, 0.03685019165277481, 0.3113533854484558]\n",
      "Step 137, loss = [0.29101070761680603, 0.03234488144516945, 0.28777623176574707]\n",
      "Step 138, loss = [0.30290666222572327, 0.07045260816812515, 0.2958613932132721]\n",
      "Step 139, loss = [0.2999837398529053, 0.05706333369016647, 0.2942773997783661]\n",
      "Step 140, loss = [0.31090375781059265, 0.06726059317588806, 0.3041777014732361]\n",
      "Step 141, loss = [0.29413241147994995, 0.046663880348205566, 0.2894660234451294]\n",
      "Step 142, loss = [0.293346643447876, 0.029949402436614037, 0.2903516888618469]\n",
      "Step 143, loss = [0.3124184310436249, 0.03182871267199516, 0.3092355728149414]\n",
      "Step 144, loss = [0.29854679107666016, 0.07440509647130966, 0.29110628366470337]\n",
      "Step 145, loss = [0.28938543796539307, 0.017951201647520065, 0.2875903248786926]\n",
      "Step 146, loss = [0.29189562797546387, 0.016647163778543472, 0.2902309000492096]\n",
      "Step 147, loss = [0.2876637578010559, 0.03751460462808609, 0.2839123010635376]\n",
      "Step 148, loss = [0.3192466199398041, 0.04337790235877037, 0.31490883231163025]\n",
      "Step 149, loss = [0.3011915385723114, 0.08734189718961716, 0.29245734214782715]\n",
      "Step 150, loss = [0.29764524102211, 0.07448621094226837, 0.29019662737846375]\n",
      "Step 151, loss = [0.30903559923171997, 0.05487797409296036, 0.30354779958724976]\n",
      "Step 152, loss = [0.2973790168762207, 0.03079158067703247, 0.2942998707294464]\n",
      "Step 153, loss = [0.30628690123558044, 0.0717327892780304, 0.2991136312484741]\n",
      "Step 154, loss = [0.2966095805168152, 0.07798466086387634, 0.2888111174106598]\n",
      "Step 155, loss = [0.2994847297668457, 0.04962141811847687, 0.29452258348464966]\n",
      "Step 156, loss = [0.31100985407829285, 0.028498224914073944, 0.30816003680229187]\n",
      "Step 157, loss = [0.2998315095901489, 0.03105163760483265, 0.2967263460159302]\n",
      "Step 158, loss = [0.3038697838783264, 0.0672161877155304, 0.2971481680870056]\n",
      "Step 159, loss = [0.311931848526001, 0.07252722978591919, 0.30467912554740906]\n",
      "Step 160, loss = [0.30138465762138367, 0.04010597988963127, 0.29737406969070435]\n",
      "Step 161, loss = [0.30730679631233215, 0.05762879550457001, 0.3015439212322235]\n",
      "Step 162, loss = [0.305446058511734, 0.03333955258131027, 0.3021121025085449]\n",
      "Step 163, loss = [0.30836448073387146, 0.043692342936992645, 0.3039952516555786]\n",
      "Step 164, loss = [0.3068637251853943, 0.02756505459547043, 0.3041072189807892]\n",
      "Step 165, loss = [0.2940213680267334, 0.06977394968271255, 0.287043958902359]\n",
      "Step 166, loss = [0.3007740378379822, 0.011319545097649097, 0.29964208602905273]\n",
      "Step 167, loss = [0.31302690505981445, 0.03392355516552925, 0.3096345365047455]\n",
      "Step 168, loss = [0.30039897561073303, 0.035277314484119415, 0.29687124490737915]\n",
      "Step 169, loss = [0.29702168703079224, 0.04808264970779419, 0.29221341013908386]\n",
      "Step 170, loss = [0.2924036979675293, 0.028365831822156906, 0.28956711292266846]\n",
      "Step 171, loss = [0.31413403153419495, 0.037937458604574203, 0.3103402853012085]\n",
      "Step 172, loss = [0.3102838695049286, 0.050971679389476776, 0.3051866888999939]\n",
      "Step 173, loss = [0.30617839097976685, 0.04030226171016693, 0.30214816331863403]\n",
      "Step 174, loss = [0.30625081062316895, 0.019089560955762863, 0.3043418526649475]\n",
      "Step 175, loss = [0.3157593905925751, 0.06428021937608719, 0.30933135747909546]\n",
      "Step 176, loss = [0.2996211051940918, 0.029930638149380684, 0.29662802815437317]\n",
      "Step 177, loss = [0.3134210407733917, 0.032483890652656555, 0.3101726472377777]\n",
      "Step 178, loss = [0.3086734116077423, 0.04193877428770065, 0.30447953939437866]\n",
      "Step 179, loss = [0.31601083278656006, 0.03885955363512039, 0.3121248781681061]\n",
      "Step 180, loss = [0.3151068389415741, 0.06890376657247543, 0.3082164525985718]\n",
      "Step 181, loss = [0.3034985363483429, 0.050989046692848206, 0.2983996272087097]\n",
      "Step 182, loss = [0.28364405035972595, 0.04470071941614151, 0.27917397022247314]\n",
      "Step 183, loss = [0.30163171887397766, 0.03630148246884346, 0.29800155758857727]\n",
      "Step 184, loss = [0.29003867506980896, 0.03728257864713669, 0.2863104045391083]\n",
      "Step 185, loss = [0.29095658659935, 0.011488484218716621, 0.2898077368736267]\n",
      "Step 186, loss = [0.2828744649887085, 0.041103437542915344, 0.27876412868499756]\n",
      "Step 187, loss = [0.29541197419166565, 0.012947906740009785, 0.29411718249320984]\n",
      "Step 188, loss = [0.3035975694656372, 0.04360407590866089, 0.2992371618747711]\n",
      "Step 189, loss = [0.30311790108680725, 0.05979230999946594, 0.29713866114616394]\n",
      "Step 190, loss = [0.3189999759197235, 0.06066686287522316, 0.3129332959651947]\n",
      "Step 191, loss = [0.30289822816848755, 0.026630235835909843, 0.30023521184921265]\n",
      "Step 192, loss = [0.30736881494522095, 0.03664018213748932, 0.30370479822158813]\n",
      "Step 193, loss = [0.3052351772785187, 0.023940149694681168, 0.3028411567211151]\n",
      "Step 194, loss = [0.3178240954875946, 0.05508136749267578, 0.312315970659256]\n",
      "Step 195, loss = [0.3084472417831421, 0.04931725561618805, 0.3035155236721039]\n",
      "Step 196, loss = [0.29531583189964294, 0.04031221941113472, 0.29128462076187134]\n",
      "Step 197, loss = [0.3027622401714325, 0.04328145831823349, 0.2984341084957123]\n",
      "Step 198, loss = [0.2973444163799286, 0.058387547731399536, 0.2915056645870209]\n",
      "Step 199, loss = [0.30076977610588074, 0.05698210373520851, 0.2950715720653534]\n",
      "Step 200, loss = [0.30092746019363403, 0.059966787695884705, 0.2949307858943939]\n",
      "Step 201, loss = [0.30317747592926025, 0.038133520632982254, 0.2993641197681427]\n",
      "Step 202, loss = [0.31809860467910767, 0.10125713050365448, 0.30797287821769714]\n",
      "Step 203, loss = [0.3049619495868683, 0.040825869888067245, 0.3008793592453003]\n",
      "Step 204, loss = [0.3037945330142975, 0.021959010511636734, 0.3015986382961273]\n",
      "Step 205, loss = [0.29585328698158264, 0.04135528951883316, 0.2917177677154541]\n",
      "Step 206, loss = [0.299480676651001, 0.03249531239271164, 0.29623115062713623]\n",
      "Step 207, loss = [0.29272040724754333, 0.04524712264537811, 0.2881956994533539]\n",
      "Step 208, loss = [0.31456971168518066, 0.05131344869732857, 0.3094383776187897]\n",
      "Step 209, loss = [0.310527503490448, 0.049797892570495605, 0.30554771423339844]\n",
      "Step 210, loss = [0.31205931305885315, 0.047955222427845, 0.3072637915611267]\n",
      "Step 211, loss = [0.3180088996887207, 0.04558251053094864, 0.3134506344795227]\n",
      "Step 212, loss = [0.29503965377807617, 0.03086348995566368, 0.29195329546928406]\n",
      "Step 213, loss = [0.3116402328014374, 0.07045415043830872, 0.30459481477737427]\n",
      "Step 214, loss = [0.30776628851890564, 0.04985862225294113, 0.302780419588089]\n",
      "Step 215, loss = [0.31069204211235046, 0.025318410247564316, 0.3081602156162262]\n",
      "Step 216, loss = [0.30863848328590393, 0.04492706060409546, 0.30414578318595886]\n",
      "Step 217, loss = [0.3027816414833069, 0.035805411636829376, 0.299201101064682]\n",
      "Step 218, loss = [0.31129518151283264, 0.045930370688438416, 0.3067021369934082]\n",
      "Step 219, loss = [0.3033985197544098, 0.0447688028216362, 0.2989216446876526]\n",
      "Step 220, loss = [0.3106935918331146, 0.04198464751243591, 0.30649513006210327]\n",
      "Step 221, loss = [0.2933625876903534, 0.013470358215272427, 0.29201555252075195]\n",
      "Step 222, loss = [0.3010144829750061, 0.045434314757585526, 0.2964710593223572]\n",
      "Step 223, loss = [0.3043304681777954, 0.057997435331344604, 0.2985307276248932]\n",
      "Step 224, loss = [0.3066678047180176, 0.050528645515441895, 0.3016149401664734]\n",
      "Step 225, loss = [0.3092334270477295, 0.030677642673254013, 0.3061656653881073]\n",
      "Step 226, loss = [0.3136872351169586, 0.048030223697423935, 0.3088842034339905]\n",
      "Step 227, loss = [0.29566308856010437, 0.033656612038612366, 0.2922974228858948]\n",
      "Step 228, loss = [0.3159307837486267, 0.037177301943302155, 0.31221306324005127]\n",
      "Step 229, loss = [0.3057743310928345, 0.05357521027326584, 0.3004167973995209]\n",
      "Step 230, loss = [0.31025195121765137, 0.020349454134702682, 0.30821701884269714]\n",
      "Step 231, loss = [0.3047357201576233, 0.06083747372031212, 0.29865196347236633]\n",
      "Step 232, loss = [0.301999032497406, 0.03480982780456543, 0.2985180616378784]\n",
      "Step 233, loss = [0.3131515383720398, 0.04457177221775055, 0.30869436264038086]\n",
      "Step 234, loss = [0.318283349275589, 0.03524354472756386, 0.3147589862346649]\n",
      "Step 235, loss = [0.3123014569282532, 0.040459588170051575, 0.30825549364089966]\n",
      "Step 236, loss = [0.29919561743736267, 0.03588686138391495, 0.29560694098472595]\n",
      "Step 237, loss = [0.2968045473098755, 0.025093819946050644, 0.29429516196250916]\n",
      "Step 238, loss = [0.26544666290283203, 0.03168391436338425, 0.2622782588005066]\n",
      "Step 239, loss = [0.30823612213134766, 0.038238149136304855, 0.304412305355072]\n",
      "Step 240, loss = [0.2918841242790222, 0.0404052771627903, 0.28784358501434326]\n",
      "Step 241, loss = [0.2902255952358246, 0.024636806920170784, 0.287761926651001]\n",
      "Step 242, loss = [0.3126591444015503, 0.06560198962688446, 0.30609893798828125]\n",
      "Step 243, loss = [0.2835880219936371, 0.026972297579050064, 0.2808907926082611]\n",
      "Step 244, loss = [0.2941758334636688, 0.04921039938926697, 0.2892547845840454]\n",
      "Step 245, loss = [0.29698440432548523, 0.09198413789272308, 0.28778597712516785]\n",
      "Step 246, loss = [0.28565043210983276, 0.04057569429278374, 0.28159287571907043]\n",
      "Step 247, loss = [0.29808473587036133, 0.04904038459062576, 0.2931807041168213]\n",
      "Step 248, loss = [0.2991953492164612, 0.025345567613840103, 0.29666078090667725]\n",
      "Step 249, loss = [0.3062869906425476, 0.034546807408332825, 0.30283230543136597]\n",
      "Step 250, loss = [0.29885178804397583, 0.036857910454273224, 0.2951659858226776]\n",
      "Step 251, loss = [0.29695695638656616, 0.07462148368358612, 0.2894948124885559]\n",
      "Step 252, loss = [0.2938424050807953, 0.06953367590904236, 0.28688904643058777]\n",
      "Step 253, loss = [0.29130318760871887, 0.02340167574584484, 0.2889630198478699]\n",
      "Step 254, loss = [0.30039337277412415, 0.0381675623357296, 0.2965766191482544]\n",
      "Step 255, loss = [0.3122479021549225, 0.06909876316785812, 0.3053380250930786]\n",
      "Step 256, loss = [0.31801214814186096, 0.05844657123088837, 0.3121674954891205]\n",
      "Step 257, loss = [0.3043789863586426, 0.02421259693801403, 0.30195772647857666]\n",
      "Step 258, loss = [0.29216232895851135, 0.03752987086772919, 0.28840935230255127]\n",
      "Step 259, loss = [0.3017663061618805, 0.0251811221241951, 0.29924818873405457]\n",
      "Step 260, loss = [0.2942236065864563, 0.02149192988872528, 0.29207441210746765]\n",
      "Step 261, loss = [0.30317768454551697, 0.06333530694246292, 0.29684415459632874]\n",
      "Step 262, loss = [0.2961919903755188, 0.06691601127386093, 0.2895003855228424]\n",
      "Step 263, loss = [0.31331658363342285, 0.03452468663454056, 0.3098641037940979]\n",
      "Step 264, loss = [0.30675405263900757, 0.055723775178194046, 0.3011816740036011]\n",
      "Step 265, loss = [0.316864550113678, 0.05369006097316742, 0.3114955425262451]\n",
      "Step 266, loss = [0.30814263224601746, 0.04189187288284302, 0.3039534389972687]\n",
      "Step 267, loss = [0.30265554785728455, 0.044876087456941605, 0.2981679439544678]\n",
      "Step 268, loss = [0.3119819462299347, 0.050761304795742035, 0.3069058060646057]\n",
      "Step 269, loss = [0.31119775772094727, 0.09122572839260101, 0.30207517743110657]\n",
      "Step 270, loss = [0.3034631907939911, 0.019368287175893784, 0.30152636766433716]\n",
      "Step 271, loss = [0.2909735441207886, 0.02340765669941902, 0.2886327803134918]\n",
      "Step 272, loss = [0.2980343699455261, 0.05397212505340576, 0.2926371693611145]\n",
      "Step 273, loss = [0.3033423125743866, 0.06463752686977386, 0.29687854647636414]\n",
      "Step 274, loss = [0.29992568492889404, 0.033506445586681366, 0.29657503962516785]\n",
      "Step 275, loss = [0.31905218958854675, 0.035608671605587006, 0.31549131870269775]\n",
      "Step 276, loss = [0.3111788034439087, 0.055592671036720276, 0.3056195378303528]\n",
      "Step 277, loss = [0.3079485297203064, 0.030992476269602776, 0.3048492968082428]\n",
      "Step 278, loss = [0.3061543405056, 0.04347866028547287, 0.3018064796924591]\n",
      "Step 279, loss = [0.3076450228691101, 0.016335804015398026, 0.306011438369751]\n",
      "Step 280, loss = [0.2961772084236145, 0.007486055605113506, 0.295428603887558]\n",
      "Step 281, loss = [0.2942001223564148, 0.050389669835567474, 0.28916114568710327]\n",
      "Step 282, loss = [0.315443217754364, 0.05321957916021347, 0.3101212680339813]\n",
      "Step 283, loss = [0.3162367641925812, 0.04119236767292023, 0.3121175169944763]\n",
      "Step 284, loss = [0.3045855760574341, 0.059085823595523834, 0.298676997423172]\n",
      "Step 285, loss = [0.2923990786075592, 0.038352590054273605, 0.2885638177394867]\n",
      "Step 286, loss = [0.31585538387298584, 0.08401846885681152, 0.30745354294776917]\n",
      "Step 287, loss = [0.2919797897338867, 0.05357372760772705, 0.28662240505218506]\n",
      "Step 288, loss = [0.30479034781455994, 0.026000946760177612, 0.30219024419784546]\n",
      "Step 289, loss = [0.30355650186538696, 0.056127503514289856, 0.29794374108314514]\n",
      "Step 290, loss = [0.3021165430545807, 0.02845916897058487, 0.2992706298828125]\n",
      "Step 291, loss = [0.28828465938568115, 0.018663078546524048, 0.2864183485507965]\n",
      "Step 292, loss = [0.3079225420951843, 0.04884946718811989, 0.3030375838279724]\n",
      "Step 293, loss = [0.315674364566803, 0.0391412228345871, 0.3117602467536926]\n",
      "Step 294, loss = [0.3075703978538513, 0.057158201932907104, 0.30185458064079285]\n",
      "Step 295, loss = [0.3207276463508606, 0.05808117985725403, 0.31491953134536743]\n",
      "Step 296, loss = [0.31198209524154663, 0.04397086799144745, 0.3075850009918213]\n",
      "Step 297, loss = [0.28671249747276306, 0.05673401802778244, 0.2810390889644623]\n",
      "Step 298, loss = [0.3126358091831207, 0.030327439308166504, 0.3096030652523041]\n",
      "Step 299, loss = [0.302967369556427, 0.040260132402181625, 0.29894134402275085]\n",
      "Step 300, loss = [0.30662623047828674, 0.04059804230928421, 0.30256643891334534]\n",
      "Step 301, loss = [0.3012474775314331, 0.03247421607375145, 0.2980000674724579]\n",
      "Step 302, loss = [0.2892095446586609, 0.015365295112133026, 0.2876730263233185]\n",
      "Step 303, loss = [0.3058581054210663, 0.03955467790365219, 0.3019026517868042]\n",
      "Step 304, loss = [0.2975199520587921, 0.0242631696164608, 0.2950936257839203]\n",
      "Step 305, loss = [0.2979069650173187, 0.06857065856456757, 0.29104989767074585]\n",
      "Step 306, loss = [0.30696019530296326, 0.0469423308968544, 0.3022659718990326]\n",
      "Step 307, loss = [0.29264333844184875, 0.03789931535720825, 0.28885340690612793]\n",
      "Step 308, loss = [0.3179796040058136, 0.03921815752983093, 0.3140577971935272]\n",
      "Step 309, loss = [0.31567803025245667, 0.03960275277495384, 0.3117177486419678]\n",
      "Step 310, loss = [0.30904054641723633, 0.031210582703351974, 0.30591949820518494]\n",
      "Step 311, loss = [0.2906773090362549, 0.04189417511224747, 0.286487877368927]\n",
      "Step 312, loss = [0.29810547828674316, 0.028925307095050812, 0.2952129542827606]\n",
      "Step 313, loss = [0.3104918599128723, 0.042564477771520615, 0.3062354028224945]\n",
      "Step 314, loss = [0.3084378242492676, 0.05000747740268707, 0.30343708395957947]\n",
      "Step 315, loss = [0.3068583607673645, 0.034842632710933685, 0.30337411165237427]\n",
      "Step 316, loss = [0.3062587380409241, 0.054802462458610535, 0.30077847838401794]\n",
      "Step 317, loss = [0.3033091425895691, 0.03742801025509834, 0.2995663285255432]\n",
      "Step 318, loss = [0.3108820617198944, 0.05518622696399689, 0.3053634464740753]\n",
      "Step 319, loss = [0.307455837726593, 0.024609502404928207, 0.3049948811531067]\n",
      "Step 320, loss = [0.30478060245513916, 0.051739245653152466, 0.29960668087005615]\n",
      "Step 321, loss = [0.2877909243106842, 0.03231474384665489, 0.2845594584941864]\n",
      "Step 322, loss = [0.3030322790145874, 0.03682395815849304, 0.2993498742580414]\n",
      "Step 323, loss = [0.298352986574173, 0.04561782628297806, 0.2937912046909332]\n",
      "Step 324, loss = [0.2960200607776642, 0.04440139979124069, 0.291579931974411]\n",
      "Step 325, loss = [0.3071902096271515, 0.027545245364308357, 0.304435670375824]\n",
      "Step 326, loss = [0.30694499611854553, 0.03353887051343918, 0.3035911023616791]\n",
      "Step 327, loss = [0.3035701513290405, 0.03858815133571625, 0.29971134662628174]\n",
      "Step 328, loss = [0.30452191829681396, 0.04651477187871933, 0.29987043142318726]\n",
      "Step 329, loss = [0.3017066419124603, 0.04066259041428566, 0.2976403832435608]\n",
      "Step 330, loss = [0.30914169549942017, 0.07327470928430557, 0.3018142282962799]\n",
      "Step 331, loss = [0.2953105866909027, 0.02379317209124565, 0.2929312586784363]\n",
      "Step 332, loss = [0.30009040236473083, 0.030028196051716805, 0.29708757996559143]\n",
      "Step 333, loss = [0.30244001746177673, 0.03465382382273674, 0.2989746332168579]\n",
      "Step 334, loss = [0.3108636140823364, 0.05808699131011963, 0.3050549030303955]\n",
      "Step 335, loss = [0.2960529029369354, 0.04312934726476669, 0.29173997044563293]\n",
      "Step 336, loss = [0.289835661649704, 0.0446331761777401, 0.2853723466396332]\n",
      "Step 337, loss = [0.30963805317878723, 0.042004719376564026, 0.3054375946521759]\n",
      "Step 338, loss = [0.29813992977142334, 0.060671307146549225, 0.2920728027820587]\n",
      "Step 339, loss = [0.2862365245819092, 0.036375317722558975, 0.28259900212287903]\n",
      "Step 340, loss = [0.30475249886512756, 0.04873425140976906, 0.2998790740966797]\n",
      "Step 341, loss = [0.31314998865127563, 0.04218363016843796, 0.3089316189289093]\n",
      "Step 342, loss = [0.2970333397388458, 0.03681665286421776, 0.2933516800403595]\n",
      "Step 343, loss = [0.31604906916618347, 0.06265602260828018, 0.3097834587097168]\n",
      "Step 344, loss = [0.29728952050209045, 0.05823344737291336, 0.2914661765098572]\n",
      "Step 345, loss = [0.29161536693573, 0.046009451150894165, 0.2870144248008728]\n",
      "Step 346, loss = [0.3007301390171051, 0.04378429055213928, 0.29635170102119446]\n",
      "Step 347, loss = [0.3010086715221405, 0.040789004415273666, 0.2969297766685486]\n",
      "Step 348, loss = [0.3178049921989441, 0.04975436255335808, 0.31282955408096313]\n",
      "Step 349, loss = [0.3015263080596924, 0.031430989503860474, 0.2983832061290741]\n",
      "Step 350, loss = [0.3038164973258972, 0.03204067423939705, 0.3006124198436737]\n",
      "Step 351, loss = [0.2929815649986267, 0.0491316094994545, 0.28806841373443604]\n",
      "Step 352, loss = [0.30834293365478516, 0.030380774289369583, 0.3053048551082611]\n",
      "Step 353, loss = [0.30020835995674133, 0.0378965362906456, 0.296418696641922]\n",
      "Step 354, loss = [0.31156277656555176, 0.054311297833919525, 0.30613166093826294]\n",
      "Step 355, loss = [0.2866593301296234, 0.04485262930393219, 0.28217408061027527]\n",
      "Step 356, loss = [0.3018498420715332, 0.04493735358119011, 0.29735609889030457]\n",
      "Step 357, loss = [0.28811073303222656, 0.02415885403752327, 0.28569483757019043]\n",
      "Step 358, loss = [0.3076868951320648, 0.04396568611264229, 0.30329033732414246]\n",
      "Step 359, loss = [0.3002842962741852, 0.02512144297361374, 0.2977721393108368]\n",
      "Step 360, loss = [0.31362950801849365, 0.03097640909254551, 0.3105318546295166]\n",
      "Step 361, loss = [0.30542832612991333, 0.0750252828001976, 0.29792580008506775]\n",
      "Step 362, loss = [0.293684720993042, 0.032630350440740585, 0.2904216945171356]\n",
      "Step 363, loss = [0.30501511693000793, 0.02856474369764328, 0.3021586537361145]\n",
      "Step 364, loss = [0.3008154034614563, 0.05427759513258934, 0.2953876554965973]\n",
      "Step 365, loss = [0.30888426303863525, 0.018331173807382584, 0.3070511519908905]\n",
      "Step 366, loss = [0.29588067531585693, 0.05422312766313553, 0.2904583513736725]\n",
      "Step 367, loss = [0.31566670536994934, 0.05798129737377167, 0.30986857414245605]\n",
      "Step 368, loss = [0.31802618503570557, 0.07338251918554306, 0.31068792939186096]\n",
      "Step 369, loss = [0.3014315366744995, 0.04302520304918289, 0.2971290051937103]\n",
      "Step 370, loss = [0.2961024045944214, 0.040382809937000275, 0.2920641303062439]\n",
      "Step 371, loss = [0.2976592183113098, 0.05193573236465454, 0.2924656569957733]\n",
      "Step 372, loss = [0.29518190026283264, 0.049858011305332184, 0.29019609093666077]\n",
      "Step 373, loss = [0.3079613447189331, 0.04306434094905853, 0.30365490913391113]\n",
      "Step 374, loss = [0.3024127185344696, 0.07575023919343948, 0.29483768343925476]\n",
      "Step 375, loss = [0.31296825408935547, 0.04465166851878166, 0.30850309133529663]\n",
      "Step 376, loss = [0.3066081404685974, 0.0402548648416996, 0.30258265137672424]\n",
      "Step 377, loss = [0.29975956678390503, 0.0482320562005043, 0.2949363589286804]\n",
      "Step 378, loss = [0.2979826331138611, 0.0121149942278862, 0.2967711389064789]\n",
      "Step 379, loss = [0.30156806111335754, 0.05274747684597969, 0.29629331827163696]\n",
      "Step 380, loss = [0.3118983209133148, 0.054802097380161285, 0.30641812086105347]\n",
      "Step 381, loss = [0.30323144793510437, 0.032862331718206406, 0.299945205450058]\n",
      "Step 382, loss = [0.2900387942790985, 0.050614647567272186, 0.2849773168563843]\n",
      "Step 383, loss = [0.31156858801841736, 0.0456724651157856, 0.30700135231018066]\n",
      "Step 384, loss = [0.30099061131477356, 0.03862606734037399, 0.29712799191474915]\n",
      "Step 385, loss = [0.2898528277873993, 0.055411506444215775, 0.2843116819858551]\n",
      "Step 386, loss = [0.30133476853370667, 0.043832339346408844, 0.2969515323638916]\n",
      "Step 387, loss = [0.2922348380088806, 0.07146348059177399, 0.2850884795188904]\n",
      "Step 388, loss = [0.3022809624671936, 0.028828617185354233, 0.2993980944156647]\n",
      "Step 389, loss = [0.3056369721889496, 0.03293836489319801, 0.30234313011169434]\n",
      "Step 390, loss = [0.3082796633243561, 0.055378179997205734, 0.3027418553829193]\n",
      "Step 391, loss = [0.31788086891174316, 0.07407388091087341, 0.3104734718799591]\n",
      "Step 392, loss = [0.3008061349391937, 0.03324093669652939, 0.29748204350471497]\n",
      "Step 393, loss = [0.3070792555809021, 0.02107195556163788, 0.3049720525741577]\n",
      "Step 394, loss = [0.30026906728744507, 0.029056459665298462, 0.29736343026161194]\n",
      "Step 395, loss = [0.3066128194332123, 0.036877699196338654, 0.3029250502586365]\n",
      "Step 396, loss = [0.2789295017719269, 0.03167268633842468, 0.2757622301578522]\n",
      "Step 397, loss = [0.30628228187561035, 0.04060447961091995, 0.3022218346595764]\n",
      "Step 398, loss = [0.2858833372592926, 0.03410171717405319, 0.2824731767177582]\n",
      "Step 399, loss = [0.31499525904655457, 0.06304412335157394, 0.3086908459663391]\n",
      "Step 400, loss = [0.30965033173561096, 0.041238754987716675, 0.305526465177536]\n",
      "Step 401, loss = [0.3032422661781311, 0.07193608582019806, 0.29604867100715637]\n",
      "Step 402, loss = [0.29378750920295715, 0.013243152759969234, 0.29246318340301514]\n",
      "Step 403, loss = [0.29679957032203674, 0.04027402773499489, 0.29277217388153076]\n",
      "Step 404, loss = [0.3110639750957489, 0.033992163836956024, 0.30766475200653076]\n",
      "Step 405, loss = [0.3074251413345337, 0.043735090643167496, 0.303051620721817]\n",
      "Step 406, loss = [0.31808406114578247, 0.05652017891407013, 0.31243205070495605]\n",
      "Step 407, loss = [0.3065303862094879, 0.04014222323894501, 0.3025161623954773]\n",
      "Step 408, loss = [0.31609565019607544, 0.06902942806482315, 0.3091927170753479]\n",
      "Step 409, loss = [0.2915286123752594, 0.020352894440293312, 0.2894933223724365]\n",
      "Step 410, loss = [0.3167332708835602, 0.07121774554252625, 0.3096114993095398]\n",
      "Step 411, loss = [0.30877992510795593, 0.04529773071408272, 0.30425015091896057]\n",
      "Step 412, loss = [0.3120502829551697, 0.04843860864639282, 0.3072064220905304]\n",
      "Step 413, loss = [0.309855192899704, 0.05091986060142517, 0.30476319789886475]\n",
      "Step 414, loss = [0.27787235379219055, 0.0453997477889061, 0.2733323872089386]\n",
      "Step 415, loss = [0.30134207010269165, 0.055071376264095306, 0.2958349287509918]\n",
      "Step 416, loss = [0.3134547770023346, 0.04917248338460922, 0.3085375428199768]\n",
      "Step 417, loss = [0.3082137405872345, 0.03614372760057449, 0.3045993745326996]\n",
      "Step 418, loss = [0.3141661286354065, 0.07345020771026611, 0.3068211078643799]\n",
      "Step 419, loss = [0.29091358184814453, 0.047888241708278656, 0.2861247658729553]\n",
      "Step 420, loss = [0.2977435290813446, 0.02545960247516632, 0.29519757628440857]\n",
      "Step 421, loss = [0.30322474241256714, 0.05162363499403, 0.29806238412857056]\n",
      "Step 422, loss = [0.3048778474330902, 0.04496248811483383, 0.300381600856781]\n",
      "Step 423, loss = [0.31439632177352905, 0.04719715937972069, 0.3096766173839569]\n",
      "Step 424, loss = [0.3018915057182312, 0.06444364786148071, 0.29544714093208313]\n",
      "Step 425, loss = [0.31135252118110657, 0.05160393938422203, 0.3061921298503876]\n",
      "Step 426, loss = [0.29392215609550476, 0.051591657102108, 0.28876298666000366]\n",
      "Step 427, loss = [0.31606993079185486, 0.05770965665578842, 0.31029897928237915]\n",
      "Step 428, loss = [0.2990676164627075, 0.019870854914188385, 0.29708051681518555]\n",
      "Step 429, loss = [0.303266704082489, 0.07546360790729523, 0.29572033882141113]\n",
      "Step 430, loss = [0.3058280944824219, 0.03454095125198364, 0.302374005317688]\n",
      "Step 431, loss = [0.3234642744064331, 0.07281020283699036, 0.3161832392215729]\n",
      "Step 432, loss = [0.2999989688396454, 0.026682835072278976, 0.29733067750930786]\n",
      "Step 433, loss = [0.3015437722206116, 0.028431270271539688, 0.29870063066482544]\n",
      "Step 434, loss = [0.3038623631000519, 0.03643956780433655, 0.300218403339386]\n",
      "Step 435, loss = [0.30078867077827454, 0.040256716310977936, 0.29676300287246704]\n",
      "Step 436, loss = [0.30634385347366333, 0.07922183722257614, 0.2984216809272766]\n",
      "Step 437, loss = [0.3056122362613678, 0.06828666478395462, 0.2987835705280304]\n",
      "Step 438, loss = [0.3016977906227112, 0.05080448091030121, 0.296617329120636]\n",
      "Step 439, loss = [0.30386999249458313, 0.0447719544172287, 0.29939278960227966]\n",
      "Step 440, loss = [0.29536008834838867, 0.04518663138151169, 0.2908414304256439]\n",
      "Step 441, loss = [0.2978375256061554, 0.032751284539699554, 0.2945623993873596]\n",
      "Step 442, loss = [0.2879749536514282, 0.08418769389390945, 0.27955618500709534]\n",
      "Step 443, loss = [0.30962714552879333, 0.06113940477371216, 0.30351319909095764]\n",
      "Step 444, loss = [0.2924252450466156, 0.0357201024889946, 0.2888532280921936]\n",
      "Step 445, loss = [0.2970367968082428, 0.019196264445781708, 0.29511716961860657]\n",
      "Step 446, loss = [0.2981407642364502, 0.05225122720003128, 0.2929156422615051]\n",
      "Step 447, loss = [0.3005143404006958, 0.040422581136226654, 0.29647207260131836]\n",
      "Step 448, loss = [0.2979794442653656, 0.03709234297275543, 0.29427021741867065]\n",
      "Step 449, loss = [0.28705674409866333, 0.0608908049762249, 0.280967652797699]\n",
      "Step 450, loss = [0.3033636212348938, 0.03614523261785507, 0.29974910616874695]\n",
      "Step 451, loss = [0.30685916543006897, 0.030557189136743546, 0.3038034439086914]\n",
      "Step 452, loss = [0.29629161953926086, 0.04349764063954353, 0.2919418513774872]\n",
      "Step 453, loss = [0.2947165369987488, 0.06837119162082672, 0.28787940740585327]\n",
      "Step 454, loss = [0.3089889585971832, 0.04309479147195816, 0.3046794831752777]\n",
      "Step 455, loss = [0.29599785804748535, 0.03839395195245743, 0.29215845465660095]\n",
      "Step 456, loss = [0.30741220712661743, 0.04023616015911102, 0.3033885955810547]\n",
      "Step 457, loss = [0.3032369613647461, 0.08054865896701813, 0.29518210887908936]\n",
      "Step 458, loss = [0.2996166944503784, 0.07186064124107361, 0.2924306392669678]\n",
      "Step 459, loss = [0.30740681290626526, 0.0372941717505455, 0.30367740988731384]\n",
      "Step 460, loss = [0.2968440651893616, 0.044941194355487823, 0.2923499345779419]\n",
      "Step 461, loss = [0.32177504897117615, 0.04342922940850258, 0.31743213534355164]\n",
      "Step 462, loss = [0.3145124018192291, 0.03974743187427521, 0.3105376660823822]\n",
      "Step 463, loss = [0.30689695477485657, 0.051692891865968704, 0.30172765254974365]\n",
      "Step 464, loss = [0.30570101737976074, 0.0488862507045269, 0.30081239342689514]\n",
      "Step 465, loss = [0.3164912462234497, 0.03407953679561615, 0.313083291053772]\n",
      "Step 466, loss = [0.31521978974342346, 0.043703436851501465, 0.31084945797920227]\n",
      "Step 467, loss = [0.30369997024536133, 0.028837893158197403, 0.3008161783218384]\n",
      "Step 468, loss = [0.29307103157043457, 0.032336991280317307, 0.2898373305797577]\n",
      "Step 469, loss = [0.3115675151348114, 0.05764281749725342, 0.30580323934555054]\n",
      "Step 470, loss = [0.3098181188106537, 0.015846921131014824, 0.30823343992233276]\n",
      "Step 471, loss = [0.3020783066749573, 0.028197942301630974, 0.2992585003376007]\n",
      "Step 472, loss = [0.2971206605434418, 0.07311328500509262, 0.28980934619903564]\n",
      "Step 473, loss = [0.29852378368377686, 0.04616573825478554, 0.29390719532966614]\n",
      "Step 474, loss = [0.2927515506744385, 0.040601909160614014, 0.28869137167930603]\n",
      "Step 475, loss = [0.30529361963272095, 0.04748987779021263, 0.3005446195602417]\n",
      "Step 476, loss = [0.299348384141922, 0.029954370111227036, 0.29635295271873474]\n",
      "Step 477, loss = [0.31107863783836365, 0.03909730911254883, 0.3071689009666443]\n",
      "Step 478, loss = [0.3104287385940552, 0.04433722048997879, 0.30599501729011536]\n",
      "Step 479, loss = [0.30279213190078735, 0.015800822526216507, 0.30121204257011414]\n",
      "Step 480, loss = [0.2967723309993744, 0.08365035057067871, 0.2884072959423065]\n",
      "Step 481, loss = [0.2891833186149597, 0.05386224016547203, 0.28379708528518677]\n",
      "Step 482, loss = [0.27195167541503906, 0.03318401426076889, 0.26863327622413635]\n",
      "Step 483, loss = [0.30076485872268677, 0.0466437004506588, 0.2961004972457886]\n",
      "Step 484, loss = [0.3042951226234436, 0.05348968505859375, 0.2989461421966553]\n",
      "Step 485, loss = [0.30632561445236206, 0.04005108401179314, 0.3023205101490021]\n",
      "Step 486, loss = [0.3161284327507019, 0.0616188570857048, 0.3099665343761444]\n",
      "Step 487, loss = [0.30896997451782227, 0.04785048961639404, 0.3041849136352539]\n",
      "Step 488, loss = [0.31557369232177734, 0.06878568232059479, 0.30869513750076294]\n",
      "Pos. rate:0.36666666666666664 Neg. rate:0.6333333333333333\n",
      "delta_label  0.9509103283988429\n",
      "Step 489, loss = [0.3016916513442993, 0.008263862691819668, 0.3008652627468109]\n",
      "Step 490, loss = [0.29560694098472595, 0.008538013324141502, 0.29475313425064087]\n",
      "Step 491, loss = [0.3071288764476776, 0.009551511146128178, 0.306173712015152]\n",
      "Step 492, loss = [0.3089520037174225, 0.009680790826678276, 0.307983934879303]\n",
      "Step 493, loss = [0.30579400062561035, 0.012681091204285622, 0.30452588200569153]\n",
      "Step 494, loss = [0.2872883677482605, 0.00979674607515335, 0.2863087058067322]\n",
      "Step 495, loss = [0.30551281571388245, 0.004865096881985664, 0.3050262928009033]\n",
      "Step 496, loss = [0.3045693337917328, 0.006038641557097435, 0.3039654791355133]\n",
      "Step 497, loss = [0.3031628131866455, 0.007658126763999462, 0.3023970127105713]\n",
      "Step 498, loss = [0.2893764078617096, 0.006534488871693611, 0.288722962141037]\n",
      "Step 499, loss = [0.3010576069355011, 0.009952118620276451, 0.3000623881816864]\n",
      "Step 500, loss = [0.2975141406059265, 0.008042898029088974, 0.2967098653316498]\n",
      "Step 501, loss = [0.2765621840953827, 0.007351350039243698, 0.2758270502090454]\n",
      "Step 502, loss = [0.30056679248809814, 0.0038028096314519644, 0.30018651485443115]\n",
      "Step 503, loss = [0.30422061681747437, 0.008322499692440033, 0.3033883571624756]\n",
      "Step 504, loss = [0.2959998548030853, 0.011459747329354286, 0.2948538661003113]\n",
      "Step 505, loss = [0.30592045187950134, 0.004101258236914873, 0.3055103123188019]\n",
      "Step 506, loss = [0.29667940735816956, 0.009883834049105644, 0.29569101333618164]\n",
      "Step 507, loss = [0.303180992603302, 0.006661760620772839, 0.30251482129096985]\n",
      "Step 508, loss = [0.2914363443851471, 0.007897484116256237, 0.29064658284187317]\n",
      "Step 509, loss = [0.28316107392311096, 0.011271046474575996, 0.2820339798927307]\n",
      "Step 510, loss = [0.30847522616386414, 0.006748814135789871, 0.3078003525733948]\n",
      "Step 511, loss = [0.2809690833091736, 0.00812564603984356, 0.28015652298927307]\n",
      "Step 512, loss = [0.2898196876049042, 0.008295658975839615, 0.2889901101589203]\n",
      "Step 513, loss = [0.3049842417240143, 0.012957796454429626, 0.3036884665489197]\n",
      "Step 514, loss = [0.30272170901298523, 0.006221030373126268, 0.3020996153354645]\n",
      "Step 515, loss = [0.29977673292160034, 0.003394811414182186, 0.2994372546672821]\n",
      "Step 516, loss = [0.30913424491882324, 0.004805821925401688, 0.30865365266799927]\n",
      "Step 517, loss = [0.3009781539440155, 0.002222292125225067, 0.30075591802597046]\n",
      "Step 518, loss = [0.29125574231147766, 0.009511944837868214, 0.2903045415878296]\n",
      "Step 519, loss = [0.3009517788887024, 0.006362637504935265, 0.3003155291080475]\n",
      "Step 520, loss = [0.2860747277736664, 0.009031897410750389, 0.2851715385913849]\n",
      "Step 521, loss = [0.2981913387775421, 0.0026771032717078924, 0.29792362451553345]\n",
      "Step 522, loss = [0.2981082797050476, 0.0046363635919988155, 0.29764464497566223]\n",
      "Step 523, loss = [0.2947380244731903, 0.008040925487875938, 0.2939339280128479]\n",
      "Step 524, loss = [0.3178010880947113, 0.004478973802179098, 0.31735318899154663]\n",
      "Step 525, loss = [0.2981749475002289, 0.009133962914347649, 0.2972615659236908]\n",
      "Step 526, loss = [0.3013168275356293, 0.006142518483102322, 0.3007025718688965]\n",
      "Step 527, loss = [0.2881852090358734, 0.006232063286006451, 0.2875620126724243]\n",
      "Step 528, loss = [0.28688278794288635, 0.004318617284297943, 0.28645092248916626]\n",
      "Step 529, loss = [0.3096877634525299, 0.010529091581702232, 0.30863484740257263]\n",
      "Step 530, loss = [0.30387163162231445, 0.007041657343506813, 0.303167462348938]\n",
      "Step 531, loss = [0.3062475025653839, 0.008514635264873505, 0.30539605021476746]\n",
      "Step 532, loss = [0.2904585301876068, 0.010706724599003792, 0.28938785195350647]\n",
      "Step 533, loss = [0.29349201917648315, 0.006011532619595528, 0.29289087653160095]\n",
      "Step 534, loss = [0.3054257035255432, 0.007923465222120285, 0.30463334918022156]\n",
      "Step 535, loss = [0.31151968240737915, 0.011200983077287674, 0.31039959192276]\n",
      "Step 536, loss = [0.30271926522254944, 0.0031789836939424276, 0.3024013638496399]\n",
      "Step 537, loss = [0.3023722469806671, 0.008938196115195751, 0.30147841572761536]\n",
      "Step 538, loss = [0.3050179183483124, 0.00851706974208355, 0.30416619777679443]\n",
      "Step 539, loss = [0.306714802980423, 0.0053687142208218575, 0.30617794394493103]\n",
      "Step 540, loss = [0.29243841767311096, 0.0071274894289672375, 0.29172566533088684]\n",
      "Step 541, loss = [0.2816597819328308, 0.00871726032346487, 0.2807880640029907]\n",
      "Step 542, loss = [0.28476929664611816, 0.0043549793772399426, 0.2843337953090668]\n",
      "Step 543, loss = [0.3026336133480072, 0.003169258125126362, 0.30231669545173645]\n",
      "Step 544, loss = [0.311306893825531, 0.0023515159264206886, 0.3110717535018921]\n",
      "Step 545, loss = [0.2929519712924957, 0.008036715909838676, 0.29214829206466675]\n",
      "Step 546, loss = [0.29915422201156616, 0.006979178637266159, 0.2984563112258911]\n",
      "Step 547, loss = [0.28193002939224243, 0.007294537965208292, 0.2812005877494812]\n",
      "Step 548, loss = [0.3099178671836853, 0.006390195805579424, 0.30927884578704834]\n",
      "Step 549, loss = [0.30422770977020264, 0.005693600978702307, 0.3036583364009857]\n",
      "Step 550, loss = [0.2921295166015625, 0.007279525510966778, 0.29140156507492065]\n",
      "Step 551, loss = [0.31000855565071106, 0.0036824035923928022, 0.3096403181552887]\n",
      "Step 552, loss = [0.30214008688926697, 0.003770191455259919, 0.30176305770874023]\n",
      "Step 553, loss = [0.2969197630882263, 0.005433537997305393, 0.29637640714645386]\n",
      "Step 554, loss = [0.286945641040802, 0.008523009717464447, 0.2860933542251587]\n",
      "Step 555, loss = [0.29490071535110474, 0.007477173116058111, 0.29415300488471985]\n",
      "Step 556, loss = [0.2855775058269501, 0.004879774525761604, 0.28508952260017395]\n",
      "Step 557, loss = [0.284721314907074, 0.006751623935997486, 0.28404614329338074]\n",
      "Step 558, loss = [0.3124968707561493, 0.007420871406793594, 0.3117547929286957]\n",
      "Step 559, loss = [0.306634783744812, 0.005987165495753288, 0.3060360550880432]\n",
      "Step 560, loss = [0.30095434188842773, 0.004939435049891472, 0.3004603981971741]\n",
      "Step 561, loss = [0.30677345395088196, 0.010046100243926048, 0.30576884746551514]\n",
      "Step 562, loss = [0.3090287744998932, 0.007720602676272392, 0.30825671553611755]\n",
      "Step 563, loss = [0.30606088042259216, 0.008839167654514313, 0.3051769733428955]\n",
      "Step 564, loss = [0.2932748794555664, 0.009979438968002796, 0.29227694869041443]\n",
      "Step 565, loss = [0.2964886724948883, 0.007097558118402958, 0.29577893018722534]\n",
      "Step 566, loss = [0.2880428433418274, 0.0041057756170630455, 0.2876322567462921]\n",
      "Step 567, loss = [0.30612796545028687, 0.00802670232951641, 0.30532529950141907]\n",
      "Step 568, loss = [0.2999340891838074, 0.010336419567465782, 0.29890045523643494]\n",
      "Step 569, loss = [0.29527491331100464, 0.009153347462415695, 0.29435956478118896]\n",
      "Step 570, loss = [0.30491432547569275, 0.007466726005077362, 0.30416765809059143]\n",
      "Step 571, loss = [0.28590887784957886, 0.004873110447078943, 0.28542158007621765]\n",
      "Step 572, loss = [0.30048269033432007, 0.010915459133684635, 0.29939115047454834]\n",
      "Step 573, loss = [0.3051185607910156, 0.009614359587430954, 0.3041571378707886]\n",
      "Step 574, loss = [0.3044353127479553, 0.007048027589917183, 0.3037305176258087]\n",
      "Step 575, loss = [0.29455921053886414, 0.007818300276994705, 0.29377737641334534]\n",
      "Step 576, loss = [0.30092179775238037, 0.00717239361256361, 0.3002045452594757]\n",
      "Step 577, loss = [0.29585346579551697, 0.007048255763947964, 0.295148640871048]\n",
      "Step 578, loss = [0.29216277599334717, 0.007657638285309076, 0.29139700531959534]\n",
      "Step 579, loss = [0.2897661328315735, 0.007326055318117142, 0.28903353214263916]\n",
      "Step 580, loss = [0.28688737750053406, 0.008905325084924698, 0.28599685430526733]\n",
      "Step 581, loss = [0.30668482184410095, 0.005042305216193199, 0.30618059635162354]\n",
      "Step 582, loss = [0.2981744408607483, 0.0056427158415317535, 0.29761016368865967]\n",
      "Step 583, loss = [0.29461541771888733, 0.0030624489299952984, 0.29430916905403137]\n",
      "Step 584, loss = [0.2987932562828064, 0.007136614993214607, 0.29807958006858826]\n",
      "Step 585, loss = [0.271477609872818, 0.009291626513004303, 0.27054843306541443]\n",
      "Step 586, loss = [0.28678613901138306, 0.00833019521087408, 0.2859531342983246]\n",
      "Step 587, loss = [0.30889350175857544, 0.008572882041335106, 0.308036208152771]\n",
      "Step 588, loss = [0.2947690784931183, 0.013362497091293335, 0.2934328317642212]\n",
      "Step 589, loss = [0.30225279927253723, 0.0037480357568711042, 0.3018780052661896]\n",
      "Step 590, loss = [0.3045463562011719, 0.006133196875452995, 0.3039330244064331]\n",
      "Step 591, loss = [0.2899327576160431, 0.012690925039350986, 0.2886636555194855]\n",
      "Step 592, loss = [0.29478147625923157, 0.012622561305761337, 0.2935192286968231]\n",
      "Step 593, loss = [0.29666370153427124, 0.007475340273231268, 0.2959161698818207]\n",
      "Step 594, loss = [0.2841702401638031, 0.004607056267559528, 0.2837095260620117]\n",
      "Step 595, loss = [0.2995200455188751, 0.007247339002788067, 0.29879531264305115]\n",
      "Step 596, loss = [0.28668922185897827, 0.004049777053296566, 0.2862842381000519]\n",
      "Step 597, loss = [0.3005489110946655, 0.0054006148129701614, 0.3000088632106781]\n",
      "Step 598, loss = [0.2946516275405884, 0.0033395709469914436, 0.29431766271591187]\n",
      "Step 599, loss = [0.30362606048583984, 0.008898179978132248, 0.3027362525463104]\n",
      "Step 600, loss = [0.3011099100112915, 0.0039482684805989265, 0.3007150888442993]\n",
      "Step 601, loss = [0.3019123375415802, 0.007486407645046711, 0.3011637032032013]\n",
      "Step 602, loss = [0.2979092299938202, 0.008957699872553349, 0.29701346158981323]\n",
      "Step 603, loss = [0.30377551913261414, 0.006750437896698713, 0.30310046672821045]\n",
      "Step 604, loss = [0.29993006587028503, 0.007866276428103447, 0.2991434335708618]\n",
      "Step 605, loss = [0.29981204867362976, 0.004936533514410257, 0.2993184030056]\n",
      "Step 606, loss = [0.3122194707393646, 0.006655271630734205, 0.311553955078125]\n",
      "Step 607, loss = [0.2977049648761749, 0.012661180458962917, 0.2964388430118561]\n",
      "Step 608, loss = [0.30243581533432007, 0.010881899856030941, 0.30134761333465576]\n",
      "Step 609, loss = [0.29098615050315857, 0.00715784914791584, 0.2902703583240509]\n",
      "Step 610, loss = [0.3012564182281494, 0.013693437911570072, 0.2998870611190796]\n",
      "Step 611, loss = [0.2975251376628876, 0.009110839106142521, 0.29661405086517334]\n",
      "Step 612, loss = [0.30389368534088135, 0.004908034112304449, 0.3034028708934784]\n",
      "Step 613, loss = [0.2980281114578247, 0.0069816638715565205, 0.29732993245124817]\n",
      "Step 614, loss = [0.29879534244537354, 0.006830423139035702, 0.29811230301856995]\n",
      "Step 615, loss = [0.3056921660900116, 0.01018044725060463, 0.3046741187572479]\n",
      "Step 616, loss = [0.3103518784046173, 0.006132655311375856, 0.3097386062145233]\n",
      "Step 617, loss = [0.30974143743515015, 0.007016878575086594, 0.30903974175453186]\n",
      "Step 618, loss = [0.29957816004753113, 0.0046713463962078094, 0.2991110384464264]\n",
      "Step 619, loss = [0.30066531896591187, 0.007660385221242905, 0.29989928007125854]\n",
      "Step 620, loss = [0.30927640199661255, 0.00414462573826313, 0.30886194109916687]\n",
      "Step 621, loss = [0.29723304510116577, 0.00938913132995367, 0.29629412293434143]\n",
      "Step 622, loss = [0.29466524720191956, 0.014362755231559277, 0.29322898387908936]\n",
      "Step 623, loss = [0.3103112280368805, 0.009787413291633129, 0.3093324899673462]\n",
      "Step 624, loss = [0.2900390923023224, 0.005109999794512987, 0.28952810168266296]\n",
      "Step 625, loss = [0.30041250586509705, 0.00595824234187603, 0.29981666803359985]\n",
      "Step 626, loss = [0.30972912907600403, 0.010060256347060204, 0.3087230920791626]\n",
      "Step 627, loss = [0.30330556631088257, 0.009111263789236546, 0.30239444971084595]\n",
      "Step 628, loss = [0.3020433783531189, 0.005983164068311453, 0.3014450669288635]\n",
      "Step 629, loss = [0.3071066439151764, 0.00909490417689085, 0.3061971664428711]\n",
      "Step 630, loss = [0.2976140081882477, 0.008870013989508152, 0.2967270016670227]\n",
      "Step 631, loss = [0.3044975697994232, 0.0116359181702137, 0.30333396792411804]\n",
      "Step 632, loss = [0.3074279725551605, 0.0054655601270496845, 0.3068814277648926]\n",
      "Step 633, loss = [0.31035926938056946, 0.01197209395468235, 0.30916205048561096]\n",
      "Step 634, loss = [0.31156882643699646, 0.008835040032863617, 0.31068533658981323]\n",
      "Step 635, loss = [0.3058169186115265, 0.008381279185414314, 0.30497878789901733]\n",
      "Step 636, loss = [0.2781192660331726, 0.0073139420710504055, 0.2773878574371338]\n",
      "Step 637, loss = [0.29854288697242737, 0.0038596303202211857, 0.2981569170951843]\n",
      "Step 638, loss = [0.29288381338119507, 0.0075538610108196735, 0.29212841391563416]\n",
      "Step 639, loss = [0.29953527450561523, 0.0044573824852705, 0.29908955097198486]\n",
      "Step 640, loss = [0.2918868958950043, 0.007798578590154648, 0.29110702872276306]\n",
      "Step 641, loss = [0.28842538595199585, 0.008714241907000542, 0.28755396604537964]\n",
      "Step 642, loss = [0.3013538420200348, 0.009726433083415031, 0.30038121342658997]\n",
      "Step 643, loss = [0.3024975657463074, 0.006481260526925325, 0.3018494248390198]\n",
      "Step 644, loss = [0.31034982204437256, 0.005999273620545864, 0.30974990129470825]\n",
      "Step 645, loss = [0.306389182806015, 0.005529845133423805, 0.30583620071411133]\n",
      "Step 646, loss = [0.2867622375488281, 0.006188998930156231, 0.28614333271980286]\n",
      "Step 647, loss = [0.29244083166122437, 0.007879666052758694, 0.2916528582572937]\n",
      "Step 648, loss = [0.28858762979507446, 0.009162629954516888, 0.28767135739326477]\n",
      "Step 649, loss = [0.3136596083641052, 0.009740585461258888, 0.3126855492591858]\n",
      "Step 650, loss = [0.3033376932144165, 0.005381619557738304, 0.3027995228767395]\n",
      "Step 651, loss = [0.29565688967704773, 0.0044728540815413, 0.2952096164226532]\n",
      "Step 652, loss = [0.2871599793434143, 0.005398626439273357, 0.2866201102733612]\n",
      "Step 653, loss = [0.30675944685935974, 0.006555949337780476, 0.3061038553714752]\n",
      "Step 654, loss = [0.2932889759540558, 0.0039047112222760916, 0.2928985059261322]\n",
      "Step 655, loss = [0.28968924283981323, 0.006186609622091055, 0.28907057642936707]\n",
      "Step 656, loss = [0.3101898729801178, 0.015249773859977722, 0.30866488814353943]\n",
      "Step 657, loss = [0.29736432433128357, 0.008250979706645012, 0.29653921723365784]\n",
      "Step 658, loss = [0.2969127893447876, 0.004611075855791569, 0.2964516878128052]\n",
      "Step 659, loss = [0.28570640087127686, 0.006557484157383442, 0.2850506603717804]\n",
      "Step 660, loss = [0.2953008711338043, 0.015472388826310635, 0.29375362396240234]\n",
      "Step 661, loss = [0.3002903461456299, 0.0041757551953196526, 0.2998727560043335]\n",
      "Step 662, loss = [0.29244837164878845, 0.002665276639163494, 0.2921818494796753]\n",
      "Step 663, loss = [0.2894408404827118, 0.009791288524866104, 0.28846171498298645]\n",
      "Step 664, loss = [0.3034219741821289, 0.007049215026199818, 0.30271705985069275]\n",
      "Step 665, loss = [0.29325902462005615, 0.0011930065229535103, 0.2931397259235382]\n",
      "Step 666, loss = [0.29076138138771057, 0.005606275983154774, 0.29020074009895325]\n",
      "Step 667, loss = [0.30695199966430664, 0.006619329564273357, 0.30629006028175354]\n",
      "Step 668, loss = [0.3075932264328003, 0.01163426972925663, 0.30642980337142944]\n",
      "Step 669, loss = [0.2978229820728302, 0.004875233396887779, 0.2973354458808899]\n",
      "Step 670, loss = [0.3087281584739685, 0.012339195236563683, 0.30749425292015076]\n",
      "Step 671, loss = [0.3017657697200775, 0.0033496327232569456, 0.3014307916164398]\n",
      "Step 672, loss = [0.31075599789619446, 0.008454694412648678, 0.30991053581237793]\n",
      "Step 673, loss = [0.2986676096916199, 0.0080324187874794, 0.2978643774986267]\n",
      "Step 674, loss = [0.28468990325927734, 0.00602375902235508, 0.28408753871917725]\n",
      "Step 675, loss = [0.31362491846084595, 0.008085612207651138, 0.3128163516521454]\n",
      "Step 676, loss = [0.29836389422416687, 0.005602858494967222, 0.2978036105632782]\n",
      "Step 677, loss = [0.3076874017715454, 0.004443883430212736, 0.3072430193424225]\n",
      "Step 678, loss = [0.2915111482143402, 0.0018351140897721052, 0.2913276255130768]\n",
      "Step 679, loss = [0.3005144000053406, 0.006326752714812756, 0.2998817265033722]\n",
      "Step 680, loss = [0.29400870203971863, 0.0035919209476560354, 0.2936495244503021]\n",
      "Step 681, loss = [0.3079797923564911, 0.010727094486355782, 0.3069070875644684]\n",
      "Step 682, loss = [0.2998114824295044, 0.004549722652882338, 0.29935652017593384]\n",
      "Step 683, loss = [0.30271658301353455, 0.005511540919542313, 0.3021654188632965]\n",
      "Step 684, loss = [0.2882096469402313, 0.008447959087789059, 0.2873648405075073]\n",
      "Step 685, loss = [0.3073258697986603, 0.006674863398075104, 0.30665838718414307]\n",
      "Step 686, loss = [0.30517497658729553, 0.0026784769725054502, 0.3049071431159973]\n",
      "Step 687, loss = [0.29850107431411743, 0.005676383152604103, 0.297933429479599]\n",
      "Step 688, loss = [0.29287320375442505, 0.005398604087531567, 0.29233333468437195]\n",
      "Step 689, loss = [0.29577913880348206, 0.0036066435277462006, 0.29541847109794617]\n",
      "Step 690, loss = [0.3038770854473114, 0.005232484545558691, 0.30335384607315063]\n",
      "Step 691, loss = [0.31035280227661133, 0.003756473772227764, 0.30997714400291443]\n",
      "Step 692, loss = [0.2908431589603424, 0.005599110387265682, 0.2902832627296448]\n",
      "Step 693, loss = [0.30712512135505676, 0.02665753662586212, 0.3044593632221222]\n",
      "Step 694, loss = [0.29902222752571106, 0.012521266005933285, 0.29777011275291443]\n",
      "Step 695, loss = [0.30478784441947937, 0.007376979570835829, 0.30405014753341675]\n",
      "Step 696, loss = [0.30735406279563904, 0.0029996742960065603, 0.3070541024208069]\n",
      "Step 697, loss = [0.30478087067604065, 0.007068556733429432, 0.3040740191936493]\n",
      "Step 698, loss = [0.2991316318511963, 0.003231397829949856, 0.2988084852695465]\n",
      "Step 699, loss = [0.29863861203193665, 0.004333574790507555, 0.29820525646209717]\n",
      "Step 700, loss = [0.3093832731246948, 0.007145348936319351, 0.30866873264312744]\n",
      "Step 701, loss = [0.29404163360595703, 0.012818697839975357, 0.2927597761154175]\n",
      "Step 702, loss = [0.29637178778648376, 0.010060464963316917, 0.29536575078964233]\n",
      "Step 703, loss = [0.2783805727958679, 0.010635064914822578, 0.277317076921463]\n",
      "Step 704, loss = [0.29697126150131226, 0.0023164371959865093, 0.2967396080493927]\n",
      "Step 705, loss = [0.30630436539649963, 0.007868234068155289, 0.3055175542831421]\n",
      "Step 706, loss = [0.3063875734806061, 0.0016628948505967855, 0.30622127652168274]\n",
      "Step 707, loss = [0.2919176518917084, 0.006191845051944256, 0.2912984788417816]\n",
      "Step 708, loss = [0.2984430491924286, 0.0017042511608451605, 0.29827260971069336]\n",
      "Step 709, loss = [0.30476897954940796, 0.00925279501825571, 0.3038437068462372]\n",
      "Step 710, loss = [0.307468444108963, 0.006975086871534586, 0.306770920753479]\n",
      "Step 711, loss = [0.275679349899292, 0.008204413577914238, 0.27485892176628113]\n",
      "Step 712, loss = [0.3148385286331177, 0.009184042923152447, 0.31392011046409607]\n",
      "Step 713, loss = [0.30034923553466797, 0.01008499227464199, 0.29934072494506836]\n",
      "Step 714, loss = [0.28886327147483826, 0.005704138427972794, 0.28829285502433777]\n",
      "Step 715, loss = [0.29591473937034607, 0.006857120431959629, 0.2952290177345276]\n",
      "Step 716, loss = [0.30679991841316223, 0.009139779023826122, 0.3058859407901764]\n",
      "Step 717, loss = [0.30578792095184326, 0.011725186370313168, 0.30461540818214417]\n",
      "Step 718, loss = [0.28915154933929443, 0.00929504819214344, 0.2882220447063446]\n",
      "Step 719, loss = [0.31190967559814453, 0.004658110905438662, 0.31144386529922485]\n",
      "Step 720, loss = [0.30722716450691223, 0.005503376014530659, 0.30667683482170105]\n",
      "Step 721, loss = [0.3014015555381775, 0.009293362498283386, 0.300472229719162]\n",
      "Step 722, loss = [0.2956991195678711, 0.003998951055109501, 0.295299232006073]\n",
      "Step 723, loss = [0.31283068656921387, 0.004430482164025307, 0.3123876452445984]\n",
      "Step 724, loss = [0.27505964040756226, 0.019164729863405228, 0.2731431722640991]\n",
      "Step 725, loss = [0.3092942535877228, 0.007795702666044235, 0.30851468443870544]\n",
      "Step 726, loss = [0.3029418885707855, 0.01241065002977848, 0.3017008304595947]\n",
      "Step 727, loss = [0.3060576021671295, 0.007062249816954136, 0.3053513765335083]\n",
      "Step 728, loss = [0.3119146525859833, 0.006380455568432808, 0.3112766146659851]\n",
      "Step 729, loss = [0.2909438908100128, 0.0030784804839640856, 0.2906360328197479]\n",
      "Step 730, loss = [0.292959600687027, 0.0069017489440739155, 0.29226943850517273]\n",
      "Step 731, loss = [0.3007306158542633, 0.009876989759504795, 0.2997429072856903]\n",
      "Step 732, loss = [0.27619078755378723, 0.00543435662984848, 0.2756473422050476]\n",
      "Step 733, loss = [0.3015732169151306, 0.0036802194081246853, 0.30120518803596497]\n",
      "Step 734, loss = [0.3054945468902588, 0.00808295700699091, 0.3046862483024597]\n",
      "Step 735, loss = [0.29218363761901855, 0.004663018975406885, 0.2917173504829407]\n",
      "Step 736, loss = [0.3013894855976105, 0.009400291368365288, 0.3004494607448578]\n",
      "Step 737, loss = [0.3132363259792328, 0.005177865736186504, 0.31271854043006897]\n",
      "Step 738, loss = [0.30499130487442017, 0.007687595207244158, 0.30422255396842957]\n",
      "Step 739, loss = [0.30376312136650085, 0.006237973924726248, 0.303139328956604]\n",
      "Step 740, loss = [0.3061496615409851, 0.006086412817239761, 0.3055410087108612]\n",
      "Step 741, loss = [0.2981322407722473, 0.00522894412279129, 0.2976093590259552]\n",
      "Step 742, loss = [0.2917206287384033, 0.0022748447954654694, 0.29149314761161804]\n",
      "Step 743, loss = [0.31005534529685974, 0.005839662626385689, 0.30947136878967285]\n",
      "Step 744, loss = [0.306584894657135, 0.006833656225353479, 0.30590152740478516]\n",
      "Step 745, loss = [0.30022555589675903, 0.013613414019346237, 0.2988642156124115]\n",
      "Step 746, loss = [0.29406511783599854, 0.002377233700826764, 0.2938273847103119]\n",
      "Step 747, loss = [0.30950385332107544, 0.011308923363685608, 0.30837297439575195]\n",
      "Step 748, loss = [0.3015615940093994, 0.00427279993891716, 0.301134318113327]\n",
      "Step 749, loss = [0.3008734881877899, 0.0037335967645049095, 0.30050012469291687]\n",
      "Step 750, loss = [0.30675560235977173, 0.005073650740087032, 0.3062482476234436]\n",
      "Step 751, loss = [0.29623332619667053, 0.004472360014915466, 0.2957860827445984]\n",
      "Step 752, loss = [0.2904028594493866, 0.006700238212943077, 0.28973284363746643]\n",
      "Step 753, loss = [0.2966332733631134, 0.010164577513933182, 0.2956168055534363]\n",
      "Step 754, loss = [0.3052440583705902, 0.009664664044976234, 0.30427759885787964]\n",
      "Step 755, loss = [0.28610193729400635, 0.005118603352457285, 0.2855900824069977]\n",
      "Step 756, loss = [0.3142389953136444, 0.011025656014680862, 0.31313642859458923]\n",
      "Step 757, loss = [0.29605090618133545, 0.010911560617387295, 0.29495975375175476]\n",
      "Step 758, loss = [0.2999197542667389, 0.007270669098943472, 0.2991926968097687]\n",
      "Step 759, loss = [0.3010041117668152, 0.006880583707243204, 0.30031606554985046]\n",
      "Step 760, loss = [0.31190401315689087, 0.004999223630875349, 0.3114040791988373]\n",
      "Step 761, loss = [0.3003228008747101, 0.00354163465090096, 0.2999686300754547]\n",
      "Step 762, loss = [0.2935000956058502, 0.009405253455042839, 0.29255956411361694]\n",
      "Step 763, loss = [0.28242161870002747, 0.007949871942400932, 0.2816266417503357]\n",
      "Step 764, loss = [0.28206512331962585, 0.010266336612403393, 0.2810384929180145]\n",
      "Step 765, loss = [0.2912042737007141, 0.004887439776211977, 0.2907155156135559]\n",
      "Step 766, loss = [0.2976744771003723, 0.013116272166371346, 0.29636284708976746]\n",
      "Step 767, loss = [0.2962743043899536, 0.0024371477775275707, 0.29603058099746704]\n",
      "Step 768, loss = [0.3029102087020874, 0.005939466878771782, 0.30231624841690063]\n",
      "Step 769, loss = [0.2869773805141449, 0.005703192204236984, 0.2864070534706116]\n",
      "Step 770, loss = [0.3074548840522766, 0.008601068519055843, 0.3065947890281677]\n",
      "Step 771, loss = [0.3020946979522705, 0.006058371625840664, 0.30148884654045105]\n",
      "Step 772, loss = [0.30721986293792725, 0.009252779185771942, 0.30629459023475647]\n",
      "Step 773, loss = [0.3105943500995636, 0.006979060359299183, 0.30989643931388855]\n",
      "Step 774, loss = [0.30527371168136597, 0.008471248671412468, 0.3044265806674957]\n",
      "Step 775, loss = [0.30199843645095825, 0.007731053978204727, 0.30122533440589905]\n",
      "Step 776, loss = [0.3005397319793701, 0.00789354182779789, 0.2997503876686096]\n",
      "Step 777, loss = [0.3102518618106842, 0.006071250420063734, 0.3096447288990021]\n",
      "Step 778, loss = [0.30120569467544556, 0.008554192259907722, 0.30035027861595154]\n",
      "Step 779, loss = [0.2828097641468048, 0.007848234847187996, 0.28202494978904724]\n",
      "Step 780, loss = [0.2872600555419922, 0.0056680114939808846, 0.2866932451725006]\n",
      "Step 781, loss = [0.3079184293746948, 0.006455474998801947, 0.30727288126945496]\n",
      "Step 782, loss = [0.301725834608078, 0.005564789287745953, 0.30116936564445496]\n",
      "Step 783, loss = [0.3038454055786133, 0.003579218639060855, 0.30348747968673706]\n",
      "Step 784, loss = [0.3114628493785858, 0.00503204669803381, 0.3109596371650696]\n",
      "Step 785, loss = [0.3008049428462982, 0.005089337006211281, 0.30029600858688354]\n",
      "Step 786, loss = [0.3004828095436096, 0.006203607656061649, 0.29986244440078735]\n",
      "Step 787, loss = [0.2991320788860321, 0.005246084649115801, 0.2986074686050415]\n",
      "Step 788, loss = [0.296363890171051, 0.009278204292058945, 0.2954360842704773]\n",
      "Step 789, loss = [0.31091707944869995, 0.004220617935061455, 0.3104950189590454]\n",
      "Step 790, loss = [0.29266297817230225, 0.008641532622277737, 0.29179883003234863]\n",
      "Step 791, loss = [0.30358561873435974, 0.00522354431450367, 0.3030632734298706]\n",
      "Step 792, loss = [0.29323989152908325, 0.0065060085617005825, 0.2925892770290375]\n",
      "Step 793, loss = [0.3092634081840515, 0.002123974496498704, 0.3090510070323944]\n",
      "Step 794, loss = [0.3081516921520233, 0.0037280735559761524, 0.30777889490127563]\n",
      "Step 795, loss = [0.30944931507110596, 0.005405051168054342, 0.3089088201522827]\n",
      "Step 796, loss = [0.2816411852836609, 0.002047379733994603, 0.2814364433288574]\n",
      "Step 797, loss = [0.2908162474632263, 0.005830001085996628, 0.2902332544326782]\n",
      "Step 798, loss = [0.29534193873405457, 0.0039000711403787136, 0.2949519455432892]\n",
      "Step 799, loss = [0.287239134311676, 0.00611508684232831, 0.2866276204586029]\n",
      "Step 800, loss = [0.29078489542007446, 0.009428005665540695, 0.2898420989513397]\n",
      "Step 801, loss = [0.28922513127326965, 0.005653006955981255, 0.28865984082221985]\n",
      "Step 802, loss = [0.30917033553123474, 0.01024308055639267, 0.30814602971076965]\n",
      "Step 803, loss = [0.2965555489063263, 0.002971664536744356, 0.2962583899497986]\n",
      "Step 804, loss = [0.3013237416744232, 0.008942896500229836, 0.30042946338653564]\n",
      "Step 805, loss = [0.2927790582180023, 0.0045981574803590775, 0.29231923818588257]\n",
      "Step 806, loss = [0.29786795377731323, 0.005415288731455803, 0.2973264157772064]\n",
      "Step 807, loss = [0.29768335819244385, 0.0054988316260278225, 0.2971334755420685]\n",
      "Step 808, loss = [0.2967316210269928, 0.011008565314114094, 0.2956307530403137]\n",
      "Step 809, loss = [0.30870047211647034, 0.005910023115575314, 0.30810946226119995]\n",
      "Step 810, loss = [0.31118205189704895, 0.005676470696926117, 0.3106144070625305]\n",
      "Step 811, loss = [0.3008207678794861, 0.005792748183012009, 0.30024150013923645]\n",
      "Step 812, loss = [0.3094111979007721, 0.004712144378572702, 0.30893999338150024]\n",
      "Step 813, loss = [0.28748977184295654, 0.005071788094937801, 0.28698259592056274]\n",
      "Step 814, loss = [0.305166631937027, 0.004279593471437693, 0.3047386705875397]\n",
      "Step 815, loss = [0.29000696539878845, 0.008315087296068668, 0.28917545080184937]\n",
      "Step 816, loss = [0.3101317882537842, 0.0037245023995637894, 0.30975934863090515]\n",
      "Step 817, loss = [0.308916836977005, 0.006135380826890469, 0.3083032965660095]\n",
      "Step 818, loss = [0.31747761368751526, 0.005787601228803396, 0.3168988525867462]\n",
      "Step 819, loss = [0.2946878969669342, 0.005910179577767849, 0.2940968871116638]\n",
      "Step 820, loss = [0.30323874950408936, 0.006088436581194401, 0.30262991786003113]\n",
      "Step 821, loss = [0.2931286096572876, 0.01147242821753025, 0.29198136925697327]\n",
      "Step 822, loss = [0.2867741584777832, 0.003071673447266221, 0.2864669859409332]\n",
      "Step 823, loss = [0.29436445236206055, 0.0058764684945344925, 0.29377681016921997]\n",
      "Step 824, loss = [0.2816920280456543, 0.008995591662824154, 0.2807924747467041]\n",
      "Step 825, loss = [0.29824957251548767, 0.007579925004392862, 0.29749158024787903]\n",
      "Step 826, loss = [0.29108723998069763, 0.001933821476995945, 0.2908938527107239]\n",
      "Step 827, loss = [0.30960261821746826, 0.009984955191612244, 0.3086041212081909]\n",
      "Step 828, loss = [0.29335373640060425, 0.005892986431717873, 0.29276442527770996]\n",
      "Step 829, loss = [0.31699249148368835, 0.007503849919885397, 0.3162420988082886]\n",
      "Step 830, loss = [0.30656659603118896, 0.012491251341998577, 0.3053174614906311]\n",
      "Step 831, loss = [0.30855414271354675, 0.009650599211454391, 0.3075890839099884]\n",
      "Step 832, loss = [0.275351345539093, 0.007263299077749252, 0.2746250033378601]\n",
      "Step 833, loss = [0.3050062358379364, 0.006977003999054432, 0.30430853366851807]\n",
      "Step 834, loss = [0.3001909852027893, 0.011063121259212494, 0.2990846633911133]\n",
      "Step 835, loss = [0.3019530475139618, 0.009835494682192802, 0.30096951127052307]\n",
      "Step 836, loss = [0.2956836521625519, 0.005154935177415609, 0.2951681613922119]\n",
      "Step 837, loss = [0.3054976463317871, 0.006534075364470482, 0.3048442304134369]\n",
      "Step 838, loss = [0.29890167713165283, 0.006671639159321785, 0.2982345223426819]\n",
      "Step 839, loss = [0.29155048727989197, 0.010500305332243443, 0.2905004620552063]\n",
      "Step 840, loss = [0.29659754037857056, 0.0032537151128053665, 0.2962721586227417]\n",
      "Step 841, loss = [0.3098078966140747, 0.005436121020466089, 0.30926427245140076]\n",
      "Step 842, loss = [0.3085581660270691, 0.006134110502898693, 0.30794474482536316]\n",
      "Step 843, loss = [0.3021726906299591, 0.008851533755660057, 0.30128753185272217]\n",
      "Step 844, loss = [0.29693737626075745, 0.006717575713992119, 0.2962656319141388]\n",
      "Step 845, loss = [0.3082565367221832, 0.01112181507050991, 0.3071443438529968]\n",
      "Step 846, loss = [0.2956923246383667, 0.006205356679856777, 0.2950717806816101]\n",
      "Step 847, loss = [0.31127429008483887, 0.004969741217792034, 0.31077730655670166]\n",
      "Step 848, loss = [0.3108798861503601, 0.004267535172402859, 0.3104531466960907]\n",
      "Step 849, loss = [0.30358654260635376, 0.0056758662685751915, 0.3030189573764801]\n",
      "Step 850, loss = [0.2910202741622925, 0.005958054214715958, 0.2904244661331177]\n",
      "Step 851, loss = [0.2916189134120941, 0.006947223097085953, 0.29092419147491455]\n",
      "Step 852, loss = [0.30180463194847107, 0.006864766590297222, 0.3011181652545929]\n",
      "Step 853, loss = [0.294369637966156, 0.00756284361705184, 0.29361334443092346]\n",
      "Step 854, loss = [0.2824881672859192, 0.008473947644233704, 0.28164076805114746]\n",
      "Step 855, loss = [0.28575971722602844, 0.005979122128337622, 0.2851617932319641]\n",
      "Step 856, loss = [0.30597639083862305, 0.007401272654533386, 0.30523625016212463]\n",
      "Step 857, loss = [0.2886784076690674, 0.007027367129921913, 0.2879756689071655]\n",
      "Step 858, loss = [0.30264315009117126, 0.004435006063431501, 0.30219966173171997]\n",
      "Step 859, loss = [0.3042677640914917, 0.004589742980897427, 0.3038087785243988]\n",
      "Step 860, loss = [0.30349811911582947, 0.009015971794724464, 0.3025965094566345]\n",
      "Step 861, loss = [0.2922726273536682, 0.003668924793601036, 0.2919057309627533]\n",
      "Step 862, loss = [0.30328214168548584, 0.006833401974290609, 0.3025988042354584]\n",
      "Step 863, loss = [0.29780611395835876, 0.009930816479027271, 0.296813040971756]\n",
      "Step 864, loss = [0.30871888995170593, 0.005230553913861513, 0.3081958293914795]\n",
      "Step 865, loss = [0.30054715275764465, 0.006434032693505287, 0.2999037504196167]\n",
      "Step 866, loss = [0.29845643043518066, 0.006811114959418774, 0.2977753281593323]\n",
      "Step 867, loss = [0.2967069447040558, 0.007051082793623209, 0.2960018217563629]\n",
      "Step 868, loss = [0.29396650195121765, 0.011268221773207188, 0.2928396761417389]\n",
      "Step 869, loss = [0.31745830178260803, 0.0166911743581295, 0.31578919291496277]\n",
      "Step 870, loss = [0.29615873098373413, 0.009908905252814293, 0.29516783356666565]\n",
      "Step 871, loss = [0.30214056372642517, 0.00401048269122839, 0.30173951387405396]\n",
      "Step 872, loss = [0.298448771238327, 0.00607523787766695, 0.29784125089645386]\n",
      "Step 873, loss = [0.29931148886680603, 0.008315193466842175, 0.29847997426986694]\n",
      "Step 874, loss = [0.3105318248271942, 0.005636087618768215, 0.3099682033061981]\n",
      "Step 875, loss = [0.281571626663208, 0.009041426703333855, 0.2806674838066101]\n",
      "Step 876, loss = [0.31444260478019714, 0.007740446832031012, 0.31366854906082153]\n",
      "Step 877, loss = [0.29000139236450195, 0.004170295782387257, 0.28958436846733093]\n",
      "Step 878, loss = [0.290652871131897, 0.005491737276315689, 0.2901037037372589]\n",
      "Step 879, loss = [0.3061407208442688, 0.00597983505576849, 0.3055427372455597]\n",
      "Step 880, loss = [0.3124944567680359, 0.005341349635273218, 0.3119603097438812]\n",
      "Step 881, loss = [0.3070361018180847, 0.006592608522623777, 0.3063768446445465]\n",
      "Step 882, loss = [0.2930135428905487, 0.00473050819709897, 0.2925404906272888]\n",
      "Step 883, loss = [0.2980476915836334, 0.0018609677208587527, 0.29786160588264465]\n",
      "Step 884, loss = [0.3062954843044281, 0.006464713253080845, 0.3056490123271942]\n",
      "Step 885, loss = [0.31076139211654663, 0.006243579555302858, 0.3101370334625244]\n",
      "Step 886, loss = [0.29537081718444824, 0.004791344050318003, 0.29489168524742126]\n",
      "Step 887, loss = [0.30873411893844604, 0.01100627239793539, 0.30763348937034607]\n",
      "Step 888, loss = [0.2984183132648468, 0.007295365445315838, 0.2976887822151184]\n",
      "Step 889, loss = [0.29869523644447327, 0.008006899617612362, 0.29789453744888306]\n",
      "Step 890, loss = [0.29677239060401917, 0.009475825354456902, 0.29582479596138]\n",
      "Step 891, loss = [0.3046184480190277, 0.00784316100180149, 0.30383414030075073]\n",
      "Step 892, loss = [0.31350135803222656, 0.00773541908711195, 0.31272780895233154]\n",
      "Step 893, loss = [0.30728480219841003, 0.0035782509949058294, 0.306926965713501]\n",
      "Step 894, loss = [0.30066412687301636, 0.005474820267409086, 0.3001166582107544]\n",
      "Step 895, loss = [0.3013487458229065, 0.009524643421173096, 0.30039629340171814]\n",
      "Step 896, loss = [0.2978934049606323, 0.00772884301841259, 0.29712051153182983]\n",
      "Step 897, loss = [0.3076724112033844, 0.005688998848199844, 0.3071035146713257]\n",
      "Step 898, loss = [0.2941240668296814, 0.006406784988939762, 0.2934833765029907]\n",
      "Step 899, loss = [0.30309727787971497, 0.011479733511805534, 0.30194929242134094]\n",
      "Step 900, loss = [0.3007449209690094, 0.015965312719345093, 0.2991483807563782]\n",
      "Step 901, loss = [0.3107258081436157, 0.013376355171203613, 0.3093881607055664]\n",
      "Step 902, loss = [0.31190916895866394, 0.010171287693083286, 0.3108920454978943]\n",
      "Step 903, loss = [0.29858899116516113, 0.00582332955673337, 0.29800665378570557]\n",
      "Step 904, loss = [0.30004701018333435, 0.007758737076073885, 0.2992711365222931]\n",
      "Step 905, loss = [0.3049876093864441, 0.004232387989759445, 0.30456435680389404]\n",
      "Step 906, loss = [0.296186625957489, 0.009485457092523575, 0.29523807764053345]\n",
      "Step 907, loss = [0.29782044887542725, 0.004258674569427967, 0.2973945736885071]\n",
      "Step 908, loss = [0.3036700189113617, 0.004235425498336554, 0.30324646830558777]\n",
      "Step 909, loss = [0.30689379572868347, 0.0066263098269701, 0.30623117089271545]\n",
      "Step 910, loss = [0.30880895256996155, 0.009560082107782364, 0.30785295367240906]\n",
      "Step 911, loss = [0.29564955830574036, 0.006561048794537783, 0.29499346017837524]\n",
      "Step 912, loss = [0.2867818772792816, 0.004154430702328682, 0.28636643290519714]\n",
      "Step 913, loss = [0.30614304542541504, 0.007469874806702137, 0.30539605021476746]\n",
      "Step 914, loss = [0.29872703552246094, 0.0072050248272717, 0.298006534576416]\n",
      "Step 915, loss = [0.2991642951965332, 0.005911009386181831, 0.29857319593429565]\n",
      "Step 916, loss = [0.30499836802482605, 0.009161517024040222, 0.3040822148323059]\n",
      "Step 917, loss = [0.3022931218147278, 0.0037627157289534807, 0.30191683769226074]\n",
      "Step 918, loss = [0.3036225438117981, 0.017691515386104584, 0.30185338854789734]\n",
      "Step 919, loss = [0.3001645505428314, 0.007746253162622452, 0.29938992857933044]\n",
      "Step 920, loss = [0.3053267300128937, 0.0027366774156689644, 0.30505305528640747]\n",
      "Step 921, loss = [0.27676838636398315, 0.00662565603852272, 0.2761058211326599]\n",
      "Step 922, loss = [0.2926965355873108, 0.003218816826120019, 0.2923746407032013]\n",
      "Step 923, loss = [0.30027008056640625, 0.008983543142676353, 0.29937171936035156]\n",
      "Step 924, loss = [0.28853946924209595, 0.005036588758230209, 0.2880358099937439]\n",
      "Step 925, loss = [0.30069684982299805, 0.0042385077103972435, 0.30027300119400024]\n",
      "Step 926, loss = [0.3171568810939789, 0.006249993573874235, 0.3165318965911865]\n",
      "Step 927, loss = [0.28542307019233704, 0.0037952617276459932, 0.28504353761672974]\n",
      "Step 928, loss = [0.3010046184062958, 0.009574705734848976, 0.3000471591949463]\n",
      "Step 929, loss = [0.3095962405204773, 0.011456355452537537, 0.3084506094455719]\n",
      "Step 930, loss = [0.28875038027763367, 0.005094014108181, 0.2882409691810608]\n",
      "Step 931, loss = [0.287941575050354, 0.004564482718706131, 0.28748512268066406]\n",
      "Step 932, loss = [0.2868179380893707, 0.007066883146762848, 0.2861112356185913]\n",
      "Step 933, loss = [0.307850182056427, 0.00872921384871006, 0.3069772720336914]\n",
      "Step 934, loss = [0.3059866726398468, 0.006470486521720886, 0.30533963441848755]\n",
      "Step 935, loss = [0.3029577434062958, 0.008023506961762905, 0.30215540528297424]\n",
      "Step 936, loss = [0.28895488381385803, 0.008990142494440079, 0.2880558669567108]\n",
      "Step 937, loss = [0.30235183238983154, 0.003512006951496005, 0.30200064182281494]\n",
      "Step 938, loss = [0.3058723211288452, 0.016330907121300697, 0.3042392432689667]\n",
      "Step 939, loss = [0.29154834151268005, 0.007542601320892572, 0.2907940745353699]\n",
      "Step 940, loss = [0.2980221211910248, 0.005824359133839607, 0.29743969440460205]\n",
      "Step 941, loss = [0.29813462495803833, 0.00684716273099184, 0.29744991660118103]\n",
      "Step 942, loss = [0.2828175127506256, 0.00585499033331871, 0.28223201632499695]\n",
      "Step 943, loss = [0.2838830053806305, 0.006156990770250559, 0.2832673192024231]\n",
      "Step 944, loss = [0.29862910509109497, 0.005490350537002087, 0.29808005690574646]\n",
      "Step 945, loss = [0.2969737648963928, 0.007301904261112213, 0.2962435781955719]\n",
      "Step 946, loss = [0.3097434341907501, 0.005246744025498629, 0.30921876430511475]\n",
      "Step 947, loss = [0.2996247112751007, 0.00719102518633008, 0.298905611038208]\n",
      "Step 948, loss = [0.3074537515640259, 0.00904887355864048, 0.3065488636493683]\n",
      "Step 949, loss = [0.3024899959564209, 0.009318006224930286, 0.3015581965446472]\n",
      "Step 950, loss = [0.3082033097743988, 0.006811654660850763, 0.30752214789390564]\n",
      "Step 951, loss = [0.29775044322013855, 0.0033569135703146458, 0.29741474986076355]\n",
      "Step 952, loss = [0.30365630984306335, 0.010019326582551003, 0.3026543855667114]\n",
      "Step 953, loss = [0.2942544221878052, 0.00938660278916359, 0.2933157682418823]\n",
      "Step 954, loss = [0.3087523281574249, 0.001965707167983055, 0.3085557520389557]\n",
      "Step 955, loss = [0.30364012718200684, 0.009198439307510853, 0.3027202785015106]\n",
      "Step 956, loss = [0.3015313148498535, 0.008135585114359856, 0.3007177710533142]\n",
      "Step 957, loss = [0.3040666878223419, 0.006447918713092804, 0.30342188477516174]\n",
      "Step 958, loss = [0.29792308807373047, 0.007542979903519154, 0.2971687912940979]\n",
      "Step 959, loss = [0.3026685416698456, 0.00986642949283123, 0.30168190598487854]\n",
      "Step 960, loss = [0.30888766050338745, 0.005713209975510836, 0.30831634998321533]\n",
      "Step 961, loss = [0.30694127082824707, 0.007945655845105648, 0.3061467111110687]\n",
      "Step 962, loss = [0.3054259419441223, 0.0030835596844553947, 0.30511757731437683]\n",
      "Step 963, loss = [0.3111132085323334, 0.005413017235696316, 0.31057190895080566]\n",
      "Step 964, loss = [0.30556225776672363, 0.006987411063164473, 0.30486351251602173]\n",
      "Step 965, loss = [0.28713759779930115, 0.0067750513553619385, 0.28646010160446167]\n",
      "Step 966, loss = [0.29712459444999695, 0.007936019450426102, 0.296330988407135]\n",
      "Step 967, loss = [0.2842015027999878, 0.009656822308897972, 0.2832358181476593]\n",
      "Step 968, loss = [0.29268375039100647, 0.004698899574577808, 0.2922138571739197]\n",
      "Step 969, loss = [0.29640913009643555, 0.00739776436239481, 0.2956693470478058]\n",
      "Step 970, loss = [0.30665868520736694, 0.005813184659928083, 0.30607736110687256]\n",
      "Step 971, loss = [0.2910923957824707, 0.005296112969517708, 0.29056277871131897]\n",
      "Step 972, loss = [0.29322218894958496, 0.011099027469754219, 0.2921122908592224]\n",
      "Step 973, loss = [0.28342339396476746, 0.006161811761558056, 0.28280720114707947]\n",
      "Step 974, loss = [0.29357969760894775, 0.006233145482838154, 0.2929563820362091]\n",
      "Step 975, loss = [0.3053068518638611, 0.010350948199629784, 0.30427175760269165]\n",
      "Step 976, loss = [0.2926822006702423, 0.0038713596295565367, 0.29229506850242615]\n",
      "Step 977, loss = [0.29976555705070496, 0.007313622161746025, 0.2990342080593109]\n",
      "Pos. rate:0.37450980392156863 Neg. rate:0.6254901960784314\n",
      "delta_label  0.01582440020418581\n",
      "Step 978, loss = [0.3036894202232361, 0.011676265858113766, 0.30252179503440857]\n",
      "Step 979, loss = [0.3152005076408386, 0.005551137495785952, 0.314645379781723]\n",
      "Step 980, loss = [0.3047626316547394, 0.00937529094517231, 0.30382511019706726]\n",
      "Step 981, loss = [0.2977452874183655, 0.00203392980620265, 0.29754188656806946]\n",
      "Step 982, loss = [0.3020491302013397, 0.008247070014476776, 0.30122441053390503]\n",
      "Step 983, loss = [0.29289236664772034, 0.008127576671540737, 0.2920795977115631]\n",
      "Step 984, loss = [0.3058231472969055, 0.003909510560333729, 0.30543220043182373]\n",
      "Step 985, loss = [0.297698974609375, 0.0026015560142695904, 0.2974388301372528]\n",
      "Step 986, loss = [0.30117085576057434, 0.011330246925354004, 0.30003783106803894]\n",
      "Step 987, loss = [0.31074967980384827, 0.007037506438791752, 0.31004592776298523]\n",
      "Step 988, loss = [0.2960802912712097, 0.0069886757992208, 0.29538142681121826]\n",
      "Step 989, loss = [0.2860475182533264, 0.008304083719849586, 0.2852171063423157]\n",
      "Step 990, loss = [0.30258241295814514, 0.007065223064273596, 0.30187588930130005]\n",
      "Step 991, loss = [0.3079847991466522, 0.003628017846494913, 0.3076219856739044]\n",
      "Step 992, loss = [0.31051477789878845, 0.008423486724495888, 0.30967241525650024]\n",
      "Step 993, loss = [0.3003259599208832, 0.004219417925924063, 0.2999040186405182]\n",
      "Step 994, loss = [0.3072986900806427, 0.0070048244670033455, 0.3065982162952423]\n",
      "Step 995, loss = [0.29078078269958496, 0.004657529294490814, 0.29031503200531006]\n",
      "Step 996, loss = [0.2896293103694916, 0.011088860221207142, 0.2885204255580902]\n",
      "Step 997, loss = [0.3039061427116394, 0.00533314049243927, 0.3033728301525116]\n",
      "Step 998, loss = [0.3018278181552887, 0.002687303815037012, 0.30155909061431885]\n",
      "Step 999, loss = [0.303642213344574, 0.007529653608798981, 0.30288925766944885]\n",
      "Step 1000, loss = [0.2991691827774048, 0.009113532491028309, 0.29825782775878906]\n",
      "Step 1001, loss = [0.30812475085258484, 0.007134539540857077, 0.3074112832546234]\n",
      "Step 1002, loss = [0.299451619386673, 0.0044343797490000725, 0.29900819063186646]\n",
      "Step 1003, loss = [0.2947207987308502, 0.004221760667860508, 0.29429861903190613]\n",
      "Step 1004, loss = [0.31124043464660645, 0.004415140021592379, 0.31079891324043274]\n",
      "Step 1005, loss = [0.29845380783081055, 0.006591593846678734, 0.2977946400642395]\n",
      "Step 1006, loss = [0.3021482825279236, 0.008673352189362049, 0.3012809455394745]\n",
      "Step 1007, loss = [0.30207011103630066, 0.005254614166915417, 0.3015446364879608]\n",
      "Step 1008, loss = [0.2976979613304138, 0.005332176573574543, 0.2971647381782532]\n",
      "Step 1009, loss = [0.29578572511672974, 0.0063723307102918625, 0.29514849185943604]\n",
      "Step 1010, loss = [0.29673826694488525, 0.008309238590300083, 0.2959073483943939]\n",
      "Step 1011, loss = [0.30262210965156555, 0.009478598833084106, 0.3016742467880249]\n",
      "Step 1012, loss = [0.29927003383636475, 0.0027021425776183605, 0.2989998161792755]\n",
      "Step 1013, loss = [0.3091074824333191, 0.007367959711700678, 0.3083706796169281]\n",
      "Step 1014, loss = [0.31099680066108704, 0.008197513408958912, 0.3101770579814911]\n",
      "Step 1015, loss = [0.31531110405921936, 0.008550025522708893, 0.31445610523223877]\n",
      "Step 1016, loss = [0.2912837266921997, 0.003601042553782463, 0.2909236252307892]\n",
      "Step 1017, loss = [0.29849135875701904, 0.0071760304272174835, 0.2977737486362457]\n",
      "Step 1018, loss = [0.3057421147823334, 0.008991498500108719, 0.3048429787158966]\n",
      "Step 1019, loss = [0.30511075258255005, 0.00665592635050416, 0.30444514751434326]\n",
      "Step 1020, loss = [0.3056606352329254, 0.00999358482658863, 0.30466127395629883]\n",
      "Step 1021, loss = [0.2919909656047821, 0.005340432748198509, 0.2914569079875946]\n",
      "Step 1022, loss = [0.2828353941440582, 0.010469788685441017, 0.2817884087562561]\n",
      "Step 1023, loss = [0.2874844968318939, 0.00644387723878026, 0.2868401110172272]\n",
      "Step 1024, loss = [0.30884578824043274, 0.006282113026827574, 0.3082175850868225]\n",
      "Step 1025, loss = [0.29349109530448914, 0.005784216802567244, 0.29291266202926636]\n",
      "Step 1026, loss = [0.30101293325424194, 0.006148486398160458, 0.3003980815410614]\n",
      "Step 1027, loss = [0.29197636246681213, 0.011019266210496426, 0.2908744215965271]\n",
      "Step 1028, loss = [0.3113432824611664, 0.006644158624112606, 0.3106788694858551]\n",
      "Step 1029, loss = [0.29929235577583313, 0.006878593936562538, 0.29860448837280273]\n",
      "Step 1030, loss = [0.30240464210510254, 0.009433267638087273, 0.3014613091945648]\n",
      "Step 1031, loss = [0.2934551239013672, 0.0036030339542776346, 0.29309481382369995]\n",
      "Step 1032, loss = [0.2883569300174713, 0.0071707312017679214, 0.287639856338501]\n",
      "Step 1033, loss = [0.3099173903465271, 0.006240236572921276, 0.30929335951805115]\n",
      "Step 1034, loss = [0.3057437539100647, 0.007322364021092653, 0.305011510848999]\n",
      "Step 1035, loss = [0.3009684681892395, 0.005736790597438812, 0.30039480328559875]\n",
      "Step 1036, loss = [0.301893025636673, 0.010129431262612343, 0.3008800745010376]\n",
      "Step 1037, loss = [0.29613110423088074, 0.0073617794550955296, 0.2953949272632599]\n",
      "Step 1038, loss = [0.302876353263855, 0.006858865264803171, 0.3021904528141022]\n",
      "Step 1039, loss = [0.30751025676727295, 0.004950651898980141, 0.30701518058776855]\n",
      "Step 1040, loss = [0.2999270558357239, 0.007004446815699339, 0.2992266118526459]\n",
      "Step 1041, loss = [0.2987184226512909, 0.0038951695896685123, 0.2983289062976837]\n",
      "Step 1042, loss = [0.30538684129714966, 0.002881361171603203, 0.3050987124443054]\n",
      "Step 1043, loss = [0.29000455141067505, 0.005149396602064371, 0.28948959708213806]\n",
      "Step 1044, loss = [0.303404301404953, 0.0072290487587451935, 0.3026813864707947]\n",
      "Step 1045, loss = [0.28138673305511475, 0.005810666363686323, 0.28080567717552185]\n",
      "Step 1046, loss = [0.2981339693069458, 0.006149869412183762, 0.2975189685821533]\n",
      "Step 1047, loss = [0.2977831959724426, 0.007181287743151188, 0.2970650792121887]\n",
      "Step 1048, loss = [0.2880426049232483, 0.009335264563560486, 0.2871090769767761]\n",
      "Step 1049, loss = [0.3123061954975128, 0.010185008868575096, 0.31128770112991333]\n",
      "Step 1050, loss = [0.30771833658218384, 0.010690368711948395, 0.3066492974758148]\n",
      "Step 1051, loss = [0.30580970644950867, 0.009354261681437492, 0.3048742711544037]\n",
      "Step 1052, loss = [0.3017607629299164, 0.00876739714294672, 0.3008840084075928]\n",
      "Step 1053, loss = [0.3145759403705597, 0.004586116410791874, 0.31411734223365784]\n",
      "Step 1054, loss = [0.2968991994857788, 0.009699834510684013, 0.2959292232990265]\n",
      "Step 1055, loss = [0.30548933148384094, 0.009230407886207104, 0.30456629395484924]\n",
      "Step 1056, loss = [0.29782384634017944, 0.004786253906786442, 0.29734522104263306]\n",
      "Step 1057, loss = [0.29554513096809387, 0.006075370591133833, 0.2949375808238983]\n",
      "Step 1058, loss = [0.30597394704818726, 0.004969554953277111, 0.30547699332237244]\n",
      "Step 1059, loss = [0.2947497069835663, 0.0051865833811461926, 0.2942310571670532]\n",
      "Step 1060, loss = [0.29953792691230774, 0.012455826625227928, 0.298292338848114]\n",
      "Step 1061, loss = [0.30587106943130493, 0.0048839859664440155, 0.3053826689720154]\n",
      "Step 1062, loss = [0.3052661716938019, 0.008966910652816296, 0.3043694794178009]\n",
      "Step 1063, loss = [0.3124859631061554, 0.007334673777222633, 0.3117524981498718]\n",
      "Step 1064, loss = [0.28502821922302246, 0.006063861772418022, 0.2844218313694]\n",
      "Step 1065, loss = [0.3145720958709717, 0.00731401052325964, 0.31384068727493286]\n",
      "Step 1066, loss = [0.3101377785205841, 0.007940743118524551, 0.30934369564056396]\n",
      "Step 1067, loss = [0.2910267412662506, 0.002204940188676119, 0.29080623388290405]\n",
      "Step 1068, loss = [0.2925918698310852, 0.00440390408039093, 0.29215148091316223]\n",
      "Step 1069, loss = [0.2886942923069, 0.009051784873008728, 0.28778910636901855]\n",
      "Step 1070, loss = [0.2906043529510498, 0.009138820692896843, 0.28969046473503113]\n",
      "Step 1071, loss = [0.3008923828601837, 0.0020519515965133905, 0.30068719387054443]\n",
      "Step 1072, loss = [0.30027058720588684, 0.00902482308447361, 0.29936811327934265]\n",
      "Step 1073, loss = [0.3074811100959778, 0.003967566415667534, 0.3070843517780304]\n",
      "Step 1074, loss = [0.28444117307662964, 0.005175372585654259, 0.2839236259460449]\n",
      "Step 1075, loss = [0.3122310936450958, 0.0071083358488976955, 0.3115202486515045]\n",
      "Step 1076, loss = [0.3006431460380554, 0.010018463246524334, 0.29964131116867065]\n",
      "Step 1077, loss = [0.29249879717826843, 0.009037377312779427, 0.29159507155418396]\n",
      "Step 1078, loss = [0.29474836587905884, 0.005381112918257713, 0.2942102551460266]\n",
      "Step 1079, loss = [0.3109818994998932, 0.0045872582122683525, 0.3105231821537018]\n",
      "Step 1080, loss = [0.3042429983615875, 0.0071606263518333435, 0.30352693796157837]\n",
      "Step 1081, loss = [0.3066211938858032, 0.006507343612611294, 0.3059704601764679]\n",
      "Step 1082, loss = [0.30698975920677185, 0.007397872861474752, 0.3062499761581421]\n",
      "Step 1083, loss = [0.3008972406387329, 0.0064008054323494434, 0.30025714635849]\n",
      "Step 1084, loss = [0.2983887791633606, 0.010204785503447056, 0.29736828804016113]\n",
      "Step 1085, loss = [0.31090450286865234, 0.008552148938179016, 0.31004929542541504]\n",
      "Step 1086, loss = [0.28773462772369385, 0.006505926139652729, 0.28708404302597046]\n",
      "Step 1087, loss = [0.30273401737213135, 0.007494899444282055, 0.3019845187664032]\n",
      "Step 1088, loss = [0.28381118178367615, 0.011843062937259674, 0.2826268672943115]\n",
      "Step 1089, loss = [0.3073824346065521, 0.00444272393360734, 0.30693817138671875]\n",
      "Step 1090, loss = [0.3003799319267273, 0.004762846045196056, 0.29990366101264954]\n",
      "Step 1091, loss = [0.2951793074607849, 0.008488250896334648, 0.2943304777145386]\n",
      "Step 1092, loss = [0.31013888120651245, 0.006314704194664955, 0.3095073997974396]\n",
      "Step 1093, loss = [0.28854864835739136, 0.005715843290090561, 0.28797706961631775]\n",
      "Step 1094, loss = [0.3020196557044983, 0.005863674450665712, 0.3014332950115204]\n",
      "Step 1095, loss = [0.285687118768692, 0.005530154332518578, 0.28513410687446594]\n",
      "Step 1096, loss = [0.3026425242424011, 0.0082665104418993, 0.30181586742401123]\n",
      "Step 1097, loss = [0.3054966628551483, 0.005138452164828777, 0.3049828112125397]\n",
      "Step 1098, loss = [0.29853522777557373, 0.005748526193201542, 0.2979603707790375]\n",
      "Step 1099, loss = [0.2945268750190735, 0.00758385518565774, 0.2937684953212738]\n",
      "Step 1100, loss = [0.30236852169036865, 0.0044628637842834, 0.3019222319126129]\n",
      "Step 1101, loss = [0.29749542474746704, 0.005821750964969397, 0.2969132363796234]\n",
      "Step 1102, loss = [0.30303525924682617, 0.006903229281306267, 0.30234494805336]\n",
      "Step 1103, loss = [0.31116101145744324, 0.005574915558099747, 0.310603529214859]\n",
      "Step 1104, loss = [0.29630714654922485, 0.007934180088341236, 0.29551371932029724]\n",
      "Step 1105, loss = [0.29484131932258606, 0.009234687313437462, 0.29391786456108093]\n",
      "Step 1106, loss = [0.27610957622528076, 0.004675580188632011, 0.2756420075893402]\n",
      "Step 1107, loss = [0.31474146246910095, 0.009671378880739212, 0.31377431750297546]\n",
      "Step 1108, loss = [0.2996848225593567, 0.003401283174753189, 0.2993446886539459]\n",
      "Step 1109, loss = [0.29976892471313477, 0.008683980442583561, 0.2989005148410797]\n",
      "Step 1110, loss = [0.30887216329574585, 0.0049348860047757626, 0.308378666639328]\n",
      "Step 1111, loss = [0.3035237193107605, 0.006043547764420509, 0.3029193580150604]\n",
      "Step 1112, loss = [0.29919102787971497, 0.007726073730736971, 0.29841843247413635]\n",
      "Step 1113, loss = [0.31314894556999207, 0.008375768549740314, 0.3123113811016083]\n",
      "Step 1114, loss = [0.29567182064056396, 0.004786296747624874, 0.2951931953430176]\n",
      "Step 1115, loss = [0.3076459467411041, 0.007414652034640312, 0.30690449476242065]\n",
      "Step 1116, loss = [0.31751111149787903, 0.008952239528298378, 0.31661587953567505]\n",
      "Step 1117, loss = [0.31489133834838867, 0.009771151468157768, 0.3139142096042633]\n",
      "Step 1118, loss = [0.2955595850944519, 0.009184367954730988, 0.2946411371231079]\n",
      "Step 1119, loss = [0.31534743309020996, 0.005171968601644039, 0.3148302435874939]\n",
      "Step 1120, loss = [0.3025836646556854, 0.002671699970960617, 0.30231648683547974]\n",
      "Step 1121, loss = [0.27926310896873474, 0.011560974642634392, 0.27810701727867126]\n",
      "Step 1122, loss = [0.2922515869140625, 0.003987472038716078, 0.29185283184051514]\n",
      "Step 1123, loss = [0.30447497963905334, 0.008238537237048149, 0.3036511242389679]\n",
      "Step 1124, loss = [0.29743707180023193, 0.004389073699712753, 0.29699817299842834]\n",
      "Step 1125, loss = [0.30271071195602417, 0.005483623594045639, 0.3021623492240906]\n",
      "Step 1126, loss = [0.2870228886604309, 0.004012467339634895, 0.286621630191803]\n",
      "Step 1127, loss = [0.301679402589798, 0.0064345113933086395, 0.30103594064712524]\n",
      "Step 1128, loss = [0.29894793033599854, 0.0075966971926391125, 0.2981882691383362]\n",
      "Step 1129, loss = [0.31075501441955566, 0.010489383712410927, 0.30970606207847595]\n",
      "Step 1130, loss = [0.30265742540359497, 0.005156547762453556, 0.3021417558193207]\n",
      "Step 1131, loss = [0.28293606638908386, 0.010344717651605606, 0.28190159797668457]\n",
      "Step 1132, loss = [0.2919512391090393, 0.004222432151436806, 0.29152899980545044]\n",
      "Step 1133, loss = [0.3081924617290497, 0.007226865738630295, 0.30746978521347046]\n",
      "Step 1134, loss = [0.31159356236457825, 0.005748114548623562, 0.31101876497268677]\n",
      "Step 1135, loss = [0.27551501989364624, 0.011903800070285797, 0.2743246257305145]\n",
      "Step 1136, loss = [0.2933630347251892, 0.012731555849313736, 0.29208987951278687]\n",
      "Step 1137, loss = [0.3106446862220764, 0.00458349846303463, 0.31018632650375366]\n",
      "Step 1138, loss = [0.28357750177383423, 0.0054646991193294525, 0.28303104639053345]\n",
      "Step 1139, loss = [0.3007710874080658, 0.005337798967957497, 0.3002372980117798]\n",
      "Step 1140, loss = [0.29910096526145935, 0.009798936545848846, 0.29812106490135193]\n",
      "Step 1141, loss = [0.3118305802345276, 0.008301627822220325, 0.31100040674209595]\n",
      "Step 1142, loss = [0.29246965050697327, 0.006269426085054874, 0.2918426990509033]\n",
      "Step 1143, loss = [0.3078553080558777, 0.006631825584918261, 0.3071921169757843]\n",
      "Step 1144, loss = [0.3051498234272003, 0.004640776664018631, 0.3046857416629791]\n",
      "Step 1145, loss = [0.3120054006576538, 0.009666616097092628, 0.3110387325286865]\n",
      "Step 1146, loss = [0.3022313416004181, 0.003794403513893485, 0.30185189843177795]\n",
      "Step 1147, loss = [0.2842656373977661, 0.008400407619774342, 0.28342559933662415]\n",
      "Step 1148, loss = [0.30102330446243286, 0.005708243697881699, 0.30045247077941895]\n",
      "Step 1149, loss = [0.30618298053741455, 0.009004848077893257, 0.30528250336647034]\n",
      "Step 1150, loss = [0.29770296812057495, 0.00420676963403821, 0.29728227853775024]\n",
      "Step 1151, loss = [0.30398043990135193, 0.006342848762869835, 0.3033461570739746]\n",
      "Step 1152, loss = [0.2991490960121155, 0.010092033073306084, 0.29813989996910095]\n",
      "Step 1153, loss = [0.29897311329841614, 0.0068717822432518005, 0.29828593134880066]\n",
      "Step 1154, loss = [0.30655884742736816, 0.006863818503916264, 0.30587247014045715]\n",
      "Step 1155, loss = [0.29207444190979004, 0.006505861412733793, 0.29142385721206665]\n",
      "Step 1156, loss = [0.2857600748538971, 0.007805180735886097, 0.28497955203056335]\n",
      "Step 1157, loss = [0.29767340421676636, 0.00585655402392149, 0.29708775877952576]\n",
      "Step 1158, loss = [0.2972649037837982, 0.004798966459929943, 0.29678499698638916]\n",
      "Step 1159, loss = [0.2996257245540619, 0.0017889832379296422, 0.29944682121276855]\n",
      "Step 1160, loss = [0.3044658899307251, 0.006163810379803181, 0.3038495182991028]\n",
      "Step 1161, loss = [0.30190786719322205, 0.011964255012571812, 0.300711452960968]\n",
      "Step 1162, loss = [0.30165624618530273, 0.005721378140151501, 0.30108410120010376]\n",
      "Step 1163, loss = [0.3144334554672241, 0.012180490419268608, 0.3132154047489166]\n",
      "Step 1164, loss = [0.2922385334968567, 0.007447130978107452, 0.29149383306503296]\n",
      "Step 1165, loss = [0.3053729832172394, 0.008165376260876656, 0.3045564591884613]\n",
      "Step 1166, loss = [0.312562495470047, 0.0031126474495977163, 0.3122512400150299]\n",
      "Step 1167, loss = [0.3021661639213562, 0.004300438798964024, 0.30173611640930176]\n",
      "Step 1168, loss = [0.2987437844276428, 0.010776115581393242, 0.29766616225242615]\n",
      "Step 1169, loss = [0.3065824508666992, 0.006874220911413431, 0.30589503049850464]\n",
      "Step 1170, loss = [0.3079657554626465, 0.006156939081847668, 0.3073500692844391]\n",
      "Step 1171, loss = [0.3083987534046173, 0.006231235805898905, 0.307775616645813]\n",
      "Step 1172, loss = [0.30389708280563354, 0.009861243888735771, 0.3029109537601471]\n",
      "Step 1173, loss = [0.29453790187835693, 0.002904077060520649, 0.29424750804901123]\n",
      "Step 1174, loss = [0.3120540976524353, 0.010846988297998905, 0.31096941232681274]\n",
      "Step 1175, loss = [0.3020821213722229, 0.003413528436794877, 0.30174076557159424]\n",
      "Step 1176, loss = [0.30655744671821594, 0.0017961581470444798, 0.3063778281211853]\n",
      "Step 1177, loss = [0.30361589789390564, 0.010232373140752316, 0.3025926649570465]\n",
      "Step 1178, loss = [0.30271822214126587, 0.005208437331020832, 0.3021973669528961]\n",
      "Step 1179, loss = [0.2941286265850067, 0.005893873982131481, 0.29353922605514526]\n",
      "Step 1180, loss = [0.3047809302806854, 0.00331136048771441, 0.30444979667663574]\n",
      "Step 1181, loss = [0.3086923062801361, 0.010704884305596352, 0.3076218068599701]\n",
      "Step 1182, loss = [0.3125471770763397, 0.0074254474602639675, 0.3118046224117279]\n",
      "Step 1183, loss = [0.29987069964408875, 0.006170890759676695, 0.2992536127567291]\n",
      "Step 1184, loss = [0.2969331741333008, 0.0069802189245820045, 0.2962351441383362]\n",
      "Step 1185, loss = [0.2851082384586334, 0.006550200283527374, 0.28445321321487427]\n",
      "Step 1186, loss = [0.2851888835430145, 0.007611391134560108, 0.2844277322292328]\n",
      "Step 1187, loss = [0.2932503819465637, 0.004871975164860487, 0.2927631735801697]\n",
      "Step 1188, loss = [0.30499476194381714, 0.006080783437937498, 0.3043866753578186]\n",
      "Step 1189, loss = [0.30214738845825195, 0.008160892874002457, 0.3013313114643097]\n",
      "Step 1190, loss = [0.3125954270362854, 0.005159389227628708, 0.3120794892311096]\n",
      "Step 1191, loss = [0.28583824634552, 0.0027187829837203026, 0.2855663597583771]\n",
      "Step 1192, loss = [0.294613778591156, 0.005675674416124821, 0.29404622316360474]\n",
      "Step 1193, loss = [0.31164100766181946, 0.0052370731718838215, 0.3111172914505005]\n",
      "Step 1194, loss = [0.2911430895328522, 0.005825913976877928, 0.2905604839324951]\n",
      "Step 1195, loss = [0.30186834931373596, 0.0075988867320120335, 0.3011084496974945]\n",
      "Step 1196, loss = [0.3027346730232239, 0.0050879125483334064, 0.30222588777542114]\n",
      "Step 1197, loss = [0.30852505564689636, 0.006814937107264996, 0.30784356594085693]\n",
      "Step 1198, loss = [0.3075406551361084, 0.0050967480055987835, 0.30703097581863403]\n",
      "Step 1199, loss = [0.2987670302391052, 0.016633611172437668, 0.2971036732196808]\n",
      "Step 1200, loss = [0.2910926043987274, 0.00519392266869545, 0.29057320952415466]\n",
      "Step 1201, loss = [0.30088961124420166, 0.007629735395312309, 0.3001266419887543]\n",
      "Step 1202, loss = [0.2961936891078949, 0.009636137634515762, 0.29523006081581116]\n",
      "Step 1203, loss = [0.2994316816329956, 0.007467981427907944, 0.29868489503860474]\n",
      "Step 1204, loss = [0.2779141366481781, 0.003403622657060623, 0.27757376432418823]\n",
      "Step 1205, loss = [0.29515597224235535, 0.00447715912014246, 0.294708251953125]\n",
      "Step 1206, loss = [0.2974459230899811, 0.008864441886544228, 0.29655948281288147]\n",
      "Step 1207, loss = [0.2807691693305969, 0.0071631669998168945, 0.2800528407096863]\n",
      "Step 1208, loss = [0.2644517421722412, 0.006572979036718607, 0.2637944519519806]\n",
      "Step 1209, loss = [0.3010150194168091, 0.0039601800963282585, 0.3006190061569214]\n",
      "Step 1210, loss = [0.31132107973098755, 0.004186884965747595, 0.3109023869037628]\n",
      "Step 1211, loss = [0.29002460837364197, 0.00834142230451107, 0.28919047117233276]\n",
      "Step 1212, loss = [0.2930225431919098, 0.01003171131014824, 0.2920193672180176]\n",
      "Step 1213, loss = [0.31230926513671875, 0.00961347483098507, 0.31134793162345886]\n",
      "Step 1214, loss = [0.3016318082809448, 0.0037233508192002773, 0.30125948786735535]\n",
      "Step 1215, loss = [0.3049863576889038, 0.009736509993672371, 0.3040127158164978]\n",
      "Step 1216, loss = [0.29937538504600525, 0.0052075982093811035, 0.29885461926460266]\n",
      "Step 1217, loss = [0.28207430243492126, 0.011585574597120285, 0.2809157371520996]\n",
      "Step 1218, loss = [0.2984541058540344, 0.005442775320261717, 0.29790982604026794]\n",
      "Step 1219, loss = [0.296099990606308, 0.0044693658128380775, 0.2956530451774597]\n",
      "Step 1220, loss = [0.3041946589946747, 0.01108650490641594, 0.3030860126018524]\n",
      "Step 1221, loss = [0.30322709679603577, 0.006003634072840214, 0.30262672901153564]\n",
      "Step 1222, loss = [0.31153178215026855, 0.005225582513958216, 0.3110092282295227]\n",
      "Step 1223, loss = [0.28986144065856934, 0.00942491739988327, 0.28891894221305847]\n",
      "Step 1224, loss = [0.30216342210769653, 0.00803640391677618, 0.30135977268218994]\n",
      "Step 1225, loss = [0.3092610836029053, 0.005379498470574617, 0.308723121881485]\n",
      "Step 1226, loss = [0.2824955880641937, 0.007744086906313896, 0.28172117471694946]\n",
      "Step 1227, loss = [0.28235670924186707, 0.006988042034208775, 0.2816579043865204]\n",
      "Step 1228, loss = [0.2894035577774048, 0.008666489273309708, 0.2885369062423706]\n",
      "Step 1229, loss = [0.29803138971328735, 0.00301038078032434, 0.29773035645484924]\n",
      "Step 1230, loss = [0.290134072303772, 0.008214038796722889, 0.2893126606941223]\n",
      "Step 1231, loss = [0.3133541941642761, 0.004256182350218296, 0.31292858719825745]\n",
      "Step 1232, loss = [0.31480705738067627, 0.010789165273308754, 0.3137281537055969]\n",
      "Step 1233, loss = [0.3085048198699951, 0.009803377091884613, 0.3075244724750519]\n",
      "Step 1234, loss = [0.30657535791397095, 0.007473350036889315, 0.3058280348777771]\n",
      "Step 1235, loss = [0.31046971678733826, 0.0064034974202513695, 0.30982935428619385]\n",
      "Step 1236, loss = [0.3138568103313446, 0.005010072607547045, 0.31335580348968506]\n",
      "Step 1237, loss = [0.2816888689994812, 0.014112349599599838, 0.28027763962745667]\n",
      "Step 1238, loss = [0.29814693331718445, 0.003207971341907978, 0.2978261411190033]\n",
      "Step 1239, loss = [0.30389830470085144, 0.010652022436261177, 0.30283311009407043]\n",
      "Step 1240, loss = [0.30402493476867676, 0.003474198281764984, 0.3036775290966034]\n",
      "Step 1241, loss = [0.30257007479667664, 0.00477749016135931, 0.3020923137664795]\n",
      "Step 1242, loss = [0.3012202978134155, 0.008191544562578201, 0.30040115118026733]\n",
      "Step 1243, loss = [0.31092631816864014, 0.00560060515999794, 0.3103662431240082]\n",
      "Step 1244, loss = [0.30825892090797424, 0.006179677322506905, 0.307640939950943]\n",
      "Step 1245, loss = [0.30259689688682556, 0.001644787727855146, 0.30243241786956787]\n",
      "Step 1246, loss = [0.3058454096317291, 0.005560343619436026, 0.3052893877029419]\n",
      "Step 1247, loss = [0.31176915764808655, 0.010441750288009644, 0.31072497367858887]\n",
      "Step 1248, loss = [0.2946424186229706, 0.009258686564862728, 0.29371654987335205]\n",
      "Step 1249, loss = [0.296718567609787, 0.005096989683806896, 0.29620885848999023]\n",
      "Step 1250, loss = [0.30372220277786255, 0.007109888829290867, 0.3030112087726593]\n",
      "Step 1251, loss = [0.30369898676872253, 0.003726945724338293, 0.303326278924942]\n",
      "Step 1252, loss = [0.31073033809661865, 0.005057510454207659, 0.31022459268569946]\n",
      "Step 1253, loss = [0.29932481050491333, 0.007411655969917774, 0.29858365654945374]\n",
      "Step 1254, loss = [0.30774030089378357, 0.003670323407277465, 0.3073732554912567]\n",
      "Step 1255, loss = [0.2732018828392029, 0.004558898508548737, 0.2727459967136383]\n",
      "Step 1256, loss = [0.31436285376548767, 0.003341109724715352, 0.3140287399291992]\n",
      "Step 1257, loss = [0.30016499757766724, 0.006444142200052738, 0.2995205819606781]\n",
      "Step 1258, loss = [0.30228015780448914, 0.00430289888754487, 0.3018498718738556]\n",
      "Step 1259, loss = [0.29642534255981445, 0.006335679441690445, 0.29579177498817444]\n",
      "Step 1260, loss = [0.2987814247608185, 0.006954743526875973, 0.2980859577655792]\n",
      "Step 1261, loss = [0.3152101933956146, 0.00781091907992959, 0.3144291043281555]\n",
      "Step 1262, loss = [0.30866575241088867, 0.0089055635035038, 0.30777519941329956]\n",
      "Step 1263, loss = [0.3066639304161072, 0.006185377947986126, 0.30604538321495056]\n",
      "Step 1264, loss = [0.306841105222702, 0.00764258299022913, 0.30607685446739197]\n",
      "Step 1265, loss = [0.2979595363140106, 0.002860770095139742, 0.29767346382141113]\n",
      "Step 1266, loss = [0.2861441671848297, 0.008983440697193146, 0.2852458357810974]\n",
      "Step 1267, loss = [0.2851114273071289, 0.009979967027902603, 0.28411343693733215]\n",
      "Step 1268, loss = [0.3006618022918701, 0.009020928293466568, 0.29975971579551697]\n",
      "Step 1269, loss = [0.3108833134174347, 0.012369203381240368, 0.3096463978290558]\n",
      "Step 1270, loss = [0.30156636238098145, 0.004254025872796774, 0.3011409640312195]\n",
      "Step 1271, loss = [0.30567997694015503, 0.008147921413183212, 0.30486518144607544]\n",
      "Step 1272, loss = [0.28169262409210205, 0.007966771721839905, 0.2808959484100342]\n",
      "Step 1273, loss = [0.29717954993247986, 0.0053371405228972435, 0.296645849943161]\n",
      "Step 1274, loss = [0.30036231875419617, 0.0047030323185026646, 0.29989200830459595]\n",
      "Step 1275, loss = [0.2999216914176941, 0.008500801399350166, 0.29907160997390747]\n",
      "Step 1276, loss = [0.30097994208335876, 0.006970956921577454, 0.3002828359603882]\n",
      "Step 1277, loss = [0.29703468084335327, 0.004788998048752546, 0.2965557873249054]\n",
      "Step 1278, loss = [0.27797412872314453, 0.004592834040522575, 0.27751484513282776]\n",
      "Step 1279, loss = [0.30058756470680237, 0.004536508582532406, 0.30013391375541687]\n",
      "Step 1280, loss = [0.3074202835559845, 0.0035960909444838762, 0.30706068873405457]\n",
      "Step 1281, loss = [0.28759366273880005, 0.00494241900742054, 0.2870994210243225]\n",
      "Step 1282, loss = [0.2985461354255676, 0.006313488818705082, 0.2979147732257843]\n",
      "Step 1283, loss = [0.30391404032707214, 0.008860031142830849, 0.30302804708480835]\n",
      "Step 1284, loss = [0.2853403687477112, 0.001923470525071025, 0.285148024559021]\n",
      "Step 1285, loss = [0.3080051839351654, 0.004975849762558937, 0.30750760436058044]\n",
      "Step 1286, loss = [0.3007928431034088, 0.009431806392967701, 0.29984965920448303]\n",
      "Step 1287, loss = [0.3103366494178772, 0.007381497882306576, 0.30959850549697876]\n",
      "Step 1288, loss = [0.29762959480285645, 0.005591250956058502, 0.2970704734325409]\n",
      "Step 1289, loss = [0.30695003271102905, 0.005691663362085819, 0.30638086795806885]\n",
      "Step 1290, loss = [0.3149307072162628, 0.007384651340544224, 0.3141922354698181]\n",
      "Step 1291, loss = [0.28789007663726807, 0.005892016924917698, 0.28730088472366333]\n",
      "Step 1292, loss = [0.30069392919540405, 0.0051712458953261375, 0.30017679929733276]\n",
      "Step 1293, loss = [0.30115172266960144, 0.003718148684129119, 0.30077990889549255]\n",
      "Step 1294, loss = [0.30524784326553345, 0.006376562640070915, 0.3046101927757263]\n",
      "Step 1295, loss = [0.2980111539363861, 0.008065527305006981, 0.2972046136856079]\n",
      "Step 1296, loss = [0.28934958577156067, 0.010031821206212044, 0.28834640979766846]\n",
      "Step 1297, loss = [0.294742226600647, 0.0031844773329794407, 0.29442378878593445]\n",
      "Step 1298, loss = [0.2780432403087616, 0.007569190580397844, 0.2772863209247589]\n",
      "Step 1299, loss = [0.29565832018852234, 0.0041170744225382805, 0.29524660110473633]\n",
      "Step 1300, loss = [0.28680354356765747, 0.0090900007635355, 0.2858945429325104]\n",
      "Step 1301, loss = [0.29953238368034363, 0.00764708686619997, 0.29876768589019775]\n",
      "Step 1302, loss = [0.2923679053783417, 0.006927646696567535, 0.2916751503944397]\n",
      "Step 1303, loss = [0.2960752844810486, 0.007345477119088173, 0.29534074664115906]\n",
      "Step 1304, loss = [0.2972503900527954, 0.005572647787630558, 0.2966931164264679]\n",
      "Step 1305, loss = [0.3107328414916992, 0.007941939868032932, 0.3099386394023895]\n",
      "Step 1306, loss = [0.2995239496231079, 0.003179789986461401, 0.2992059588432312]\n",
      "Step 1307, loss = [0.31076040863990784, 0.0071105388924479485, 0.3100493550300598]\n",
      "Step 1308, loss = [0.2943441569805145, 0.013089794665575027, 0.2930351793766022]\n",
      "Step 1309, loss = [0.3028225600719452, 0.007069721352308989, 0.3021155893802643]\n",
      "Step 1310, loss = [0.3107810616493225, 0.01038794219493866, 0.309742271900177]\n",
      "Step 1311, loss = [0.3042246103286743, 0.005762491375207901, 0.30364835262298584]\n",
      "Step 1312, loss = [0.29238688945770264, 0.006269803736358881, 0.2917599081993103]\n",
      "Step 1313, loss = [0.295195609331131, 0.0034751305356621742, 0.29484808444976807]\n",
      "Step 1314, loss = [0.28343212604522705, 0.00952235609292984, 0.2824798822402954]\n",
      "Step 1315, loss = [0.29563429951667786, 0.006484056822955608, 0.29498589038848877]\n",
      "Step 1316, loss = [0.3086073696613312, 0.00830315425992012, 0.3077770471572876]\n",
      "Step 1317, loss = [0.30564573407173157, 0.008516579866409302, 0.3047940731048584]\n",
      "Step 1318, loss = [0.3115338385105133, 0.0100486408919096, 0.310528963804245]\n",
      "Step 1319, loss = [0.2755899131298065, 0.004994197282940149, 0.2750904858112335]\n",
      "Step 1320, loss = [0.30938392877578735, 0.0025147891137748957, 0.30913245677948]\n",
      "Step 1321, loss = [0.2976725697517395, 0.00502230878919363, 0.29717034101486206]\n",
      "Step 1322, loss = [0.29615429043769836, 0.011270327493548393, 0.2950272560119629]\n",
      "Step 1323, loss = [0.30413204431533813, 0.004334497265517712, 0.3036985993385315]\n",
      "Step 1324, loss = [0.31041911244392395, 0.0063220784068107605, 0.30978691577911377]\n",
      "Step 1325, loss = [0.2973419725894928, 0.008183803409337997, 0.2965236008167267]\n",
      "Step 1326, loss = [0.3103642165660858, 0.005540263839066029, 0.30981019139289856]\n",
      "Step 1327, loss = [0.29803958535194397, 0.011030668392777443, 0.2969365119934082]\n",
      "Step 1328, loss = [0.2929794490337372, 0.006755870766937733, 0.2923038601875305]\n",
      "Step 1329, loss = [0.293691486120224, 0.004918242804706097, 0.29319965839385986]\n",
      "Step 1330, loss = [0.30210021138191223, 0.0045267511159181595, 0.3016475439071655]\n",
      "Step 1331, loss = [0.31066930294036865, 0.005546212196350098, 0.31011468172073364]\n",
      "Step 1332, loss = [0.30099329352378845, 0.010067632421851158, 0.2999865412712097]\n",
      "Step 1333, loss = [0.2954774796962738, 0.006138081196695566, 0.29486367106437683]\n",
      "Step 1334, loss = [0.29772523045539856, 0.008172140456736088, 0.29690802097320557]\n",
      "Step 1335, loss = [0.2983439266681671, 0.006397746503353119, 0.29770416021347046]\n",
      "Step 1336, loss = [0.2999287545681, 0.00810980424284935, 0.299117773771286]\n",
      "Step 1337, loss = [0.2989928424358368, 0.010235371068120003, 0.2979693114757538]\n",
      "Step 1338, loss = [0.3057156503200531, 0.010555844753980637, 0.30466005206108093]\n",
      "Step 1339, loss = [0.30317991971969604, 0.003499508835375309, 0.3028299808502197]\n",
      "Step 1340, loss = [0.2817053496837616, 0.006632332224398851, 0.2810421288013458]\n",
      "Step 1341, loss = [0.29642409086227417, 0.01067187450826168, 0.2953568994998932]\n",
      "Step 1342, loss = [0.29924246668815613, 0.00612000934779644, 0.2986304759979248]\n",
      "Step 1343, loss = [0.30321764945983887, 0.004716247320175171, 0.3027460277080536]\n",
      "Step 1344, loss = [0.2861975133419037, 0.005164951551705599, 0.28568100929260254]\n",
      "Step 1345, loss = [0.3090456426143646, 0.0018520767334848642, 0.3088604211807251]\n",
      "Step 1346, loss = [0.3066021203994751, 0.003798890858888626, 0.30622223019599915]\n",
      "Step 1347, loss = [0.2920660376548767, 0.008225207217037678, 0.2912435233592987]\n",
      "Step 1348, loss = [0.2923532724380493, 0.007472279481589794, 0.29160603880882263]\n",
      "Step 1349, loss = [0.3131970763206482, 0.005140080116689205, 0.3126830756664276]\n",
      "Step 1350, loss = [0.28959742188453674, 0.007701233960688114, 0.2888273000717163]\n",
      "Step 1351, loss = [0.29594099521636963, 0.004587275441735983, 0.2954822778701782]\n",
      "Step 1352, loss = [0.2991683781147003, 0.0067310631275177, 0.29849526286125183]\n",
      "Step 1353, loss = [0.29241621494293213, 0.005606364458799362, 0.2918555736541748]\n",
      "Step 1354, loss = [0.30609622597694397, 0.010184003040194511, 0.30507782101631165]\n",
      "Step 1355, loss = [0.3066129982471466, 0.00719616562128067, 0.3058933913707733]\n",
      "Step 1356, loss = [0.28695109486579895, 0.00490889698266983, 0.28646019101142883]\n",
      "Step 1357, loss = [0.2945176362991333, 0.0054661305621266365, 0.2939710319042206]\n",
      "Step 1358, loss = [0.3035046458244324, 0.005572977010160685, 0.30294734239578247]\n",
      "Step 1359, loss = [0.2803467810153961, 0.009059891104698181, 0.2794407904148102]\n",
      "Step 1360, loss = [0.3062513470649719, 0.00711454451084137, 0.30553990602493286]\n",
      "Step 1361, loss = [0.2854522168636322, 0.0074369171634316444, 0.28470852971076965]\n",
      "Step 1362, loss = [0.2959420680999756, 0.00891571119427681, 0.2950505018234253]\n",
      "Step 1363, loss = [0.286159485578537, 0.006426727399230003, 0.28551679849624634]\n",
      "Step 1364, loss = [0.28760141134262085, 0.004689985886216164, 0.2871324121952057]\n",
      "Step 1365, loss = [0.29723218083381653, 0.002751885447651148, 0.29695698618888855]\n",
      "Step 1366, loss = [0.284027099609375, 0.007666059769690037, 0.2832604944705963]\n",
      "Step 1367, loss = [0.3041223883628845, 0.004545843228697777, 0.303667813539505]\n",
      "Step 1368, loss = [0.30407077074050903, 0.005016775336116552, 0.30356907844543457]\n",
      "Step 1369, loss = [0.29182010889053345, 0.00902507919818163, 0.29091760516166687]\n",
      "Step 1370, loss = [0.2899954915046692, 0.01047114934772253, 0.2889483869075775]\n",
      "Step 1371, loss = [0.2936497628688812, 0.008591243997216225, 0.29279065132141113]\n",
      "Step 1372, loss = [0.28921231627464294, 0.009388210251927376, 0.28827348351478577]\n",
      "Step 1373, loss = [0.31427210569381714, 0.009287502616643906, 0.313343346118927]\n",
      "Step 1374, loss = [0.29867398738861084, 0.00658834632486105, 0.29801514744758606]\n",
      "Step 1375, loss = [0.3100971579551697, 0.007990910671651363, 0.3092980682849884]\n",
      "Step 1376, loss = [0.30020031332969666, 0.007894324138760567, 0.299410879611969]\n",
      "Step 1377, loss = [0.3054697513580322, 0.010238062590360641, 0.30444595217704773]\n",
      "Step 1378, loss = [0.31561461091041565, 0.008543451316654682, 0.3147602677345276]\n",
      "Step 1379, loss = [0.30471205711364746, 0.005468845367431641, 0.30416518449783325]\n",
      "Step 1380, loss = [0.2971530258655548, 0.004635129123926163, 0.296689510345459]\n",
      "Step 1381, loss = [0.3048962354660034, 0.004124722443521023, 0.3044837713241577]\n",
      "Step 1382, loss = [0.29659247398376465, 0.007232075557112694, 0.29586926102638245]\n",
      "Step 1383, loss = [0.2939470410346985, 0.009959444403648376, 0.2929511070251465]\n",
      "Step 1384, loss = [0.2925492525100708, 0.003983420319855213, 0.29215091466903687]\n",
      "Step 1385, loss = [0.3078176975250244, 0.007694763597100973, 0.3070482313632965]\n",
      "Step 1386, loss = [0.28416118025779724, 0.00990334153175354, 0.2831708490848541]\n",
      "Step 1387, loss = [0.2831074595451355, 0.005038452334702015, 0.2826036214828491]\n",
      "Step 1388, loss = [0.29422739148139954, 0.008341771550476551, 0.29339322447776794]\n",
      "Step 1389, loss = [0.30595558881759644, 0.013269198127090931, 0.3046286702156067]\n",
      "Step 1390, loss = [0.30460435152053833, 0.00883994996547699, 0.3037203550338745]\n",
      "Step 1391, loss = [0.30552056431770325, 0.0025956747122108936, 0.3052609860897064]\n",
      "Step 1392, loss = [0.3112901747226715, 0.008183052763342857, 0.31047186255455017]\n",
      "Step 1393, loss = [0.2906860411167145, 0.007110270205885172, 0.28997501730918884]\n",
      "Step 1394, loss = [0.3019464612007141, 0.003174682380631566, 0.3016290068626404]\n",
      "Step 1395, loss = [0.30506372451782227, 0.004222547635436058, 0.304641455411911]\n",
      "Step 1396, loss = [0.31238317489624023, 0.009528392925858498, 0.31143033504486084]\n",
      "Step 1397, loss = [0.313443660736084, 0.0036486934404820204, 0.31307879090309143]\n",
      "Step 1398, loss = [0.2963164746761322, 0.00777305755764246, 0.29553917050361633]\n",
      "Step 1399, loss = [0.3057202398777008, 0.0059357983991503716, 0.3051266670227051]\n",
      "Step 1400, loss = [0.2942045331001282, 0.004220685455948114, 0.29378247261047363]\n",
      "Step 1401, loss = [0.2837468683719635, 0.007697529159486294, 0.2829771041870117]\n",
      "Step 1402, loss = [0.29118821024894714, 0.007663664873689413, 0.29042184352874756]\n",
      "Step 1403, loss = [0.30766162276268005, 0.0036300532519817352, 0.3072986304759979]\n",
      "Step 1404, loss = [0.3027397096157074, 0.007283024024218321, 0.3020114004611969]\n",
      "Step 1405, loss = [0.28077396750450134, 0.008700378239154816, 0.27990391850471497]\n",
      "Step 1406, loss = [0.3127944767475128, 0.009671228006482124, 0.3118273615837097]\n",
      "Step 1407, loss = [0.305017352104187, 0.003175322664901614, 0.3046998083591461]\n",
      "Step 1408, loss = [0.29617780447006226, 0.004154476802796125, 0.2957623600959778]\n",
      "Step 1409, loss = [0.3128760755062103, 0.004359700717031956, 0.31244009733200073]\n",
      "Step 1410, loss = [0.30114418268203735, 0.007158633321523666, 0.3004283308982849]\n",
      "Step 1411, loss = [0.2946300804615021, 0.005253272596746683, 0.29410475492477417]\n",
      "Step 1412, loss = [0.30696943402290344, 0.006831507198512554, 0.3062862753868103]\n",
      "Step 1413, loss = [0.27837488055229187, 0.007862044498324394, 0.2775886654853821]\n",
      "Step 1414, loss = [0.302631676197052, 0.0034689558669924736, 0.30228477716445923]\n",
      "Step 1415, loss = [0.3003184497356415, 0.0054884906858205795, 0.2997696101665497]\n",
      "Step 1416, loss = [0.28878360986709595, 0.006900819018483162, 0.28809353709220886]\n",
      "Step 1417, loss = [0.29693081974983215, 0.0090427715331316, 0.2960265278816223]\n",
      "Step 1418, loss = [0.27638572454452515, 0.005079540424048901, 0.27587777376174927]\n",
      "Step 1419, loss = [0.3006037771701813, 0.00781747605651617, 0.29982203245162964]\n",
      "Step 1420, loss = [0.30927321314811707, 0.009070436470210552, 0.30836617946624756]\n",
      "Step 1421, loss = [0.3035622239112854, 0.00583710428327322, 0.302978515625]\n",
      "Step 1422, loss = [0.2912595272064209, 0.008198194205760956, 0.2904396951198578]\n",
      "Step 1423, loss = [0.302534282207489, 0.006848037708550692, 0.30184948444366455]\n",
      "Step 1424, loss = [0.3027728199958801, 0.00698152557015419, 0.302074670791626]\n",
      "Step 1425, loss = [0.29918789863586426, 0.007277867756783962, 0.29846012592315674]\n",
      "Step 1426, loss = [0.273885041475296, 0.003025013953447342, 0.2735825479030609]\n",
      "Step 1427, loss = [0.2940472960472107, 0.005668482277542353, 0.29348045587539673]\n",
      "Step 1428, loss = [0.3064054846763611, 0.010566642507910728, 0.30534881353378296]\n",
      "Step 1429, loss = [0.2948404550552368, 0.005900587420910597, 0.29425039887428284]\n",
      "Step 1430, loss = [0.2909553647041321, 0.004161548335105181, 0.2905392050743103]\n",
      "Step 1431, loss = [0.3001716136932373, 0.008602067828178406, 0.29931139945983887]\n",
      "Step 1432, loss = [0.3055795729160309, 0.007006656378507614, 0.30487892031669617]\n",
      "Step 1433, loss = [0.3030238151550293, 0.007310715038329363, 0.30229273438453674]\n",
      "Step 1434, loss = [0.312814861536026, 0.005604228936135769, 0.3122544288635254]\n",
      "Step 1435, loss = [0.2929798364639282, 0.00425060885027051, 0.2925547659397125]\n",
      "Step 1436, loss = [0.3005479872226715, 0.01170375943183899, 0.2993776202201843]\n",
      "Step 1437, loss = [0.3120608925819397, 0.00796065479516983, 0.3112648129463196]\n",
      "Step 1438, loss = [0.30801746249198914, 0.014184301719069481, 0.3065990209579468]\n",
      "Step 1439, loss = [0.3033161163330078, 0.0037083327770233154, 0.3029452860355377]\n",
      "Step 1440, loss = [0.31355029344558716, 0.0054129306226968765, 0.31300899386405945]\n",
      "Step 1441, loss = [0.3048318922519684, 0.004108676221221685, 0.3044210374355316]\n",
      "Step 1442, loss = [0.3106333017349243, 0.005711385514587164, 0.3100621700286865]\n",
      "Step 1443, loss = [0.30134543776512146, 0.007506076246500015, 0.30059483647346497]\n",
      "Step 1444, loss = [0.31088799238204956, 0.007396334782242775, 0.31014835834503174]\n",
      "Step 1445, loss = [0.30192601680755615, 0.00728975236415863, 0.3011970520019531]\n",
      "Step 1446, loss = [0.3028361201286316, 0.005913743749260902, 0.30224475264549255]\n",
      "Step 1447, loss = [0.3113483190536499, 0.006852923892438412, 0.31066301465034485]\n",
      "Step 1448, loss = [0.28234878182411194, 0.006917801685631275, 0.28165701031684875]\n",
      "Step 1449, loss = [0.3079054057598114, 0.005962223280221224, 0.30730918049812317]\n",
      "Step 1450, loss = [0.30503782629966736, 0.01201784797012806, 0.30383604764938354]\n",
      "Step 1451, loss = [0.304286390542984, 0.0022889075335115194, 0.3040575087070465]\n",
      "Step 1452, loss = [0.3009612560272217, 0.009343156591057777, 0.30002695322036743]\n",
      "Step 1453, loss = [0.29768145084381104, 0.007349488325417042, 0.2969464957714081]\n",
      "Step 1454, loss = [0.30831632018089294, 0.006614453159272671, 0.30765488743782043]\n",
      "Step 1455, loss = [0.30540186166763306, 0.005641422234475613, 0.30483773350715637]\n",
      "Step 1456, loss = [0.3114199936389923, 0.006383518688380718, 0.3107816278934479]\n",
      "Step 1457, loss = [0.3106965720653534, 0.005849429871886969, 0.3101116418838501]\n",
      "Step 1458, loss = [0.3169424533843994, 0.008582058362662792, 0.31608423590660095]\n",
      "Step 1459, loss = [0.3112213909626007, 0.006231657229363918, 0.310598224401474]\n",
      "Step 1460, loss = [0.2976437509059906, 0.00449778139591217, 0.2971939742565155]\n",
      "Step 1461, loss = [0.3015652894973755, 0.007765712216496468, 0.3007887303829193]\n",
      "Step 1462, loss = [0.2982081174850464, 0.0048266020603477955, 0.29772546887397766]\n",
      "Step 1463, loss = [0.31180688738822937, 0.003693795297294855, 0.3114375174045563]\n",
      "Step 1464, loss = [0.2905602753162384, 0.007195604965090752, 0.2898407280445099]\n",
      "Step 1465, loss = [0.3113589882850647, 0.00860951654613018, 0.31049802899360657]\n",
      "Step 1466, loss = [0.30268701910972595, 0.006433665286749601, 0.3020436465740204]\n",
      "Pos. rate:0.3862745098039216 Neg. rate:0.6137254901960785\n",
      "delta_label  0.016845329249617153\n",
      "Step 1467, loss = [0.2923926115036011, 0.003829222870990634, 0.2920096814632416]\n",
      "Step 1468, loss = [0.3033392131328583, 0.00515515822917223, 0.3028236925601959]\n",
      "Step 1469, loss = [0.31822824478149414, 0.015796568244695663, 0.31664860248565674]\n",
      "Epoch 0, loss = [0.30106185 0.01958054 0.2991038 ]\n",
      "\n",
      "Start of epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11754 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update target distribution epoch 1 step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11754/11754 [1:51:23<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos. rate:0.38235294117647056 Neg. rate:0.6176470588235294\n",
      "delta_label  0.0064658839543985025\n",
      "Step 0, loss = [0.2969430983066559, 0.0020694027189165354, 0.2967361509799957]\n",
      "Step 1, loss = [0.3024516999721527, 0.00957782007753849, 0.30149391293525696]\n",
      "Step 2, loss = [0.30714958906173706, 0.00803665816783905, 0.3063459098339081]\n",
      "Step 3, loss = [0.3078710734844208, 0.0054796962067484856, 0.3073230981826782]\n",
      "Step 4, loss = [0.29297882318496704, 0.006692391820251942, 0.29230958223342896]\n",
      "Step 5, loss = [0.2881767451763153, 0.0051764328964054585, 0.2876591086387634]\n",
      "Step 6, loss = [0.2810010612010956, 0.012900471687316895, 0.2797110080718994]\n",
      "Step 7, loss = [0.29913222789764404, 0.006043660454452038, 0.29852786660194397]\n",
      "Step 8, loss = [0.2936554253101349, 0.006484492216259241, 0.2930069863796234]\n",
      "Step 9, loss = [0.29942086338996887, 0.003038830356672406, 0.29911696910858154]\n",
      "Step 10, loss = [0.3152118921279907, 0.0013570496812462807, 0.31507617235183716]\n",
      "Step 11, loss = [0.28628525137901306, 0.009439272806048393, 0.2853413224220276]\n",
      "Step 12, loss = [0.30701005458831787, 0.011003573425114155, 0.3059096932411194]\n",
      "Step 13, loss = [0.31352680921554565, 0.007069617509841919, 0.31281983852386475]\n",
      "Step 14, loss = [0.2928881049156189, 0.002962669590488076, 0.2925918400287628]\n",
      "Step 15, loss = [0.2937750816345215, 0.007505550980567932, 0.29302453994750977]\n",
      "Step 16, loss = [0.305099219083786, 0.004252382554113865, 0.304673969745636]\n",
      "Step 17, loss = [0.3029702305793762, 0.004259106703102589, 0.30254432559013367]\n",
      "Step 18, loss = [0.2995772063732147, 0.002756214700639248, 0.2993015944957733]\n",
      "Step 19, loss = [0.3000144362449646, 0.00382504565641284, 0.29963192343711853]\n",
      "Step 20, loss = [0.3033667206764221, 0.004845054820179939, 0.30288222432136536]\n",
      "Step 21, loss = [0.30835601687431335, 0.01174737885594368, 0.3071812689304352]\n",
      "Step 22, loss = [0.30197107791900635, 0.005596852861344814, 0.30141139030456543]\n",
      "Step 23, loss = [0.30362892150878906, 0.013786843977868557, 0.3022502362728119]\n",
      "Step 24, loss = [0.2930888831615448, 0.008033949881792068, 0.2922855019569397]\n",
      "Step 25, loss = [0.296553373336792, 0.0059263999573886395, 0.2959607243537903]\n",
      "Step 26, loss = [0.3102741241455078, 0.0036237468011677265, 0.3099117577075958]\n",
      "Step 27, loss = [0.2990754246711731, 0.00537931127473712, 0.2985374927520752]\n",
      "Step 28, loss = [0.2925155758857727, 0.01156531646847725, 0.2913590371608734]\n",
      "Step 29, loss = [0.30242523550987244, 0.0037582935765385628, 0.3020493984222412]\n",
      "Step 30, loss = [0.30130407214164734, 0.005909815896302462, 0.30071309208869934]\n",
      "Step 31, loss = [0.2967652678489685, 0.007222902961075306, 0.2960429787635803]\n",
      "Step 32, loss = [0.2951273024082184, 0.004863312467932701, 0.2946409583091736]\n",
      "Step 33, loss = [0.303783655166626, 0.02108011767268181, 0.3016756474971771]\n",
      "Step 34, loss = [0.30815833806991577, 0.006622910499572754, 0.307496041059494]\n",
      "Step 35, loss = [0.3101555407047272, 0.0031976806931197643, 0.3098357617855072]\n",
      "Step 36, loss = [0.2990082800388336, 0.0033145907800644636, 0.29867681860923767]\n",
      "Step 37, loss = [0.30645042657852173, 0.006931208074092865, 0.3057573139667511]\n",
      "Step 38, loss = [0.29485884308815, 0.011801382526755333, 0.2936787009239197]\n",
      "Step 39, loss = [0.30543145537376404, 0.0054199229925870895, 0.3048894703388214]\n",
      "Step 40, loss = [0.2806158661842346, 0.004405663348734379, 0.2801752984523773]\n",
      "Step 41, loss = [0.2859223186969757, 0.006283917929977179, 0.28529393672943115]\n",
      "Step 42, loss = [0.2973427474498749, 0.011404984630644321, 0.29620224237442017]\n",
      "Step 43, loss = [0.298431932926178, 0.006942920386791229, 0.29773762822151184]\n",
      "Step 44, loss = [0.28435489535331726, 0.0021640483755618334, 0.2841385006904602]\n",
      "Step 45, loss = [0.30657801032066345, 0.006785382516682148, 0.3058994710445404]\n",
      "Step 46, loss = [0.3094111979007721, 0.006442958489060402, 0.3087669014930725]\n",
      "Step 47, loss = [0.3104279637336731, 0.003915401175618172, 0.31003642082214355]\n",
      "Step 48, loss = [0.3010827302932739, 0.005460014566779137, 0.30053672194480896]\n",
      "Step 49, loss = [0.3105429708957672, 0.006188216153532267, 0.3099241554737091]\n",
      "Step 50, loss = [0.3061982989311218, 0.0067072100937366486, 0.30552756786346436]\n",
      "Step 51, loss = [0.2927047610282898, 0.006858141161501408, 0.29201894998550415]\n",
      "Step 52, loss = [0.3029605448246002, 0.008081695064902306, 0.3021523654460907]\n",
      "Step 53, loss = [0.3119785785675049, 0.0032482733950018883, 0.3116537630558014]\n",
      "Step 54, loss = [0.2892988920211792, 0.006811306346207857, 0.2886177599430084]\n",
      "Step 55, loss = [0.29882439970970154, 0.005513548851013184, 0.2982730567455292]\n",
      "Step 56, loss = [0.29749393463134766, 0.008493074215948582, 0.2966446280479431]\n",
      "Step 57, loss = [0.30067241191864014, 0.008238635957241058, 0.2998485565185547]\n",
      "Step 58, loss = [0.30360114574432373, 0.007501544430851936, 0.30285099148750305]\n",
      "Step 59, loss = [0.30049601197242737, 0.003417777828872204, 0.3001542389392853]\n",
      "Step 60, loss = [0.29900944232940674, 0.008768299594521523, 0.29813259840011597]\n",
      "Step 61, loss = [0.3025454580783844, 0.004608785733580589, 0.3020845651626587]\n",
      "Step 62, loss = [0.287319540977478, 0.005558834411203861, 0.28676366806030273]\n",
      "Step 63, loss = [0.285027414560318, 0.0047474452294409275, 0.284552663564682]\n",
      "Step 64, loss = [0.30612751841545105, 0.005674086511135101, 0.3055601119995117]\n",
      "Step 65, loss = [0.30154669284820557, 0.005013830028474331, 0.301045298576355]\n",
      "Step 66, loss = [0.2891062796115875, 0.00695447064936161, 0.28841084241867065]\n",
      "Step 67, loss = [0.3052961826324463, 0.005532020702958107, 0.3047429919242859]\n",
      "Step 68, loss = [0.3085204064846039, 0.0025917505845427513, 0.3082612454891205]\n",
      "Step 69, loss = [0.30459272861480713, 0.008205197751522064, 0.3037722110748291]\n",
      "Step 70, loss = [0.30381232500076294, 0.005390963517129421, 0.3032732307910919]\n",
      "Step 71, loss = [0.29200980067253113, 0.004426015540957451, 0.29156720638275146]\n",
      "Step 72, loss = [0.28321874141693115, 0.004988991189748049, 0.28271985054016113]\n",
      "Step 73, loss = [0.31376203894615173, 0.005956106819212437, 0.31316643953323364]\n",
      "Step 74, loss = [0.29954203963279724, 0.0029749120585620403, 0.29924455285072327]\n",
      "Step 75, loss = [0.3066684603691101, 0.0036072339862585068, 0.30630773305892944]\n",
      "Step 76, loss = [0.3128727674484253, 0.005548532120883465, 0.3123179078102112]\n",
      "Step 77, loss = [0.29577329754829407, 0.0071418103761971, 0.29505911469459534]\n",
      "Step 78, loss = [0.28647056221961975, 0.0032525104470551014, 0.28614529967308044]\n",
      "Step 79, loss = [0.3114147484302521, 0.006938542239367962, 0.31072089076042175]\n",
      "Step 80, loss = [0.2882729768753052, 0.00991271622478962, 0.28728169202804565]\n",
      "Step 81, loss = [0.2992130517959595, 0.0046224091202020645, 0.2987508177757263]\n",
      "Step 82, loss = [0.309635728597641, 0.007168885786086321, 0.308918833732605]\n",
      "Step 83, loss = [0.30411234498023987, 0.0059551456943154335, 0.30351683497428894]\n",
      "Step 84, loss = [0.298019677400589, 0.006712587550282478, 0.29734840989112854]\n",
      "Step 85, loss = [0.29228976368904114, 0.006236018612980843, 0.2916661500930786]\n",
      "Step 86, loss = [0.3089045584201813, 0.006329759955406189, 0.308271586894989]\n",
      "Step 87, loss = [0.2984534800052643, 0.012280205264687538, 0.2972254455089569]\n",
      "Step 88, loss = [0.2986748516559601, 0.010771197266876698, 0.297597736120224]\n",
      "Step 89, loss = [0.2936702072620392, 0.005552391987293959, 0.29311496019363403]\n",
      "Step 90, loss = [0.2718508839607239, 0.006538659334182739, 0.27119702100753784]\n",
      "Step 91, loss = [0.29947635531425476, 0.0057471804320812225, 0.29890164732933044]\n",
      "Step 92, loss = [0.2921049892902374, 0.005440275650471449, 0.29156094789505005]\n",
      "Step 93, loss = [0.2879832684993744, 0.008686373010277748, 0.28711462020874023]\n",
      "Step 94, loss = [0.3007797300815582, 0.004472005181014538, 0.30033251643180847]\n",
      "Step 95, loss = [0.29016581177711487, 0.004300219006836414, 0.2897357940673828]\n",
      "Step 96, loss = [0.2909935712814331, 0.003411882324144244, 0.29065239429473877]\n",
      "Step 97, loss = [0.310303270816803, 0.004722528159618378, 0.30983102321624756]\n",
      "Step 98, loss = [0.30152973532676697, 0.00784868374466896, 0.3007448613643646]\n",
      "Step 99, loss = [0.30092549324035645, 0.007055643945932388, 0.30021992325782776]\n",
      "Step 100, loss = [0.2942757308483124, 0.006512083113193512, 0.29362452030181885]\n",
      "Step 101, loss = [0.29412195086479187, 0.005665925797075033, 0.293555349111557]\n",
      "Step 102, loss = [0.2914530038833618, 0.005577791947871447, 0.2908952236175537]\n",
      "Step 103, loss = [0.292163610458374, 0.0037920023314654827, 0.291784405708313]\n",
      "Step 104, loss = [0.29233330488204956, 0.008400872349739075, 0.2914932072162628]\n",
      "Step 105, loss = [0.3040461540222168, 0.0044767544604837894, 0.3035984933376312]\n",
      "Step 106, loss = [0.3072202503681183, 0.005321740638464689, 0.3066880702972412]\n",
      "Step 107, loss = [0.2943325936794281, 0.003356743371114135, 0.2939969301223755]\n",
      "Step 108, loss = [0.3087012767791748, 0.0050091673620045185, 0.3082003593444824]\n",
      "Step 109, loss = [0.2965373992919922, 0.005352132488042116, 0.29600217938423157]\n",
      "Step 110, loss = [0.3087950348854065, 0.005111062899231911, 0.3082839250564575]\n",
      "Step 111, loss = [0.30961349606513977, 0.005180311389267445, 0.30909547209739685]\n",
      "Step 112, loss = [0.3058922588825226, 0.008046996779739857, 0.3050875663757324]\n",
      "Step 113, loss = [0.3057662546634674, 0.01283382810652256, 0.3044828772544861]\n",
      "Step 114, loss = [0.2864093482494354, 0.006703579798340797, 0.285739004611969]\n",
      "Step 115, loss = [0.3017054498195648, 0.0038936797063797712, 0.3013160824775696]\n",
      "Step 116, loss = [0.306991308927536, 0.003693055361509323, 0.3066219985485077]\n",
      "Step 117, loss = [0.296132892370224, 0.00821322575211525, 0.2953115701675415]\n",
      "Step 118, loss = [0.2861121892929077, 0.005980237387120724, 0.2855141758918762]\n",
      "Step 119, loss = [0.3038672208786011, 0.004658432211726904, 0.303401380777359]\n",
      "Step 120, loss = [0.3090948760509491, 0.0052755228243768215, 0.3085673153400421]\n",
      "Step 121, loss = [0.3095351755619049, 0.00734281400218606, 0.30880090594291687]\n",
      "Step 122, loss = [0.2815284729003906, 0.007220616564154625, 0.28080642223358154]\n",
      "Step 123, loss = [0.30131426453590393, 0.003194532822817564, 0.3009948134422302]\n",
      "Step 124, loss = [0.2908438742160797, 0.00952034443616867, 0.2898918390274048]\n",
      "Step 125, loss = [0.2948646545410156, 0.004049557261168957, 0.2944597005844116]\n",
      "Step 126, loss = [0.30655279755592346, 0.0029339869506657124, 0.3062593936920166]\n",
      "Step 127, loss = [0.30840009450912476, 0.006083718501031399, 0.30779170989990234]\n",
      "Step 128, loss = [0.28668877482414246, 0.008684610947966576, 0.2858203053474426]\n",
      "Step 129, loss = [0.29488033056259155, 0.004049972631037235, 0.29447534680366516]\n",
      "Step 130, loss = [0.30595874786376953, 0.006051081232726574, 0.30535364151000977]\n",
      "Step 131, loss = [0.29842647910118103, 0.00794956088066101, 0.29763153195381165]\n",
      "Step 132, loss = [0.3040784001350403, 0.00631768349558115, 0.30344662070274353]\n",
      "Step 133, loss = [0.2991192936897278, 0.005555338226258755, 0.29856374859809875]\n",
      "Step 134, loss = [0.308292418718338, 0.002708584535866976, 0.30802154541015625]\n",
      "Step 135, loss = [0.3005191683769226, 0.006330414675176144, 0.29988613724708557]\n",
      "Step 136, loss = [0.31232404708862305, 0.0048790317960083485, 0.3118361532688141]\n",
      "Step 137, loss = [0.29241907596588135, 0.005864813923835754, 0.2918325960636139]\n",
      "Step 138, loss = [0.2908385694026947, 0.003866812214255333, 0.29045188426971436]\n",
      "Step 139, loss = [0.3069407343864441, 0.006022130139172077, 0.30633851885795593]\n",
      "Step 140, loss = [0.30667027831077576, 0.0032736517023295164, 0.3063428997993469]\n",
      "Step 141, loss = [0.3008829355239868, 0.003773423610255122, 0.3005055785179138]\n",
      "Step 142, loss = [0.3041914105415344, 0.005656267516314983, 0.30362579226493835]\n",
      "Step 143, loss = [0.30875977873802185, 0.00696625467389822, 0.3080631494522095]\n",
      "Step 144, loss = [0.2692743241786957, 0.0058985440991818905, 0.2686844766139984]\n",
      "Step 145, loss = [0.3035193383693695, 0.008214805275201797, 0.3026978671550751]\n",
      "Step 146, loss = [0.29058873653411865, 0.00969565100967884, 0.28961917757987976]\n",
      "Step 147, loss = [0.30278685688972473, 0.007070254534482956, 0.30207982659339905]\n",
      "Step 148, loss = [0.29796797037124634, 0.008962650783360004, 0.2970716953277588]\n",
      "Step 149, loss = [0.29657307267189026, 0.0036147641949355602, 0.2962116003036499]\n",
      "Step 150, loss = [0.3097260296344757, 0.008597303181886673, 0.3088662922382355]\n",
      "Step 151, loss = [0.2929235100746155, 0.004420795477926731, 0.2924814224243164]\n",
      "Step 152, loss = [0.303457647562027, 0.006197699345648289, 0.30283787846565247]\n",
      "Step 153, loss = [0.299512654542923, 0.00495749618858099, 0.29901689291000366]\n",
      "Step 154, loss = [0.28503677248954773, 0.006227271631360054, 0.28441405296325684]\n",
      "Step 155, loss = [0.2880646586418152, 0.004593761172145605, 0.28760528564453125]\n",
      "Step 156, loss = [0.3090619742870331, 0.006374434567987919, 0.30842453241348267]\n",
      "Step 157, loss = [0.2971554398536682, 0.0073160575702786446, 0.2964238226413727]\n",
      "Step 158, loss = [0.2995821237564087, 0.002845283132046461, 0.29929760098457336]\n",
      "Step 159, loss = [0.30128493905067444, 0.004329311661422253, 0.3008520007133484]\n",
      "Step 160, loss = [0.28737324476242065, 0.008318045176565647, 0.2865414321422577]\n",
      "Step 161, loss = [0.30856144428253174, 0.007908841595053673, 0.3077705502510071]\n",
      "Step 162, loss = [0.2914154529571533, 0.01086706668138504, 0.2903287410736084]\n",
      "Step 163, loss = [0.30928435921669006, 0.006131516769528389, 0.3086712062358856]\n",
      "Step 164, loss = [0.30772286653518677, 0.006925014313310385, 0.3070303797721863]\n",
      "Step 165, loss = [0.30031079053878784, 0.007488850969821215, 0.29956191778182983]\n",
      "Step 166, loss = [0.30969586968421936, 0.007523841690272093, 0.3089434802532196]\n",
      "Step 167, loss = [0.29813000559806824, 0.00576922670006752, 0.29755309224128723]\n",
      "Step 168, loss = [0.28309959173202515, 0.008160911500453949, 0.2822835147380829]\n",
      "Step 169, loss = [0.2976698875427246, 0.008168057538568974, 0.29685309529304504]\n",
      "Step 170, loss = [0.2925756871700287, 0.010655640624463558, 0.29151013493537903]\n",
      "Step 171, loss = [0.29816532135009766, 0.0024449455086141825, 0.297920823097229]\n",
      "Step 172, loss = [0.29943573474884033, 0.004063927102833986, 0.2990293502807617]\n",
      "Step 173, loss = [0.3074948191642761, 0.00581216998398304, 0.3069136142730713]\n",
      "Step 174, loss = [0.31293946504592896, 0.003718389431014657, 0.3125676214694977]\n",
      "Step 175, loss = [0.2880266010761261, 0.00658929068595171, 0.28736767172813416]\n",
      "Step 176, loss = [0.30409184098243713, 0.007310010027140379, 0.30336084961891174]\n",
      "Step 177, loss = [0.3140871226787567, 0.00659447256475687, 0.3134276866912842]\n",
      "Step 178, loss = [0.303296834230423, 0.008777542971074581, 0.3024190664291382]\n",
      "Step 179, loss = [0.2742158770561218, 0.006128737702965736, 0.27360299229621887]\n",
      "Step 180, loss = [0.2795068025588989, 0.005858846474438906, 0.2789209187030792]\n",
      "Step 181, loss = [0.29308566451072693, 0.004774647764861584, 0.29260820150375366]\n",
      "Step 182, loss = [0.30614230036735535, 0.007361886091530323, 0.3054061233997345]\n",
      "Step 183, loss = [0.2842603325843811, 0.00620229309424758, 0.2836401164531708]\n",
      "Step 184, loss = [0.2940201163291931, 0.009062035009264946, 0.29311391711235046]\n",
      "Step 185, loss = [0.2958948314189911, 0.006375388242304325, 0.2952573001384735]\n",
      "Step 186, loss = [0.2916446328163147, 0.007468467578291893, 0.29089778661727905]\n",
      "Step 187, loss = [0.3042272925376892, 0.005661479197442532, 0.30366113781929016]\n",
      "Step 188, loss = [0.3001583218574524, 0.006544838659465313, 0.2995038330554962]\n",
      "Step 189, loss = [0.3034074604511261, 0.006177120376378298, 0.30278974771499634]\n",
      "Step 190, loss = [0.3008902668952942, 0.0059506772086024284, 0.3002952039241791]\n",
      "Step 191, loss = [0.29733994603157043, 0.004879754967987537, 0.2968519628047943]\n",
      "Step 192, loss = [0.29382720589637756, 0.007403211202472448, 0.2930868864059448]\n",
      "Step 193, loss = [0.28302741050720215, 0.010470203123986721, 0.28198039531707764]\n",
      "Step 194, loss = [0.30580341815948486, 0.0073281461372971535, 0.3050706088542938]\n",
      "Step 195, loss = [0.29642099142074585, 0.0050985123962163925, 0.29591113328933716]\n",
      "Step 196, loss = [0.3073813319206238, 0.008219903334975243, 0.30655935406684875]\n",
      "Step 197, loss = [0.30021199584007263, 0.003875319380313158, 0.29982447624206543]\n",
      "Step 198, loss = [0.293282687664032, 0.014735566452145576, 0.2918091416358948]\n",
      "Step 199, loss = [0.30832579731941223, 0.00601590471342206, 0.3077242076396942]\n",
      "Step 200, loss = [0.3022417426109314, 0.004772205371409655, 0.30176451802253723]\n",
      "Step 201, loss = [0.3002135455608368, 0.0034795652609318495, 0.29986560344696045]\n",
      "Step 202, loss = [0.30355027318000793, 0.005844153929501772, 0.30296584963798523]\n",
      "Step 203, loss = [0.30766841769218445, 0.003973732702434063, 0.3072710335254669]\n",
      "Step 204, loss = [0.30361005663871765, 0.003089750884100795, 0.3033010959625244]\n",
      "Step 205, loss = [0.30021408200263977, 0.00852903164923191, 0.2993611693382263]\n",
      "Step 206, loss = [0.31210577487945557, 0.006623788736760616, 0.31144338846206665]\n",
      "Step 207, loss = [0.31385159492492676, 0.005965897813439369, 0.3132550120353699]\n",
      "Step 208, loss = [0.29004165530204773, 0.00901708658784628, 0.2891399562358856]\n",
      "Step 209, loss = [0.2803126275539398, 0.005835363175719976, 0.27972909808158875]\n",
      "Step 210, loss = [0.3050083518028259, 0.005705138668417931, 0.3044378459453583]\n",
      "Step 211, loss = [0.2999252378940582, 0.012324247509241104, 0.29869282245635986]\n",
      "Step 212, loss = [0.29845064878463745, 0.007356504909694195, 0.2977150082588196]\n",
      "Step 213, loss = [0.31432846188545227, 0.005444212816655636, 0.31378403306007385]\n",
      "Step 214, loss = [0.3044182360172272, 0.009169714525341988, 0.30350127816200256]\n",
      "Step 215, loss = [0.3069630265235901, 0.010108619928359985, 0.30595216155052185]\n",
      "Step 216, loss = [0.29654401540756226, 0.0046118078753352165, 0.29608282446861267]\n",
      "Step 217, loss = [0.29309991002082825, 0.008028537966310978, 0.2922970652580261]\n",
      "Step 218, loss = [0.30145901441574097, 0.00600797776132822, 0.3008582293987274]\n",
      "Step 219, loss = [0.29415440559387207, 0.004638311453163624, 0.29369056224823]\n",
      "Step 220, loss = [0.3052949607372284, 0.007885193452239037, 0.30450645089149475]\n",
      "Step 221, loss = [0.30214861035346985, 0.009523400105535984, 0.30119627714157104]\n",
      "Step 222, loss = [0.2941572070121765, 0.005237788427621126, 0.29363343119621277]\n",
      "Step 223, loss = [0.29962900280952454, 0.007334117311984301, 0.29889559745788574]\n",
      "Step 224, loss = [0.29898375272750854, 0.005968691315501928, 0.2983868718147278]\n",
      "Step 225, loss = [0.2963179051876068, 0.013357279822230339, 0.2949821650981903]\n",
      "Step 226, loss = [0.2876397967338562, 0.0028494070284068584, 0.28735485672950745]\n",
      "Step 227, loss = [0.30519354343414307, 0.009334240108728409, 0.30426010489463806]\n",
      "Step 228, loss = [0.2906359136104584, 0.008467710576951504, 0.2897891402244568]\n",
      "Step 229, loss = [0.3017600178718567, 0.004261261783540249, 0.3013339042663574]\n",
      "Step 230, loss = [0.30500108003616333, 0.008621621876955032, 0.3041389286518097]\n",
      "Step 231, loss = [0.29078471660614014, 0.009571559727191925, 0.2898275554180145]\n",
      "Step 232, loss = [0.3038507103919983, 0.0070541673339903355, 0.30314528942108154]\n",
      "Step 233, loss = [0.3022375702857971, 0.0065905991941690445, 0.3015785217285156]\n",
      "Step 234, loss = [0.29851818084716797, 0.005490048788487911, 0.29796916246414185]\n",
      "Step 235, loss = [0.2910911440849304, 0.00695121381431818, 0.2903960347175598]\n",
      "Step 236, loss = [0.2792431116104126, 0.007225553505122662, 0.2785205543041229]\n",
      "Step 237, loss = [0.30258697271347046, 0.004111044574528933, 0.3021758794784546]\n",
      "Step 238, loss = [0.3075166344642639, 0.00663762167096138, 0.30685287714004517]\n",
      "Step 239, loss = [0.30140024423599243, 0.0026822336949408054, 0.3011320233345032]\n",
      "Step 240, loss = [0.31545355916023254, 0.007999742403626442, 0.31465357542037964]\n",
      "Step 241, loss = [0.28191280364990234, 0.004998719319701195, 0.28141292929649353]\n",
      "Step 242, loss = [0.2750798165798187, 0.0038005653768777847, 0.27469974756240845]\n",
      "Step 243, loss = [0.2737143635749817, 0.009111584164202213, 0.2728032171726227]\n",
      "Step 244, loss = [0.30176031589508057, 0.008477620780467987, 0.3009125590324402]\n",
      "Step 245, loss = [0.3112528324127197, 0.008194511756300926, 0.31043338775634766]\n",
      "Step 246, loss = [0.29795539379119873, 0.0032789844553917646, 0.2976275086402893]\n",
      "Step 247, loss = [0.2958025634288788, 0.007569470442831516, 0.2950456142425537]\n",
      "Step 248, loss = [0.2995142936706543, 0.009922362864017487, 0.29852205514907837]\n",
      "Step 249, loss = [0.2921450436115265, 0.004079253412783146, 0.2917371094226837]\n",
      "Step 250, loss = [0.30104345083236694, 0.007623469922691584, 0.3002811074256897]\n",
      "Step 251, loss = [0.29376229643821716, 0.004320427309721708, 0.29333025217056274]\n",
      "Step 252, loss = [0.30226969718933105, 0.005297326948493719, 0.30173996090888977]\n",
      "Step 253, loss = [0.3021426498889923, 0.006967442110180855, 0.3014459013938904]\n",
      "Step 254, loss = [0.28124696016311646, 0.005298543721437454, 0.2807171046733856]\n",
      "Step 255, loss = [0.3081349730491638, 0.007356863934546709, 0.30739927291870117]\n",
      "Step 256, loss = [0.31281009316444397, 0.007028092630207539, 0.31210729479789734]\n",
      "Step 257, loss = [0.29889369010925293, 0.006993436254560947, 0.29819434881210327]\n",
      "Step 258, loss = [0.28730377554893494, 0.0029157581739127636, 0.2870121896266937]\n",
      "Step 259, loss = [0.3031693994998932, 0.005159903317689896, 0.30265340209007263]\n",
      "Step 260, loss = [0.30552560091018677, 0.003908438142389059, 0.30513474345207214]\n",
      "Step 261, loss = [0.30163902044296265, 0.007625322323292494, 0.3008764982223511]\n",
      "Step 262, loss = [0.29704809188842773, 0.007714833132922649, 0.29627659916877747]\n",
      "Step 263, loss = [0.29542654752731323, 0.00813461933284998, 0.2946130931377411]\n",
      "Step 264, loss = [0.29012808203697205, 0.00552730355411768, 0.28957533836364746]\n",
      "Step 265, loss = [0.29906460642814636, 0.003532768227159977, 0.2987113296985626]\n",
      "Step 266, loss = [0.30957627296447754, 0.0030620242469012737, 0.30927008390426636]\n",
      "Step 267, loss = [0.3170793354511261, 0.00271327281370759, 0.3168080151081085]\n",
      "Step 268, loss = [0.30881956219673157, 0.00906001590192318, 0.30791357159614563]\n",
      "Step 269, loss = [0.304188996553421, 0.004245992749929428, 0.3037644028663635]\n",
      "Step 270, loss = [0.30323487520217896, 0.00490375142544508, 0.3027445077896118]\n",
      "Step 271, loss = [0.29986172914505005, 0.004950469359755516, 0.29936668276786804]\n",
      "Step 272, loss = [0.29760584235191345, 0.005550738424062729, 0.2970507740974426]\n",
      "Step 273, loss = [0.3093263804912567, 0.006251498591154814, 0.30870121717453003]\n",
      "Step 274, loss = [0.3039044439792633, 0.006362504325807095, 0.3032681941986084]\n",
      "Step 275, loss = [0.28124430775642395, 0.002893871394917369, 0.28095492720603943]\n",
      "Step 276, loss = [0.29716140031814575, 0.005646376870572567, 0.2965967655181885]\n",
      "Step 277, loss = [0.3106434643268585, 0.004721235018223524, 0.31017133593559265]\n",
      "Step 278, loss = [0.2946512997150421, 0.005972948856651783, 0.2940540015697479]\n",
      "Step 279, loss = [0.2950744330883026, 0.006382370367646217, 0.29443618655204773]\n",
      "Step 280, loss = [0.30388784408569336, 0.010133890435099602, 0.30287444591522217]\n",
      "Step 281, loss = [0.29641321301460266, 0.008474720641970634, 0.29556575417518616]\n",
      "Step 282, loss = [0.3101443350315094, 0.012034531682729721, 0.3089408874511719]\n",
      "Step 283, loss = [0.2930203080177307, 0.010305358096957207, 0.2919897735118866]\n",
      "Step 284, loss = [0.2883201837539673, 0.006001878529787064, 0.2877199947834015]\n",
      "Step 285, loss = [0.29433396458625793, 0.009449770674109459, 0.2933889925479889]\n",
      "Step 286, loss = [0.30233234167099, 0.010863354429602623, 0.3012460172176361]\n",
      "Step 287, loss = [0.27808210253715515, 0.004827348981052637, 0.27759936451911926]\n",
      "Step 288, loss = [0.30331647396087646, 0.003641771152615547, 0.30295228958129883]\n",
      "Step 289, loss = [0.30926546454429626, 0.005865394603461027, 0.30867892503738403]\n",
      "Step 290, loss = [0.29806873202323914, 0.003468420123681426, 0.29772189259529114]\n",
      "Step 291, loss = [0.2966845631599426, 0.003782032523304224, 0.2963063716888428]\n",
      "Step 292, loss = [0.2840001583099365, 0.00765205267816782, 0.28323495388031006]\n",
      "Step 293, loss = [0.2843628525733948, 0.010541165247559547, 0.283308744430542]\n",
      "Step 294, loss = [0.3068142533302307, 0.007059209980070591, 0.3061083257198334]\n",
      "Step 295, loss = [0.28646960854530334, 0.004365157801657915, 0.28603309392929077]\n",
      "Step 296, loss = [0.2803229093551636, 0.0030802348628640175, 0.28001487255096436]\n",
      "Step 297, loss = [0.30130115151405334, 0.0048469072207808495, 0.30081644654273987]\n",
      "Step 298, loss = [0.31736183166503906, 0.007153402082622051, 0.3166464865207672]\n",
      "Step 299, loss = [0.3124353885650635, 0.008834123611450195, 0.3115519881248474]\n",
      "Step 300, loss = [0.301786869764328, 0.006930084899067879, 0.30109384655952454]\n",
      "Step 301, loss = [0.30545198917388916, 0.0086534908041358, 0.30458664894104004]\n",
      "Step 302, loss = [0.2763345539569855, 0.007038375828415155, 0.2756307125091553]\n",
      "Step 303, loss = [0.30699241161346436, 0.00599458534270525, 0.30639296770095825]\n",
      "Step 304, loss = [0.2978602349758148, 0.005234476178884506, 0.29733678698539734]\n",
      "Step 305, loss = [0.3068993091583252, 0.005154735408723354, 0.3063838481903076]\n",
      "Step 306, loss = [0.2997802793979645, 0.007894689217209816, 0.29899081587791443]\n",
      "Step 307, loss = [0.2967679798603058, 0.005632540211081505, 0.29620471596717834]\n",
      "Step 308, loss = [0.2964777946472168, 0.002747695427387953, 0.29620301723480225]\n",
      "Step 309, loss = [0.29323533177375793, 0.009225500747561455, 0.29231277108192444]\n",
      "Step 310, loss = [0.30338671803474426, 0.004400331061333418, 0.30294668674468994]\n",
      "Step 311, loss = [0.2958865165710449, 0.007591083645820618, 0.29512742161750793]\n",
      "Step 312, loss = [0.29627883434295654, 0.0050926306284964085, 0.2957695722579956]\n",
      "Step 313, loss = [0.2974757254123688, 0.005629382096230984, 0.2969127893447876]\n",
      "Step 314, loss = [0.29382550716400146, 0.006801296025514603, 0.29314538836479187]\n",
      "Step 315, loss = [0.2949531376361847, 0.007548934780061245, 0.29419824481010437]\n",
      "Step 316, loss = [0.2833154797554016, 0.0038766248617321253, 0.28292781114578247]\n",
      "Step 317, loss = [0.31302738189697266, 0.010705521330237389, 0.31195682287216187]\n",
      "Step 318, loss = [0.29524996876716614, 0.0028222352266311646, 0.29496774077415466]\n",
      "Step 319, loss = [0.31512826681137085, 0.00961349718272686, 0.3141669034957886]\n",
      "Step 320, loss = [0.3046526312828064, 0.005778037011623383, 0.30407482385635376]\n",
      "Step 321, loss = [0.292873352766037, 0.007414310239255428, 0.2921319305896759]\n",
      "Step 322, loss = [0.29687127470970154, 0.0034626382403075695, 0.2965250015258789]\n",
      "Step 323, loss = [0.30210837721824646, 0.004977996926754713, 0.3016105890274048]\n",
      "Step 324, loss = [0.2999434471130371, 0.004217823967337608, 0.29952165484428406]\n",
      "Step 325, loss = [0.307915061712265, 0.007908307015895844, 0.30712422728538513]\n",
      "Step 326, loss = [0.31094419956207275, 0.010452899150550365, 0.30989891290664673]\n",
      "Step 327, loss = [0.2980045676231384, 0.007204368244856596, 0.2972841262817383]\n",
      "Step 328, loss = [0.2904539406299591, 0.010262985713779926, 0.28942763805389404]\n",
      "Step 329, loss = [0.30236661434173584, 0.006010664626955986, 0.3017655611038208]\n",
      "Step 330, loss = [0.30665677785873413, 0.0037797363474965096, 0.306278795003891]\n",
      "Step 331, loss = [0.2973732352256775, 0.006781723350286484, 0.2966950535774231]\n",
      "Step 332, loss = [0.29473876953125, 0.014099130406975746, 0.2933288514614105]\n",
      "Step 333, loss = [0.3001288175582886, 0.00578262796625495, 0.2995505630970001]\n",
      "Step 334, loss = [0.29132381081581116, 0.004028718918561935, 0.2909209430217743]\n",
      "Step 335, loss = [0.29653340578079224, 0.006026516668498516, 0.29593074321746826]\n",
      "Step 336, loss = [0.27565309405326843, 0.004679461475461721, 0.27518513798713684]\n",
      "Step 337, loss = [0.29137080907821655, 0.003991906531155109, 0.2909716069698334]\n",
      "Step 338, loss = [0.30274733901023865, 0.007239648140966892, 0.30202338099479675]\n",
      "Step 339, loss = [0.2984684407711029, 0.006632499396800995, 0.29780519008636475]\n",
      "Step 340, loss = [0.3002045452594757, 0.0025269887410104275, 0.29995185136795044]\n",
      "Step 341, loss = [0.3025531768798828, 0.0068790423683822155, 0.30186527967453003]\n",
      "Step 342, loss = [0.2802892029285431, 0.007027612067759037, 0.27958643436431885]\n",
      "Step 343, loss = [0.30034902691841125, 0.005218213889747858, 0.2998272180557251]\n",
      "Step 344, loss = [0.3040398955345154, 0.004612806253135204, 0.30357861518859863]\n",
      "Step 345, loss = [0.30649077892303467, 0.010816633701324463, 0.30540910363197327]\n",
      "Step 346, loss = [0.3100605309009552, 0.010957779362797737, 0.3089647591114044]\n",
      "Step 347, loss = [0.2997839152812958, 0.0027791322208940983, 0.2995060086250305]\n",
      "Step 348, loss = [0.2962528169155121, 0.005855938419699669, 0.29566723108291626]\n",
      "Step 349, loss = [0.3057314157485962, 0.002856148174032569, 0.3054457902908325]\n",
      "Step 350, loss = [0.2963261604309082, 0.005540582351386547, 0.29577210545539856]\n",
      "Step 351, loss = [0.2863213121891022, 0.006043408066034317, 0.2857169806957245]\n",
      "Step 352, loss = [0.30734169483184814, 0.005343395285308361, 0.30680736899375916]\n",
      "Step 353, loss = [0.29953083395957947, 0.004849151708185673, 0.2990459203720093]\n",
      "Step 354, loss = [0.2961503267288208, 0.0031405650079250336, 0.29583626985549927]\n",
      "Step 355, loss = [0.29323938488960266, 0.006964055821299553, 0.2925429940223694]\n",
      "Step 356, loss = [0.29243922233581543, 0.003148800227791071, 0.29212433099746704]\n",
      "Step 357, loss = [0.29953742027282715, 0.0031711789779365063, 0.2992202937602997]\n",
      "Step 358, loss = [0.3109372854232788, 0.009003359824419022, 0.31003695726394653]\n",
      "Step 359, loss = [0.30766597390174866, 0.008417051285505295, 0.306824266910553]\n",
      "Step 360, loss = [0.2961746156215668, 0.005650966893881559, 0.2956095337867737]\n",
      "Step 361, loss = [0.2940879762172699, 0.005627823062241077, 0.29352518916130066]\n",
      "Step 362, loss = [0.29836899042129517, 0.0032217511907219887, 0.2980468273162842]\n",
      "Step 363, loss = [0.30020615458488464, 0.002983739599585533, 0.29990777373313904]\n",
      "Step 364, loss = [0.30161434412002563, 0.010295793414115906, 0.3005847632884979]\n",
      "Step 365, loss = [0.2980685830116272, 0.004632480442523956, 0.29760533571243286]\n",
      "Step 366, loss = [0.3007606565952301, 0.006125589832663536, 0.3001480996608734]\n",
      "Step 367, loss = [0.30577942728996277, 0.006758032366633415, 0.3051036298274994]\n",
      "Step 368, loss = [0.2838587760925293, 0.009569481015205383, 0.2829018235206604]\n",
      "Step 369, loss = [0.3092934787273407, 0.005150727927684784, 0.30877840518951416]\n",
      "Step 370, loss = [0.30602338910102844, 0.009000069461762905, 0.30512338876724243]\n",
      "Step 371, loss = [0.30146610736846924, 0.006092951633036137, 0.3008567988872528]\n",
      "Step 372, loss = [0.2864959239959717, 0.00837707705795765, 0.28565821051597595]\n",
      "Step 373, loss = [0.311926007270813, 0.0038709668442606926, 0.3115389049053192]\n",
      "Step 374, loss = [0.31775155663490295, 0.004358371719717979, 0.3173157274723053]\n",
      "Step 375, loss = [0.30560848116874695, 0.006773700471967459, 0.304931104183197]\n",
      "Step 376, loss = [0.29408130049705505, 0.005990480072796345, 0.29348224401474]\n",
      "Step 377, loss = [0.3039056062698364, 0.00756832817569375, 0.3031487762928009]\n",
      "Step 378, loss = [0.2869342863559723, 0.011752632446587086, 0.2857590317726135]\n",
      "Step 379, loss = [0.30676552653312683, 0.002756288042291999, 0.30648988485336304]\n",
      "Step 380, loss = [0.3000900447368622, 0.0021871402859687805, 0.2998713254928589]\n",
      "Step 381, loss = [0.2815650999546051, 0.011091554537415504, 0.28045594692230225]\n",
      "Step 382, loss = [0.30827322602272034, 0.0026499733794480562, 0.30800822377204895]\n",
      "Step 383, loss = [0.29575115442276, 0.00675266282632947, 0.2950758934020996]\n",
      "Step 384, loss = [0.29665741324424744, 0.005152431316673756, 0.2961421608924866]\n",
      "Step 385, loss = [0.2998586595058441, 0.009953344240784645, 0.29886332154273987]\n",
      "Step 386, loss = [0.30291450023651123, 0.005159442313015461, 0.30239856243133545]\n",
      "Step 387, loss = [0.2744351625442505, 0.004652587231248617, 0.2739699184894562]\n",
      "Step 388, loss = [0.30064332485198975, 0.01226198673248291, 0.2994171380996704]\n",
      "Step 389, loss = [0.29663974046707153, 0.005814295262098312, 0.2960582971572876]\n",
      "Step 390, loss = [0.289777934551239, 0.0074884542264044285, 0.2890290915966034]\n",
      "Step 391, loss = [0.2894042134284973, 0.005080820992588997, 0.2888961434364319]\n",
      "Step 392, loss = [0.2830054759979248, 0.004522279836237431, 0.2825532555580139]\n",
      "Step 393, loss = [0.3008235991001129, 0.006021297536790371, 0.3002214729785919]\n",
      "Step 394, loss = [0.288068950176239, 0.005943347234278917, 0.2874746024608612]\n",
      "Step 395, loss = [0.3154650330543518, 0.009661965072154999, 0.3144988417625427]\n",
      "Step 396, loss = [0.30940720438957214, 0.00751097034662962, 0.30865609645843506]\n",
      "Step 397, loss = [0.29788070917129517, 0.003424152033403516, 0.29753828048706055]\n",
      "Step 398, loss = [0.29584959149360657, 0.007363930810242891, 0.295113205909729]\n",
      "Step 399, loss = [0.2976270020008087, 0.007954753004014492, 0.29683151841163635]\n",
      "Step 400, loss = [0.292636901140213, 0.008717963472008705, 0.29176509380340576]\n",
      "Step 401, loss = [0.3032035827636719, 0.00468515045940876, 0.3027350604534149]\n",
      "Step 402, loss = [0.3111379146575928, 0.003815317992120981, 0.3107563853263855]\n",
      "Step 403, loss = [0.2988447844982147, 0.009324725717306137, 0.2979122996330261]\n",
      "Step 404, loss = [0.2874257266521454, 0.00868098996579647, 0.2865576148033142]\n",
      "Step 405, loss = [0.29957956075668335, 0.00545869767665863, 0.2990337014198303]\n",
      "Step 406, loss = [0.3061644732952118, 0.009436043910682201, 0.3052208721637726]\n",
      "Step 407, loss = [0.30309581756591797, 0.006093376316130161, 0.30248647928237915]\n",
      "Step 408, loss = [0.3136883080005646, 0.0049669938161969185, 0.31319162249565125]\n",
      "Step 409, loss = [0.29622915387153625, 0.01050819642841816, 0.2951783239841461]\n",
      "Step 410, loss = [0.3037791848182678, 0.005526319146156311, 0.3032265603542328]\n",
      "Step 411, loss = [0.29469063878059387, 0.010544368997216225, 0.2936362028121948]\n",
      "Step 412, loss = [0.30363038182258606, 0.009038180112838745, 0.3027265667915344]\n",
      "Step 413, loss = [0.3024425208568573, 0.00637710839509964, 0.3018048107624054]\n",
      "Step 414, loss = [0.3030780255794525, 0.0028300932608544827, 0.30279502272605896]\n",
      "Step 415, loss = [0.30867335200309753, 0.00680471770465374, 0.3079928755760193]\n",
      "Step 416, loss = [0.30696070194244385, 0.006734131835401058, 0.3062872886657715]\n",
      "Step 417, loss = [0.31549474596977234, 0.006448375526815653, 0.3148499131202698]\n",
      "Step 418, loss = [0.29734793305397034, 0.002421899000182748, 0.29710572957992554]\n",
      "Step 419, loss = [0.3148222267627716, 0.005550154019147158, 0.31426721811294556]\n",
      "Step 420, loss = [0.29836103320121765, 0.012823776341974735, 0.2970786690711975]\n",
      "Step 421, loss = [0.2939251661300659, 0.00770652387291193, 0.2931545078754425]\n",
      "Step 422, loss = [0.2950971722602844, 0.0099619310349226, 0.29410097002983093]\n",
      "Step 423, loss = [0.3007461428642273, 0.014531314373016357, 0.29929301142692566]\n",
      "Step 424, loss = [0.3132488429546356, 0.0033472771756350994, 0.312914103269577]\n",
      "Step 425, loss = [0.2961314916610718, 0.011092416942119598, 0.29502224922180176]\n",
      "Step 426, loss = [0.2928727865219116, 0.0030494423117488623, 0.2925678491592407]\n",
      "Step 427, loss = [0.31034043431282043, 0.004837492480874062, 0.30985668301582336]\n",
      "Step 428, loss = [0.2914081811904907, 0.005016431678086519, 0.29090654850006104]\n",
      "Step 429, loss = [0.28096768260002136, 0.008357672020792961, 0.28013190627098083]\n",
      "Step 430, loss = [0.306576669216156, 0.005195167846977711, 0.3060571551322937]\n",
      "Step 431, loss = [0.30301815271377563, 0.0038248635828495026, 0.30263566970825195]\n",
      "Step 432, loss = [0.29991453886032104, 0.006893589161336422, 0.29922518134117126]\n",
      "Step 433, loss = [0.29811757802963257, 0.0036580683663487434, 0.297751784324646]\n",
      "Step 434, loss = [0.3037477731704712, 0.00918835960328579, 0.30282893776893616]\n",
      "Step 435, loss = [0.30994823575019836, 0.008213290944695473, 0.30912691354751587]\n",
      "Step 436, loss = [0.29977312684059143, 0.007720449473708868, 0.2990010678768158]\n",
      "Step 437, loss = [0.3011476397514343, 0.009396224282681942, 0.30020803213119507]\n",
      "Step 438, loss = [0.29963743686676025, 0.007352188695222139, 0.2989022135734558]\n",
      "Step 439, loss = [0.308734267950058, 0.0066464608535170555, 0.3080696165561676]\n",
      "Step 440, loss = [0.29677140712738037, 0.003320184536278248, 0.29643937945365906]\n",
      "Step 441, loss = [0.3046486973762512, 0.006414841860532761, 0.3040072023868561]\n",
      "Step 442, loss = [0.30540770292282104, 0.013586648739874363, 0.3040490448474884]\n",
      "Step 443, loss = [0.3048705756664276, 0.006900358479470015, 0.3041805326938629]\n",
      "Step 444, loss = [0.2961055338382721, 0.006771508604288101, 0.29542839527130127]\n",
      "Step 445, loss = [0.3077498972415924, 0.0024532678071409464, 0.3075045645236969]\n",
      "Step 446, loss = [0.2877779006958008, 0.004587175324559212, 0.2873191833496094]\n",
      "Step 447, loss = [0.29571449756622314, 0.011286361142992973, 0.29458585381507874]\n",
      "Step 448, loss = [0.2981984615325928, 0.00880342721939087, 0.29731813073158264]\n",
      "Step 449, loss = [0.30870601534843445, 0.0048318952322006226, 0.30822283029556274]\n",
      "Step 450, loss = [0.2804109454154968, 0.006741330958902836, 0.27973681688308716]\n",
      "Step 451, loss = [0.3033950924873352, 0.005830295383930206, 0.3028120696544647]\n",
      "Step 452, loss = [0.3041764795780182, 0.005251645110547543, 0.3036513030529022]\n",
      "Step 453, loss = [0.28400859236717224, 0.0037230756133794785, 0.28363627195358276]\n",
      "Step 454, loss = [0.2953996956348419, 0.005718874745070934, 0.29482781887054443]\n",
      "Step 455, loss = [0.30052414536476135, 0.0068201590329408646, 0.29984211921691895]\n",
      "Step 456, loss = [0.31170859932899475, 0.006117683835327625, 0.31109681725502014]\n",
      "Step 457, loss = [0.2876478135585785, 0.006363703869283199, 0.28701144456863403]\n",
      "Step 458, loss = [0.2829659879207611, 0.0065668318420648575, 0.28230929374694824]\n",
      "Step 459, loss = [0.29324281215667725, 0.004243380390107632, 0.29281848669052124]\n",
      "Step 460, loss = [0.305919349193573, 0.005357126705348492, 0.3053836226463318]\n",
      "Step 461, loss = [0.2955380976200104, 0.005633050575852394, 0.29497480392456055]\n",
      "Step 462, loss = [0.29730239510536194, 0.00730912433937192, 0.2965714931488037]\n",
      "Step 463, loss = [0.3012749254703522, 0.005347423255443573, 0.30074018239974976]\n",
      "Step 464, loss = [0.2882484495639801, 0.010919177904725075, 0.28715652227401733]\n",
      "Step 465, loss = [0.30123215913772583, 0.00440131826326251, 0.30079203844070435]\n",
      "Step 466, loss = [0.29527097940444946, 0.0062898811884224415, 0.29464200139045715]\n",
      "Step 467, loss = [0.29339444637298584, 0.009085706435143948, 0.2924858629703522]\n",
      "Step 468, loss = [0.2977781593799591, 0.006610460113734007, 0.29711711406707764]\n",
      "Step 469, loss = [0.3088066875934601, 0.005869759246706963, 0.30821970105171204]\n",
      "Step 470, loss = [0.3095225393772125, 0.009128793142735958, 0.30860966444015503]\n",
      "Step 471, loss = [0.2993825674057007, 0.011807240545749664, 0.2982018291950226]\n",
      "Step 472, loss = [0.3056793510913849, 0.00701302383095026, 0.30497804284095764]\n",
      "Step 473, loss = [0.30130651593208313, 0.007928424514830112, 0.30051368474960327]\n",
      "Step 474, loss = [0.31312161684036255, 0.005754980258643627, 0.31254610419273376]\n",
      "Step 475, loss = [0.28192558884620667, 0.0053188554011285305, 0.28139370679855347]\n",
      "Step 476, loss = [0.29825809597969055, 0.006488470360636711, 0.29760923981666565]\n",
      "Step 477, loss = [0.3008694648742676, 0.008883361704647541, 0.29998111724853516]\n",
      "Step 478, loss = [0.2819986045360565, 0.00365841225720942, 0.28163275122642517]\n",
      "Step 479, loss = [0.3061445951461792, 0.004565470851957798, 0.3056880533695221]\n",
      "Step 480, loss = [0.30377230048179626, 0.011135567910969257, 0.30265873670578003]\n",
      "Step 481, loss = [0.3034672737121582, 0.003112723585218191, 0.3031559884548187]\n",
      "Step 482, loss = [0.30219006538391113, 0.0057088350877165794, 0.30161917209625244]\n",
      "Step 483, loss = [0.2912702262401581, 0.006533079780638218, 0.2906169295310974]\n",
      "Step 484, loss = [0.2874637246131897, 0.005320850759744644, 0.2869316339492798]\n",
      "Step 485, loss = [0.2740657329559326, 0.012805966660380363, 0.27278512716293335]\n",
      "Step 486, loss = [0.30950701236724854, 0.00808189157396555, 0.308698832988739]\n",
      "Step 487, loss = [0.29282522201538086, 0.00753556564450264, 0.292071670293808]\n",
      "Step 488, loss = [0.28735238313674927, 0.011460889130830765, 0.28620630502700806]\n",
      "Update target distribution epoch 1 step 489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11754/11754 [1:51:30<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos. rate:0.396078431372549 Neg. rate:0.6039215686274509\n",
      "delta_label  0.01854687765866939\n",
      "Step 489, loss = [0.30026906728744507, 0.009291257709264755, 0.2993399500846863]\n",
      "Step 490, loss = [0.31034553050994873, 0.013877131044864655, 0.3089578151702881]\n",
      "Step 491, loss = [0.2987422049045563, 0.005805268883705139, 0.29816168546676636]\n",
      "Step 492, loss = [0.3038690984249115, 0.010019796900451183, 0.3028671145439148]\n",
      "Step 493, loss = [0.3178218603134155, 0.011620685458183289, 0.3166597783565521]\n",
      "Step 494, loss = [0.30361703038215637, 0.002441869815811515, 0.3033728301525116]\n",
      "Step 495, loss = [0.29861974716186523, 0.006850799545645714, 0.2979346811771393]\n",
      "Step 496, loss = [0.30067184567451477, 0.006962387822568417, 0.29997560381889343]\n",
      "Step 497, loss = [0.2864098846912384, 0.012894143350422382, 0.2851204574108124]\n",
      "Step 498, loss = [0.2970559895038605, 0.004406703170388937, 0.296615332365036]\n",
      "Step 499, loss = [0.3025527894496918, 0.00671278964728117, 0.3018815219402313]\n",
      "Step 500, loss = [0.28558358550071716, 0.006395281758159399, 0.2849440574645996]\n",
      "Step 501, loss = [0.30006903409957886, 0.006432596128433943, 0.29942578077316284]\n",
      "Step 502, loss = [0.29617390036582947, 0.005817457567900419, 0.29559215903282166]\n",
      "Step 503, loss = [0.2936103045940399, 0.006432045251131058, 0.2929671108722687]\n",
      "Step 504, loss = [0.29477760195732117, 0.009797057136893272, 0.29379791021347046]\n",
      "Step 505, loss = [0.31002679467201233, 0.006819975562393665, 0.3093447983264923]\n",
      "Step 506, loss = [0.28566974401474, 0.0061899530701339245, 0.28505074977874756]\n",
      "Step 507, loss = [0.2879011034965515, 0.010552805848419666, 0.2868458330631256]\n",
      "Step 508, loss = [0.3084806501865387, 0.00523429736495018, 0.3079572319984436]\n",
      "Step 509, loss = [0.31398725509643555, 0.003846685402095318, 0.31360259652137756]\n",
      "Step 510, loss = [0.2929247319698334, 0.010743897408246994, 0.29185032844543457]\n",
      "Step 511, loss = [0.30767524242401123, 0.004077736288309097, 0.3072674572467804]\n",
      "Step 512, loss = [0.3109664022922516, 0.010075853206217289, 0.309958815574646]\n",
      "Step 513, loss = [0.3024235665798187, 0.007987957447767258, 0.30162477493286133]\n",
      "Step 514, loss = [0.30357885360717773, 0.009251645766198635, 0.3026537001132965]\n",
      "Step 515, loss = [0.3104671835899353, 0.007832682691514492, 0.3096839189529419]\n",
      "Step 516, loss = [0.3009953200817108, 0.00839925929903984, 0.3001554012298584]\n",
      "Step 517, loss = [0.3140997588634491, 0.007330261170864105, 0.31336674094200134]\n",
      "Step 518, loss = [0.29612603783607483, 0.00854338239878416, 0.29527169466018677]\n",
      "Step 519, loss = [0.2953261733055115, 0.0052679190412163734, 0.2947993874549866]\n",
      "Step 520, loss = [0.2868839502334595, 0.01158393919467926, 0.28572556376457214]\n",
      "Step 521, loss = [0.2992788553237915, 0.0013445345684885979, 0.2991443872451782]\n",
      "Step 522, loss = [0.30634579062461853, 0.009499105624854565, 0.30539587140083313]\n",
      "Step 523, loss = [0.29768136143684387, 0.0060986243188381195, 0.2970714867115021]\n",
      "Step 524, loss = [0.3135705888271332, 0.0047308518551290035, 0.3130975067615509]\n",
      "Step 525, loss = [0.2905679941177368, 0.007059138733893633, 0.2898620665073395]\n",
      "Step 526, loss = [0.3028036952018738, 0.013565612025558949, 0.3014471232891083]\n",
      "Step 527, loss = [0.2957168519496918, 0.0040055145509541035, 0.29531630873680115]\n",
      "Step 528, loss = [0.3030736744403839, 0.006487080827355385, 0.30242496728897095]\n",
      "Step 529, loss = [0.2828846275806427, 0.0013651950284838676, 0.28274810314178467]\n",
      "Step 530, loss = [0.29671862721443176, 0.004000086337327957, 0.2963186204433441]\n",
      "Step 531, loss = [0.3026139736175537, 0.010725734755396843, 0.30154138803482056]\n",
      "Step 532, loss = [0.30638355016708374, 0.008002158254384995, 0.30558332800865173]\n",
      "Step 533, loss = [0.303059458732605, 0.012162039056420326, 0.3018432557582855]\n",
      "Step 534, loss = [0.30008023977279663, 0.008486342616379261, 0.299231618642807]\n",
      "Step 535, loss = [0.3094821572303772, 0.008748138323426247, 0.3086073398590088]\n",
      "Step 536, loss = [0.2959519326686859, 0.005749126896262169, 0.2953770160675049]\n",
      "Step 537, loss = [0.30468225479125977, 0.0042364345863461494, 0.3042586147785187]\n",
      "Step 538, loss = [0.29674240946769714, 0.004332404583692551, 0.2963091731071472]\n",
      "Step 539, loss = [0.30985304713249207, 0.006407438311725855, 0.3092122972011566]\n",
      "Step 540, loss = [0.30063897371292114, 0.006586222909390926, 0.2999803423881531]\n",
      "Step 541, loss = [0.30885881185531616, 0.011363120749592781, 0.3077225089073181]\n",
      "Step 542, loss = [0.30364105105400085, 0.009077937342226505, 0.30273324251174927]\n",
      "Step 543, loss = [0.2907916307449341, 0.0042505026794970036, 0.29036659002304077]\n",
      "Step 544, loss = [0.29823532700538635, 0.00451812706887722, 0.2977835237979889]\n",
      "Step 545, loss = [0.3102235198020935, 0.008848044089972973, 0.3093387186527252]\n",
      "Step 546, loss = [0.31003737449645996, 0.007979528047144413, 0.3092394173145294]\n",
      "Step 547, loss = [0.2922082841396332, 0.004965553991496563, 0.2917117178440094]\n",
      "Step 548, loss = [0.2967069447040558, 0.007278176490217447, 0.2959791123867035]\n",
      "Step 549, loss = [0.3120903968811035, 0.00845843181014061, 0.31124454736709595]\n",
      "Step 550, loss = [0.2937438488006592, 0.007418379653245211, 0.29300200939178467]\n",
      "Step 551, loss = [0.29873886704444885, 0.011709017679095268, 0.2975679636001587]\n",
      "Step 552, loss = [0.3128722012042999, 0.00715044979006052, 0.31215715408325195]\n",
      "Step 553, loss = [0.2833747863769531, 0.004374321550130844, 0.28293734788894653]\n",
      "Step 554, loss = [0.29770931601524353, 0.0057424139231443405, 0.2971350848674774]\n",
      "Step 555, loss = [0.29375576972961426, 0.007181461900472641, 0.29303762316703796]\n",
      "Step 556, loss = [0.29483240842819214, 0.008056861348450184, 0.2940267324447632]\n",
      "Step 557, loss = [0.28689250349998474, 0.009690099395811558, 0.28592348098754883]\n",
      "Step 558, loss = [0.30491259694099426, 0.007594248279929161, 0.304153174161911]\n",
      "Step 559, loss = [0.29603999853134155, 0.007229099981486797, 0.2953170835971832]\n",
      "Step 560, loss = [0.3101933002471924, 0.007678528316318989, 0.3094254434108734]\n",
      "Step 561, loss = [0.3000115156173706, 0.006833724211901426, 0.29932814836502075]\n",
      "Step 562, loss = [0.295455664396286, 0.004565536975860596, 0.2949991226196289]\n",
      "Step 563, loss = [0.29869455099105835, 0.01078278012573719, 0.29761627316474915]\n",
      "Step 564, loss = [0.2941129505634308, 0.0071672010235488415, 0.2933962345123291]\n",
      "Step 565, loss = [0.29397305846214294, 0.006213255692273378, 0.2933517396450043]\n",
      "Step 566, loss = [0.3056832253932953, 0.006337125785648823, 0.30504950881004333]\n",
      "Step 567, loss = [0.28829899430274963, 0.005403721239417791, 0.28775861859321594]\n",
      "Step 568, loss = [0.2929406762123108, 0.0035346343647688627, 0.2925872206687927]\n",
      "Step 569, loss = [0.27828171849250793, 0.007261822000145912, 0.27755552530288696]\n",
      "Step 570, loss = [0.2824227511882782, 0.008390109986066818, 0.2815837264060974]\n",
      "Step 571, loss = [0.2991396486759186, 0.0059584928676486015, 0.2985438108444214]\n",
      "Step 572, loss = [0.312320351600647, 0.009542971849441528, 0.3113660514354706]\n",
      "Step 573, loss = [0.3065641224384308, 0.006918909028172493, 0.30587223172187805]\n",
      "Step 574, loss = [0.29089605808258057, 0.008835803717374802, 0.2900124788284302]\n",
      "Step 575, loss = [0.3055632412433624, 0.0035095331259071827, 0.3052122890949249]\n",
      "Step 576, loss = [0.30966565012931824, 0.009787270799279213, 0.30868691205978394]\n",
      "Step 577, loss = [0.29050710797309875, 0.004212195053696632, 0.29008588194847107]\n",
      "Step 578, loss = [0.29816752672195435, 0.008263230323791504, 0.2973411977291107]\n",
      "Step 579, loss = [0.28223684430122375, 0.007502120919525623, 0.2814866304397583]\n",
      "Step 580, loss = [0.30531075596809387, 0.0044050393626093864, 0.30487024784088135]\n",
      "Step 581, loss = [0.3018653988838196, 0.003713705111294985, 0.3014940321445465]\n",
      "Step 582, loss = [0.3018629252910614, 0.0032576918601989746, 0.3015371561050415]\n",
      "Step 583, loss = [0.30180519819259644, 0.00699367793276906, 0.3011058270931244]\n",
      "Step 584, loss = [0.3013153076171875, 0.0078246109187603, 0.30053284764289856]\n",
      "Step 585, loss = [0.30186396837234497, 0.0023743375204503536, 0.3016265332698822]\n",
      "Step 586, loss = [0.2983418107032776, 0.003445022739470005, 0.29799729585647583]\n",
      "Step 587, loss = [0.29912665486335754, 0.0052869487553834915, 0.29859796166419983]\n",
      "Step 588, loss = [0.3096922039985657, 0.008834687992930412, 0.30880874395370483]\n",
      "Step 589, loss = [0.3003537058830261, 0.004503055475652218, 0.29990339279174805]\n",
      "Step 590, loss = [0.28662192821502686, 0.0020695775747299194, 0.2864149808883667]\n",
      "Step 591, loss = [0.28697532415390015, 0.0022072596475481987, 0.2867546081542969]\n",
      "Step 592, loss = [0.300272136926651, 0.008810599334537983, 0.29939109086990356]\n",
      "Step 593, loss = [0.30004197359085083, 0.009174926206469536, 0.29912447929382324]\n",
      "Step 594, loss = [0.3105795979499817, 0.0027508216444402933, 0.31030452251434326]\n",
      "Step 595, loss = [0.30535954236984253, 0.00843125581741333, 0.30451640486717224]\n",
      "Step 596, loss = [0.29815322160720825, 0.006425108294934034, 0.29751071333885193]\n",
      "Step 597, loss = [0.31234318017959595, 0.007865462452173233, 0.3115566372871399]\n",
      "Step 598, loss = [0.29575949907302856, 0.005480624735355377, 0.29521143436431885]\n",
      "Step 599, loss = [0.2905921936035156, 0.004986009560525417, 0.2900936007499695]\n",
      "Step 600, loss = [0.30911785364151, 0.006194601766765118, 0.3084983825683594]\n",
      "Step 601, loss = [0.30069589614868164, 0.0037815782707184553, 0.3003177344799042]\n",
      "Step 602, loss = [0.29991647601127625, 0.006904109846800566, 0.2992260754108429]\n",
      "Step 603, loss = [0.3045591115951538, 0.009799403138458729, 0.303579181432724]\n",
      "Step 604, loss = [0.3019803464412689, 0.006399231031537056, 0.3013404309749603]\n",
      "Step 605, loss = [0.29344847798347473, 0.009015260264277458, 0.29254695773124695]\n",
      "Step 606, loss = [0.3008537292480469, 0.005978580564260483, 0.3002558648586273]\n",
      "Step 607, loss = [0.30489978194236755, 0.008370606228709221, 0.30406272411346436]\n",
      "Step 608, loss = [0.30455586314201355, 0.008873345330357552, 0.3036685287952423]\n",
      "Step 609, loss = [0.31133392453193665, 0.003133529331535101, 0.3110205829143524]\n",
      "Step 610, loss = [0.30265435576438904, 0.01078200526535511, 0.301576167345047]\n",
      "Step 611, loss = [0.3066256046295166, 0.00641762837767601, 0.30598384141921997]\n",
      "Step 612, loss = [0.31127431988716125, 0.006617062259465456, 0.31061261892318726]\n",
      "Step 613, loss = [0.29627421498298645, 0.006967581808567047, 0.2955774664878845]\n",
      "Step 614, loss = [0.3090437650680542, 0.01088414154946804, 0.3079553544521332]\n",
      "Step 615, loss = [0.31045886874198914, 0.009153749793767929, 0.3095434904098511]\n",
      "Step 616, loss = [0.3124011754989624, 0.004439802840352058, 0.3119571805000305]\n",
      "Step 617, loss = [0.29606127738952637, 0.0078109512105584145, 0.29528018832206726]\n",
      "Step 618, loss = [0.2989811599254608, 0.005812617018818855, 0.2983998954296112]\n",
      "Step 619, loss = [0.29776766896247864, 0.004610343370586634, 0.297306627035141]\n",
      "Step 620, loss = [0.3071122169494629, 0.009474081918597221, 0.30616480112075806]\n",
      "Step 621, loss = [0.27800118923187256, 0.006343334913253784, 0.27736684679985046]\n",
      "Step 622, loss = [0.2910330891609192, 0.008826445788145065, 0.2901504337787628]\n",
      "Step 623, loss = [0.29011115431785583, 0.008934352546930313, 0.2892177104949951]\n",
      "Step 624, loss = [0.283405601978302, 0.0039885262958705425, 0.2830067574977875]\n",
      "Step 625, loss = [0.30323415994644165, 0.010891874320805073, 0.30214497447013855]\n",
      "Step 626, loss = [0.2960186004638672, 0.0030302051454782486, 0.2957155704498291]\n",
      "Step 627, loss = [0.31051650643348694, 0.005441972985863686, 0.3099723160266876]\n",
      "Step 628, loss = [0.31320077180862427, 0.008569443598389626, 0.3123438358306885]\n",
      "Step 629, loss = [0.30324745178222656, 0.003893202869221568, 0.3028581440448761]\n",
      "Step 630, loss = [0.30034390091896057, 0.0038750674575567245, 0.29995638132095337]\n",
      "Step 631, loss = [0.3084510862827301, 0.0049491263926029205, 0.30795615911483765]\n",
      "Step 632, loss = [0.3027418255805969, 0.008462483994662762, 0.3018955886363983]\n",
      "Step 633, loss = [0.3097759187221527, 0.007603176869452, 0.3090156018733978]\n",
      "Step 634, loss = [0.30023497343063354, 0.006746112369000912, 0.2995603680610657]\n",
      "Step 635, loss = [0.29695796966552734, 0.007302817422896624, 0.29622769355773926]\n",
      "Step 636, loss = [0.3047355115413666, 0.002962683793157339, 0.3044392466545105]\n",
      "Step 637, loss = [0.2977718412876129, 0.005904262885451317, 0.2971814274787903]\n",
      "Step 638, loss = [0.29452019929885864, 0.006321313790977001, 0.29388806223869324]\n",
      "Step 639, loss = [0.30545690655708313, 0.00402848981320858, 0.30505406856536865]\n",
      "Step 640, loss = [0.30518272519111633, 0.005263345781713724, 0.30465638637542725]\n",
      "Step 641, loss = [0.2937780022621155, 0.008998371660709381, 0.2928781509399414]\n",
      "Step 642, loss = [0.30761510133743286, 0.00792006403207779, 0.30682310461997986]\n",
      "Step 643, loss = [0.30999085307121277, 0.005998333916068077, 0.3093910217285156]\n",
      "Step 644, loss = [0.30008795857429504, 0.004516058601438999, 0.2996363639831543]\n",
      "Step 645, loss = [0.3094453513622284, 0.005890414118766785, 0.3088563084602356]\n",
      "Step 646, loss = [0.29723092913627625, 0.007302275393158197, 0.29650071263313293]\n",
      "Step 647, loss = [0.3078729212284088, 0.009449644945561886, 0.30692794919013977]\n",
      "Step 648, loss = [0.29184088110923767, 0.005120514426380396, 0.2913288176059723]\n",
      "Step 649, loss = [0.30097612738609314, 0.0077279419638216496, 0.3002033233642578]\n",
      "Step 650, loss = [0.28146183490753174, 0.007234326563775539, 0.2807384133338928]\n",
      "Step 651, loss = [0.2861168682575226, 0.006945578847080469, 0.28542232513427734]\n",
      "Step 652, loss = [0.3078869581222534, 0.007406214252114296, 0.3071463406085968]\n",
      "Step 653, loss = [0.300229549407959, 0.004181272350251675, 0.2998114228248596]\n",
      "Step 654, loss = [0.30029013752937317, 0.007604914717376232, 0.29952964186668396]\n",
      "Step 655, loss = [0.2871094346046448, 0.004367880057543516, 0.2866726517677307]\n",
      "Step 656, loss = [0.31156203150749207, 0.0039073750376701355, 0.311171293258667]\n",
      "Step 657, loss = [0.299769788980484, 0.002087565138936043, 0.2995610237121582]\n",
      "Step 658, loss = [0.3018188774585724, 0.010447371751070023, 0.30077412724494934]\n",
      "Step 659, loss = [0.3036051392555237, 0.004520943388342857, 0.30315303802490234]\n",
      "Step 660, loss = [0.2864852547645569, 0.006177202798426151, 0.2858675420284271]\n",
      "Step 661, loss = [0.30329740047454834, 0.012941794469952583, 0.30200323462486267]\n",
      "Step 662, loss = [0.3001697063446045, 0.003013074630871415, 0.2998684048652649]\n",
      "Step 663, loss = [0.30593639612197876, 0.005756451282650232, 0.3053607642650604]\n",
      "Step 664, loss = [0.3053340017795563, 0.00672745518386364, 0.30466124415397644]\n",
      "Step 665, loss = [0.29701530933380127, 0.007880366407334805, 0.29622727632522583]\n",
      "Step 666, loss = [0.3078070282936096, 0.00998472236096859, 0.30680856108665466]\n",
      "Step 667, loss = [0.29544854164123535, 0.008949045091867447, 0.29455363750457764]\n",
      "Step 668, loss = [0.30196863412857056, 0.007359943352639675, 0.30123263597488403]\n",
      "Step 669, loss = [0.2943556010723114, 0.011234430596232414, 0.29323217272758484]\n",
      "Step 670, loss = [0.30435246229171753, 0.00947682373225689, 0.3034047782421112]\n",
      "Step 671, loss = [0.29422059655189514, 0.00904650054872036, 0.29331594705581665]\n",
      "Step 672, loss = [0.3013359010219574, 0.009341216646134853, 0.3004017770290375]\n",
      "Step 673, loss = [0.30161648988723755, 0.0069083161652088165, 0.30092567205429077]\n",
      "Step 674, loss = [0.30892303586006165, 0.004503777250647545, 0.3084726631641388]\n",
      "Step 675, loss = [0.29256030917167664, 0.005901041440665722, 0.2919701933860779]\n",
      "Step 676, loss = [0.29129117727279663, 0.008074050769209862, 0.2904837727546692]\n",
      "Step 677, loss = [0.300796777009964, 0.004241061396896839, 0.3003726601600647]\n",
      "Step 678, loss = [0.30004066228866577, 0.005522138439118862, 0.29948845505714417]\n",
      "Step 679, loss = [0.3131389915943146, 0.004551105201244354, 0.3126838803291321]\n",
      "Step 680, loss = [0.2946690022945404, 0.012529618106782436, 0.2934160530567169]\n",
      "Step 681, loss = [0.31528565287590027, 0.007012226153165102, 0.3145844340324402]\n",
      "Step 682, loss = [0.301278293132782, 0.004427521955221891, 0.3008355498313904]\n",
      "Step 683, loss = [0.30182474851608276, 0.004765519872307777, 0.3013482093811035]\n",
      "Step 684, loss = [0.30091592669487, 0.00805220752954483, 0.30011069774627686]\n",
      "Step 685, loss = [0.29200950264930725, 0.006365201435983181, 0.29137298464775085]\n",
      "Step 686, loss = [0.3076479732990265, 0.0069266315549612045, 0.3069553077220917]\n",
      "Step 687, loss = [0.28958359360694885, 0.00712804589420557, 0.28887078166007996]\n",
      "Step 688, loss = [0.3044605255126953, 0.006579128559678793, 0.30380260944366455]\n",
      "Step 689, loss = [0.3075478672981262, 0.005610688589513302, 0.30698680877685547]\n",
      "Step 690, loss = [0.308188259601593, 0.006150947883725166, 0.3075731694698334]\n",
      "Step 691, loss = [0.29746612906455994, 0.006747967563569546, 0.29679134488105774]\n",
      "Step 692, loss = [0.29230260848999023, 0.007170579396188259, 0.2915855646133423]\n",
      "Step 693, loss = [0.2986646592617035, 0.008738463744521141, 0.2977908253669739]\n",
      "Step 694, loss = [0.28984975814819336, 0.0069873277097940445, 0.28915101289749146]\n",
      "Step 695, loss = [0.2872820198535919, 0.00287021417170763, 0.286994993686676]\n",
      "Step 696, loss = [0.30758702754974365, 0.0034171240404248238, 0.30724531412124634]\n",
      "Step 697, loss = [0.28972721099853516, 0.0028153713792562485, 0.2894456684589386]\n",
      "Step 698, loss = [0.2964400351047516, 0.006280198693275452, 0.2958120107650757]\n",
      "Step 699, loss = [0.2856590449810028, 0.0075618065893650055, 0.2849028706550598]\n",
      "Step 700, loss = [0.3066495954990387, 0.004636409692466259, 0.3061859607696533]\n",
      "Step 701, loss = [0.3040628731250763, 0.003308368381112814, 0.3037320375442505]\n",
      "Step 702, loss = [0.2916085124015808, 0.002808457240462303, 0.29132765531539917]\n",
      "Step 703, loss = [0.29334914684295654, 0.008536769077181816, 0.292495459318161]\n",
      "Step 704, loss = [0.29789939522743225, 0.005073294509202242, 0.2973920702934265]\n",
      "Step 705, loss = [0.3063971996307373, 0.006762298755347729, 0.3057209551334381]\n",
      "Step 706, loss = [0.2934935390949249, 0.007828641682863235, 0.29271066188812256]\n",
      "Step 707, loss = [0.3043880760669708, 0.010733513161540031, 0.3033147156238556]\n",
      "Step 708, loss = [0.3048188388347626, 0.0031350580975413322, 0.304505318403244]\n",
      "Step 709, loss = [0.30761435627937317, 0.010183293372392654, 0.306596040725708]\n",
      "Step 710, loss = [0.31150510907173157, 0.009218464605510235, 0.3105832636356354]\n",
      "Step 711, loss = [0.3040563762187958, 0.008474486880004406, 0.3032089173793793]\n",
      "Step 712, loss = [0.29610928893089294, 0.006964090280234814, 0.2954128682613373]\n",
      "Step 713, loss = [0.3110300898551941, 0.009039247408509254, 0.3101261556148529]\n",
      "Step 714, loss = [0.2991383373737335, 0.00444468017667532, 0.29869386553764343]\n",
      "Step 715, loss = [0.3049562871456146, 0.0037761214189231396, 0.30457866191864014]\n",
      "Step 716, loss = [0.3107842206954956, 0.008144406601786613, 0.30996978282928467]\n",
      "Step 717, loss = [0.29363879561424255, 0.003314078552648425, 0.2933073937892914]\n",
      "Step 718, loss = [0.31190094351768494, 0.005862342659384012, 0.3113147020339966]\n",
      "Step 719, loss = [0.317654550075531, 0.008933945558965206, 0.31676116585731506]\n",
      "Step 720, loss = [0.3033323585987091, 0.008705283515155315, 0.3024618327617645]\n",
      "Step 721, loss = [0.2970269024372101, 0.0083481939509511, 0.29619207978248596]\n",
      "Step 722, loss = [0.3079994320869446, 0.004654970020055771, 0.30753394961357117]\n",
      "Step 723, loss = [0.3080352544784546, 0.006113135255873203, 0.3074239492416382]\n",
      "Step 724, loss = [0.2852681577205658, 0.012772046029567719, 0.2839909493923187]\n",
      "Step 725, loss = [0.3008417785167694, 0.004979298450052738, 0.3003438413143158]\n",
      "Step 726, loss = [0.3012285828590393, 0.0034237117506563663, 0.30088621377944946]\n",
      "Step 727, loss = [0.28278249502182007, 0.004566500429064035, 0.2823258340358734]\n",
      "Step 728, loss = [0.2998189330101013, 0.0066718910820782185, 0.299151748418808]\n",
      "Step 729, loss = [0.30687758326530457, 0.007849128916859627, 0.30609267950057983]\n",
      "Step 730, loss = [0.30071473121643066, 0.005870487540960312, 0.30012768507003784]\n",
      "Step 731, loss = [0.291498601436615, 0.00444029550999403, 0.2910545766353607]\n",
      "Step 732, loss = [0.3037550449371338, 0.008386911824345589, 0.30291634798049927]\n",
      "Step 733, loss = [0.30189549922943115, 0.007436432875692844, 0.301151841878891]\n",
      "Step 734, loss = [0.2930430471897125, 0.005746114067733288, 0.29246842861175537]\n",
      "Step 735, loss = [0.2832667827606201, 0.003848080523312092, 0.2828819751739502]\n",
      "Step 736, loss = [0.3123454451560974, 0.007474932353943586, 0.31159794330596924]\n",
      "Step 737, loss = [0.2909943461418152, 0.002513774437829852, 0.290742963552475]\n",
      "Step 738, loss = [0.3056682348251343, 0.005145762115716934, 0.30515366792678833]\n",
      "Step 739, loss = [0.30107757449150085, 0.009084454737603664, 0.3001691401004791]\n",
      "Step 740, loss = [0.30824247002601624, 0.008264021016657352, 0.30741608142852783]\n",
      "Step 741, loss = [0.3023471236228943, 0.004295026417821646, 0.3019176125526428]\n",
      "Step 742, loss = [0.3053511679172516, 0.007603669539093971, 0.30459079146385193]\n",
      "Step 743, loss = [0.3056078553199768, 0.005243026651442051, 0.3050835430622101]\n",
      "Step 744, loss = [0.305954247713089, 0.006357657723128796, 0.3053184747695923]\n",
      "Step 745, loss = [0.3002048134803772, 0.008677894249558449, 0.2993370294570923]\n",
      "Step 746, loss = [0.3090262711048126, 0.0048809172585606575, 0.30853816866874695]\n",
      "Step 747, loss = [0.3120647370815277, 0.005268699489533901, 0.31153786182403564]\n",
      "Step 748, loss = [0.31395214796066284, 0.008829774335026741, 0.3130691647529602]\n",
      "Step 749, loss = [0.31170445680618286, 0.008385196328163147, 0.31086593866348267]\n",
      "Step 750, loss = [0.29714688658714294, 0.004735675640404224, 0.29667332768440247]\n",
      "Step 751, loss = [0.2959495484828949, 0.010095473378896713, 0.2949399948120117]\n",
      "Step 752, loss = [0.2997453510761261, 0.005853652022778988, 0.299159973859787]\n",
      "Step 753, loss = [0.2987951934337616, 0.008362485095858574, 0.29795894026756287]\n",
      "Step 754, loss = [0.29954424500465393, 0.00388526963070035, 0.29915571212768555]\n",
      "Step 755, loss = [0.30680111050605774, 0.010564979165792465, 0.30574461817741394]\n",
      "Step 756, loss = [0.30844932794570923, 0.008090730756521225, 0.3076402544975281]\n",
      "Step 757, loss = [0.3101039230823517, 0.009277116507291794, 0.3091762065887451]\n",
      "Step 758, loss = [0.2958845794200897, 0.009117305278778076, 0.29497283697128296]\n",
      "Step 759, loss = [0.30433833599090576, 0.006828425917774439, 0.3036555051803589]\n",
      "Step 760, loss = [0.28696781396865845, 0.006613645236939192, 0.2863064408302307]\n",
      "Step 761, loss = [0.28626537322998047, 0.008339778520166874, 0.2854313850402832]\n",
      "Step 762, loss = [0.3069916069507599, 0.007119754329323769, 0.30627962946891785]\n",
      "Step 763, loss = [0.2921578288078308, 0.006184573285281658, 0.29153937101364136]\n",
      "Step 764, loss = [0.3047075867652893, 0.009376402013003826, 0.30376994609832764]\n",
      "Step 765, loss = [0.3145250678062439, 0.006833971943706274, 0.31384167075157166]\n",
      "Step 766, loss = [0.29961609840393066, 0.005850238725543022, 0.2990310788154602]\n",
      "Step 767, loss = [0.30501970648765564, 0.007710881531238556, 0.3042486310005188]\n",
      "Step 768, loss = [0.30053818225860596, 0.005230712704360485, 0.3000151216983795]\n",
      "Step 769, loss = [0.2946817874908447, 0.003888134378939867, 0.29429298639297485]\n",
      "Step 770, loss = [0.3005111515522003, 0.008395127952098846, 0.29967164993286133]\n",
      "Step 771, loss = [0.3099837005138397, 0.010759154334664345, 0.30890777707099915]\n",
      "Step 772, loss = [0.2950827181339264, 0.005594393238425255, 0.2945232689380646]\n",
      "Step 773, loss = [0.30165788531303406, 0.0030563657637685537, 0.30135226249694824]\n",
      "Step 774, loss = [0.30172860622406006, 0.0029880686197429895, 0.301429808139801]\n",
      "Step 775, loss = [0.30701684951782227, 0.0052172234281897545, 0.30649513006210327]\n",
      "Step 776, loss = [0.2937942445278168, 0.006336125545203686, 0.293160617351532]\n",
      "Step 777, loss = [0.31289345026016235, 0.005372986663132906, 0.3123561441898346]\n",
      "Step 778, loss = [0.2946629226207733, 0.0060960836708545685, 0.294053316116333]\n",
      "Step 779, loss = [0.30147045850753784, 0.007337925489991903, 0.300736665725708]\n",
      "Step 780, loss = [0.27251994609832764, 0.010671285912394524, 0.27145281434059143]\n",
      "Step 781, loss = [0.3063165545463562, 0.0030301117803901434, 0.3060135543346405]\n",
      "Step 782, loss = [0.30667343735694885, 0.006005246192216873, 0.3060729205608368]\n",
      "Step 783, loss = [0.3076741695404053, 0.011478202417492867, 0.3065263628959656]\n",
      "Step 784, loss = [0.30768364667892456, 0.007955270819365978, 0.3068881332874298]\n",
      "Step 785, loss = [0.30615678429603577, 0.006080487743020058, 0.3055487275123596]\n",
      "Step 786, loss = [0.30855318903923035, 0.008926133625209332, 0.3076605796813965]\n",
      "Step 787, loss = [0.30220291018486023, 0.007070929743349552, 0.30149582028388977]\n",
      "Step 788, loss = [0.2997420132160187, 0.011477148160338402, 0.29859429597854614]\n",
      "Step 789, loss = [0.287285715341568, 0.007372413761913776, 0.2865484654903412]\n",
      "Step 790, loss = [0.3044901490211487, 0.007213430479168892, 0.3037688136100769]\n",
      "Step 791, loss = [0.3084056079387665, 0.005834526848047972, 0.30782216787338257]\n",
      "Step 792, loss = [0.2970404028892517, 0.011381275951862335, 0.295902281999588]\n",
      "Step 793, loss = [0.3053548336029053, 0.008052339777350426, 0.30454960465431213]\n",
      "Step 794, loss = [0.2908514738082886, 0.006881233304738998, 0.2901633381843567]\n",
      "Step 795, loss = [0.2955543100833893, 0.004619184415787458, 0.2950924038887024]\n",
      "Step 796, loss = [0.29262253642082214, 0.008016001433134079, 0.2918209433555603]\n",
      "Step 797, loss = [0.2865654528141022, 0.007532057352364063, 0.28581225872039795]\n",
      "Step 798, loss = [0.29291221499443054, 0.0049454569816589355, 0.2924176752567291]\n",
      "Step 799, loss = [0.2805944085121155, 0.008046603761613369, 0.2797897458076477]\n",
      "Step 800, loss = [0.3132714331150055, 0.014551185071468353, 0.3118163049221039]\n",
      "Step 801, loss = [0.3014903664588928, 0.005348788574337959, 0.30095547437667847]\n",
      "Step 802, loss = [0.29068681597709656, 0.0040266988798975945, 0.2902841567993164]\n",
      "Step 803, loss = [0.3035699427127838, 0.005768236704170704, 0.30299311876296997]\n",
      "Step 804, loss = [0.30082160234451294, 0.007698568049818277, 0.300051748752594]\n",
      "Step 805, loss = [0.294548898935318, 0.006202764343470335, 0.2939286231994629]\n",
      "Step 806, loss = [0.27306750416755676, 0.003857796546071768, 0.27268171310424805]\n",
      "Step 807, loss = [0.28801730275154114, 0.006285841576755047, 0.28738871216773987]\n",
      "Step 808, loss = [0.2964194416999817, 0.010667643509805202, 0.29535266757011414]\n",
      "Step 809, loss = [0.30514320731163025, 0.01793432980775833, 0.3033497631549835]\n",
      "Step 810, loss = [0.2927526533603668, 0.012150978669524193, 0.2915375530719757]\n",
      "Step 811, loss = [0.30404260754585266, 0.01326022855937481, 0.30271658301353455]\n",
      "Step 812, loss = [0.3040998876094818, 0.006707075983285904, 0.30342918634414673]\n",
      "Step 813, loss = [0.30256107449531555, 0.00765475258231163, 0.3017956018447876]\n",
      "Step 814, loss = [0.3091685473918915, 0.0036245142109692097, 0.30880609154701233]\n",
      "Step 815, loss = [0.30297261476516724, 0.00782076921314001, 0.30219054222106934]\n",
      "Step 816, loss = [0.3105493485927582, 0.006505847908556461, 0.3098987638950348]\n",
      "Step 817, loss = [0.3082433044910431, 0.005995078943669796, 0.3076438009738922]\n",
      "Step 818, loss = [0.3026885390281677, 0.004785485565662384, 0.3022100031375885]\n",
      "Step 819, loss = [0.2893969416618347, 0.006345133297145367, 0.2887624204158783]\n",
      "Step 820, loss = [0.2861004173755646, 0.007250559516251087, 0.28537535667419434]\n",
      "Step 821, loss = [0.3069368302822113, 0.007065022364258766, 0.3062303364276886]\n",
      "Step 822, loss = [0.2931579649448395, 0.007349653169512749, 0.2924230098724365]\n",
      "Step 823, loss = [0.291765958070755, 0.011607153341174126, 0.29060524702072144]\n",
      "Step 824, loss = [0.30918994545936584, 0.0103879664093256, 0.30815115571022034]\n",
      "Step 825, loss = [0.3082734942436218, 0.005578175187110901, 0.30771568417549133]\n",
      "Step 826, loss = [0.28264182806015015, 0.006310426630079746, 0.2820107936859131]\n",
      "Step 827, loss = [0.2974599003791809, 0.006464180536568165, 0.2968134880065918]\n",
      "Step 828, loss = [0.29050368070602417, 0.010949745774269104, 0.28940871357917786]\n",
      "Step 829, loss = [0.28255292773246765, 0.0067881084978580475, 0.2818741202354431]\n",
      "Step 830, loss = [0.2875339090824127, 0.0034228553995490074, 0.28719162940979004]\n",
      "Step 831, loss = [0.3032401204109192, 0.004660435952246189, 0.3027740716934204]\n",
      "Step 832, loss = [0.30449485778808594, 0.007336704060435295, 0.30376118421554565]\n",
      "Step 833, loss = [0.2900380492210388, 0.006540676578879356, 0.28938397765159607]\n",
      "Step 834, loss = [0.3024228513240814, 0.007848412729799747, 0.30163800716400146]\n",
      "Step 835, loss = [0.28806090354919434, 0.004832921549677849, 0.2875775992870331]\n",
      "Step 836, loss = [0.2987673282623291, 0.006127932108938694, 0.2981545329093933]\n",
      "Step 837, loss = [0.30381953716278076, 0.006730361375957727, 0.30314651131629944]\n",
      "Step 838, loss = [0.3033286929130554, 0.002928251400589943, 0.3030358552932739]\n",
      "Step 839, loss = [0.30741602182388306, 0.005901656113564968, 0.3068258464336395]\n",
      "Step 840, loss = [0.3099089562892914, 0.006470155902206898, 0.3092619478702545]\n",
      "Step 841, loss = [0.31158965826034546, 0.006752749904990196, 0.31091439723968506]\n",
      "Step 842, loss = [0.3025699853897095, 0.0066793086007237434, 0.30190205574035645]\n",
      "Step 843, loss = [0.3062550723552704, 0.00484888069331646, 0.3057701885700226]\n",
      "Step 844, loss = [0.2937411367893219, 0.009462236426770687, 0.2927949130535126]\n",
      "Step 845, loss = [0.2930394113063812, 0.005965505726635456, 0.29244285821914673]\n",
      "Step 846, loss = [0.3073550760746002, 0.008132429793477058, 0.3065418303012848]\n",
      "Step 847, loss = [0.29918867349624634, 0.004962552804499865, 0.29869240522384644]\n",
      "Step 848, loss = [0.30145105719566345, 0.007326648570597172, 0.30071839690208435]\n",
      "Step 849, loss = [0.29671940207481384, 0.003217712976038456, 0.2963976263999939]\n",
      "Step 850, loss = [0.31003496050834656, 0.006856018211692572, 0.3093493580818176]\n",
      "Step 851, loss = [0.31028833985328674, 0.004090326838195324, 0.3098793029785156]\n",
      "Step 852, loss = [0.30076852440834045, 0.00835488736629486, 0.2999330461025238]\n",
      "Step 853, loss = [0.300224244594574, 0.005835158750414848, 0.2996407151222229]\n",
      "Step 854, loss = [0.30217236280441284, 0.0023789810948073864, 0.30193445086479187]\n",
      "Step 855, loss = [0.300540030002594, 0.007628151681274176, 0.29977720975875854]\n",
      "Step 856, loss = [0.26800960302352905, 0.0019060054328292608, 0.26781901717185974]\n",
      "Step 857, loss = [0.30449360609054565, 0.0041664536111056805, 0.3040769696235657]\n",
      "Step 858, loss = [0.30810853838920593, 0.006329952739179134, 0.3074755370616913]\n",
      "Step 859, loss = [0.298466295003891, 0.007527600042521954, 0.2977135479450226]\n",
      "Step 860, loss = [0.31119829416275024, 0.005540842190384865, 0.3106442093849182]\n",
      "Step 861, loss = [0.30850592255592346, 0.006774633191525936, 0.30782845616340637]\n",
      "Step 862, loss = [0.30109360814094543, 0.009067287668585777, 0.3001868724822998]\n",
      "Step 863, loss = [0.312183141708374, 0.003574354574084282, 0.311825692653656]\n",
      "Step 864, loss = [0.30819979310035706, 0.0038407999090850353, 0.30781570076942444]\n",
      "Step 865, loss = [0.2934187948703766, 0.008393613621592522, 0.29257944226264954]\n",
      "Step 866, loss = [0.30330297350883484, 0.003930386155843735, 0.3029099404811859]\n",
      "Step 867, loss = [0.2833622395992279, 0.0028345370665192604, 0.28307878971099854]\n",
      "Step 868, loss = [0.2922229468822479, 0.010746663436293602, 0.29114827513694763]\n",
      "Step 869, loss = [0.30323857069015503, 0.0036082076840102673, 0.3028777539730072]\n",
      "Step 870, loss = [0.2983196973800659, 0.004400169476866722, 0.2978796660900116]\n",
      "Step 871, loss = [0.3074188530445099, 0.0066795386373996735, 0.3067508935928345]\n",
      "Step 872, loss = [0.2938801944255829, 0.00675246212631464, 0.2932049334049225]\n",
      "Step 873, loss = [0.3113526701927185, 0.008950374089181423, 0.31045764684677124]\n",
      "Step 874, loss = [0.30428123474121094, 0.006027255207300186, 0.3036785125732422]\n",
      "Step 875, loss = [0.2969300150871277, 0.007709227502346039, 0.2961590886116028]\n",
      "Step 876, loss = [0.311730295419693, 0.004133325070142746, 0.31131696701049805]\n",
      "Step 877, loss = [0.3070705831050873, 0.008027436211705208, 0.3062678277492523]\n",
      "Step 878, loss = [0.3080720007419586, 0.006685782223939896, 0.30740341544151306]\n",
      "Step 879, loss = [0.3178599178791046, 0.004633820615708828, 0.31739652156829834]\n",
      "Step 880, loss = [0.2780449092388153, 0.005554019473493099, 0.2774895131587982]\n",
      "Step 881, loss = [0.2949616611003876, 0.005571799352765083, 0.2944044768810272]\n",
      "Step 882, loss = [0.30034810304641724, 0.006575007922947407, 0.2996906042098999]\n",
      "Step 883, loss = [0.30486857891082764, 0.00788048468530178, 0.3040805160999298]\n",
      "Step 884, loss = [0.2847753167152405, 0.010645726695656776, 0.2837107479572296]\n",
      "Step 885, loss = [0.30554288625717163, 0.00954689085483551, 0.3045881986618042]\n",
      "Step 886, loss = [0.3059297800064087, 0.00670262984931469, 0.3052595257759094]\n",
      "Step 887, loss = [0.29500946402549744, 0.007711153477430344, 0.2942383587360382]\n",
      "Step 888, loss = [0.2942679226398468, 0.006657661870121956, 0.2936021685600281]\n",
      "Step 889, loss = [0.31291013956069946, 0.00743086775764823, 0.31216704845428467]\n",
      "Step 890, loss = [0.3067987263202667, 0.005382831208407879, 0.30626043677330017]\n",
      "Step 891, loss = [0.3039472997188568, 0.010615510866045952, 0.3028857409954071]\n",
      "Step 892, loss = [0.28945207595825195, 0.003025170648470521, 0.28914955258369446]\n",
      "Step 893, loss = [0.2877808213233948, 0.006844979710876942, 0.2870963215827942]\n",
      "Step 894, loss = [0.30472174286842346, 0.004232815466821194, 0.304298460483551]\n",
      "Step 895, loss = [0.3074008524417877, 0.007549021393060684, 0.3066459596157074]\n",
      "Step 896, loss = [0.31275996565818787, 0.008540923707187176, 0.3119058609008789]\n",
      "Step 897, loss = [0.3091193437576294, 0.004846800118684769, 0.3086346685886383]\n",
      "Step 898, loss = [0.30199965834617615, 0.004549999721348286, 0.3015446662902832]\n",
      "Step 899, loss = [0.3015371561050415, 0.005094163119792938, 0.30102774500846863]\n",
      "Step 900, loss = [0.2993519604206085, 0.00689716637134552, 0.2986622452735901]\n",
      "Step 901, loss = [0.30945315957069397, 0.005569976754486561, 0.30889615416526794]\n",
      "Step 902, loss = [0.3037142753601074, 0.004379261285066605, 0.3032763600349426]\n",
      "Step 903, loss = [0.3078145980834961, 0.006880120374262333, 0.30712658166885376]\n",
      "Step 904, loss = [0.3097460865974426, 0.005059071350842714, 0.3092401921749115]\n",
      "Step 905, loss = [0.307921439409256, 0.003401345107704401, 0.3075813055038452]\n",
      "Step 906, loss = [0.30750301480293274, 0.004153912886977196, 0.30708763003349304]\n",
      "Step 907, loss = [0.3087891638278961, 0.007348975166678429, 0.30805426836013794]\n",
      "Step 908, loss = [0.3013131022453308, 0.011005891487002373, 0.3002125024795532]\n",
      "Step 909, loss = [0.2951197028160095, 0.008470875211060047, 0.29427260160446167]\n",
      "Step 910, loss = [0.29631656408309937, 0.007633893750607967, 0.29555317759513855]\n",
      "Step 911, loss = [0.31220972537994385, 0.009120730683207512, 0.3112976551055908]\n",
      "Step 912, loss = [0.3097951412200928, 0.005808267742395401, 0.309214323759079]\n",
      "Step 913, loss = [0.300509512424469, 0.0082791056483984, 0.2996816039085388]\n",
      "Step 914, loss = [0.29776033759117126, 0.0035826959647238255, 0.2974020540714264]\n",
      "Step 915, loss = [0.3042052984237671, 0.01130598783493042, 0.3030746877193451]\n",
      "Step 916, loss = [0.29753997921943665, 0.005861146375536919, 0.29695385694503784]\n",
      "Step 917, loss = [0.3118925392627716, 0.00626866240054369, 0.3112656772136688]\n",
      "Step 918, loss = [0.3050442636013031, 0.007133507169783115, 0.3043309152126312]\n",
      "Step 919, loss = [0.3082161545753479, 0.0072855036705732346, 0.3074876070022583]\n",
      "Step 920, loss = [0.3096477687358856, 0.01000327430665493, 0.3086474537849426]\n",
      "Step 921, loss = [0.30490508675575256, 0.006011082325130701, 0.30430397391319275]\n",
      "Step 922, loss = [0.2985294759273529, 0.005764948669821024, 0.2979529798030853]\n",
      "Step 923, loss = [0.2824578285217285, 0.006074812728911638, 0.28185033798217773]\n",
      "Step 924, loss = [0.29730555415153503, 0.006655614357441664, 0.29663997888565063]\n",
      "Step 925, loss = [0.2959200143814087, 0.009344028308987617, 0.2949856221675873]\n",
      "Step 926, loss = [0.2972676455974579, 0.008822041563689709, 0.29638543725013733]\n",
      "Step 927, loss = [0.3065131604671478, 0.0028822377789765596, 0.3062249422073364]\n",
      "Step 928, loss = [0.28511524200439453, 0.0025184438563883305, 0.2848634123802185]\n",
      "Step 929, loss = [0.31241747736930847, 0.009452952072024345, 0.31147217750549316]\n",
      "Step 930, loss = [0.29710087180137634, 0.008123595267534256, 0.29628852009773254]\n",
      "Step 931, loss = [0.29773783683776855, 0.006486749276518822, 0.297089159488678]\n",
      "Step 932, loss = [0.3080123960971832, 0.007845225743949413, 0.30722787976264954]\n",
      "Step 933, loss = [0.30549654364585876, 0.006477893330156803, 0.3048487603664398]\n",
      "Step 934, loss = [0.2977162301540375, 0.0077265407890081406, 0.2969435751438141]\n",
      "Step 935, loss = [0.3031284511089325, 0.0063064442947506905, 0.3024978041648865]\n",
      "Step 936, loss = [0.305505633354187, 0.0042644524946808815, 0.3050791919231415]\n",
      "Step 937, loss = [0.30883777141571045, 0.004980947822332382, 0.3083396852016449]\n",
      "Step 938, loss = [0.2800450623035431, 0.0067088971845805645, 0.2793741822242737]\n",
      "Step 939, loss = [0.29543495178222656, 0.00828607939183712, 0.29460635781288147]\n",
      "Step 940, loss = [0.307407945394516, 0.008433463983237743, 0.306564599275589]\n",
      "Step 941, loss = [0.29641005396842957, 0.0058684442192316055, 0.29582321643829346]\n",
      "Step 942, loss = [0.29139018058776855, 0.0036016034428030252, 0.29103001952171326]\n",
      "Step 943, loss = [0.30746451020240784, 0.014208965934813023, 0.3060436248779297]\n",
      "Step 944, loss = [0.2991326153278351, 0.005058848764747381, 0.29862672090530396]\n",
      "Step 945, loss = [0.31282007694244385, 0.0037080529145896435, 0.31244927644729614]\n",
      "Step 946, loss = [0.2941521406173706, 0.00904128048568964, 0.2932479977607727]\n",
      "Step 947, loss = [0.3083193600177765, 0.0036507747136056423, 0.3079542815685272]\n",
      "Step 948, loss = [0.31475022435188293, 0.007513971999287605, 0.313998818397522]\n",
      "Step 949, loss = [0.29483795166015625, 0.0027827490121126175, 0.29455968737602234]\n",
      "Step 950, loss = [0.2966662645339966, 0.0069101424887776375, 0.2959752380847931]\n",
      "Step 951, loss = [0.29778167605400085, 0.0032139404211193323, 0.29746028780937195]\n",
      "Step 952, loss = [0.2985496520996094, 0.005551931448280811, 0.297994464635849]\n",
      "Step 953, loss = [0.29861101508140564, 0.005045960191637278, 0.29810643196105957]\n",
      "Step 954, loss = [0.28290337324142456, 0.009069114923477173, 0.2819964587688446]\n",
      "Step 955, loss = [0.29134759306907654, 0.0038269096985459328, 0.29096490144729614]\n",
      "Step 956, loss = [0.2974855303764343, 0.00584047194570303, 0.29690149426460266]\n",
      "Step 957, loss = [0.2926594913005829, 0.005070528015494347, 0.29215243458747864]\n",
      "Step 958, loss = [0.3039017617702484, 0.0049145822413265705, 0.30341029167175293]\n",
      "Step 959, loss = [0.3028183877468109, 0.005256513599306345, 0.30229273438453674]\n",
      "Step 960, loss = [0.30861324071884155, 0.0036236231680959463, 0.30825087428092957]\n",
      "Step 961, loss = [0.2997173070907593, 0.009136320091784, 0.2988036870956421]\n",
      "Step 962, loss = [0.2866905927658081, 0.0063795726746320724, 0.2860526442527771]\n",
      "Step 963, loss = [0.2982407212257385, 0.010618597269058228, 0.29717886447906494]\n",
      "Step 964, loss = [0.30452319979667664, 0.012014001607894897, 0.30332180857658386]\n",
      "Step 965, loss = [0.2848736345767975, 0.009886074811220169, 0.28388503193855286]\n",
      "Step 966, loss = [0.28610631823539734, 0.012597592547535896, 0.28484654426574707]\n",
      "Step 967, loss = [0.292805552482605, 0.009525417350232601, 0.29185301065444946]\n",
      "Step 968, loss = [0.296507328748703, 0.005146493203938007, 0.2959926724433899]\n",
      "Step 969, loss = [0.3073798716068268, 0.008207854814827442, 0.30655908584594727]\n",
      "Step 970, loss = [0.2903510332107544, 0.006580892950296402, 0.2896929383277893]\n",
      "Step 971, loss = [0.2937226891517639, 0.00918399728834629, 0.2928043007850647]\n",
      "Step 972, loss = [0.29759418964385986, 0.00613087322562933, 0.2969810962677002]\n",
      "Step 973, loss = [0.29685068130493164, 0.007321684155613184, 0.29611852765083313]\n",
      "Step 974, loss = [0.28992876410484314, 0.003972550854086876, 0.28953149914741516]\n",
      "Step 975, loss = [0.30480241775512695, 0.008990539237856865, 0.30390337109565735]\n",
      "Step 976, loss = [0.30046984553337097, 0.0077134789898991585, 0.29969850182533264]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11754 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 977, loss = [0.307962566614151, 0.005633023101836443, 0.30739927291870117]\n",
      "Update target distribution epoch 1 step 978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11754/11754 [1:51:22<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos. rate:0.40980392156862744 Neg. rate:0.5901960784313726\n",
      "delta_label  0.012761613067891782\n",
      "Step 978, loss = [0.29053792357444763, 0.0036083657760173082, 0.2901770770549774]\n",
      "Step 979, loss = [0.30266350507736206, 0.00857090950012207, 0.30180642008781433]\n",
      "Step 980, loss = [0.28571540117263794, 0.009284494444727898, 0.2847869396209717]\n",
      "Step 981, loss = [0.29689159989356995, 0.010811381042003632, 0.2958104610443115]\n",
      "Step 982, loss = [0.29914841055870056, 0.004625881556421518, 0.29868581891059875]\n",
      "Step 983, loss = [0.31077510118484497, 0.011461075395345688, 0.30962899327278137]\n",
      "Step 984, loss = [0.2922290861606598, 0.009752285666763783, 0.29125386476516724]\n",
      "Step 985, loss = [0.2909763753414154, 0.006653623655438423, 0.2903110086917877]\n",
      "Step 986, loss = [0.30515986680984497, 0.009597420692443848, 0.30420011281967163]\n",
      "Step 987, loss = [0.3019552230834961, 0.007863106206059456, 0.30116891860961914]\n",
      "Step 988, loss = [0.29725268483161926, 0.006336969789117575, 0.2966189980506897]\n",
      "Step 989, loss = [0.2940422594547272, 0.008605899289250374, 0.2931816577911377]\n",
      "Step 990, loss = [0.3125912845134735, 0.007058638613671064, 0.31188541650772095]\n",
      "Step 991, loss = [0.30644550919532776, 0.006769427563995123, 0.30576857924461365]\n",
      "Step 992, loss = [0.31268787384033203, 0.008109765127301216, 0.31187689304351807]\n",
      "Step 993, loss = [0.2921321392059326, 0.008134865202009678, 0.2913186550140381]\n",
      "Step 994, loss = [0.28854838013648987, 0.005245057400316, 0.2880238890647888]\n",
      "Step 995, loss = [0.29134058952331543, 0.00872072670608759, 0.2904685139656067]\n",
      "Step 996, loss = [0.2998543977737427, 0.009124550968408585, 0.2989419400691986]\n",
      "Step 997, loss = [0.301321804523468, 0.010999076999723911, 0.30022189021110535]\n",
      "Step 998, loss = [0.2905791103839874, 0.0034406736958771944, 0.2902350425720215]\n",
      "Step 999, loss = [0.3077602982521057, 0.004937018267810345, 0.30726659297943115]\n",
      "Step 1000, loss = [0.3140977621078491, 0.00982160959392786, 0.31311559677124023]\n",
      "Step 1001, loss = [0.3035159707069397, 0.008068676106631756, 0.30270910263061523]\n",
      "Step 1002, loss = [0.29163023829460144, 0.005749200005084276, 0.2910553216934204]\n",
      "Step 1003, loss = [0.3022095561027527, 0.008619528263807297, 0.30134761333465576]\n",
      "Step 1004, loss = [0.30587783455848694, 0.004454643931239843, 0.30543237924575806]\n",
      "Step 1005, loss = [0.308395653963089, 0.007426554337143898, 0.30765300989151]\n",
      "Step 1006, loss = [0.2965148091316223, 0.009033811278641224, 0.2956114411354065]\n",
      "Step 1007, loss = [0.3004881739616394, 0.0030590821988880634, 0.3001822531223297]\n",
      "Step 1008, loss = [0.30098676681518555, 0.009272600524127483, 0.3000594973564148]\n",
      "Step 1009, loss = [0.30395498871803284, 0.006640204228460789, 0.3032909631729126]\n",
      "Step 1010, loss = [0.30987223982810974, 0.00711440434679389, 0.3091607987880707]\n",
      "Step 1011, loss = [0.2928137183189392, 0.007754005491733551, 0.29203832149505615]\n",
      "Step 1012, loss = [0.2963119149208069, 0.008458014577627182, 0.2954661250114441]\n",
      "Step 1013, loss = [0.29054102301597595, 0.004949349910020828, 0.2900460958480835]\n",
      "Step 1014, loss = [0.29238632321357727, 0.010341097600758076, 0.29135221242904663]\n",
      "Step 1015, loss = [0.30212077498435974, 0.007065105717629194, 0.30141425132751465]\n",
      "Step 1016, loss = [0.2923550307750702, 0.008280952461063862, 0.2915269434452057]\n",
      "Step 1017, loss = [0.29827573895454407, 0.006473971996456385, 0.29762834310531616]\n",
      "Step 1018, loss = [0.29988500475883484, 0.0032047105487436056, 0.29956454038619995]\n",
      "Step 1019, loss = [0.30424436926841736, 0.006660980172455311, 0.30357825756073]\n",
      "Step 1020, loss = [0.2931920886039734, 0.0040781209245324135, 0.29278427362442017]\n",
      "Step 1021, loss = [0.29295089840888977, 0.002446770202368498, 0.2927062213420868]\n",
      "Step 1022, loss = [0.3031929135322571, 0.006425570696592331, 0.302550345659256]\n",
      "Step 1023, loss = [0.30149590969085693, 0.00516862515360117, 0.30097904801368713]\n",
      "Step 1024, loss = [0.29727354645729065, 0.006688138470053673, 0.296604722738266]\n",
      "Step 1025, loss = [0.2922777235507965, 0.011922533623874187, 0.29108548164367676]\n",
      "Step 1026, loss = [0.29895222187042236, 0.0030121514573693275, 0.2986510097980499]\n",
      "Step 1027, loss = [0.29340001940727234, 0.0031029265373945236, 0.29308971762657166]\n",
      "Step 1028, loss = [0.2886097729206085, 0.0036473348736763, 0.2882450520992279]\n",
      "Step 1029, loss = [0.300541490316391, 0.004483959637582302, 0.3000930845737457]\n",
      "Step 1030, loss = [0.3063306212425232, 0.009417884051799774, 0.30538883805274963]\n",
      "Step 1031, loss = [0.2988321781158447, 0.0075184195302426815, 0.29808032512664795]\n",
      "Step 1032, loss = [0.3040328323841095, 0.004428256768733263, 0.30358999967575073]\n",
      "Step 1033, loss = [0.30721282958984375, 0.007077925838530064, 0.306505024433136]\n",
      "Step 1034, loss = [0.2903536558151245, 0.004434171132743359, 0.289910227060318]\n",
      "Step 1035, loss = [0.3022291362285614, 0.008453108370304108, 0.3013838231563568]\n",
      "Step 1036, loss = [0.31204044818878174, 0.007264048792421818, 0.31131404638290405]\n",
      "Step 1037, loss = [0.301960289478302, 0.005061298608779907, 0.3014541566371918]\n",
      "Step 1038, loss = [0.30940717458724976, 0.004354780539870262, 0.30897170305252075]\n",
      "Step 1039, loss = [0.2888481616973877, 0.004961356054991484, 0.28835201263427734]\n",
      "Step 1040, loss = [0.3023565113544464, 0.007955898530781269, 0.3015609085559845]\n",
      "Step 1041, loss = [0.2962818443775177, 0.005865259096026421, 0.29569530487060547]\n",
      "Step 1042, loss = [0.2997783422470093, 0.008699004538357258, 0.29890844225883484]\n",
      "Step 1043, loss = [0.31223341822624207, 0.007318419404327869, 0.31150156259536743]\n",
      "Step 1044, loss = [0.28928664326667786, 0.01254452858120203, 0.288032203912735]\n",
      "Step 1045, loss = [0.31663623452186584, 0.008861937560141087, 0.31575003266334534]\n",
      "Step 1046, loss = [0.2988166809082031, 0.00479314848780632, 0.2983373701572418]\n",
      "Step 1047, loss = [0.30819839239120483, 0.004322565160691738, 0.3077661395072937]\n",
      "Step 1048, loss = [0.30570682883262634, 0.008031802251935005, 0.30490365624427795]\n",
      "Step 1049, loss = [0.3069661557674408, 0.010419659316539764, 0.3059241771697998]\n",
      "Step 1050, loss = [0.30239129066467285, 0.004129653796553612, 0.30197831988334656]\n",
      "Step 1051, loss = [0.29663601517677307, 0.003367088967934251, 0.2962993085384369]\n",
      "Step 1052, loss = [0.29343023896217346, 0.008391555398702621, 0.2925910949707031]\n",
      "Step 1053, loss = [0.296429842710495, 0.008195169270038605, 0.29561033844947815]\n",
      "Step 1054, loss = [0.3043970465660095, 0.00398057047277689, 0.3039989769458771]\n",
      "Step 1055, loss = [0.28931939601898193, 0.007917801849544048, 0.28852760791778564]\n",
      "Step 1056, loss = [0.312363862991333, 0.00968484953045845, 0.3113953769207001]\n",
      "Step 1057, loss = [0.3080241084098816, 0.002993303816765547, 0.3077247738838196]\n",
      "Step 1058, loss = [0.31566521525382996, 0.006464467383921146, 0.31501877307891846]\n",
      "Step 1059, loss = [0.2726386487483978, 0.009703466668725014, 0.27166831493377686]\n",
      "Step 1060, loss = [0.2863389551639557, 0.004085960332304239, 0.2859303653240204]\n",
      "Step 1061, loss = [0.29386335611343384, 0.010559465736150742, 0.292807400226593]\n",
      "Step 1062, loss = [0.3002064824104309, 0.0049119554460048676, 0.2997152805328369]\n",
      "Step 1063, loss = [0.293837308883667, 0.002950576599687338, 0.2935422658920288]\n",
      "Step 1064, loss = [0.29210174083709717, 0.009031137451529503, 0.29119864106178284]\n",
      "Step 1065, loss = [0.30293142795562744, 0.0066769784316420555, 0.3022637367248535]\n",
      "Step 1066, loss = [0.3116776943206787, 0.00746826222166419, 0.31093087792396545]\n",
      "Step 1067, loss = [0.3046574592590332, 0.004993779119104147, 0.304158091545105]\n",
      "Step 1068, loss = [0.3010689914226532, 0.008481960743665695, 0.300220787525177]\n",
      "Step 1069, loss = [0.3104707896709442, 0.0024278080090880394, 0.31022801995277405]\n",
      "Step 1070, loss = [0.313124418258667, 0.006724676117300987, 0.31245195865631104]\n",
      "Step 1071, loss = [0.28008297085762024, 0.005921835079789162, 0.27949079871177673]\n",
      "Step 1072, loss = [0.30400219559669495, 0.0076371412724256516, 0.30323848128318787]\n",
      "Step 1073, loss = [0.2591017782688141, 0.0036247223615646362, 0.25873929262161255]\n",
      "Step 1074, loss = [0.3051696717739105, 0.009120850823819637, 0.3042576014995575]\n",
      "Step 1075, loss = [0.2956056296825409, 0.010535185225307941, 0.29455211758613586]\n",
      "Step 1076, loss = [0.30018192529678345, 0.01225455105304718, 0.2989564836025238]\n",
      "Step 1077, loss = [0.3058587312698364, 0.00976092554628849, 0.30488264560699463]\n",
      "Step 1078, loss = [0.2993813753128052, 0.005297018215060234, 0.2988516688346863]\n",
      "Step 1079, loss = [0.2933509349822998, 0.006682964041829109, 0.2926826477050781]\n",
      "Step 1080, loss = [0.3062599301338196, 0.004998789168894291, 0.30576005578041077]\n",
      "Step 1081, loss = [0.30374932289123535, 0.009747658856213093, 0.3027745485305786]\n",
      "Step 1082, loss = [0.2970413267612457, 0.0055037024430930614, 0.29649096727371216]\n",
      "Step 1083, loss = [0.3030553162097931, 0.005070503801107407, 0.30254825949668884]\n",
      "Step 1084, loss = [0.2811165452003479, 0.004149955697357655, 0.28070154786109924]\n",
      "Step 1085, loss = [0.29519742727279663, 0.005431243684142828, 0.29465430974960327]\n",
      "Step 1086, loss = [0.2998051047325134, 0.005022207275032997, 0.299302875995636]\n",
      "Step 1087, loss = [0.27299267053604126, 0.008742650970816612, 0.2721184194087982]\n",
      "Step 1088, loss = [0.29606419801712036, 0.0065521905198693275, 0.2954089641571045]\n",
      "Step 1089, loss = [0.279812753200531, 0.005039960611611605, 0.2793087661266327]\n",
      "Step 1090, loss = [0.294445663690567, 0.006207038648426533, 0.2938249707221985]\n",
      "Step 1091, loss = [0.3074226379394531, 0.007450120989233255, 0.3066776394844055]\n",
      "Step 1092, loss = [0.3022620379924774, 0.007147229742258787, 0.3015473186969757]\n",
      "Step 1093, loss = [0.3008055090904236, 0.01006514672189951, 0.29979899525642395]\n",
      "Step 1094, loss = [0.31753596663475037, 0.0039384313859045506, 0.317142128944397]\n",
      "Step 1095, loss = [0.2978168725967407, 0.010258878581225872, 0.2967909872531891]\n",
      "Step 1096, loss = [0.29838263988494873, 0.005008958280086517, 0.29788175225257874]\n",
      "Step 1097, loss = [0.299451619386673, 0.007933233864605427, 0.2986582815647125]\n",
      "Step 1098, loss = [0.2977076470851898, 0.004804507829248905, 0.2972272038459778]\n",
      "Step 1099, loss = [0.30437564849853516, 0.005046116188168526, 0.3038710355758667]\n",
      "Step 1100, loss = [0.3041597902774811, 0.006032300181686878, 0.30355656147003174]\n",
      "Step 1101, loss = [0.302876353263855, 0.011933349072933197, 0.3016830086708069]\n",
      "Step 1102, loss = [0.2782341539859772, 0.005692772567272186, 0.2776648700237274]\n",
      "Step 1103, loss = [0.3082447052001953, 0.008998062461614609, 0.307344913482666]\n",
      "Step 1104, loss = [0.30145010352134705, 0.01024041511118412, 0.30042606592178345]\n",
      "Step 1105, loss = [0.29348623752593994, 0.005026834085583687, 0.2929835617542267]\n",
      "Step 1106, loss = [0.28670454025268555, 0.008685934357345104, 0.28583595156669617]\n",
      "Step 1107, loss = [0.3131122887134552, 0.004608732182532549, 0.3126514256000519]\n",
      "Step 1108, loss = [0.2940710484981537, 0.005020868498831987, 0.2935689687728882]\n",
      "Step 1109, loss = [0.3076477348804474, 0.003804159350693226, 0.30726730823516846]\n",
      "Step 1110, loss = [0.30355656147003174, 0.01142867561429739, 0.3024137020111084]\n",
      "Step 1111, loss = [0.29222264885902405, 0.01107214204967022, 0.2911154329776764]\n",
      "Step 1112, loss = [0.30130720138549805, 0.01019626297056675, 0.3002875745296478]\n",
      "Step 1113, loss = [0.30090487003326416, 0.0054410649463534355, 0.300360769033432]\n",
      "Step 1114, loss = [0.30772921442985535, 0.005447233095765114, 0.30718448758125305]\n",
      "Step 1115, loss = [0.3039507269859314, 0.005083858966827393, 0.3034423291683197]\n",
      "Step 1116, loss = [0.2908676266670227, 0.005335965193808079, 0.290334016084671]\n",
      "Step 1117, loss = [0.2873317003250122, 0.0047827912494540215, 0.2868534326553345]\n",
      "Step 1118, loss = [0.29580798745155334, 0.007932249456644058, 0.29501476883888245]\n",
      "Step 1119, loss = [0.2982219457626343, 0.00479178037494421, 0.2977427542209625]\n",
      "Step 1120, loss = [0.2830852270126343, 0.003584511112421751, 0.2827267646789551]\n",
      "Step 1121, loss = [0.30798980593681335, 0.005549230147153139, 0.30743488669395447]\n",
      "Step 1122, loss = [0.2968226373195648, 0.005650185979902744, 0.2962576150894165]\n",
      "Step 1123, loss = [0.308110773563385, 0.007890749722719193, 0.307321697473526]\n",
      "Step 1124, loss = [0.2919706106185913, 0.008045002818107605, 0.2911660969257355]\n",
      "Step 1125, loss = [0.29820048809051514, 0.008885846473276615, 0.2973119020462036]\n",
      "Step 1126, loss = [0.30389857292175293, 0.004546438343822956, 0.30344393849372864]\n",
      "Step 1127, loss = [0.28301572799682617, 0.0072655947878956795, 0.28228917717933655]\n",
      "Step 1128, loss = [0.30250024795532227, 0.007708847522735596, 0.30172935128211975]\n",
      "Step 1129, loss = [0.3026227056980133, 0.005923243705183268, 0.30203038454055786]\n",
      "Step 1130, loss = [0.296204537153244, 0.009997563436627388, 0.2952047884464264]\n",
      "Step 1131, loss = [0.29421108961105347, 0.007889242842793465, 0.2934221625328064]\n",
      "Step 1132, loss = [0.30680105090141296, 0.007854989729821682, 0.3060155510902405]\n",
      "Step 1133, loss = [0.30355072021484375, 0.005441865883767605, 0.30300652980804443]\n",
      "Step 1134, loss = [0.28830787539482117, 0.004962095990777016, 0.28781166672706604]\n",
      "Step 1135, loss = [0.3017541170120239, 0.007032882422208786, 0.3010508418083191]\n",
      "Step 1136, loss = [0.3109308183193207, 0.010549167171120644, 0.3098759055137634]\n",
      "Step 1137, loss = [0.30192092061042786, 0.009206876158714294, 0.3010002374649048]\n",
      "Step 1138, loss = [0.3023669123649597, 0.008541259914636612, 0.30151277780532837]\n",
      "Step 1139, loss = [0.3006615936756134, 0.01181519590318203, 0.2994800806045532]\n",
      "Step 1140, loss = [0.3116513192653656, 0.0059838490560650826, 0.31105294823646545]\n",
      "Step 1141, loss = [0.29281991720199585, 0.0038651013746857643, 0.29243341088294983]\n",
      "Step 1142, loss = [0.3040611445903778, 0.011384565383195877, 0.30292269587516785]\n",
      "Step 1143, loss = [0.3012818694114685, 0.01195753924548626, 0.300086110830307]\n",
      "Step 1144, loss = [0.30302631855010986, 0.007403836585581303, 0.30228593945503235]\n",
      "Step 1145, loss = [0.29776856303215027, 0.004003592766821384, 0.29736819863319397]\n",
      "Step 1146, loss = [0.30513250827789307, 0.0038277003914117813, 0.3047497272491455]\n",
      "Step 1147, loss = [0.3081347644329071, 0.005225776229053736, 0.30761218070983887]\n",
      "Step 1148, loss = [0.30284079909324646, 0.00931798480451107, 0.3019089996814728]\n",
      "Step 1149, loss = [0.29769566655158997, 0.008340961299836636, 0.29686155915260315]\n",
      "Step 1150, loss = [0.2898326516151428, 0.009497497230768204, 0.28888291120529175]\n",
      "Step 1151, loss = [0.3066581189632416, 0.006080956198275089, 0.30605003237724304]\n",
      "Step 1152, loss = [0.2968776822090149, 0.006240662187337875, 0.29625362157821655]\n",
      "Step 1153, loss = [0.30766963958740234, 0.008659888058900833, 0.3068036437034607]\n",
      "Step 1154, loss = [0.3031828999519348, 0.004732958041131496, 0.3027096092700958]\n",
      "Step 1155, loss = [0.2949475347995758, 0.005519314669072628, 0.2943955957889557]\n",
      "Step 1156, loss = [0.28356727957725525, 0.0062308236956596375, 0.2829442024230957]\n",
      "Step 1157, loss = [0.30203381180763245, 0.005133547820150852, 0.3015204668045044]\n",
      "Step 1158, loss = [0.29367780685424805, 0.0017914911732077599, 0.2934986650943756]\n",
      "Step 1159, loss = [0.29928797483444214, 0.0037041015457361937, 0.2989175617694855]\n",
      "Step 1160, loss = [0.29989510774612427, 0.0014636011328548193, 0.2997487485408783]\n",
      "Step 1161, loss = [0.3073523938655853, 0.006789770442992449, 0.30667340755462646]\n",
      "Step 1162, loss = [0.3031039237976074, 0.012053439393639565, 0.3018985688686371]\n",
      "Step 1163, loss = [0.2851366698741913, 0.007898788899183273, 0.2843467891216278]\n",
      "Step 1164, loss = [0.3016888499259949, 0.009250007569789886, 0.3007638454437256]\n",
      "Step 1165, loss = [0.30212804675102234, 0.003792067989706993, 0.3017488420009613]\n",
      "Step 1166, loss = [0.3027574419975281, 0.007471579592674971, 0.30201029777526855]\n",
      "Step 1167, loss = [0.2929801344871521, 0.003087474498897791, 0.2926713824272156]\n",
      "Step 1168, loss = [0.2971668839454651, 0.00479817483574152, 0.2966870665550232]\n",
      "Step 1169, loss = [0.30118757486343384, 0.008626772090792656, 0.3003248870372772]\n",
      "Step 1170, loss = [0.3039824962615967, 0.00702295359224081, 0.30328020453453064]\n",
      "Step 1171, loss = [0.30330023169517517, 0.007696738466620445, 0.30253055691719055]\n",
      "Step 1172, loss = [0.3046567440032959, 0.0034209140576422215, 0.30431464314460754]\n",
      "Step 1173, loss = [0.2953267991542816, 0.0049990033730864525, 0.2948268949985504]\n",
      "Step 1174, loss = [0.3020099401473999, 0.007980311289429665, 0.3012119233608246]\n",
      "Step 1175, loss = [0.3012062907218933, 0.006444411352276802, 0.3005618453025818]\n",
      "Step 1176, loss = [0.29947012662887573, 0.0030252672731876373, 0.29916760325431824]\n",
      "Step 1177, loss = [0.3026437759399414, 0.009296605363488197, 0.30171412229537964]\n",
      "Step 1178, loss = [0.30459439754486084, 0.0027739384677261114, 0.30431699752807617]\n",
      "Step 1179, loss = [0.3103314936161041, 0.00334460218437016, 0.309997022151947]\n",
      "Step 1180, loss = [0.30238664150238037, 0.003866513492539525, 0.3019999861717224]\n",
      "Step 1181, loss = [0.30418848991394043, 0.003054666332900524, 0.30388301610946655]\n",
      "Step 1182, loss = [0.30318471789360046, 0.0043957168236374855, 0.30274513363838196]\n",
      "Step 1183, loss = [0.31224536895751953, 0.004460781812667847, 0.3117992877960205]\n",
      "Step 1184, loss = [0.30486050248146057, 0.00514377374202013, 0.30434611439704895]\n",
      "Step 1185, loss = [0.3045092821121216, 0.004386015236377716, 0.30407068133354187]\n",
      "Step 1186, loss = [0.30112341046333313, 0.00857755821198225, 0.3002656400203705]\n",
      "Step 1187, loss = [0.3107045888900757, 0.004613908939063549, 0.3102431893348694]\n",
      "Step 1188, loss = [0.29866042733192444, 0.006509278900921345, 0.2980094850063324]\n",
      "Step 1189, loss = [0.30235132575035095, 0.008017120882868767, 0.30154961347579956]\n",
      "Step 1190, loss = [0.29354965686798096, 0.013185791671276093, 0.29223108291625977]\n",
      "Step 1191, loss = [0.292431503534317, 0.004195283632725477, 0.29201197624206543]\n",
      "Step 1192, loss = [0.29637011885643005, 0.0047681089490652084, 0.2958933115005493]\n",
      "Step 1193, loss = [0.307017058134079, 0.007102231495082378, 0.3063068389892578]\n",
      "Step 1194, loss = [0.29733791947364807, 0.005556492600589991, 0.2967822849750519]\n",
      "Step 1195, loss = [0.30044668912887573, 0.010116368532180786, 0.2994350492954254]\n",
      "Step 1196, loss = [0.29088935256004333, 0.006423397455364466, 0.29024702310562134]\n",
      "Step 1197, loss = [0.29561230540275574, 0.009883074089884758, 0.294624000787735]\n",
      "Step 1198, loss = [0.3071533143520355, 0.005292561836540699, 0.30662405490875244]\n",
      "Step 1199, loss = [0.31483253836631775, 0.0075439102947711945, 0.314078152179718]\n",
      "Step 1200, loss = [0.3072269856929779, 0.004030732437968254, 0.3068239092826843]\n",
      "Step 1201, loss = [0.2982209324836731, 0.0059739286080002785, 0.29762354493141174]\n",
      "Step 1202, loss = [0.2783415615558624, 0.006326430011540651, 0.27770891785621643]\n",
      "Step 1203, loss = [0.30633336305618286, 0.008507663384079933, 0.3054825961589813]\n",
      "Step 1204, loss = [0.2990231215953827, 0.007120141759514809, 0.29831111431121826]\n",
      "Step 1205, loss = [0.2916887402534485, 0.006974235642701387, 0.29099130630493164]\n",
      "Step 1206, loss = [0.29962339997291565, 0.006240733899176121, 0.2989993393421173]\n",
      "Step 1207, loss = [0.29789769649505615, 0.0054653724655508995, 0.2973511517047882]\n",
      "Step 1208, loss = [0.3018178343772888, 0.007616698741912842, 0.3010561764240265]\n",
      "Step 1209, loss = [0.29401475191116333, 0.005645041354000568, 0.2934502363204956]\n",
      "Step 1210, loss = [0.3054903745651245, 0.006557646207511425, 0.30483460426330566]\n",
      "Step 1211, loss = [0.30341729521751404, 0.005682556424289942, 0.30284905433654785]\n",
      "Step 1212, loss = [0.29692453145980835, 0.005265314131975174, 0.29639801383018494]\n",
      "Step 1213, loss = [0.3030271828174591, 0.004015242215245962, 0.3026256561279297]\n",
      "Step 1214, loss = [0.29147982597351074, 0.006571302656084299, 0.29082268476486206]\n",
      "Step 1215, loss = [0.31034305691719055, 0.0067869191989302635, 0.30966436862945557]\n",
      "Step 1216, loss = [0.3018496632575989, 0.008159920573234558, 0.3010336756706238]\n",
      "Step 1217, loss = [0.2933369278907776, 0.009299606084823608, 0.29240697622299194]\n",
      "Step 1218, loss = [0.3038412928581238, 0.008416371420025826, 0.3029996454715729]\n",
      "Step 1219, loss = [0.3070540130138397, 0.012155873700976372, 0.3058384358882904]\n",
      "Step 1220, loss = [0.29618600010871887, 0.005563984625041485, 0.2956295907497406]\n",
      "Step 1221, loss = [0.28468990325927734, 0.004712085239589214, 0.2842186987400055]\n",
      "Step 1222, loss = [0.2846248745918274, 0.006526659708470106, 0.28397220373153687]\n",
      "Step 1223, loss = [0.3070428669452667, 0.005672418512403965, 0.3064756393432617]\n",
      "Step 1224, loss = [0.3016229569911957, 0.009712230414152145, 0.3006517291069031]\n",
      "Step 1225, loss = [0.28479576110839844, 0.00664234533905983, 0.2841315269470215]\n",
      "Step 1226, loss = [0.29834407567977905, 0.006438871845602989, 0.2977001965045929]\n",
      "Step 1227, loss = [0.29582953453063965, 0.006092670373618603, 0.2952202558517456]\n",
      "Step 1228, loss = [0.2900421619415283, 0.006972027476876974, 0.2893449664115906]\n",
      "Step 1229, loss = [0.2920088469982147, 0.00801084190607071, 0.29120776057243347]\n",
      "Step 1230, loss = [0.30804675817489624, 0.006766502745449543, 0.3073700964450836]\n",
      "Step 1231, loss = [0.2891506552696228, 0.010729284025728703, 0.288077712059021]\n",
      "Step 1232, loss = [0.2798410654067993, 0.015450550243258476, 0.27829602360725403]\n",
      "Step 1233, loss = [0.3080879747867584, 0.007050739601254463, 0.3073829114437103]\n",
      "Step 1234, loss = [0.2996382713317871, 0.008820882998406887, 0.2987561821937561]\n",
      "Step 1235, loss = [0.2859894037246704, 0.011422332376241684, 0.2848471701145172]\n",
      "Step 1236, loss = [0.30583450198173523, 0.008253058418631554, 0.3050091862678528]\n",
      "Step 1237, loss = [0.29015129804611206, 0.007346156053245068, 0.28941667079925537]\n",
      "Step 1238, loss = [0.2813735008239746, 0.003736148588359356, 0.28099989891052246]\n",
      "Step 1239, loss = [0.3041931688785553, 0.00352264940738678, 0.30384090542793274]\n",
      "Step 1240, loss = [0.2996549606323242, 0.009353790432214737, 0.298719584941864]\n",
      "Step 1241, loss = [0.2898597717285156, 0.005930507089942694, 0.2892667353153229]\n",
      "Step 1242, loss = [0.29924720525741577, 0.003809690475463867, 0.29886624217033386]\n",
      "Step 1243, loss = [0.3111974596977234, 0.006908740382641554, 0.31050658226013184]\n",
      "Step 1244, loss = [0.30331921577453613, 0.005788221023976803, 0.3027403950691223]\n",
      "Step 1245, loss = [0.3010866940021515, 0.006658201105892658, 0.300420880317688]\n",
      "Step 1246, loss = [0.29542601108551025, 0.0097834886983037, 0.294447660446167]\n",
      "Step 1247, loss = [0.3003564774990082, 0.004749426618218422, 0.29988154768943787]\n",
      "Step 1248, loss = [0.30376824736595154, 0.009485279209911823, 0.30281972885131836]\n",
      "Step 1249, loss = [0.30441808700561523, 0.008305232971906662, 0.30358755588531494]\n",
      "Step 1250, loss = [0.30676424503326416, 0.004427176900207996, 0.30632153153419495]\n",
      "Step 1251, loss = [0.30171898007392883, 0.005836306139826775, 0.3011353611946106]\n",
      "Step 1252, loss = [0.2916070222854614, 0.002947794273495674, 0.29131224751472473]\n",
      "Step 1253, loss = [0.3042643666267395, 0.006250352598726749, 0.30363932251930237]\n",
      "Step 1254, loss = [0.2912016212940216, 0.009086936712265015, 0.2902929186820984]\n",
      "Step 1255, loss = [0.2968202531337738, 0.007375514134764671, 0.2960827052593231]\n",
      "Step 1256, loss = [0.3002248704433441, 0.006933734752237797, 0.299531489610672]\n",
      "Step 1257, loss = [0.2992945909500122, 0.004686073400080204, 0.2988259792327881]\n",
      "Step 1258, loss = [0.29963117837905884, 0.005991585087031126, 0.2990320324897766]\n",
      "Step 1259, loss = [0.29962974786758423, 0.0063383374363183975, 0.2989959120750427]\n",
      "Step 1260, loss = [0.30990439653396606, 0.009227474220097065, 0.30898165702819824]\n",
      "Step 1261, loss = [0.28199654817581177, 0.00965646468102932, 0.28103089332580566]\n",
      "Step 1262, loss = [0.3146894574165344, 0.008336406201124191, 0.3138558268547058]\n",
      "Step 1263, loss = [0.293927401304245, 0.004849138669669628, 0.2934424877166748]\n",
      "Step 1264, loss = [0.29864057898521423, 0.005693312734365463, 0.2980712354183197]\n",
      "Step 1265, loss = [0.2988613247871399, 0.008231786079704762, 0.29803815484046936]\n",
      "Step 1266, loss = [0.30693507194519043, 0.0062561193481087685, 0.30630946159362793]\n",
      "Step 1267, loss = [0.3043595552444458, 0.006915414705872536, 0.3036680221557617]\n",
      "Step 1268, loss = [0.3100862205028534, 0.0068021961487829685, 0.30940601229667664]\n",
      "Step 1269, loss = [0.31046056747436523, 0.004965260159224272, 0.30996403098106384]\n",
      "Step 1270, loss = [0.3055655062198639, 0.0062803840264678, 0.304937481880188]\n",
      "Step 1271, loss = [0.3001036047935486, 0.003438910935074091, 0.29975971579551697]\n",
      "Step 1272, loss = [0.2777866721153259, 0.0023090688046067953, 0.27755576372146606]\n",
      "Step 1273, loss = [0.3054293096065521, 0.010792526416480541, 0.3043500483036041]\n",
      "Step 1274, loss = [0.2996092438697815, 0.0036462577991187572, 0.29924461245536804]\n",
      "Step 1275, loss = [0.30472302436828613, 0.0048126839101314545, 0.30424174666404724]\n",
      "Step 1276, loss = [0.3024802505970001, 0.006489383988082409, 0.30183130502700806]\n",
      "Step 1277, loss = [0.29742416739463806, 0.004936737008392811, 0.2969304919242859]\n",
      "Step 1278, loss = [0.29707083106040955, 0.00590935442596674, 0.29647988080978394]\n",
      "Step 1279, loss = [0.3074099123477936, 0.00612806249409914, 0.3067971169948578]\n",
      "Step 1280, loss = [0.2887042760848999, 0.0018269263673573732, 0.28852158784866333]\n",
      "Step 1281, loss = [0.3004785180091858, 0.006212176755070686, 0.2998572885990143]\n",
      "Step 1282, loss = [0.2828574478626251, 0.013752160593867302, 0.2814822196960449]\n",
      "Step 1283, loss = [0.3026766777038574, 0.004938055761158466, 0.3021828830242157]\n",
      "Step 1284, loss = [0.28571242094039917, 0.006659627892076969, 0.28504645824432373]\n",
      "Step 1285, loss = [0.29485994577407837, 0.005863575264811516, 0.29427358508110046]\n",
      "Step 1286, loss = [0.3000343143939972, 0.007295708172023296, 0.2993047535419464]\n",
      "Step 1287, loss = [0.2882167398929596, 0.008076440542936325, 0.28740909695625305]\n",
      "Step 1288, loss = [0.30945268273353577, 0.012050515040755272, 0.3082476258277893]\n",
      "Step 1289, loss = [0.30221056938171387, 0.006494377739727497, 0.3015611171722412]\n",
      "Step 1290, loss = [0.2968203127384186, 0.003194450633600354, 0.2965008616447449]\n",
      "Step 1291, loss = [0.29309505224227905, 0.004170913249254227, 0.29267796874046326]\n",
      "Step 1292, loss = [0.31106114387512207, 0.012408190406858921, 0.3098203241825104]\n",
      "Step 1293, loss = [0.29665160179138184, 0.007727073505520821, 0.29587888717651367]\n",
      "Step 1294, loss = [0.30542832612991333, 0.0018522078171372414, 0.3052431046962738]\n",
      "Step 1295, loss = [0.29899299144744873, 0.008441552519798279, 0.29814884066581726]\n",
      "Step 1296, loss = [0.30594402551651, 0.004697325173765421, 0.30547428131103516]\n",
      "Step 1297, loss = [0.3022865056991577, 0.00452074920758605, 0.30183443427085876]\n",
      "Step 1298, loss = [0.3087673783302307, 0.005271556321531534, 0.30824023485183716]\n",
      "Step 1299, loss = [0.3044104278087616, 0.0030064284801483154, 0.3041097819805145]\n",
      "Step 1300, loss = [0.30807456374168396, 0.00340685760602355, 0.3077338635921478]\n",
      "Step 1301, loss = [0.2914779782295227, 0.004823750350624323, 0.29099559783935547]\n",
      "Step 1302, loss = [0.301821768283844, 0.009604052640497684, 0.3008613586425781]\n",
      "Step 1303, loss = [0.30856505036354065, 0.007854361087083817, 0.30777961015701294]\n",
      "Step 1304, loss = [0.31129997968673706, 0.012130193412303925, 0.3100869655609131]\n",
      "Step 1305, loss = [0.30108997225761414, 0.0061912331730127335, 0.30047085881233215]\n",
      "Step 1306, loss = [0.29660168290138245, 0.005338629707694054, 0.29606783390045166]\n",
      "Step 1307, loss = [0.2897145748138428, 0.004789051599800587, 0.2892356812953949]\n",
      "Step 1308, loss = [0.2891302704811096, 0.007902698591351509, 0.2883400022983551]\n",
      "Step 1309, loss = [0.30310678482055664, 0.004783927462995052, 0.30262839794158936]\n",
      "Step 1310, loss = [0.29342135787010193, 0.004343661479651928, 0.29298698902130127]\n",
      "Step 1311, loss = [0.3030555844306946, 0.005571217741817236, 0.302498459815979]\n",
      "Step 1312, loss = [0.30158093571662903, 0.007105586584657431, 0.3008703887462616]\n",
      "Step 1313, loss = [0.30058106780052185, 0.004771575331687927, 0.30010390281677246]\n",
      "Step 1314, loss = [0.28179246187210083, 0.008468082174658775, 0.28094565868377686]\n",
      "Step 1315, loss = [0.3003599941730499, 0.00784510187804699, 0.29957547783851624]\n",
      "Step 1316, loss = [0.29183244705200195, 0.009710326790809631, 0.29086142778396606]\n",
      "Step 1317, loss = [0.3096233308315277, 0.005957756191492081, 0.3090275526046753]\n",
      "Step 1318, loss = [0.29296156764030457, 0.006553603336215019, 0.29230621457099915]\n",
      "Step 1319, loss = [0.30780211091041565, 0.00635924469679594, 0.307166188955307]\n",
      "Step 1320, loss = [0.3047564923763275, 0.006505077704787254, 0.3041059970855713]\n",
      "Step 1321, loss = [0.28957486152648926, 0.005118164233863354, 0.289063036441803]\n",
      "Step 1322, loss = [0.30015796422958374, 0.0031945323571562767, 0.29983851313591003]\n",
      "Step 1323, loss = [0.28787171840667725, 0.006598439998924732, 0.2872118651866913]\n",
      "Step 1324, loss = [0.3041470944881439, 0.004779481329023838, 0.30366915464401245]\n",
      "Step 1325, loss = [0.2934143841266632, 0.005395481362938881, 0.29287484288215637]\n",
      "Step 1326, loss = [0.30348119139671326, 0.007855536416172981, 0.302695631980896]\n",
      "Step 1327, loss = [0.3036268353462219, 0.013091989792883396, 0.30231764912605286]\n",
      "Step 1328, loss = [0.29960161447525024, 0.00909203477203846, 0.29869240522384644]\n",
      "Step 1329, loss = [0.29489052295684814, 0.006599913816899061, 0.29423052072525024]\n",
      "Step 1330, loss = [0.3006502687931061, 0.008970554918050766, 0.29975321888923645]\n",
      "Step 1331, loss = [0.30121830105781555, 0.009996490553021431, 0.3002186417579651]\n",
      "Step 1332, loss = [0.31623080372810364, 0.0047270492650568485, 0.3157581090927124]\n",
      "Step 1333, loss = [0.30821800231933594, 0.005887548439204693, 0.307629257440567]\n",
      "Step 1334, loss = [0.30219194293022156, 0.007314448244869709, 0.30146050453186035]\n",
      "Step 1335, loss = [0.3058449625968933, 0.009779185056686401, 0.30486705899238586]\n",
      "Step 1336, loss = [0.3037039339542389, 0.008460810407996178, 0.3028578460216522]\n",
      "Step 1337, loss = [0.30148643255233765, 0.007451529148966074, 0.3007412850856781]\n",
      "Step 1338, loss = [0.2946886420249939, 0.004222075454890728, 0.2942664325237274]\n",
      "Step 1339, loss = [0.302796870470047, 0.008075959980487823, 0.30198928713798523]\n",
      "Step 1340, loss = [0.29030728340148926, 0.002287090290337801, 0.2900785803794861]\n",
      "Step 1341, loss = [0.30723798274993896, 0.007378051057457924, 0.3065001666545868]\n",
      "Step 1342, loss = [0.2968345284461975, 0.008190827444195747, 0.2960154414176941]\n",
      "Step 1343, loss = [0.29361462593078613, 0.006727240048348904, 0.2929418981075287]\n",
      "Step 1344, loss = [0.3000888228416443, 0.008645794354379177, 0.29922425746917725]\n",
      "Step 1345, loss = [0.29829975962638855, 0.007217067759484053, 0.2975780665874481]\n",
      "Step 1346, loss = [0.30901864171028137, 0.0060483599081635475, 0.3084138035774231]\n",
      "Step 1347, loss = [0.30240416526794434, 0.005553483497351408, 0.301848828792572]\n",
      "Step 1348, loss = [0.30357274413108826, 0.0015289995353668928, 0.3034198582172394]\n",
      "Step 1349, loss = [0.29818201065063477, 0.004784717224538326, 0.2977035343647003]\n",
      "Step 1350, loss = [0.3043563961982727, 0.007505374029278755, 0.303605854511261]\n",
      "Step 1351, loss = [0.3078245222568512, 0.005687789525836706, 0.30725574493408203]\n",
      "Step 1352, loss = [0.31023919582366943, 0.0069491504691541195, 0.30954429507255554]\n",
      "Step 1353, loss = [0.30705782771110535, 0.009561976417899132, 0.30610162019729614]\n",
      "Step 1354, loss = [0.2999710142612457, 0.03551279753446579, 0.29641973972320557]\n",
      "Step 1355, loss = [0.30422940850257874, 0.004567424766719341, 0.3037726581096649]\n",
      "Step 1356, loss = [0.3090754747390747, 0.00704948278144002, 0.30837053060531616]\n",
      "Step 1357, loss = [0.28321129083633423, 0.0035612722858786583, 0.28285515308380127]\n",
      "Step 1358, loss = [0.29562029242515564, 0.006444100756198168, 0.2949758768081665]\n",
      "Step 1359, loss = [0.3087431788444519, 0.004976263735443354, 0.30824553966522217]\n",
      "Step 1360, loss = [0.3086926341056824, 0.009345544502139091, 0.307758092880249]\n",
      "Step 1361, loss = [0.30803433060646057, 0.006887640804052353, 0.30734556913375854]\n",
      "Step 1362, loss = [0.29633629322052, 0.010116065852344036, 0.2953246831893921]\n",
      "Step 1363, loss = [0.2970004081726074, 0.004687556531280279, 0.29653164744377136]\n",
      "Step 1364, loss = [0.3042996823787689, 0.005801752209663391, 0.30371952056884766]\n",
      "Step 1365, loss = [0.2957709729671478, 0.009377649053931236, 0.2948332130908966]\n",
      "Step 1366, loss = [0.28520646691322327, 0.007021898403763771, 0.2845042645931244]\n",
      "Step 1367, loss = [0.2964291572570801, 0.005972183309495449, 0.29583194851875305]\n",
      "Step 1368, loss = [0.2993184030056, 0.00891976710408926, 0.29842641949653625]\n",
      "Step 1369, loss = [0.28449562191963196, 0.009760916233062744, 0.28351953625679016]\n",
      "Step 1370, loss = [0.31310924887657166, 0.00324007635936141, 0.31278523802757263]\n",
      "Step 1371, loss = [0.302485853433609, 0.0075627341866493225, 0.30172958970069885]\n",
      "Step 1372, loss = [0.3115205764770508, 0.005739171057939529, 0.31094667315483093]\n",
      "Step 1373, loss = [0.30381301045417786, 0.004764116369187832, 0.30333659052848816]\n",
      "Step 1374, loss = [0.3076333701610565, 0.011906314641237259, 0.3064427375793457]\n",
      "Step 1375, loss = [0.2989809215068817, 0.008677014149725437, 0.29811322689056396]\n",
      "Step 1376, loss = [0.3092097043991089, 0.009364652447402477, 0.30827322602272034]\n",
      "Step 1377, loss = [0.29164770245552063, 0.010212453082203865, 0.2906264662742615]\n",
      "Step 1378, loss = [0.30437731742858887, 0.0031575949396938086, 0.30406156182289124]\n",
      "Step 1379, loss = [0.28946512937545776, 0.012823835015296936, 0.28818273544311523]\n",
      "Step 1380, loss = [0.3143472969532013, 0.006913656368851662, 0.31365594267845154]\n",
      "Step 1381, loss = [0.30414775013923645, 0.009025296196341515, 0.3032452166080475]\n",
      "Step 1382, loss = [0.2974369525909424, 0.003387805540114641, 0.29709815979003906]\n",
      "Step 1383, loss = [0.29520702362060547, 0.008669303730130196, 0.2943401038646698]\n",
      "Step 1384, loss = [0.3020046651363373, 0.007081632502377033, 0.30129650235176086]\n",
      "Step 1385, loss = [0.3028358519077301, 0.00664451764896512, 0.30217140913009644]\n",
      "Step 1386, loss = [0.3019755184650421, 0.011140793561935425, 0.3008614480495453]\n",
      "Step 1387, loss = [0.2885868549346924, 0.007211919873952866, 0.28786566853523254]\n",
      "Step 1388, loss = [0.3005203604698181, 0.015279681421816349, 0.298992395401001]\n",
      "Step 1389, loss = [0.28457406163215637, 0.01372910849750042, 0.2832011580467224]\n",
      "Step 1390, loss = [0.2761378884315491, 0.005826300010085106, 0.27555525302886963]\n",
      "Step 1391, loss = [0.3017652630805969, 0.0073272306472063065, 0.30103254318237305]\n",
      "Step 1392, loss = [0.2946387231349945, 0.007426659110933542, 0.29389604926109314]\n",
      "Step 1393, loss = [0.31195956468582153, 0.009044206701219082, 0.31105515360832214]\n",
      "Step 1394, loss = [0.3025800883769989, 0.009900098666548729, 0.30159008502960205]\n",
      "Step 1395, loss = [0.30951038002967834, 0.006349829025566578, 0.3088754117488861]\n",
      "Step 1396, loss = [0.29469043016433716, 0.004334530793130398, 0.2942569851875305]\n",
      "Step 1397, loss = [0.3094470500946045, 0.0048418063670396805, 0.308962881565094]\n",
      "Step 1398, loss = [0.30955737829208374, 0.00868077389895916, 0.30868929624557495]\n",
      "Step 1399, loss = [0.290633887052536, 0.0063767991960048676, 0.2899962067604065]\n",
      "Step 1400, loss = [0.2899521291255951, 0.005204495508223772, 0.28943169116973877]\n",
      "Step 1401, loss = [0.31294357776641846, 0.004121533595025539, 0.31253141164779663]\n",
      "Step 1402, loss = [0.28745725750923157, 0.007958895526826382, 0.2866613566875458]\n",
      "Step 1403, loss = [0.306189626455307, 0.003286841092631221, 0.3058609366416931]\n",
      "Step 1404, loss = [0.3023359775543213, 0.004402715712785721, 0.30189570784568787]\n",
      "Step 1405, loss = [0.3006685972213745, 0.009733702056109905, 0.29969522356987]\n",
      "Step 1406, loss = [0.30746766924858093, 0.006792896892875433, 0.3067883849143982]\n",
      "Step 1407, loss = [0.3036562502384186, 0.006330966949462891, 0.30302315950393677]\n",
      "Step 1408, loss = [0.2689947187900543, 0.00261866906657815, 0.26873284578323364]\n",
      "Step 1409, loss = [0.2853291630744934, 0.008031156845390797, 0.2845260500907898]\n",
      "Step 1410, loss = [0.30906370282173157, 0.004592176992446184, 0.30860447883605957]\n",
      "Step 1411, loss = [0.30274975299835205, 0.009180933237075806, 0.3018316626548767]\n",
      "Step 1412, loss = [0.30939024686813354, 0.006317882798612118, 0.3087584674358368]\n",
      "Step 1413, loss = [0.3013460636138916, 0.0033471425995230675, 0.3010113537311554]\n",
      "Step 1414, loss = [0.306657075881958, 0.008328269235789776, 0.30582424998283386]\n",
      "Step 1415, loss = [0.27329593896865845, 0.00699927331879735, 0.27259600162506104]\n",
      "Step 1416, loss = [0.27449601888656616, 0.007573272567242384, 0.27373868227005005]\n",
      "Step 1417, loss = [0.30115026235580444, 0.011654531583189964, 0.29998481273651123]\n",
      "Step 1418, loss = [0.2989939749240875, 0.007324664853513241, 0.29826152324676514]\n",
      "Step 1419, loss = [0.28324922919273376, 0.002985349390655756, 0.2829506993293762]\n",
      "Step 1420, loss = [0.30838578939437866, 0.003534009214490652, 0.30803239345550537]\n",
      "Step 1421, loss = [0.29459115862846375, 0.010694773867726326, 0.2935216724872589]\n",
      "Step 1422, loss = [0.3033088147640228, 0.005461304448544979, 0.3027626872062683]\n",
      "Step 1423, loss = [0.28827545046806335, 0.005637060850858688, 0.2877117395401001]\n",
      "Step 1424, loss = [0.2951284945011139, 0.009620795026421547, 0.2941664159297943]\n",
      "Step 1425, loss = [0.3013250529766083, 0.0038015220779925585, 0.30094489455223083]\n",
      "Step 1426, loss = [0.2977721095085144, 0.005023025441914797, 0.2972698211669922]\n",
      "Step 1427, loss = [0.3090232312679291, 0.011921755969524384, 0.3078310489654541]\n",
      "Step 1428, loss = [0.3096407353878021, 0.007541476748883724, 0.3088865876197815]\n",
      "Step 1429, loss = [0.2964470684528351, 0.004595407284796238, 0.2959875166416168]\n",
      "Step 1430, loss = [0.29030126333236694, 0.008304253220558167, 0.2894708514213562]\n",
      "Step 1431, loss = [0.3143927752971649, 0.00823613628745079, 0.31356915831565857]\n",
      "Step 1432, loss = [0.3143359422683716, 0.006490570027381182, 0.31368687748908997]\n",
      "Step 1433, loss = [0.3113015294075012, 0.008354831486940384, 0.31046605110168457]\n",
      "Step 1434, loss = [0.3026513159275055, 0.005233009345829487, 0.30212801694869995]\n",
      "Step 1435, loss = [0.2975296080112457, 0.0079424437135458, 0.29673537611961365]\n",
      "Step 1436, loss = [0.30791541934013367, 0.00736957648769021, 0.30717846751213074]\n",
      "Step 1437, loss = [0.280143141746521, 0.009177865460515022, 0.27922534942626953]\n",
      "Step 1438, loss = [0.2938128113746643, 0.005396654829382896, 0.2932731509208679]\n",
      "Step 1439, loss = [0.3082559108734131, 0.0019407683284953237, 0.3080618381500244]\n",
      "Step 1440, loss = [0.30487072467803955, 0.015162629075348377, 0.3033544719219208]\n",
      "Step 1441, loss = [0.29660460352897644, 0.01164754293859005, 0.29543983936309814]\n",
      "Step 1442, loss = [0.2951596677303314, 0.007477916311472654, 0.29441186785697937]\n",
      "Step 1443, loss = [0.29836827516555786, 0.010162974707782269, 0.29735198616981506]\n",
      "Step 1444, loss = [0.30464446544647217, 0.006884724833071232, 0.303956001996994]\n",
      "Step 1445, loss = [0.30063924193382263, 0.007975209504365921, 0.2998417317867279]\n",
      "Step 1446, loss = [0.3095267713069916, 0.008430223912000656, 0.30868375301361084]\n",
      "Step 1447, loss = [0.29452037811279297, 0.009307126514613628, 0.29358965158462524]\n",
      "Step 1448, loss = [0.30795156955718994, 0.008175358176231384, 0.3071340322494507]\n",
      "Step 1449, loss = [0.2991926968097687, 0.009514189325273037, 0.2982412874698639]\n",
      "Step 1450, loss = [0.3012368083000183, 0.004040186293423176, 0.3008327782154083]\n",
      "Step 1451, loss = [0.31354600191116333, 0.005252490751445293, 0.3130207657814026]\n",
      "Step 1452, loss = [0.2918170094490051, 0.00998772494494915, 0.2908182442188263]\n",
      "Step 1453, loss = [0.307573139667511, 0.010241770185530186, 0.30654895305633545]\n",
      "Step 1454, loss = [0.31413325667381287, 0.0039678593166172504, 0.3137364685535431]\n",
      "Step 1455, loss = [0.3040653467178345, 0.008034305647015572, 0.3032619059085846]\n",
      "Step 1456, loss = [0.2906314432621002, 0.011177627369761467, 0.2895136773586273]\n",
      "Step 1457, loss = [0.2982899248600006, 0.002185255754739046, 0.29807138442993164]\n",
      "Step 1458, loss = [0.29928910732269287, 0.007440301589667797, 0.29854506254196167]\n",
      "Step 1459, loss = [0.31087034940719604, 0.009688802063465118, 0.30990147590637207]\n",
      "Step 1460, loss = [0.28758201003074646, 0.0052484432235360146, 0.28705716133117676]\n",
      "Step 1461, loss = [0.3061380684375763, 0.0058430638164281845, 0.30555376410484314]\n",
      "Step 1462, loss = [0.3047480285167694, 0.0066734133288264275, 0.30408069491386414]\n",
      "Step 1463, loss = [0.3061336874961853, 0.013148559257388115, 0.3048188388347626]\n",
      "Step 1464, loss = [0.31092050671577454, 0.00456845760345459, 0.31046366691589355]\n",
      "Step 1465, loss = [0.2910052537918091, 0.008604713715612888, 0.29014477133750916]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11754 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1466, loss = [0.2954392433166504, 0.006012490950524807, 0.29483798146247864]\n",
      "Update target distribution epoch 1 step 1467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11754/11754 [1:51:08<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos. rate:0.4176470588235294 Neg. rate:0.5823529411764706\n",
      "delta_label  0.006210651693040667\n",
      "Step 1467, loss = [0.2950197756290436, 0.010236058384180069, 0.2939961552619934]\n",
      "Step 1468, loss = [0.3065062165260315, 0.00343389343470335, 0.30616283416748047]\n",
      "Step 1469, loss = [0.3156805634498596, 0.005477935075759888, 0.3151327669620514]\n",
      "Epoch 1, loss = [0.29969118 0.00674203 0.29901698]\n",
      "\n",
      "Start of epoch 2\n",
      "Update target distribution epoch 2 step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11754/11754 [1:51:26<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos. rate:0.4176470588235294 Neg. rate:0.5823529411764706\n",
      "delta_label  0.0030627871362940277\n",
      "Step 0, loss = [0.3134377896785736, 0.005261637270450592, 0.31291162967681885]\n",
      "Step 1, loss = [0.30175280570983887, 0.00592887494713068, 0.30115991830825806]\n",
      "Step 2, loss = [0.28695279359817505, 0.004799041897058487, 0.286472886800766]\n",
      "Step 3, loss = [0.2984056770801544, 0.005981056485325098, 0.29780757427215576]\n",
      "Step 4, loss = [0.29035207629203796, 0.007306183222681284, 0.2896214723587036]\n",
      "Step 5, loss = [0.30739626288414, 0.003944262862205505, 0.30700182914733887]\n",
      "Step 6, loss = [0.29538753628730774, 0.0063898698426783085, 0.29474854469299316]\n",
      "Step 7, loss = [0.2900020480155945, 0.007495085708796978, 0.28925254940986633]\n",
      "Step 8, loss = [0.3078557550907135, 0.00554576376453042, 0.30730119347572327]\n",
      "Step 9, loss = [0.3004988133907318, 0.004033398814499378, 0.30009546875953674]\n",
      "Step 10, loss = [0.3127049207687378, 0.014974873512983322, 0.31120744347572327]\n",
      "Step 11, loss = [0.290572851896286, 0.009898601099848747, 0.2895829975605011]\n",
      "Step 12, loss = [0.30235451459884644, 0.007534353993833065, 0.3016010820865631]\n",
      "Step 13, loss = [0.2994271218776703, 0.005935127846896648, 0.29883360862731934]\n",
      "Step 14, loss = [0.2971895635128021, 0.009270555339753628, 0.2962625026702881]\n",
      "Step 15, loss = [0.29969489574432373, 0.005041874945163727, 0.2991907000541687]\n",
      "Step 16, loss = [0.2959858775138855, 0.00550807686522603, 0.2954350709915161]\n",
      "Step 17, loss = [0.3138980269432068, 0.00484595587477088, 0.31341344118118286]\n",
      "Step 18, loss = [0.2950325608253479, 0.0049897534772753716, 0.2945335805416107]\n",
      "Step 19, loss = [0.31251290440559387, 0.006317708641290665, 0.3118811249732971]\n",
      "Step 20, loss = [0.30521613359451294, 0.004990503191947937, 0.304717093706131]\n",
      "Step 21, loss = [0.30073127150535583, 0.005408535245805979, 0.30019041895866394]\n",
      "Step 22, loss = [0.30058538913726807, 0.0019434605492278934, 0.3003910481929779]\n",
      "Step 23, loss = [0.30044281482696533, 0.004994870163500309, 0.29994332790374756]\n",
      "Step 24, loss = [0.30576765537261963, 0.007516274694353342, 0.30501604080200195]\n",
      "Step 25, loss = [0.28962787985801697, 0.006498188246041536, 0.28897807002067566]\n",
      "Step 26, loss = [0.30219385027885437, 0.00433974526822567, 0.30175986886024475]\n",
      "Step 27, loss = [0.30424776673316956, 0.003967783413827419, 0.3038509786128998]\n",
      "Step 28, loss = [0.2926636338233948, 0.005352410022169352, 0.29212838411331177]\n",
      "Step 29, loss = [0.30805954337120056, 0.007634973153471947, 0.3072960376739502]\n",
      "Step 30, loss = [0.2997935712337494, 0.009480664506554604, 0.298845499753952]\n",
      "Step 31, loss = [0.3067312240600586, 0.00613931892439723, 0.30611729621887207]\n",
      "Step 32, loss = [0.2967264652252197, 0.008513130247592926, 0.2958751618862152]\n",
      "Step 33, loss = [0.2927646040916443, 0.004244505427777767, 0.29234015941619873]\n",
      "Step 34, loss = [0.29716557264328003, 0.006508304737508297, 0.29651474952697754]\n",
      "Step 35, loss = [0.3073228895664215, 0.00790694821625948, 0.30653220415115356]\n",
      "Step 36, loss = [0.3044144809246063, 0.0064528388902544975, 0.30376920104026794]\n",
      "Step 37, loss = [0.31181004643440247, 0.007512747310101986, 0.31105875968933105]\n",
      "Step 38, loss = [0.2847212851047516, 0.00799693912267685, 0.28392159938812256]\n",
      "Step 39, loss = [0.30959755182266235, 0.00854458473622799, 0.30874308943748474]\n",
      "Step 40, loss = [0.29692620038986206, 0.0042881774716079235, 0.2964973747730255]\n",
      "Step 41, loss = [0.3052150309085846, 0.0051472801715135574, 0.3047003149986267]\n",
      "Step 42, loss = [0.31601807475090027, 0.006545187905430794, 0.3153635561466217]\n",
      "Step 43, loss = [0.3030186891555786, 0.006010966841131449, 0.3024176061153412]\n",
      "Step 44, loss = [0.2869148254394531, 0.0041231135837733746, 0.28650251030921936]\n",
      "Step 45, loss = [0.2927817404270172, 0.0114036425948143, 0.29164138436317444]\n",
      "Step 46, loss = [0.29954248666763306, 0.004453191068023443, 0.2990971803665161]\n",
      "Step 47, loss = [0.30141326785087585, 0.005998614244163036, 0.3008134067058563]\n",
      "Step 48, loss = [0.29472315311431885, 0.009167063981294632, 0.29380643367767334]\n",
      "Step 49, loss = [0.2959001064300537, 0.011420383118093014, 0.2947580814361572]\n",
      "Step 50, loss = [0.3078596889972687, 0.005368482321500778, 0.30732282996177673]\n",
      "Step 51, loss = [0.30752065777778625, 0.007851947098970413, 0.30673545598983765]\n",
      "Step 52, loss = [0.29267024993896484, 0.011086871847510338, 0.2915615737438202]\n",
      "Step 53, loss = [0.3001115024089813, 0.007540798280388117, 0.29935741424560547]\n",
      "Step 54, loss = [0.3076300024986267, 0.008022831752896309, 0.30682772397994995]\n",
      "Step 55, loss = [0.2956477999687195, 0.006194053217768669, 0.2950283885002136]\n",
      "Step 56, loss = [0.30084869265556335, 0.009712476283311844, 0.29987743496894836]\n",
      "Step 57, loss = [0.31067296862602234, 0.007401457987725735, 0.3099328279495239]\n",
      "Step 58, loss = [0.3080565631389618, 0.004387549124658108, 0.30761781334877014]\n",
      "Step 59, loss = [0.2937498688697815, 0.005385654512792826, 0.29321131110191345]\n",
      "Step 60, loss = [0.29572832584381104, 0.0028807988855987787, 0.2954402565956116]\n",
      "Step 61, loss = [0.2968732714653015, 0.002990751527249813, 0.296574205160141]\n",
      "Step 62, loss = [0.30006709694862366, 0.006335187703371048, 0.2994335889816284]\n",
      "Step 63, loss = [0.27230924367904663, 0.004001987166702747, 0.27190905809402466]\n",
      "Step 64, loss = [0.3006281852722168, 0.0104970782995224, 0.2995784878730774]\n",
      "Step 65, loss = [0.2979208827018738, 0.005941502749919891, 0.2973267436027527]\n",
      "Step 66, loss = [0.2932114601135254, 0.004911800846457481, 0.2927202880382538]\n",
      "Step 67, loss = [0.2921927869319916, 0.0077974083833396435, 0.2914130389690399]\n",
      "Step 68, loss = [0.2971560060977936, 0.0045045823790133, 0.29670554399490356]\n",
      "Step 69, loss = [0.30250102281570435, 0.0035771906841546297, 0.30214330554008484]\n",
      "Step 70, loss = [0.30685552954673767, 0.006142104044556618, 0.30624133348464966]\n",
      "Step 71, loss = [0.30694639682769775, 0.004181415308266878, 0.306528240442276]\n",
      "Step 72, loss = [0.288361519575119, 0.012541195377707481, 0.2871074080467224]\n",
      "Step 73, loss = [0.30241626501083374, 0.008033672347664833, 0.30161288380622864]\n",
      "Step 74, loss = [0.2962172031402588, 0.0060572074726223946, 0.2956114709377289]\n",
      "Step 75, loss = [0.30750495195388794, 0.00864240899682045, 0.30664071440696716]\n",
      "Step 76, loss = [0.3015381097793579, 0.005275384988635778, 0.3010105788707733]\n",
      "Step 77, loss = [0.2999460995197296, 0.007818774320185184, 0.2991642355918884]\n",
      "Step 78, loss = [0.2904486060142517, 0.00642987247556448, 0.2898056209087372]\n",
      "Step 79, loss = [0.3068942427635193, 0.008627503179013729, 0.3060314953327179]\n",
      "Step 80, loss = [0.2904183864593506, 0.00924646481871605, 0.28949373960494995]\n",
      "Step 81, loss = [0.2997826039791107, 0.003511573188006878, 0.2994314432144165]\n",
      "Step 82, loss = [0.29803791642189026, 0.004647882655262947, 0.29757311940193176]\n",
      "Step 83, loss = [0.3041393756866455, 0.0050925277173519135, 0.30363011360168457]\n",
      "Step 84, loss = [0.3112724721431732, 0.004846685100346804, 0.31078779697418213]\n",
      "Step 85, loss = [0.3007476031780243, 0.009577124379575253, 0.2997899055480957]\n",
      "Step 86, loss = [0.29063892364501953, 0.007452228106558323, 0.2898936867713928]\n",
      "Step 87, loss = [0.31738021969795227, 0.005460553802549839, 0.31683415174484253]\n",
      "Step 88, loss = [0.2990674078464508, 0.006767204497009516, 0.2983906865119934]\n",
      "Step 89, loss = [0.3143773674964905, 0.013039137236773968, 0.31307345628738403]\n",
      "Step 90, loss = [0.29616719484329224, 0.00711063202470541, 0.2954561412334442]\n",
      "Step 91, loss = [0.30491167306900024, 0.003083934774622321, 0.3046032786369324]\n",
      "Step 92, loss = [0.3021354377269745, 0.005000395700335503, 0.30163538455963135]\n",
      "Step 93, loss = [0.29860055446624756, 0.007344561628997326, 0.2978661060333252]\n",
      "Step 94, loss = [0.3121553063392639, 0.006492459215223789, 0.311506062746048]\n",
      "Step 95, loss = [0.2943507730960846, 0.007854500785470009, 0.2935653328895569]\n",
      "Step 96, loss = [0.30145755410194397, 0.0047279419377446175, 0.30098477005958557]\n",
      "Step 97, loss = [0.30663248896598816, 0.0029308940283954144, 0.30633941292762756]\n",
      "Step 98, loss = [0.2926895022392273, 0.0074251312762498856, 0.29194697737693787]\n",
      "Step 99, loss = [0.30303582549095154, 0.009404250420629978, 0.3020954132080078]\n",
      "Step 100, loss = [0.29704567790031433, 0.00794131401926279, 0.2962515354156494]\n",
      "Step 101, loss = [0.30473995208740234, 0.008319171145558357, 0.30390802025794983]\n",
      "Step 102, loss = [0.2980417311191559, 0.005811463575810194, 0.2974605858325958]\n",
      "Step 103, loss = [0.3007808327674866, 0.003996851854026318, 0.3003811538219452]\n",
      "Step 104, loss = [0.3032311499118805, 0.003148778108879924, 0.3029162585735321]\n",
      "Step 105, loss = [0.30527395009994507, 0.007056731730699539, 0.3045682907104492]\n",
      "Step 106, loss = [0.3017285168170929, 0.006535505875945091, 0.30107495188713074]\n",
      "Step 107, loss = [0.29214388132095337, 0.006742238067090511, 0.29146966338157654]\n",
      "Step 108, loss = [0.3005146086215973, 0.003059242619201541, 0.3002086877822876]\n",
      "Step 109, loss = [0.29172056913375854, 0.007769776042550802, 0.29094359278678894]\n",
      "Step 110, loss = [0.29545021057128906, 0.007233570795506239, 0.2947268486022949]\n",
      "Step 111, loss = [0.3084307909011841, 0.004280564375221729, 0.3080027401447296]\n",
      "Step 112, loss = [0.3003031611442566, 0.005742823705077171, 0.2997288703918457]\n",
      "Step 113, loss = [0.2953108251094818, 0.005119196139276028, 0.29479891061782837]\n",
      "Step 114, loss = [0.3109084963798523, 0.007048766128718853, 0.3102036118507385]\n",
      "Step 115, loss = [0.30644604563713074, 0.004427601583302021, 0.30600327253341675]\n",
      "Step 116, loss = [0.3015393018722534, 0.011858321726322174, 0.300353467464447]\n",
      "Step 117, loss = [0.2868176996707916, 0.010999348945915699, 0.28571775555610657]\n",
      "Step 118, loss = [0.3132387101650238, 0.008270276710391045, 0.31241169571876526]\n",
      "Step 119, loss = [0.294666051864624, 0.007340424694120884, 0.2939320206642151]\n",
      "Step 120, loss = [0.2914242148399353, 0.007159766741096973, 0.2907082438468933]\n",
      "Step 121, loss = [0.29758211970329285, 0.012194130569696426, 0.2963626980781555]\n",
      "Step 122, loss = [0.3073730170726776, 0.011248593218624592, 0.30624815821647644]\n",
      "Step 123, loss = [0.2802087366580963, 0.008489571511745453, 0.2793597877025604]\n",
      "Step 124, loss = [0.3016555905342102, 0.0026748059317469597, 0.30138811469078064]\n",
      "Step 125, loss = [0.30921146273612976, 0.008196470327675343, 0.308391809463501]\n",
      "Step 126, loss = [0.29339537024497986, 0.008907937444746494, 0.29250457882881165]\n",
      "Step 127, loss = [0.3131400942802429, 0.005252949893474579, 0.3126147985458374]\n",
      "Step 128, loss = [0.3130594789981842, 0.010913100093603134, 0.3119681775569916]\n",
      "Step 129, loss = [0.30291393399238586, 0.007655244320631027, 0.30214840173721313]\n",
      "Step 130, loss = [0.3046310544013977, 0.0055458820424973965, 0.3040764629840851]\n",
      "Step 131, loss = [0.31275323033332825, 0.00850415788590908, 0.31190282106399536]\n",
      "Step 132, loss = [0.3076976537704468, 0.010757558047771454, 0.3066219091415405]\n",
      "Step 133, loss = [0.29217851161956787, 0.002738344017416239, 0.2919046878814697]\n",
      "Step 134, loss = [0.3139882981777191, 0.009059002622961998, 0.31308239698410034]\n",
      "Step 135, loss = [0.3009890019893646, 0.004791918210685253, 0.30050981044769287]\n",
      "Step 136, loss = [0.30244600772857666, 0.011887909844517708, 0.3012572228908539]\n",
      "Step 137, loss = [0.29772311449050903, 0.0042517781257629395, 0.2972979247570038]\n",
      "Step 138, loss = [0.3056870698928833, 0.003528501372784376, 0.305334210395813]\n",
      "Step 139, loss = [0.31182488799095154, 0.015345664694905281, 0.31029030680656433]\n",
      "Step 140, loss = [0.31112316250801086, 0.007927203550934792, 0.31033045053482056]\n",
      "Step 141, loss = [0.29346755146980286, 0.006014348939061165, 0.2928661108016968]\n",
      "Step 142, loss = [0.2957739233970642, 0.007215029560029507, 0.2950524091720581]\n",
      "Step 143, loss = [0.28071120381355286, 0.002957944292575121, 0.280415415763855]\n",
      "Step 144, loss = [0.30265700817108154, 0.008582965470850468, 0.3017987012863159]\n",
      "Step 145, loss = [0.3140684962272644, 0.00693191634491086, 0.3133752942085266]\n",
      "Step 146, loss = [0.2818133234977722, 0.004139815457165241, 0.28139933943748474]\n",
      "Step 147, loss = [0.2911602854728699, 0.013602392747998238, 0.2898000478744507]\n",
      "Step 148, loss = [0.30097436904907227, 0.006307404488325119, 0.3003436326980591]\n",
      "Step 149, loss = [0.3107292950153351, 0.005515646189451218, 0.310177743434906]\n",
      "Step 150, loss = [0.29423952102661133, 0.0059489780105650425, 0.29364463686943054]\n",
      "Step 151, loss = [0.29969608783721924, 0.005245937965810299, 0.299171507358551]\n",
      "Step 152, loss = [0.29000604152679443, 0.004829293116927147, 0.2895231246948242]\n",
      "Step 153, loss = [0.3065128028392792, 0.005755571648478508, 0.305937260389328]\n",
      "Step 154, loss = [0.30398496985435486, 0.003978091292083263, 0.3035871684551239]\n",
      "Step 155, loss = [0.3121558129787445, 0.010357515886425972, 0.31112006306648254]\n",
      "Step 156, loss = [0.29730942845344543, 0.0015960134332999587, 0.2971498370170593]\n",
      "Step 157, loss = [0.2856200635433197, 0.009355898015201092, 0.2846844792366028]\n",
      "Step 158, loss = [0.3085991442203522, 0.007211009506136179, 0.3078780472278595]\n",
      "Step 159, loss = [0.2989751696586609, 0.004644508473575115, 0.29851073026657104]\n",
      "Step 160, loss = [0.2954314053058624, 0.006012843456119299, 0.2948301136493683]\n",
      "Step 161, loss = [0.30474212765693665, 0.0031778262928128242, 0.30442434549331665]\n",
      "Step 162, loss = [0.29161757230758667, 0.002872919198125601, 0.2913302779197693]\n",
      "Step 163, loss = [0.29692116379737854, 0.0074814134277403355, 0.2961730360984802]\n",
      "Step 164, loss = [0.3126586079597473, 0.010400379076600075, 0.3116185665130615]\n",
      "Step 165, loss = [0.29142048954963684, 0.010287415236234665, 0.290391743183136]\n",
      "Step 166, loss = [0.29277801513671875, 0.007473215460777283, 0.2920306921005249]\n",
      "Step 167, loss = [0.2969866394996643, 0.011469841003417969, 0.29583966732025146]\n",
      "Step 168, loss = [0.30723896622657776, 0.006915155798196793, 0.30654746294021606]\n",
      "Step 169, loss = [0.3091751039028168, 0.007602180354297161, 0.30841487646102905]\n",
      "Step 170, loss = [0.30240824818611145, 0.009275982156395912, 0.30148065090179443]\n",
      "Step 171, loss = [0.288329541683197, 0.005603283643722534, 0.28776922821998596]\n",
      "Step 172, loss = [0.2958747446537018, 0.004027581308037043, 0.29547199606895447]\n",
      "Step 173, loss = [0.30086028575897217, 0.008995632641017437, 0.299960732460022]\n",
      "Step 174, loss = [0.30507904291152954, 0.009048141539096832, 0.3041742146015167]\n",
      "Step 175, loss = [0.3026755154132843, 0.008012965321540833, 0.30187422037124634]\n",
      "Step 176, loss = [0.30132248997688293, 0.007873548194766045, 0.3005351424217224]\n",
      "Step 177, loss = [0.3067121207714081, 0.005813471972942352, 0.3061307668685913]\n",
      "Step 178, loss = [0.296405166387558, 0.0040221987292170525, 0.29600295424461365]\n",
      "Step 179, loss = [0.30075767636299133, 0.005808916408568621, 0.30017679929733276]\n",
      "Step 180, loss = [0.2986771762371063, 0.007595591712743044, 0.29791760444641113]\n",
      "Step 181, loss = [0.3010558784008026, 0.009817827492952347, 0.30007410049438477]\n",
      "Step 182, loss = [0.30630049109458923, 0.0028872950933873653, 0.30601176619529724]\n",
      "Step 183, loss = [0.29887595772743225, 0.006970596499741077, 0.29817891120910645]\n",
      "Step 184, loss = [0.30138012766838074, 0.01027150172740221, 0.3003529906272888]\n",
      "Step 185, loss = [0.3104124665260315, 0.0039232391864061356, 0.3100201487541199]\n",
      "Step 186, loss = [0.3073073923587799, 0.009593207389116287, 0.3063480854034424]\n",
      "Step 187, loss = [0.30920669436454773, 0.0029461297672241926, 0.308912068605423]\n",
      "Step 188, loss = [0.2989949584007263, 0.002391249407082796, 0.29875582456588745]\n",
      "Step 189, loss = [0.28945642709732056, 0.005746455863118172, 0.288881778717041]\n",
      "Step 190, loss = [0.3037143647670746, 0.006050806492567062, 0.3031092882156372]\n",
      "Step 191, loss = [0.3019976019859314, 0.007172113750129938, 0.3012803792953491]\n",
      "Step 192, loss = [0.2905229330062866, 0.008316603489220142, 0.2896912693977356]\n",
      "Step 193, loss = [0.31057634949684143, 0.007690286263823509, 0.30980733036994934]\n",
      "Step 194, loss = [0.3130505383014679, 0.00904202088713646, 0.3121463358402252]\n",
      "Step 195, loss = [0.31170302629470825, 0.007904651574790478, 0.310912549495697]\n",
      "Step 196, loss = [0.3008410334587097, 0.006266552023589611, 0.30021438002586365]\n",
      "Step 197, loss = [0.3013867437839508, 0.006923070177435875, 0.30069443583488464]\n",
      "Step 198, loss = [0.30704864859580994, 0.005990589968860149, 0.3064495921134949]\n",
      "Step 199, loss = [0.29966259002685547, 0.009761776775121689, 0.2986864149570465]\n",
      "Step 200, loss = [0.28753116726875305, 0.006917770020663738, 0.28683939576148987]\n",
      "Step 201, loss = [0.2993203103542328, 0.0061639826744794846, 0.2987039089202881]\n",
      "Step 202, loss = [0.2942766845226288, 0.0071077668108046055, 0.29356589913368225]\n",
      "Step 203, loss = [0.30025243759155273, 0.006369660142809153, 0.2996154725551605]\n",
      "Step 204, loss = [0.31054747104644775, 0.010399090126156807, 0.3095075488090515]\n",
      "Step 205, loss = [0.29140958189964294, 0.006506253499537706, 0.29075896739959717]\n",
      "Step 206, loss = [0.3039288818836212, 0.006042644381523132, 0.3033246099948883]\n",
      "Step 207, loss = [0.30722692608833313, 0.003804497653618455, 0.3068464696407318]\n",
      "Step 208, loss = [0.3002098798751831, 0.010523812845349312, 0.2991575002670288]\n",
      "Step 209, loss = [0.30854979157447815, 0.00831660907715559, 0.3077181279659271]\n",
      "Step 210, loss = [0.3087702691555023, 0.011339491233229637, 0.3076363205909729]\n",
      "Step 211, loss = [0.3061981797218323, 0.006208239123225212, 0.3055773675441742]\n",
      "Step 212, loss = [0.3018456697463989, 0.009869333356618881, 0.300858736038208]\n",
      "Step 213, loss = [0.30018919706344604, 0.006860198453068733, 0.2995031774044037]\n",
      "Step 214, loss = [0.3092060983181, 0.006584330927580595, 0.3085476756095886]\n",
      "Step 215, loss = [0.29281389713287354, 0.010934295132756233, 0.2917204797267914]\n",
      "Step 216, loss = [0.2724476754665375, 0.00581266637891531, 0.27186641097068787]\n",
      "Step 217, loss = [0.2990891635417938, 0.008583007380366325, 0.2982308566570282]\n",
      "Step 218, loss = [0.2930123209953308, 0.0038116639479994774, 0.2926311492919922]\n",
      "Step 219, loss = [0.28729474544525146, 0.00855568703263998, 0.2864391803741455]\n",
      "Step 220, loss = [0.27578651905059814, 0.009306158870458603, 0.27485591173171997]\n",
      "Step 221, loss = [0.3146071135997772, 0.007936719805002213, 0.3138134479522705]\n",
      "Step 222, loss = [0.30855390429496765, 0.00883578136563301, 0.30767032504081726]\n",
      "Step 223, loss = [0.2927117347717285, 0.006419881246984005, 0.2920697331428528]\n",
      "Step 224, loss = [0.3011949956417084, 0.012612076476216316, 0.2999337911605835]\n",
      "Step 225, loss = [0.2945549488067627, 0.012508454732596874, 0.29330411553382874]\n",
      "Step 226, loss = [0.2937523424625397, 0.006084463093429804, 0.2931438982486725]\n",
      "Step 227, loss = [0.2847842276096344, 0.004699298180639744, 0.2843143045902252]\n",
      "Step 228, loss = [0.2989859879016876, 0.008333886042237282, 0.2981525957584381]\n",
      "Step 229, loss = [0.2825275957584381, 0.003626785008236766, 0.28216493129730225]\n",
      "Step 230, loss = [0.3028314411640167, 0.0077720507979393005, 0.302054226398468]\n",
      "Step 231, loss = [0.29990607500076294, 0.006466819904744625, 0.29925939440727234]\n",
      "Step 232, loss = [0.3001529276371002, 0.007435197941958904, 0.299409419298172]\n",
      "Step 233, loss = [0.30486807227134705, 0.0066401176154613495, 0.3042040467262268]\n",
      "Step 234, loss = [0.3049299120903015, 0.012277763336896896, 0.30370214581489563]\n",
      "Step 235, loss = [0.29894036054611206, 0.0025994370225816965, 0.2986804246902466]\n",
      "Step 236, loss = [0.306830495595932, 0.016682984307408333, 0.3051621913909912]\n",
      "Step 237, loss = [0.30039048194885254, 0.0043271128088235855, 0.2999577820301056]\n",
      "Step 238, loss = [0.3001425564289093, 0.01045825146138668, 0.2990967333316803]\n",
      "Step 239, loss = [0.27825069427490234, 0.005104521289467812, 0.2777402400970459]\n",
      "Step 240, loss = [0.3043118715286255, 0.004321123939007521, 0.3038797676563263]\n",
      "Step 241, loss = [0.288103312253952, 0.010200915858149529, 0.2870832085609436]\n",
      "Step 242, loss = [0.301472932100296, 0.007269458379596472, 0.30074599385261536]\n",
      "Step 243, loss = [0.306183397769928, 0.011447728611528873, 0.3050386309623718]\n",
      "Step 244, loss = [0.28800249099731445, 0.005650337785482407, 0.28743746876716614]\n",
      "Step 245, loss = [0.2876172959804535, 0.005615930538624525, 0.28705570101737976]\n",
      "Step 246, loss = [0.3016434609889984, 0.005850368179380894, 0.30105841159820557]\n",
      "Step 247, loss = [0.3073370158672333, 0.005555715411901474, 0.30678144097328186]\n",
      "Step 248, loss = [0.3131876289844513, 0.011957379058003426, 0.3119919002056122]\n",
      "Step 249, loss = [0.30409693717956543, 0.010383526794612408, 0.30305859446525574]\n",
      "Step 250, loss = [0.29706233739852905, 0.005601922050118446, 0.29650214314460754]\n",
      "Step 251, loss = [0.2993030846118927, 0.006005228962749243, 0.29870256781578064]\n",
      "Step 252, loss = [0.3017621338367462, 0.012087905779480934, 0.3005533516407013]\n",
      "Step 253, loss = [0.29649943113327026, 0.008917558938264847, 0.29560768604278564]\n",
      "Step 254, loss = [0.29948851466178894, 0.003958593588322401, 0.2990926504135132]\n",
      "Step 255, loss = [0.28444164991378784, 0.004990415647625923, 0.2839426100254059]\n",
      "Step 256, loss = [0.3048410415649414, 0.007364178076386452, 0.30410462617874146]\n",
      "Step 257, loss = [0.28249305486679077, 0.0058302427642047405, 0.2819100320339203]\n",
      "Step 258, loss = [0.3041490316390991, 0.008645158261060715, 0.30328452587127686]\n",
      "Step 259, loss = [0.30620133876800537, 0.006624470464885235, 0.3055388927459717]\n",
      "Step 260, loss = [0.2998740077018738, 0.009951943531632423, 0.29887881875038147]\n",
      "Step 261, loss = [0.29966306686401367, 0.005149408243596554, 0.2991481125354767]\n",
      "Step 262, loss = [0.28500428795814514, 0.005642469506710768, 0.2844400405883789]\n",
      "Step 263, loss = [0.28667131066322327, 0.010089742951095104, 0.28566232323646545]\n",
      "Step 264, loss = [0.30011340975761414, 0.006274167913943529, 0.299485981464386]\n",
      "Step 265, loss = [0.3118918240070343, 0.007873834110796452, 0.3111044466495514]\n",
      "Step 266, loss = [0.3047053813934326, 0.0058635082095861435, 0.3041190207004547]\n",
      "Step 267, loss = [0.308572918176651, 0.010678168386220932, 0.3075051009654999]\n",
      "Step 268, loss = [0.3017289638519287, 0.009359349496662617, 0.30079302191734314]\n",
      "Step 269, loss = [0.3016146123409271, 0.005293503403663635, 0.3010852634906769]\n",
      "Step 270, loss = [0.3004539906978607, 0.004212670028209686, 0.30003273487091064]\n",
      "Step 271, loss = [0.298184335231781, 0.010314741171896458, 0.2971528470516205]\n",
      "Step 272, loss = [0.3052866756916046, 0.009865070693194866, 0.3043001592159271]\n",
      "Step 273, loss = [0.3085026741027832, 0.008331812918186188, 0.3076694905757904]\n",
      "Step 274, loss = [0.29557880759239197, 0.004802295938134193, 0.29509857296943665]\n",
      "Step 275, loss = [0.2995721697807312, 0.006570378318428993, 0.2989151179790497]\n",
      "Step 276, loss = [0.2837529182434082, 0.006579238921403885, 0.28309500217437744]\n",
      "Step 277, loss = [0.3043835461139679, 0.010344279929995537, 0.303349107503891]\n",
      "Step 278, loss = [0.30316075682640076, 0.004677408374845982, 0.3026930093765259]\n",
      "Step 279, loss = [0.2992038428783417, 0.003721424611285329, 0.2988317012786865]\n",
      "Step 280, loss = [0.2885201573371887, 0.006006509996950626, 0.2879194915294647]\n",
      "Step 281, loss = [0.29723894596099854, 0.0067708613350987434, 0.2965618669986725]\n",
      "Step 282, loss = [0.27431750297546387, 0.006801656447350979, 0.2736373245716095]\n",
      "Step 283, loss = [0.30004754662513733, 0.00769065460190177, 0.29927846789360046]\n",
      "Step 284, loss = [0.28371086716651917, 0.0029880781657993793, 0.28341206908226013]\n",
      "Step 285, loss = [0.3064612150192261, 0.004226468037813902, 0.3060385584831238]\n",
      "Step 286, loss = [0.3017873466014862, 0.0034223883412778378, 0.3014450967311859]\n",
      "Step 287, loss = [0.29465457797050476, 0.009456481784582138, 0.2937089204788208]\n",
      "Step 288, loss = [0.29431554675102234, 0.002343960804864764, 0.2940811514854431]\n",
      "Step 289, loss = [0.29414522647857666, 0.007874231785535812, 0.293357789516449]\n",
      "Step 290, loss = [0.30905142426490784, 0.00885612703859806, 0.3081658184528351]\n",
      "Step 291, loss = [0.29378995299339294, 0.007203183136880398, 0.29306963086128235]\n",
      "Step 292, loss = [0.29786068201065063, 0.0043217833153903484, 0.29742851853370667]\n",
      "Step 293, loss = [0.2959035634994507, 0.01115199364721775, 0.2947883605957031]\n",
      "Step 294, loss = [0.30548974871635437, 0.0054850163869559765, 0.30494123697280884]\n",
      "Step 295, loss = [0.29052892327308655, 0.007613340392708778, 0.2897675931453705]\n",
      "Step 296, loss = [0.291501522064209, 0.006059243343770504, 0.29089561104774475]\n",
      "Step 297, loss = [0.29512372612953186, 0.00845783855766058, 0.29427793622016907]\n",
      "Step 298, loss = [0.30589601397514343, 0.005397750064730644, 0.3053562343120575]\n",
      "Step 299, loss = [0.30091387033462524, 0.002526644617319107, 0.30066120624542236]\n",
      "Step 300, loss = [0.3058928847312927, 0.006737486459314823, 0.3052191436290741]\n",
      "Step 301, loss = [0.2922849655151367, 0.006930644623935223, 0.29159191250801086]\n",
      "Step 302, loss = [0.30925822257995605, 0.00820454116910696, 0.3084377646446228]\n",
      "Step 303, loss = [0.28216221928596497, 0.011886587366461754, 0.28097355365753174]\n",
      "Step 304, loss = [0.2869035005569458, 0.00867716409265995, 0.28603577613830566]\n",
      "Step 305, loss = [0.30251628160476685, 0.006905266549438238, 0.30182576179504395]\n",
      "Step 306, loss = [0.28732985258102417, 0.004947388079017401, 0.28683510422706604]\n",
      "Step 307, loss = [0.30290812253952026, 0.007760864682495594, 0.3021320402622223]\n",
      "Step 308, loss = [0.30818575620651245, 0.007072052918374538, 0.30747854709625244]\n",
      "Step 309, loss = [0.28922921419143677, 0.008337130770087242, 0.288395494222641]\n",
      "Step 310, loss = [0.31268593668937683, 0.008299468085169792, 0.3118560016155243]\n",
      "Step 311, loss = [0.30375349521636963, 0.007594665512442589, 0.302994042634964]\n",
      "Step 312, loss = [0.3019848167896271, 0.007636813446879387, 0.3012211322784424]\n",
      "Step 313, loss = [0.310791939496994, 0.006630143150687218, 0.31012892723083496]\n",
      "Step 314, loss = [0.2940632700920105, 0.0036220792680978775, 0.29370105266571045]\n",
      "Step 315, loss = [0.29317858815193176, 0.0038620035629719496, 0.2927923798561096]\n",
      "Step 316, loss = [0.29547828435897827, 0.004119135905057192, 0.29506635665893555]\n",
      "Step 317, loss = [0.3030701279640198, 0.004748739767819643, 0.30259525775909424]\n",
      "Step 318, loss = [0.3071041703224182, 0.00940730981528759, 0.3061634302139282]\n",
      "Step 319, loss = [0.30498918890953064, 0.01007058098912239, 0.303982138633728]\n",
      "Step 320, loss = [0.3099720776081085, 0.008356314152479172, 0.30913645029067993]\n",
      "Step 321, loss = [0.300651490688324, 0.007363964803516865, 0.2999151051044464]\n",
      "Step 322, loss = [0.2981792986392975, 0.01000603474676609, 0.2971786856651306]\n",
      "Step 323, loss = [0.28060004115104675, 0.0053484439849853516, 0.2800652086734772]\n",
      "Step 324, loss = [0.3129286766052246, 0.006709051318466663, 0.3122577667236328]\n",
      "Step 325, loss = [0.30535438656806946, 0.007755444385111332, 0.30457884073257446]\n",
      "Step 326, loss = [0.3042633831501007, 0.0034473049454391003, 0.30391865968704224]\n",
      "Step 327, loss = [0.30692335963249207, 0.00607713358476758, 0.30631566047668457]\n",
      "Step 328, loss = [0.3143916726112366, 0.008167748339474201, 0.3135749101638794]\n",
      "Step 329, loss = [0.3020459711551666, 0.006566875614225864, 0.30138927698135376]\n",
      "Step 330, loss = [0.305043488740921, 0.005962309427559376, 0.3044472634792328]\n",
      "Step 331, loss = [0.303739994764328, 0.005860218778252602, 0.30315396189689636]\n",
      "Step 332, loss = [0.30115804076194763, 0.007030196487903595, 0.3004550337791443]\n",
      "Step 333, loss = [0.2996012568473816, 0.006495879963040352, 0.298951655626297]\n",
      "Step 334, loss = [0.2984520494937897, 0.008279560133814812, 0.2976240813732147]\n",
      "Step 335, loss = [0.29682666063308716, 0.009113479405641556, 0.29591530561447144]\n",
      "Step 336, loss = [0.3012947142124176, 0.0027504274621605873, 0.30101966857910156]\n",
      "Step 337, loss = [0.3082304894924164, 0.007722214795649052, 0.3074582815170288]\n",
      "Step 338, loss = [0.29544922709465027, 0.007926061749458313, 0.2946566343307495]\n",
      "Step 339, loss = [0.31179550290107727, 0.012786250561475754, 0.3105168640613556]\n",
      "Step 340, loss = [0.30717039108276367, 0.008829694241285324, 0.30628740787506104]\n",
      "Step 341, loss = [0.2997232675552368, 0.006056774873286486, 0.2991175949573517]\n",
      "Step 342, loss = [0.3043861985206604, 0.006446757819503546, 0.3037415146827698]\n",
      "Step 343, loss = [0.2932966947555542, 0.008744903840124607, 0.29242220520973206]\n",
      "Step 344, loss = [0.30011847615242004, 0.0067932698875665665, 0.2994391620159149]\n",
      "Step 345, loss = [0.29795512557029724, 0.003506772220134735, 0.29760444164276123]\n",
      "Step 346, loss = [0.2824362516403198, 0.006581263151019812, 0.28177812695503235]\n",
      "Step 347, loss = [0.2891782224178314, 0.004651685245335102, 0.2887130677700043]\n",
      "Step 348, loss = [0.30547454953193665, 0.006406662054359913, 0.30483388900756836]\n",
      "Step 349, loss = [0.3083651661872864, 0.006602171808481216, 0.30770495533943176]\n",
      "Step 350, loss = [0.2942033112049103, 0.003452013246715069, 0.2938581109046936]\n",
      "Step 351, loss = [0.2984680235385895, 0.007221238221973181, 0.2977459132671356]\n",
      "Step 352, loss = [0.28913959860801697, 0.008307103998959064, 0.28830888867378235]\n",
      "Step 353, loss = [0.30130699276924133, 0.008640010841190815, 0.30044299364089966]\n",
      "Step 354, loss = [0.29127126932144165, 0.008899630978703499, 0.2903813123703003]\n",
      "Step 355, loss = [0.3063403367996216, 0.007436950691044331, 0.30559664964675903]\n",
      "Step 356, loss = [0.3034103810787201, 0.009135350584983826, 0.30249685049057007]\n",
      "Step 357, loss = [0.30119451880455017, 0.0075801098719239235, 0.30043649673461914]\n",
      "Step 358, loss = [0.29701635241508484, 0.007022061385214329, 0.29631415009498596]\n",
      "Step 359, loss = [0.30319690704345703, 0.006257286295294762, 0.302571177482605]\n",
      "Step 360, loss = [0.29250460863113403, 0.004368431866168976, 0.2920677661895752]\n",
      "Step 361, loss = [0.29174235463142395, 0.0037339627742767334, 0.2913689613342285]\n",
      "Step 362, loss = [0.30859044194221497, 0.012341126799583435, 0.3073563277721405]\n",
      "Step 363, loss = [0.29681211709976196, 0.009261293336749077, 0.29588598012924194]\n",
      "Step 364, loss = [0.28843405842781067, 0.0033698834013193846, 0.288097083568573]\n",
      "Step 365, loss = [0.30086421966552734, 0.008719122037291527, 0.29999229311943054]\n",
      "Step 366, loss = [0.2897615432739258, 0.005783953703939915, 0.2891831398010254]\n",
      "Step 367, loss = [0.2948037385940552, 0.009186433628201485, 0.29388508200645447]\n",
      "Step 368, loss = [0.3069519102573395, 0.01024171244353056, 0.30592775344848633]\n",
      "Step 369, loss = [0.30496567487716675, 0.010908680036664009, 0.30387482047080994]\n",
      "Step 370, loss = [0.3025156557559967, 0.008665837347507477, 0.3016490638256073]\n",
      "Step 371, loss = [0.30869901180267334, 0.004060626961290836, 0.308292955160141]\n",
      "Step 372, loss = [0.30388572812080383, 0.008456260897219181, 0.303040087223053]\n",
      "Step 373, loss = [0.3036029040813446, 0.005530963186174631, 0.30304980278015137]\n",
      "Step 374, loss = [0.2852693796157837, 0.007076118607074022, 0.28456175327301025]\n",
      "Step 375, loss = [0.2856297492980957, 0.007314022164791822, 0.2848983407020569]\n",
      "Step 376, loss = [0.29677021503448486, 0.005793035961687565, 0.29619091749191284]\n",
      "Step 377, loss = [0.3028174340724945, 0.004463100805878639, 0.3023711144924164]\n",
      "Step 378, loss = [0.30237245559692383, 0.004443754907697439, 0.3019280731678009]\n",
      "Step 379, loss = [0.2994431257247925, 0.008588384836912155, 0.2985842823982239]\n",
      "Step 380, loss = [0.3021656274795532, 0.014880117028951645, 0.300677627325058]\n",
      "Step 381, loss = [0.284258633852005, 0.004615764133632183, 0.2837970554828644]\n",
      "Step 382, loss = [0.28784850239753723, 0.007532806601375341, 0.28709521889686584]\n",
      "Step 383, loss = [0.30044102668762207, 0.006714562885463238, 0.2997695803642273]\n",
      "Step 384, loss = [0.30651769042015076, 0.007906541228294373, 0.3057270348072052]\n",
      "Step 385, loss = [0.2977273762226105, 0.007621533237397671, 0.29696521162986755]\n",
      "Step 386, loss = [0.29505836963653564, 0.009728807024657726, 0.2940855026245117]\n",
      "Step 387, loss = [0.30475640296936035, 0.011151693761348724, 0.3036412298679352]\n",
      "Step 388, loss = [0.30046579241752625, 0.008261553011834621, 0.29963964223861694]\n",
      "Step 389, loss = [0.2939557731151581, 0.005476363003253937, 0.2934081256389618]\n",
      "Step 390, loss = [0.29091542959213257, 0.0041107358410954475, 0.2905043661594391]\n",
      "Step 391, loss = [0.31536099314689636, 0.005957352928817272, 0.31476524472236633]\n",
      "Step 392, loss = [0.2934782803058624, 0.003248085267841816, 0.29315346479415894]\n",
      "Step 393, loss = [0.3084986209869385, 0.01017233170568943, 0.3074813783168793]\n",
      "Step 394, loss = [0.2858167290687561, 0.004887606017291546, 0.2853279709815979]\n",
      "Step 395, loss = [0.29329538345336914, 0.0027926084585487843, 0.29301613569259644]\n",
      "Step 396, loss = [0.3047126829624176, 0.00796662550419569, 0.30391600728034973]\n",
      "Step 397, loss = [0.30296292901039124, 0.00561509421095252, 0.30240142345428467]\n",
      "Step 398, loss = [0.30793845653533936, 0.007201171014457941, 0.3072183430194855]\n",
      "Step 399, loss = [0.29663750529289246, 0.011779727414250374, 0.2954595386981964]\n",
      "Step 400, loss = [0.30556657910346985, 0.020601579919457436, 0.30350643396377563]\n",
      "Step 401, loss = [0.29490965604782104, 0.014664258807897568, 0.2934432327747345]\n",
      "Step 402, loss = [0.30476945638656616, 0.009128618985414505, 0.30385658144950867]\n",
      "Step 403, loss = [0.30407923460006714, 0.006955097429454327, 0.3033837378025055]\n",
      "Step 404, loss = [0.3013603091239929, 0.011251860298216343, 0.3002351224422455]\n",
      "Step 405, loss = [0.2989274263381958, 0.005154598504304886, 0.2984119653701782]\n",
      "Step 406, loss = [0.2909846007823944, 0.01032060943543911, 0.2899525463581085]\n",
      "Step 407, loss = [0.3041163980960846, 0.005690970923751593, 0.30354729294776917]\n",
      "Step 408, loss = [0.30263325572013855, 0.003963577561080456, 0.3022368848323822]\n",
      "Step 409, loss = [0.2927425801753998, 0.005154046230018139, 0.292227178812027]\n",
      "Step 410, loss = [0.30140256881713867, 0.007693576160818338, 0.3006332218647003]\n",
      "Step 411, loss = [0.2825935184955597, 0.006555848754942417, 0.28193792700767517]\n",
      "Step 412, loss = [0.30526530742645264, 0.007621557917445898, 0.3045031428337097]\n",
      "Step 413, loss = [0.27431952953338623, 0.010816794820129871, 0.27323785424232483]\n",
      "Step 414, loss = [0.30008459091186523, 0.005890361033380032, 0.29949554800987244]\n",
      "Step 415, loss = [0.30187278985977173, 0.005540281534194946, 0.3013187646865845]\n",
      "Step 416, loss = [0.3070550560951233, 0.002849096432328224, 0.3067701458930969]\n",
      "Step 417, loss = [0.2956415116786957, 0.0028429976664483547, 0.29535719752311707]\n",
      "Step 418, loss = [0.30587461590766907, 0.011187300086021423, 0.30475589632987976]\n",
      "Step 419, loss = [0.29750579595565796, 0.007599459029734135, 0.29674583673477173]\n",
      "Step 420, loss = [0.30027568340301514, 0.00954241119325161, 0.2993214428424835]\n",
      "Step 421, loss = [0.29638057947158813, 0.007870253175497055, 0.2955935597419739]\n",
      "Step 422, loss = [0.2947373688220978, 0.005697997286915779, 0.29416757822036743]\n",
      "Step 423, loss = [0.281607985496521, 0.0069855754263699055, 0.2809094190597534]\n",
      "Step 424, loss = [0.3087860345840454, 0.011244475841522217, 0.30766159296035767]\n",
      "Step 425, loss = [0.28205758333206177, 0.0076383366249501705, 0.28129374980926514]\n",
      "Step 426, loss = [0.30619120597839355, 0.0066112615168094635, 0.3055300712585449]\n",
      "Step 427, loss = [0.2921525835990906, 0.006604042369872332, 0.29149219393730164]\n",
      "Step 428, loss = [0.28488513827323914, 0.0034714429639279842, 0.28453800082206726]\n",
      "Step 429, loss = [0.2928365170955658, 0.006704892497509718, 0.2921660244464874]\n",
      "Step 430, loss = [0.3042909502983093, 0.00988681335002184, 0.30330225825309753]\n",
      "Step 431, loss = [0.3002331852912903, 0.008506153710186481, 0.2993825674057007]\n",
      "Step 432, loss = [0.30717733502388, 0.009415716864168644, 0.30623576045036316]\n",
      "Step 433, loss = [0.30238690972328186, 0.015110431239008904, 0.30087587237358093]\n",
      "Step 434, loss = [0.30185139179229736, 0.006020353175699711, 0.30124935507774353]\n",
      "Step 435, loss = [0.3099817633628845, 0.002588700968772173, 0.309722900390625]\n",
      "Step 436, loss = [0.2939653694629669, 0.008512621745467186, 0.2931140959262848]\n",
      "Step 437, loss = [0.3005821704864502, 0.009290678426623344, 0.2996531128883362]\n",
      "Step 438, loss = [0.30926448106765747, 0.006733844988048077, 0.3085910975933075]\n",
      "Step 439, loss = [0.29760733246803284, 0.003723298665136099, 0.29723501205444336]\n",
      "Step 440, loss = [0.2894876301288605, 0.010011876001954079, 0.28848645091056824]\n",
      "Step 441, loss = [0.3035006523132324, 0.001655697706155479, 0.3033350706100464]\n",
      "Step 442, loss = [0.3124387264251709, 0.0038562417030334473, 0.3120531141757965]\n",
      "Step 443, loss = [0.2826116681098938, 0.008837132714688778, 0.28172796964645386]\n",
      "Step 444, loss = [0.25814053416252136, 0.005371876992285252, 0.25760334730148315]\n",
      "Step 445, loss = [0.3044760227203369, 0.011694412678480148, 0.30330657958984375]\n",
      "Step 446, loss = [0.3066789507865906, 0.008221643045544624, 0.3058567941188812]\n",
      "Step 447, loss = [0.30403268337249756, 0.010088831186294556, 0.3030237853527069]\n",
      "Step 448, loss = [0.3005053997039795, 0.00789951253682375, 0.29971545934677124]\n",
      "Step 449, loss = [0.29596835374832153, 0.009635593742132187, 0.29500478506088257]\n",
      "Step 450, loss = [0.29044613242149353, 0.004499904345721006, 0.2899961471557617]\n",
      "Step 451, loss = [0.3008318543434143, 0.0072259013541042805, 0.30010926723480225]\n",
      "Step 452, loss = [0.2813577651977539, 0.004731370136141777, 0.28088462352752686]\n",
      "Step 453, loss = [0.30665433406829834, 0.004525420255959034, 0.3062017858028412]\n",
      "Step 454, loss = [0.29933488368988037, 0.005650301463901997, 0.29876986145973206]\n",
      "Step 455, loss = [0.30227169394493103, 0.007100136484950781, 0.3015616834163666]\n",
      "Step 456, loss = [0.30101993680000305, 0.004635583609342575, 0.30055639147758484]\n",
      "Step 457, loss = [0.2978323698043823, 0.004507090896368027, 0.2973816692829132]\n",
      "Step 458, loss = [0.3120526373386383, 0.00468960776925087, 0.31158366799354553]\n",
      "Step 459, loss = [0.30183282494544983, 0.005648837890475988, 0.30126795172691345]\n",
      "Step 460, loss = [0.3095051348209381, 0.008091730065643787, 0.3086959719657898]\n",
      "Step 461, loss = [0.3081364333629608, 0.0034821818117052317, 0.307788223028183]\n",
      "Step 462, loss = [0.2913420796394348, 0.012550676241517067, 0.2900870144367218]\n",
      "Step 463, loss = [0.3035060167312622, 0.005147515796124935, 0.30299127101898193]\n",
      "Step 464, loss = [0.30370405316352844, 0.008503890596330166, 0.30285367369651794]\n",
      "Step 465, loss = [0.31529501080513, 0.007255908101797104, 0.3145694136619568]\n",
      "Step 466, loss = [0.29871341586112976, 0.007905470207333565, 0.29792287945747375]\n",
      "Step 467, loss = [0.3034045100212097, 0.002677489072084427, 0.30313676595687866]\n",
      "Step 468, loss = [0.3027128577232361, 0.01236705295741558, 0.3014761507511139]\n",
      "Step 469, loss = [0.30295902490615845, 0.0035028939601033926, 0.3026087284088135]\n",
      "Step 470, loss = [0.2942965030670166, 0.00292372889816761, 0.2940041422843933]\n",
      "Step 471, loss = [0.2941611409187317, 0.007473594043403864, 0.29341378808021545]\n",
      "Step 472, loss = [0.3069988787174225, 0.003832868067547679, 0.30661559104919434]\n",
      "Step 473, loss = [0.3019259572029114, 0.008079414255917072, 0.30111801624298096]\n",
      "Step 474, loss = [0.3025621473789215, 0.00567150954157114, 0.30199500918388367]\n",
      "Step 475, loss = [0.2977577745914459, 0.009337477385997772, 0.29682403802871704]\n",
      "Step 476, loss = [0.29954251646995544, 0.007695797365158796, 0.298772931098938]\n",
      "Step 477, loss = [0.306608110666275, 0.008772604167461395, 0.3057308495044708]\n",
      "Step 478, loss = [0.3011769652366638, 0.009436726570129395, 0.30023330450057983]\n",
      "Step 479, loss = [0.2926533818244934, 0.009611770510673523, 0.29169219732284546]\n",
      "Step 480, loss = [0.29659610986709595, 0.0059929280541837215, 0.2959968149662018]\n",
      "Step 481, loss = [0.31258633732795715, 0.006802234798669815, 0.311906099319458]\n",
      "Step 482, loss = [0.3050491511821747, 0.017582060769200325, 0.3032909333705902]\n",
      "Step 483, loss = [0.3132469952106476, 0.006314230617135763, 0.3126155734062195]\n",
      "Step 484, loss = [0.2990347445011139, 0.005076275207102299, 0.2985271215438843]\n",
      "Step 485, loss = [0.290624737739563, 0.006672443822026253, 0.2899574935436249]\n",
      "Step 486, loss = [0.2963075637817383, 0.003774096956476569, 0.2959301471710205]\n",
      "Step 487, loss = [0.2977471351623535, 0.0021831579506397247, 0.29752883315086365]\n",
      "Step 488, loss = [0.2983322739601135, 0.004823231138288975, 0.29784995317459106]\n",
      "Update target distribution epoch 2 step 489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11754/11754 [1:51:31<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos. rate:0.4176470588235294 Neg. rate:0.5823529411764706\n",
      "delta_label  0.003998638761272758\n",
      "Step 489, loss = [0.30521124601364136, 0.006304966285824776, 0.3045807480812073]\n",
      "Step 490, loss = [0.3080470561981201, 0.0046505737118422985, 0.30758199095726013]\n",
      "Step 491, loss = [0.3099399209022522, 0.011409539729356766, 0.30879896879196167]\n",
      "Step 492, loss = [0.2961086332798004, 0.01034894771873951, 0.2950737476348877]\n",
      "Step 493, loss = [0.296893835067749, 0.010744022205471992, 0.2958194315433502]\n",
      "Step 494, loss = [0.30129677057266235, 0.006990245543420315, 0.30059775710105896]\n",
      "Step 495, loss = [0.29660195112228394, 0.004634194541722536, 0.2961385250091553]\n",
      "Step 496, loss = [0.31351685523986816, 0.014045238494873047, 0.31211233139038086]\n",
      "Step 497, loss = [0.3058115243911743, 0.006942409556359053, 0.30511727929115295]\n",
      "Step 498, loss = [0.2980431318283081, 0.008271805010735989, 0.29721593856811523]\n",
      "Step 499, loss = [0.3114778995513916, 0.009081028401851654, 0.31056979298591614]\n",
      "Step 500, loss = [0.2710314989089966, 0.012167548760771751, 0.26981472969055176]\n",
      "Step 501, loss = [0.30115702748298645, 0.006269842851907015, 0.3005300462245941]\n",
      "Step 502, loss = [0.3095267117023468, 0.004236966371536255, 0.30910301208496094]\n",
      "Step 503, loss = [0.3037928640842438, 0.0053440723568201065, 0.3032584488391876]\n",
      "Step 504, loss = [0.3048953115940094, 0.005702612921595573, 0.30432504415512085]\n",
      "Step 505, loss = [0.30814966559410095, 0.005414560437202454, 0.3076082170009613]\n",
      "Step 506, loss = [0.30984899401664734, 0.003136006649583578, 0.3095353841781616]\n",
      "Step 507, loss = [0.3060583472251892, 0.008879194036126137, 0.3051704168319702]\n",
      "Step 508, loss = [0.29698076844215393, 0.005683609750121832, 0.2964124083518982]\n",
      "Step 509, loss = [0.2896772623062134, 0.005473149009048939, 0.28912994265556335]\n",
      "Step 510, loss = [0.3093840479850769, 0.004484912380576134, 0.3089355528354645]\n",
      "Step 511, loss = [0.30062395334243774, 0.00605569826439023, 0.3000183701515198]\n",
      "Step 512, loss = [0.2900642454624176, 0.01027557160705328, 0.28903669118881226]\n",
      "Step 513, loss = [0.2950891852378845, 0.005939698312431574, 0.29449522495269775]\n",
      "Step 514, loss = [0.29917439818382263, 0.009498067200183868, 0.2982245981693268]\n",
      "Step 515, loss = [0.29122868180274963, 0.006569461897015572, 0.29057174921035767]\n",
      "Step 516, loss = [0.2997869551181793, 0.005003649275749922, 0.2992866039276123]\n",
      "Step 517, loss = [0.3000135123729706, 0.0045154220424592495, 0.2995619773864746]\n",
      "Step 518, loss = [0.29444554448127747, 0.009404554963111877, 0.29350510239601135]\n",
      "Step 519, loss = [0.2945740818977356, 0.003968586679548025, 0.29417723417282104]\n",
      "Step 520, loss = [0.31668710708618164, 0.005479738581925631, 0.3161391317844391]\n",
      "Step 521, loss = [0.2996951639652252, 0.008660480380058289, 0.2988291084766388]\n",
      "Step 522, loss = [0.2950982451438904, 0.0031349649652838707, 0.2947847545146942]\n",
      "Step 523, loss = [0.3002391457557678, 0.0055132643319666386, 0.29968783259391785]\n",
      "Step 524, loss = [0.3113797903060913, 0.0065118675120174885, 0.31072860956192017]\n",
      "Step 525, loss = [0.2862883508205414, 0.006152573972940445, 0.2856730818748474]\n",
      "Step 526, loss = [0.2864071726799011, 0.007454141974449158, 0.2856617569923401]\n",
      "Step 527, loss = [0.29601314663887024, 0.010672181844711304, 0.29494592547416687]\n",
      "Step 528, loss = [0.307228684425354, 0.005104426294565201, 0.30671823024749756]\n",
      "Step 529, loss = [0.29703161120414734, 0.0070344507694244385, 0.2963281571865082]\n",
      "Step 530, loss = [0.2868726849555969, 0.006043681874871254, 0.28626832365989685]\n",
      "Step 531, loss = [0.3038974702358246, 0.00492934649810195, 0.3034045398235321]\n",
      "Step 532, loss = [0.3083822727203369, 0.00700770877301693, 0.30768150091171265]\n",
      "Step 533, loss = [0.2985691726207733, 0.0038988394662737846, 0.2981792986392975]\n",
      "Step 534, loss = [0.3036547601222992, 0.009700771421194077, 0.3026846945285797]\n",
      "Step 535, loss = [0.3016658127307892, 0.012927504256367683, 0.30037304759025574]\n",
      "Step 536, loss = [0.3147224187850952, 0.004188711754977703, 0.31430354714393616]\n",
      "Step 537, loss = [0.3082916736602783, 0.007613389287143946, 0.30753034353256226]\n",
      "Step 538, loss = [0.30549874901771545, 0.009398204274475574, 0.3045589327812195]\n",
      "Step 539, loss = [0.2944731116294861, 0.004195765126496553, 0.2940535247325897]\n",
      "Step 540, loss = [0.2806134819984436, 0.01009868923574686, 0.27960360050201416]\n",
      "Step 541, loss = [0.2971944808959961, 0.0033165705390274525, 0.29686281085014343]\n",
      "Step 542, loss = [0.2953874170780182, 0.006848658435046673, 0.29470255970954895]\n",
      "Step 543, loss = [0.3042716681957245, 0.006333786528557539, 0.3036382794380188]\n",
      "Step 544, loss = [0.3063757121562958, 0.007798610255122185, 0.30559584498405457]\n",
      "Step 545, loss = [0.28905269503593445, 0.007105250842869282, 0.2883421778678894]\n",
      "Step 546, loss = [0.30792587995529175, 0.006984046194702387, 0.3072274625301361]\n",
      "Step 547, loss = [0.29249459505081177, 0.006329424679279327, 0.2918616533279419]\n",
      "Step 548, loss = [0.2845468521118164, 0.009121870622038841, 0.28363466262817383]\n",
      "Step 549, loss = [0.3085624873638153, 0.0075986385345458984, 0.30780261754989624]\n",
      "Step 550, loss = [0.30035942792892456, 0.009882530197501183, 0.2993711829185486]\n",
      "Step 551, loss = [0.30215808749198914, 0.003228449961170554, 0.30183523893356323]\n",
      "Step 552, loss = [0.3088477551937103, 0.011228492483496666, 0.30772489309310913]\n",
      "Step 553, loss = [0.3011019229888916, 0.004683387465775013, 0.30063357949256897]\n",
      "Step 554, loss = [0.29990819096565247, 0.005453995428979397, 0.29936277866363525]\n",
      "Step 555, loss = [0.26499640941619873, 0.009090607985854149, 0.26408734917640686]\n",
      "Step 556, loss = [0.31126633286476135, 0.01314875204116106, 0.30995145440101624]\n",
      "Step 557, loss = [0.2870864272117615, 0.004729551263153553, 0.28661346435546875]\n",
      "Step 558, loss = [0.302350252866745, 0.007973678410053253, 0.3015528917312622]\n",
      "Step 559, loss = [0.3024625778198242, 0.00875061471015215, 0.3015875220298767]\n",
      "Step 560, loss = [0.3142673671245575, 0.004833634942770004, 0.31378400325775146]\n",
      "Step 561, loss = [0.2918321192264557, 0.007177312392741442, 0.2911143898963928]\n",
      "Step 562, loss = [0.29997894167900085, 0.00937806349247694, 0.29904112219810486]\n",
      "Step 563, loss = [0.29462653398513794, 0.0076919011771678925, 0.2938573360443115]\n",
      "Step 564, loss = [0.30622395873069763, 0.009499214589595795, 0.30527403950691223]\n",
      "Step 565, loss = [0.29289475083351135, 0.0029206397011876106, 0.29260268807411194]\n",
      "Step 566, loss = [0.2973465323448181, 0.003094886662438512, 0.2970370352268219]\n",
      "Step 567, loss = [0.3070353865623474, 0.0069684553891420364, 0.3063385486602783]\n",
      "Step 568, loss = [0.3043244183063507, 0.0062409876845777035, 0.30370032787323]\n",
      "Step 569, loss = [0.30121883749961853, 0.0073148892261087894, 0.30048733949661255]\n",
      "Step 570, loss = [0.3123628497123718, 0.004949552938342094, 0.311867892742157]\n",
      "Step 571, loss = [0.30486366152763367, 0.008832788094878197, 0.30398038029670715]\n",
      "Step 572, loss = [0.28753000497817993, 0.005838769022375345, 0.2869461178779602]\n",
      "Step 573, loss = [0.29564982652664185, 0.006792566739022732, 0.2949705719947815]\n",
      "Step 574, loss = [0.2973785102367401, 0.009458629414439201, 0.29643264412879944]\n",
      "Step 575, loss = [0.2958868145942688, 0.004029302857816219, 0.29548388719558716]\n",
      "Step 576, loss = [0.30220043659210205, 0.009496425278484821, 0.30125078558921814]\n",
      "Step 577, loss = [0.30579933524131775, 0.007828728295862675, 0.3050164580345154]\n",
      "Step 578, loss = [0.30509883165359497, 0.002871741307899356, 0.30481165647506714]\n",
      "Step 579, loss = [0.30682939291000366, 0.0036612895783036947, 0.3064632713794708]\n",
      "Step 580, loss = [0.2935115396976471, 0.00765280332416296, 0.29274624586105347]\n",
      "Step 581, loss = [0.3040827214717865, 0.01189945824444294, 0.3028927743434906]\n",
      "Step 582, loss = [0.2883893847465515, 0.010961386375129223, 0.2872932553291321]\n",
      "Step 583, loss = [0.2851649224758148, 0.004157092422246933, 0.28474920988082886]\n",
      "Step 584, loss = [0.29218536615371704, 0.011589162051677704, 0.29102644324302673]\n",
      "Step 585, loss = [0.28720149397850037, 0.00812985748052597, 0.2863885164260864]\n",
      "Step 586, loss = [0.3113373816013336, 0.0074765924364328384, 0.3105897307395935]\n",
      "Step 587, loss = [0.29602012038230896, 0.005259301513433456, 0.2954941987991333]\n",
      "Step 588, loss = [0.30624935030937195, 0.006370099261403084, 0.30561232566833496]\n",
      "Step 589, loss = [0.30207589268684387, 0.007592097856104374, 0.30131667852401733]\n",
      "Step 590, loss = [0.28682568669319153, 0.005914812441915274, 0.28623420000076294]\n",
      "Step 591, loss = [0.29782620072364807, 0.006240951828658581, 0.29720211029052734]\n",
      "Step 592, loss = [0.2901577949523926, 0.008915706537663937, 0.2892662286758423]\n",
      "Step 593, loss = [0.30257514119148254, 0.005079303868114948, 0.30206722021102905]\n",
      "Step 594, loss = [0.29410597681999207, 0.008648707531392574, 0.29324111342430115]\n",
      "Step 595, loss = [0.3021094501018524, 0.008519875817000866, 0.301257461309433]\n",
      "Step 596, loss = [0.30196085572242737, 0.0077840304002165794, 0.30118244886398315]\n",
      "Step 597, loss = [0.3033372163772583, 0.006457032635807991, 0.3026915192604065]\n",
      "Step 598, loss = [0.30029821395874023, 0.009385093115270138, 0.2993597090244293]\n",
      "Step 599, loss = [0.29520314931869507, 0.008006490767002106, 0.29440250992774963]\n",
      "Step 600, loss = [0.29185235500335693, 0.008525671437382698, 0.29099979996681213]\n",
      "Step 601, loss = [0.2895223796367645, 0.006990441121160984, 0.28882333636283875]\n",
      "Step 602, loss = [0.29707664251327515, 0.005398835986852646, 0.29653677344322205]\n",
      "Step 603, loss = [0.3006763160228729, 0.007784249261021614, 0.2998978793621063]\n",
      "Step 604, loss = [0.3100644052028656, 0.008167327381670475, 0.3092476725578308]\n",
      "Step 605, loss = [0.30638960003852844, 0.003865704406052828, 0.30600303411483765]\n",
      "Step 606, loss = [0.3044929504394531, 0.00467575341463089, 0.3040253818035126]\n",
      "Step 607, loss = [0.3069240152835846, 0.008246921002864838, 0.3060993254184723]\n",
      "Step 608, loss = [0.2977902591228485, 0.004891516640782356, 0.29730111360549927]\n",
      "Step 609, loss = [0.3052217960357666, 0.007787410169839859, 0.3044430613517761]\n",
      "Step 610, loss = [0.3054463565349579, 0.007296523079276085, 0.30471670627593994]\n",
      "Step 611, loss = [0.2991146147251129, 0.007519859354943037, 0.2983626425266266]\n",
      "Step 612, loss = [0.31474989652633667, 0.00507775554433465, 0.3142421245574951]\n",
      "Step 613, loss = [0.30719080567359924, 0.006339970510452986, 0.3065568208694458]\n",
      "Step 614, loss = [0.2980310618877411, 0.007655407302081585, 0.29726552963256836]\n",
      "Step 615, loss = [0.305889368057251, 0.004669811576604843, 0.3054223954677582]\n",
      "Step 616, loss = [0.29822614789009094, 0.002491083461791277, 0.2979770302772522]\n",
      "Step 617, loss = [0.29525136947631836, 0.007159397006034851, 0.29453542828559875]\n",
      "Step 618, loss = [0.30493786931037903, 0.005787516478449106, 0.30435910820961]\n",
      "Step 619, loss = [0.3025282919406891, 0.008571045473217964, 0.30167117714881897]\n",
      "Step 620, loss = [0.2966315746307373, 0.005407858639955521, 0.2960907816886902]\n",
      "Step 621, loss = [0.3034920394420624, 0.004107384942471981, 0.30308130383491516]\n",
      "Step 622, loss = [0.30326956510543823, 0.010107038542628288, 0.30225884914398193]\n",
      "Step 623, loss = [0.2996157109737396, 0.006516586989164352, 0.2989640533924103]\n",
      "Step 624, loss = [0.30089911818504333, 0.006145542953163385, 0.30028456449508667]\n",
      "Step 625, loss = [0.29709842801094055, 0.007379821501672268, 0.29636043310165405]\n",
      "Step 626, loss = [0.31200358271598816, 0.004238795954734087, 0.31157970428466797]\n",
      "Step 627, loss = [0.29252147674560547, 0.007823188789188862, 0.29173916578292847]\n",
      "Step 628, loss = [0.3075227737426758, 0.00375430379062891, 0.307147353887558]\n",
      "Step 629, loss = [0.2784263491630554, 0.0056242733262479305, 0.27786391973495483]\n",
      "Step 630, loss = [0.31306856870651245, 0.006870475132018328, 0.3123815357685089]\n",
      "Step 631, loss = [0.29186248779296875, 0.006121971644461155, 0.2912502884864807]\n",
      "Step 632, loss = [0.29476988315582275, 0.004115499556064606, 0.29435834288597107]\n",
      "Step 633, loss = [0.3031889796257019, 0.005050801206380129, 0.30268388986587524]\n",
      "Step 634, loss = [0.297911673784256, 0.010185676626861095, 0.2968931198120117]\n",
      "Step 635, loss = [0.28761982917785645, 0.014001909643411636, 0.28621962666511536]\n",
      "Step 636, loss = [0.3016175925731659, 0.004595572594553232, 0.30115804076194763]\n",
      "Step 637, loss = [0.29616087675094604, 0.005779607221484184, 0.29558292031288147]\n",
      "Step 638, loss = [0.30161580443382263, 0.006534164771437645, 0.3009623885154724]\n",
      "Step 639, loss = [0.3105103075504303, 0.006265754345804453, 0.3098837435245514]\n",
      "Step 640, loss = [0.30052000284194946, 0.008029645308852196, 0.2997170388698578]\n",
      "Step 641, loss = [0.30042576789855957, 0.004095613956451416, 0.3000161945819855]\n",
      "Step 642, loss = [0.2962629497051239, 0.005476645193994045, 0.2957152724266052]\n",
      "Step 643, loss = [0.3044246733188629, 0.006157931871712208, 0.30380886793136597]\n",
      "Step 644, loss = [0.3025474548339844, 0.006370741408318281, 0.3019103705883026]\n",
      "Step 645, loss = [0.2987646162509918, 0.008862409740686417, 0.2978783845901489]\n",
      "Step 646, loss = [0.29100438952445984, 0.01155928149819374, 0.2898484468460083]\n",
      "Step 647, loss = [0.3089011311531067, 0.008024495095014572, 0.3080986738204956]\n",
      "Step 648, loss = [0.30903661251068115, 0.008443031460046768, 0.30819231271743774]\n",
      "Step 649, loss = [0.29883259534835815, 0.007195605896413326, 0.29811304807662964]\n",
      "Step 650, loss = [0.30475327372550964, 0.011695532128214836, 0.30358371138572693]\n",
      "Step 651, loss = [0.29816699028015137, 0.004391351714730263, 0.2977278530597687]\n",
      "Step 652, loss = [0.2993116080760956, 0.007285417523235083, 0.298583060503006]\n",
      "Step 653, loss = [0.2988026738166809, 0.010052908211946487, 0.29779738187789917]\n",
      "Step 654, loss = [0.297626256942749, 0.0016241339035332203, 0.2974638342857361]\n",
      "Step 655, loss = [0.3096150755882263, 0.004887138959020376, 0.3091263473033905]\n",
      "Step 656, loss = [0.3074733018875122, 0.005145708564668894, 0.30695873498916626]\n",
      "Step 657, loss = [0.2982144355773926, 0.004938455298542976, 0.2977205812931061]\n",
      "Step 658, loss = [0.30212876200675964, 0.0075546447187662125, 0.30137330293655396]\n",
      "Step 659, loss = [0.2953767776489258, 0.007333178073167801, 0.29464346170425415]\n",
      "Step 660, loss = [0.2888471782207489, 0.0040126158855855465, 0.28844591975212097]\n",
      "Step 661, loss = [0.29741206765174866, 0.004161864519119263, 0.2969958782196045]\n",
      "Step 662, loss = [0.29735130071640015, 0.003229744266718626, 0.2970283329486847]\n",
      "Step 663, loss = [0.30492281913757324, 0.009543145075440407, 0.30396851897239685]\n",
      "Step 664, loss = [0.2960067689418793, 0.004735497757792473, 0.2955332100391388]\n",
      "Step 665, loss = [0.29649344086647034, 0.006872490048408508, 0.2958061993122101]\n",
      "Step 666, loss = [0.2842501997947693, 0.008608663454651833, 0.2833893299102783]\n",
      "Step 667, loss = [0.30038607120513916, 0.0075959027744829655, 0.2996264696121216]\n",
      "Step 668, loss = [0.3012065589427948, 0.007911784574389458, 0.30041536688804626]\n",
      "Step 669, loss = [0.2971249222755432, 0.012982912361621857, 0.29582664370536804]\n",
      "Step 670, loss = [0.28451910614967346, 0.009844215586781502, 0.2835346758365631]\n",
      "Step 671, loss = [0.2952670753002167, 0.00864256452769041, 0.2944028079509735]\n",
      "Step 672, loss = [0.30475956201553345, 0.0039394311606884, 0.3043656051158905]\n",
      "Step 673, loss = [0.30772829055786133, 0.007246249355375767, 0.3070036768913269]\n",
      "Step 674, loss = [0.29331985116004944, 0.005519595928490162, 0.29276788234710693]\n",
      "Step 675, loss = [0.3044580817222595, 0.005330683663487434, 0.3039250075817108]\n",
      "Step 676, loss = [0.31146329641342163, 0.007060999050736427, 0.31075718998908997]\n",
      "Step 677, loss = [0.2935350835323334, 0.005891868844628334, 0.29294589161872864]\n",
      "Step 678, loss = [0.30789268016815186, 0.00987747311592102, 0.30690494179725647]\n",
      "Step 679, loss = [0.3001096844673157, 0.006878143176436424, 0.29942187666893005]\n",
      "Step 680, loss = [0.3065933287143707, 0.009215866215527058, 0.305671751499176]\n",
      "Step 681, loss = [0.30788978934288025, 0.008492125198245049, 0.30704057216644287]\n",
      "Step 682, loss = [0.3033711016178131, 0.00559863168746233, 0.30281123518943787]\n",
      "Step 683, loss = [0.30559107661247253, 0.010944307781755924, 0.3044966459274292]\n",
      "Step 684, loss = [0.29390302300453186, 0.007645092438906431, 0.2931385040283203]\n",
      "Step 685, loss = [0.29998061060905457, 0.010622070170938969, 0.29891839623451233]\n",
      "Step 686, loss = [0.2698606252670288, 0.007237437646836042, 0.26913687586784363]\n",
      "Step 687, loss = [0.29033908247947693, 0.004876081366091967, 0.28985148668289185]\n",
      "Step 688, loss = [0.300750732421875, 0.005545042920857668, 0.30019623041152954]\n",
      "Step 689, loss = [0.3048884868621826, 0.0038088413421064615, 0.30450761318206787]\n",
      "Step 690, loss = [0.3084079325199127, 0.0049705058336257935, 0.30791088938713074]\n",
      "Step 691, loss = [0.30969399213790894, 0.008946585468947887, 0.3087993264198303]\n",
      "Step 692, loss = [0.2975718379020691, 0.006407474167644978, 0.29693108797073364]\n",
      "Step 693, loss = [0.3023698627948761, 0.00653521902859211, 0.30171632766723633]\n",
      "Step 694, loss = [0.28890085220336914, 0.009630769491195679, 0.2879377603530884]\n",
      "Step 695, loss = [0.30044424533843994, 0.004632145632058382, 0.299981027841568]\n",
      "Step 696, loss = [0.29274043440818787, 0.005214547738432884, 0.29221898317337036]\n",
      "Step 697, loss = [0.29601144790649414, 0.006648187525570393, 0.29534661769866943]\n",
      "Step 698, loss = [0.2917505204677582, 0.007575585041195154, 0.29099297523498535]\n",
      "Step 699, loss = [0.27307701110839844, 0.009351388551294804, 0.27214187383651733]\n",
      "Step 700, loss = [0.3002063035964966, 0.005104878917336464, 0.29969581961631775]\n",
      "Step 701, loss = [0.31071338057518005, 0.008464179933071136, 0.3098669648170471]\n",
      "Step 702, loss = [0.2714189887046814, 0.007449781522154808, 0.2706740200519562]\n",
      "Step 703, loss = [0.29376527667045593, 0.004459718707948923, 0.29331931471824646]\n",
      "Step 704, loss = [0.29707592725753784, 0.00353062991052866, 0.2967228591442108]\n",
      "Step 705, loss = [0.302049458026886, 0.003787652589380741, 0.30167070031166077]\n",
      "Step 706, loss = [0.30615735054016113, 0.011051231063902378, 0.3050522208213806]\n",
      "Step 707, loss = [0.3017873466014862, 0.0078064873814582825, 0.3010067045688629]\n",
      "Step 708, loss = [0.28930625319480896, 0.004540520254522562, 0.2888522148132324]\n",
      "Step 709, loss = [0.2814736068248749, 0.007359493523836136, 0.28073766827583313]\n",
      "Step 710, loss = [0.2960509657859802, 0.005148330703377724, 0.2955361306667328]\n",
      "Step 711, loss = [0.30524957180023193, 0.008589481934905052, 0.3043906092643738]\n",
      "Step 712, loss = [0.28876540064811707, 0.00904817134141922, 0.28786057233810425]\n",
      "Step 713, loss = [0.3063957691192627, 0.004958060570061207, 0.305899977684021]\n",
      "Step 714, loss = [0.3036201000213623, 0.006094519514590502, 0.30301064252853394]\n",
      "Step 715, loss = [0.2892285883426666, 0.008304942399263382, 0.2883980870246887]\n",
      "Step 716, loss = [0.2989082932472229, 0.008184978738427162, 0.29808980226516724]\n",
      "Step 717, loss = [0.309433251619339, 0.008154761977493763, 0.3086177706718445]\n",
      "Step 718, loss = [0.29613450169563293, 0.007473963312804699, 0.2953871190547943]\n",
      "Step 719, loss = [0.3025687336921692, 0.010744187980890274, 0.301494300365448]\n",
      "Step 720, loss = [0.3012462854385376, 0.0043295142240822315, 0.30081334710121155]\n",
      "Step 721, loss = [0.29661062359809875, 0.006615952122956514, 0.2959490418434143]\n",
      "Step 722, loss = [0.30542421340942383, 0.00976303219795227, 0.3044479191303253]\n",
      "Step 723, loss = [0.294586718082428, 0.007104183081537485, 0.2938762903213501]\n",
      "Step 724, loss = [0.30866339802742004, 0.005937286652624607, 0.3080696761608124]\n",
      "Step 725, loss = [0.3046324849128723, 0.0032255419064313173, 0.3043099343776703]\n",
      "Step 726, loss = [0.3048366904258728, 0.00765271857380867, 0.30407142639160156]\n",
      "Step 727, loss = [0.31267136335372925, 0.006165338680148125, 0.312054842710495]\n",
      "Step 728, loss = [0.2945014536380768, 0.006263463757932186, 0.2938750982284546]\n",
      "Step 729, loss = [0.3004857301712036, 0.006571349687874317, 0.29982858896255493]\n",
      "Step 730, loss = [0.3012242913246155, 0.006592791527509689, 0.3005650043487549]\n",
      "Step 731, loss = [0.29456308484077454, 0.008677218109369278, 0.2936953604221344]\n",
      "Step 732, loss = [0.29814499616622925, 0.008110282942652702, 0.2973339557647705]\n",
      "Step 733, loss = [0.2969990372657776, 0.006676008924841881, 0.2963314354419708]\n",
      "Step 734, loss = [0.3063954710960388, 0.009865814819931984, 0.30540889501571655]\n",
      "Step 735, loss = [0.2902259826660156, 0.008424749597907066, 0.28938350081443787]\n",
      "Step 736, loss = [0.3025768995285034, 0.01202167198061943, 0.30137473344802856]\n",
      "Step 737, loss = [0.30380183458328247, 0.008003273978829384, 0.3030014932155609]\n",
      "Step 738, loss = [0.304735004901886, 0.007825029082596302, 0.30395251512527466]\n",
      "Step 739, loss = [0.29095831513404846, 0.0041876924224197865, 0.29053953289985657]\n",
      "Step 740, loss = [0.2924867868423462, 0.00670810928568244, 0.29181596636772156]\n",
      "Step 741, loss = [0.2874322533607483, 0.006510802544653416, 0.2867811620235443]\n",
      "Step 742, loss = [0.2994531989097595, 0.0069978320971131325, 0.29875341057777405]\n",
      "Step 743, loss = [0.3067719340324402, 0.007297783624380827, 0.3060421645641327]\n",
      "Step 744, loss = [0.3019317388534546, 0.0023486188147217035, 0.30169686675071716]\n",
      "Step 745, loss = [0.29681387543678284, 0.0072844666428864, 0.2960854172706604]\n",
      "Step 746, loss = [0.2894971966743469, 0.006663994863629341, 0.28883078694343567]\n",
      "Step 747, loss = [0.3010835647583008, 0.0065006474032998085, 0.300433486700058]\n",
      "Step 748, loss = [0.285055935382843, 0.005530860275030136, 0.28450286388397217]\n",
      "Step 749, loss = [0.3002281188964844, 0.006639072671532631, 0.2995642125606537]\n",
      "Step 750, loss = [0.30593249201774597, 0.0053846752271056175, 0.3053940236568451]\n",
      "Step 751, loss = [0.30359575152397156, 0.005492689553648233, 0.30304649472236633]\n",
      "Step 752, loss = [0.2977542281150818, 0.0057159364223480225, 0.2971826493740082]\n",
      "Step 753, loss = [0.28696250915527344, 0.008583423681557178, 0.2861041724681854]\n",
      "Step 754, loss = [0.2967560589313507, 0.009332452900707722, 0.29582279920578003]\n",
      "Step 755, loss = [0.3038524389266968, 0.004593801684677601, 0.30339306592941284]\n",
      "Step 756, loss = [0.2828935384750366, 0.004676394630223513, 0.2824259102344513]\n",
      "Step 757, loss = [0.3141807019710541, 0.012058479711413383, 0.31297484040260315]\n",
      "Step 758, loss = [0.285408079624176, 0.004960704129189253, 0.28491201996803284]\n",
      "Step 759, loss = [0.3079853355884552, 0.009442019276320934, 0.30704113841056824]\n",
      "Step 760, loss = [0.28791946172714233, 0.005036079324781895, 0.28741586208343506]\n",
      "Step 761, loss = [0.29153385758399963, 0.006925950758159161, 0.2908412516117096]\n",
      "Step 762, loss = [0.29636290669441223, 0.008129563182592392, 0.2955499589443207]\n",
      "Step 763, loss = [0.31308630108833313, 0.009905220940709114, 0.3120957911014557]\n",
      "Step 764, loss = [0.28956031799316406, 0.004061136394739151, 0.28915420174598694]\n",
      "Step 765, loss = [0.2704697847366333, 0.005601509474217892, 0.2699096202850342]\n",
      "Step 766, loss = [0.30391547083854675, 0.0060160113498568535, 0.30331388115882874]\n",
      "Step 767, loss = [0.2980262041091919, 0.0030070445500314236, 0.29772549867630005]\n",
      "Step 768, loss = [0.3030311167240143, 0.005011406727135181, 0.3025299906730652]\n",
      "Step 769, loss = [0.3087596595287323, 0.00922434963285923, 0.30783721804618835]\n",
      "Step 770, loss = [0.2946278154850006, 0.004803748801350594, 0.29414743185043335]\n",
      "Step 771, loss = [0.29429560899734497, 0.005981241352856159, 0.2936974763870239]\n",
      "Step 772, loss = [0.31015023589134216, 0.009664436802268028, 0.309183806180954]\n",
      "Step 773, loss = [0.2980092465877533, 0.011358387768268585, 0.29687342047691345]\n",
      "Step 774, loss = [0.2872784435749054, 0.002967496169731021, 0.2869817018508911]\n",
      "Step 775, loss = [0.2911026179790497, 0.0009378427639603615, 0.2910088300704956]\n",
      "Step 776, loss = [0.2889528274536133, 0.011192278936505318, 0.2878336012363434]\n",
      "Step 777, loss = [0.2907198369503021, 0.004420415032655001, 0.2902778089046478]\n",
      "Step 778, loss = [0.3046856224536896, 0.007641365751624107, 0.30392149090766907]\n",
      "Step 779, loss = [0.29381322860717773, 0.010789732448756695, 0.2927342653274536]\n",
      "Step 780, loss = [0.28884536027908325, 0.0075850337743759155, 0.288086861371994]\n",
      "Step 781, loss = [0.29374903440475464, 0.011970321647822857, 0.29255199432373047]\n",
      "Step 782, loss = [0.30666834115982056, 0.005259115248918533, 0.3061424195766449]\n",
      "Step 783, loss = [0.2861844003200531, 0.004782523028552532, 0.28570616245269775]\n",
      "Step 784, loss = [0.2837029695510864, 0.008255131542682648, 0.28287744522094727]\n",
      "Step 785, loss = [0.3041389286518097, 0.006035453639924526, 0.3035353720188141]\n",
      "Step 786, loss = [0.30695876479148865, 0.005423042923212051, 0.30641645193099976]\n",
      "Step 787, loss = [0.30353251099586487, 0.004119692370295525, 0.30312055349349976]\n",
      "Step 788, loss = [0.2891947329044342, 0.006851439364254475, 0.2885095775127411]\n",
      "Step 789, loss = [0.2940252721309662, 0.009624479338526726, 0.29306283593177795]\n",
      "Step 790, loss = [0.29649150371551514, 0.005800370592623949, 0.2959114611148834]\n",
      "Step 791, loss = [0.3083930015563965, 0.00592705188319087, 0.30780029296875]\n",
      "Step 792, loss = [0.30975058674812317, 0.010999289341270924, 0.3086506724357605]\n",
      "Step 793, loss = [0.28971487283706665, 0.009012941271066666, 0.28881359100341797]\n",
      "Step 794, loss = [0.2906643748283386, 0.0027961009182035923, 0.29038476943969727]\n",
      "Step 795, loss = [0.2967642545700073, 0.007037034723907709, 0.29606056213378906]\n",
      "Step 796, loss = [0.29722121357917786, 0.007119525223970413, 0.2965092658996582]\n",
      "Step 797, loss = [0.30724141001701355, 0.007596862036734819, 0.3064817190170288]\n",
      "Step 798, loss = [0.3072872757911682, 0.00492551364004612, 0.3067947328090668]\n",
      "Step 799, loss = [0.29171404242515564, 0.003639234695583582, 0.2913501262664795]\n",
      "Step 800, loss = [0.30887332558631897, 0.005883514415472746, 0.3082849681377411]\n",
      "Step 801, loss = [0.29508835077285767, 0.0067362105473876, 0.2944147288799286]\n",
      "Step 802, loss = [0.30297911167144775, 0.005385672673583031, 0.3024405539035797]\n",
      "Step 803, loss = [0.29861611127853394, 0.008475510403513908, 0.29776856303215027]\n",
      "Step 804, loss = [0.2899905741214752, 0.003122655674815178, 0.28967830538749695]\n",
      "Step 805, loss = [0.30337339639663696, 0.014257727190852165, 0.30194762349128723]\n",
      "Step 806, loss = [0.28710120916366577, 0.008285332471132278, 0.28627267479896545]\n",
      "Step 807, loss = [0.2948194742202759, 0.007091441657394171, 0.29411032795906067]\n",
      "Step 808, loss = [0.29588600993156433, 0.004453519359230995, 0.2954406440258026]\n",
      "Step 809, loss = [0.3116939067840576, 0.007647670805454254, 0.31092914938926697]\n",
      "Step 810, loss = [0.2918449342250824, 0.006412262562662363, 0.29120370745658875]\n",
      "Step 811, loss = [0.2907271981239319, 0.00444544292986393, 0.290282666683197]\n",
      "Step 812, loss = [0.3051598072052002, 0.004859237931668758, 0.3046738803386688]\n",
      "Step 813, loss = [0.30137765407562256, 0.005264120176434517, 0.3008512556552887]\n",
      "Step 814, loss = [0.30496203899383545, 0.003944897558540106, 0.3045675456523895]\n",
      "Step 815, loss = [0.30745095014572144, 0.008862447924911976, 0.30656471848487854]\n",
      "Step 816, loss = [0.28949302434921265, 0.00548611581325531, 0.28894442319869995]\n",
      "Step 817, loss = [0.30050167441368103, 0.0021266392432153225, 0.30028900504112244]\n",
      "Step 818, loss = [0.30305808782577515, 0.004971785470843315, 0.3025608956813812]\n",
      "Step 819, loss = [0.2985519468784332, 0.007070071995258331, 0.29784494638442993]\n",
      "Step 820, loss = [0.2935045063495636, 0.010840358212590218, 0.29242047667503357]\n",
      "Step 821, loss = [0.2953975796699524, 0.0019537650514394045, 0.29520219564437866]\n",
      "Step 822, loss = [0.30346280336380005, 0.009638242423534393, 0.3024989664554596]\n",
      "Step 823, loss = [0.2874271869659424, 0.002924656728282571, 0.28713470697402954]\n",
      "Step 824, loss = [0.2900726795196533, 0.010925717651844025, 0.288980096578598]\n",
      "Step 825, loss = [0.30445536971092224, 0.01325283944606781, 0.3031300902366638]\n",
      "Step 826, loss = [0.3089636266231537, 0.007697785273194313, 0.3081938624382019]\n",
      "Step 827, loss = [0.3036787509918213, 0.00846773199737072, 0.3028319776058197]\n",
      "Step 828, loss = [0.2877624034881592, 0.00741882948204875, 0.2870205342769623]\n",
      "Step 829, loss = [0.30398738384246826, 0.005138677079230547, 0.30347350239753723]\n",
      "Step 830, loss = [0.28318652510643005, 0.00946984626352787, 0.28223952651023865]\n",
      "Step 831, loss = [0.29977819323539734, 0.005791503936052322, 0.29919904470443726]\n",
      "Step 832, loss = [0.31146785616874695, 0.006451527588069439, 0.3108226954936981]\n",
      "Step 833, loss = [0.3091799020767212, 0.008107208646833897, 0.3083691895008087]\n",
      "Step 834, loss = [0.296814501285553, 0.005597942043095827, 0.2962546944618225]\n",
      "Step 835, loss = [0.29560285806655884, 0.00884336419403553, 0.29471853375434875]\n",
      "Step 836, loss = [0.3106946349143982, 0.00585637241601944, 0.3101089894771576]\n",
      "Step 837, loss = [0.2858006954193115, 0.009413364343345165, 0.2848593592643738]\n",
      "Step 838, loss = [0.30387750267982483, 0.006420869380235672, 0.30323541164398193]\n",
      "Step 839, loss = [0.30940937995910645, 0.0073461104184389114, 0.30867478251457214]\n",
      "Step 840, loss = [0.30333173274993896, 0.005242944695055485, 0.30280745029449463]\n",
      "Step 841, loss = [0.2991037666797638, 0.0039205728098750114, 0.29871171712875366]\n",
      "Step 842, loss = [0.3113860785961151, 0.010427132248878479, 0.31034335494041443]\n",
      "Step 843, loss = [0.3071969151496887, 0.004939431324601173, 0.30670297145843506]\n",
      "Step 844, loss = [0.304649293422699, 0.010158834978938103, 0.3036334216594696]\n",
      "Step 845, loss = [0.30893048644065857, 0.005468539893627167, 0.30838364362716675]\n",
      "Step 846, loss = [0.305111289024353, 0.008793964050710201, 0.3042318820953369]\n",
      "Step 847, loss = [0.2946128249168396, 0.002173610730096698, 0.29439547657966614]\n",
      "Step 848, loss = [0.29520779848098755, 0.00951929111033678, 0.2942558825016022]\n",
      "Step 849, loss = [0.3063734471797943, 0.0044763050973415375, 0.30592581629753113]\n",
      "Step 850, loss = [0.2999262511730194, 0.007578942459076643, 0.29916834831237793]\n",
      "Step 851, loss = [0.3083459138870239, 0.0089283287525177, 0.30745306611061096]\n",
      "Step 852, loss = [0.31454193592071533, 0.029009787365794182, 0.3116409480571747]\n",
      "Step 853, loss = [0.3030788004398346, 0.0075963204726576805, 0.30231916904449463]\n",
      "Step 854, loss = [0.3173830509185791, 0.004268893972039223, 0.31695616245269775]\n",
      "Step 855, loss = [0.30331575870513916, 0.0035877088084816933, 0.3029569983482361]\n",
      "Step 856, loss = [0.29808735847473145, 0.008009377866983414, 0.29728642106056213]\n",
      "Step 857, loss = [0.306799978017807, 0.00654617790132761, 0.3061453700065613]\n",
      "Step 858, loss = [0.31391671299934387, 0.006315281614661217, 0.3132851719856262]\n",
      "Step 859, loss = [0.2752859592437744, 0.00859813578426838, 0.274426132440567]\n",
      "Step 860, loss = [0.28269508481025696, 0.00764333363622427, 0.28193074464797974]\n",
      "Step 861, loss = [0.3041318953037262, 0.006489696446806192, 0.30348291993141174]\n",
      "Step 862, loss = [0.2975030243396759, 0.005489312577992678, 0.29695409536361694]\n",
      "Step 863, loss = [0.29928654432296753, 0.005122635513544083, 0.29877427220344543]\n",
      "Step 864, loss = [0.3007185459136963, 0.005958497524261475, 0.3001227080821991]\n",
      "Step 865, loss = [0.2918926477432251, 0.0041831121779978275, 0.2914743423461914]\n",
      "Step 866, loss = [0.30150407552719116, 0.005834090989083052, 0.30092066526412964]\n",
      "Step 867, loss = [0.30750373005867004, 0.00525266956537962, 0.3069784641265869]\n",
      "Step 868, loss = [0.28643861413002014, 0.007077998481690884, 0.2857308089733124]\n",
      "Step 869, loss = [0.2960736155509949, 0.009328776970505714, 0.29514074325561523]\n",
      "Step 870, loss = [0.3021000921726227, 0.0066508157178759575, 0.30143502354621887]\n",
      "Step 871, loss = [0.2978231906890869, 0.005060876719653606, 0.29731711745262146]\n",
      "Step 872, loss = [0.31166812777519226, 0.010137149132788181, 0.3106544017791748]\n",
      "Step 873, loss = [0.2915465831756592, 0.005202045664191246, 0.29102638363838196]\n",
      "Step 874, loss = [0.3014870285987854, 0.002895504701882601, 0.30119746923446655]\n",
      "Step 875, loss = [0.2813590466976166, 0.003744938876479864, 0.2809845507144928]\n",
      "Step 876, loss = [0.2962457239627838, 0.008775891736149788, 0.29536813497543335]\n",
      "Step 877, loss = [0.3166622817516327, 0.005761380307376385, 0.31608614325523376]\n",
      "Step 878, loss = [0.31530240178108215, 0.006303534377366304, 0.31467205286026]\n",
      "Step 879, loss = [0.3096121847629547, 0.006757767405360937, 0.3089364171028137]\n",
      "Step 880, loss = [0.29416245222091675, 0.005555003881454468, 0.2936069369316101]\n",
      "Step 881, loss = [0.30547216534614563, 0.007448027841746807, 0.30472737550735474]\n",
      "Step 882, loss = [0.30796295404434204, 0.011618387885391712, 0.30680111050605774]\n",
      "Step 883, loss = [0.2969246804714203, 0.00899277999997139, 0.2960253953933716]\n",
      "Step 884, loss = [0.30826207995414734, 0.010934406891465187, 0.3071686327457428]\n",
      "Step 885, loss = [0.30198439955711365, 0.010524909943342209, 0.3009319007396698]\n",
      "Step 886, loss = [0.28902101516723633, 0.006636530160903931, 0.28835734724998474]\n",
      "Step 887, loss = [0.2829715311527252, 0.005297234281897545, 0.28244179487228394]\n",
      "Step 888, loss = [0.3122807741165161, 0.007176805753260851, 0.311563104391098]\n",
      "Step 889, loss = [0.3016272783279419, 0.004016616847366095, 0.30122560262680054]\n",
      "Step 890, loss = [0.28885596990585327, 0.00998120941221714, 0.28785786032676697]\n",
      "Step 891, loss = [0.30692058801651, 0.006089905276894569, 0.30631160736083984]\n",
      "Step 892, loss = [0.29651886224746704, 0.009144866839051247, 0.2956043779850006]\n",
      "Step 893, loss = [0.29832136631011963, 0.009818846359848976, 0.29733946919441223]\n",
      "Step 894, loss = [0.2987152934074402, 0.005042636301368475, 0.2982110381126404]\n",
      "Step 895, loss = [0.2939482629299164, 0.0021848618052899837, 0.2937297821044922]\n",
      "Step 896, loss = [0.30225899815559387, 0.006535065360367298, 0.3016054928302765]\n",
      "Step 897, loss = [0.31252455711364746, 0.005116714164614677, 0.3120128810405731]\n",
      "Step 898, loss = [0.3021094501018524, 0.01106410101056099, 0.30100303888320923]\n",
      "Step 899, loss = [0.2831856310367584, 0.007121614180505276, 0.28247347474098206]\n",
      "Step 900, loss = [0.2899788022041321, 0.007637537084519863, 0.2892150580883026]\n",
      "Step 901, loss = [0.3067261576652527, 0.004961187019944191, 0.3062300384044647]\n",
      "Step 902, loss = [0.3037886321544647, 0.006896038539707661, 0.30309903621673584]\n",
      "Step 903, loss = [0.2947474718093872, 0.006518295966088772, 0.29409563541412354]\n",
      "Step 904, loss = [0.2968633770942688, 0.0024244494270533323, 0.2966209352016449]\n",
      "Step 905, loss = [0.3107318878173828, 0.003219444304704666, 0.31040993332862854]\n",
      "Step 906, loss = [0.29871368408203125, 0.0046822987496852875, 0.29824545979499817]\n",
      "Step 907, loss = [0.3001193106174469, 0.008476145565509796, 0.29927170276641846]\n",
      "Step 908, loss = [0.29661378264427185, 0.005295101087540388, 0.29608428478240967]\n",
      "Step 909, loss = [0.31093350052833557, 0.004432766232639551, 0.310490220785141]\n",
      "Step 910, loss = [0.2977631688117981, 0.008919750340282917, 0.2968711853027344]\n",
      "Step 911, loss = [0.30690622329711914, 0.0051949722692370415, 0.3063867390155792]\n",
      "Step 912, loss = [0.2839128375053406, 0.0071449121460318565, 0.28319835662841797]\n",
      "Step 913, loss = [0.30095309019088745, 0.007064434699714184, 0.3002466559410095]\n",
      "Step 914, loss = [0.3052235543727875, 0.012991802766919136, 0.3039243817329407]\n",
      "Step 915, loss = [0.297088623046875, 0.005455671809613705, 0.29654306173324585]\n",
      "Step 916, loss = [0.29080063104629517, 0.013585086911916733, 0.28944212198257446]\n",
      "Step 917, loss = [0.2941551208496094, 0.007857893593609333, 0.293369323015213]\n",
      "Step 918, loss = [0.3117949962615967, 0.007569063920527697, 0.311038076877594]\n",
      "Step 919, loss = [0.3006417453289032, 0.005659374408423901, 0.30007579922676086]\n",
      "Step 920, loss = [0.30289238691329956, 0.004198763053864241, 0.3024725019931793]\n",
      "Step 921, loss = [0.3018966615200043, 0.00637366808950901, 0.301259309053421]\n",
      "Step 922, loss = [0.29519036412239075, 0.005047468468546867, 0.29468563199043274]\n",
      "Step 923, loss = [0.29131418466567993, 0.010552102699875832, 0.2902589738368988]\n",
      "Step 924, loss = [0.29889237880706787, 0.008125743828713894, 0.29807981848716736]\n",
      "Step 925, loss = [0.2901776432991028, 0.00533690582960844, 0.28964394330978394]\n",
      "Step 926, loss = [0.29691314697265625, 0.005910228006541729, 0.29632213711738586]\n",
      "Step 927, loss = [0.2904271185398102, 0.010205858387053013, 0.28940653800964355]\n",
      "Step 928, loss = [0.30779141187667847, 0.004956869408488274, 0.3072957396507263]\n",
      "Step 929, loss = [0.3061152696609497, 0.01004873402416706, 0.3051103949546814]\n",
      "Step 930, loss = [0.299577534198761, 0.007501522079110146, 0.2988273799419403]\n",
      "Step 931, loss = [0.30271366238594055, 0.007154081016778946, 0.3019982576370239]\n",
      "Step 932, loss = [0.29372739791870117, 0.008506967686116695, 0.2928766906261444]\n",
      "Step 933, loss = [0.29828765988349915, 0.007651016116142273, 0.29752254486083984]\n",
      "Step 934, loss = [0.3085629343986511, 0.006734196562319994, 0.30788952112197876]\n",
      "Step 935, loss = [0.2975502014160156, 0.005722841247916222, 0.2969779074192047]\n",
      "Step 936, loss = [0.2877728044986725, 0.006212751846760511, 0.2871515154838562]\n",
      "Step 937, loss = [0.3020579218864441, 0.009783006273210049, 0.3010796308517456]\n",
      "Step 938, loss = [0.3077473044395447, 0.007733128033578396, 0.30697399377822876]\n",
      "Step 939, loss = [0.3029487133026123, 0.008019231259822845, 0.3021467924118042]\n",
      "Step 940, loss = [0.30918681621551514, 0.007807360030710697, 0.3084060847759247]\n",
      "Step 941, loss = [0.2822608947753906, 0.0025829682126641273, 0.28200259804725647]\n",
      "Step 942, loss = [0.3106618821620941, 0.013227653689682484, 0.30933910608291626]\n",
      "Step 943, loss = [0.30135825276374817, 0.007883787155151367, 0.3005698621273041]\n",
      "Step 944, loss = [0.2919357419013977, 0.004496677778661251, 0.29148608446121216]\n",
      "Step 945, loss = [0.28891798853874207, 0.008966056630015373, 0.28802138566970825]\n",
      "Step 946, loss = [0.2946076989173889, 0.00407711137086153, 0.29419997334480286]\n",
      "Step 947, loss = [0.3096548020839691, 0.004579474218189716, 0.3091968595981598]\n",
      "Step 948, loss = [0.2877318561077118, 0.008405163884162903, 0.2868913412094116]\n",
      "Step 949, loss = [0.2962080240249634, 0.006437400821596384, 0.29556429386138916]\n",
      "Step 950, loss = [0.29838845133781433, 0.004467597231268883, 0.2979416847229004]\n",
      "Step 951, loss = [0.27914702892303467, 0.0038006475660949945, 0.2787669599056244]\n",
      "Step 952, loss = [0.2984824776649475, 0.006928899325430393, 0.2977895736694336]\n",
      "Step 953, loss = [0.30730950832366943, 0.00458770664408803, 0.30685073137283325]\n",
      "Step 954, loss = [0.31023329496383667, 0.007436057552695274, 0.3094896972179413]\n",
      "Step 955, loss = [0.2872277498245239, 0.007656965404748917, 0.28646203875541687]\n",
      "Step 956, loss = [0.28193119168281555, 0.005322122015058994, 0.2813989818096161]\n",
      "Step 957, loss = [0.28379517793655396, 0.006937568075954914, 0.2831014096736908]\n",
      "Step 958, loss = [0.307229220867157, 0.007130425423383713, 0.306516170501709]\n",
      "Step 959, loss = [0.30104395747184753, 0.0037371190264821053, 0.30067023634910583]\n",
      "Step 960, loss = [0.29924434423446655, 0.003978686407208443, 0.2988464832305908]\n",
      "Step 961, loss = [0.28429386019706726, 0.008631190285086632, 0.2834307551383972]\n",
      "Step 962, loss = [0.29952141642570496, 0.006149710156023502, 0.29890644550323486]\n",
      "Step 963, loss = [0.3017982244491577, 0.005544088780879974, 0.3012438118457794]\n",
      "Step 964, loss = [0.2970535159111023, 0.008914981968700886, 0.2961620092391968]\n",
      "Step 965, loss = [0.28745773434638977, 0.005593985319137573, 0.28689834475517273]\n",
      "Step 966, loss = [0.3110686242580414, 0.008110915310680866, 0.31025752425193787]\n",
      "Step 967, loss = [0.3026886284351349, 0.004781510680913925, 0.3022104799747467]\n",
      "Step 968, loss = [0.2997399568557739, 0.0072403885424137115, 0.29901590943336487]\n",
      "Step 969, loss = [0.30789583921432495, 0.009586041793227196, 0.30693724751472473]\n",
      "Step 970, loss = [0.2915772795677185, 0.004664713982492685, 0.2911108136177063]\n",
      "Step 971, loss = [0.29394829273223877, 0.00373377394862473, 0.2935749292373657]\n",
      "Step 972, loss = [0.30931198596954346, 0.008911540731787682, 0.3084208369255066]\n",
      "Step 973, loss = [0.30734679102897644, 0.006134495139122009, 0.3067333400249481]\n",
      "Step 974, loss = [0.29412904381752014, 0.007029050029814243, 0.29342612624168396]\n",
      "Step 975, loss = [0.3060742914676666, 0.00522560253739357, 0.3055517375469208]\n",
      "Step 976, loss = [0.3062542676925659, 0.0066232322715222836, 0.3055919408798218]\n",
      "Step 977, loss = [0.2909337282180786, 0.009414377622306347, 0.2899923026561737]\n",
      "Update target distribution epoch 2 step 978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11754/11754 [1:51:21<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos. rate:0.42549019607843136 Neg. rate:0.5745098039215686\n",
      "delta_label  0.009443593670239918\n",
      "Step 978, loss = [0.29557284712791443, 0.006345708388835192, 0.29493826627731323]\n",
      "Step 979, loss = [0.2893085181713104, 0.013514295220375061, 0.287957102060318]\n",
      "Step 980, loss = [0.31023767590522766, 0.003355881664901972, 0.3099021017551422]\n",
      "Step 981, loss = [0.2841787040233612, 0.003749028081074357, 0.283803790807724]\n",
      "Step 982, loss = [0.30528968572616577, 0.005114567931741476, 0.30477821826934814]\n",
      "Step 983, loss = [0.31317752599716187, 0.00923988875001669, 0.31225353479385376]\n",
      "Step 984, loss = [0.29895973205566406, 0.005239953752607107, 0.2984357476234436]\n",
      "Step 985, loss = [0.2990339994430542, 0.006044352892786264, 0.29842957854270935]\n",
      "Step 986, loss = [0.3078877031803131, 0.004064326174557209, 0.3074812591075897]\n",
      "Step 987, loss = [0.30019667744636536, 0.005986347328871489, 0.2995980381965637]\n",
      "Step 988, loss = [0.2988449037075043, 0.007917700335383415, 0.29805314540863037]\n",
      "Step 989, loss = [0.3035202622413635, 0.00898749753832817, 0.3026215136051178]\n",
      "Step 990, loss = [0.30251893401145935, 0.00873192586004734, 0.30164575576782227]\n",
      "Step 991, loss = [0.30960559844970703, 0.009952295571565628, 0.30861037969589233]\n",
      "Step 992, loss = [0.30237823724746704, 0.003668765537440777, 0.3020113706588745]\n",
      "Step 993, loss = [0.2883376181125641, 0.0066572194918990135, 0.28767189383506775]\n",
      "Step 994, loss = [0.30124130845069885, 0.004742327146232128, 0.30076706409454346]\n",
      "Step 995, loss = [0.29499566555023193, 0.005562825594097376, 0.2944393754005432]\n",
      "Step 996, loss = [0.285128116607666, 0.009324108250439167, 0.2841956913471222]\n",
      "Step 997, loss = [0.29017820954322815, 0.007414218969643116, 0.28943678736686707]\n",
      "Step 998, loss = [0.29142752289772034, 0.004843664355576038, 0.2909431457519531]\n",
      "Step 999, loss = [0.3047173023223877, 0.007139146327972412, 0.30400338768959045]\n",
      "Step 1000, loss = [0.30030667781829834, 0.007691475097090006, 0.2995375394821167]\n",
      "Step 1001, loss = [0.30445393919944763, 0.0019324369495734572, 0.3042607009410858]\n",
      "Step 1002, loss = [0.3060128092765808, 0.005933060776442289, 0.3054195046424866]\n",
      "Step 1003, loss = [0.29679936170578003, 0.00697533180937171, 0.296101838350296]\n",
      "Step 1004, loss = [0.3079666197299957, 0.01004533190280199, 0.3069620728492737]\n",
      "Step 1005, loss = [0.2988082766532898, 0.007366538979113102, 0.29807162284851074]\n",
      "Step 1006, loss = [0.30580461025238037, 0.01199868880212307, 0.30460473895072937]\n",
      "Step 1007, loss = [0.30791160464286804, 0.007217485923320055, 0.30718985199928284]\n",
      "Step 1008, loss = [0.3048570156097412, 0.012761964462697506, 0.3035808205604553]\n",
      "Step 1009, loss = [0.28964313864707947, 0.003077277448028326, 0.2893353998661041]\n",
      "Step 1010, loss = [0.30923423171043396, 0.007010383997112513, 0.3085331916809082]\n",
      "Step 1011, loss = [0.3042270839214325, 0.006239654961973429, 0.3036031126976013]\n",
      "Step 1012, loss = [0.2970012128353119, 0.010563341900706291, 0.29594486951828003]\n",
      "Step 1013, loss = [0.3117780387401581, 0.006175365298986435, 0.31116050481796265]\n",
      "Step 1014, loss = [0.2991166114807129, 0.009401533752679825, 0.29817646741867065]\n",
      "Step 1015, loss = [0.2951470911502838, 0.007998001761734486, 0.29434728622436523]\n",
      "Step 1016, loss = [0.29433003067970276, 0.006670283153653145, 0.29366299510002136]\n",
      "Step 1017, loss = [0.28079551458358765, 0.005617348477244377, 0.280233770608902]\n",
      "Step 1018, loss = [0.3030836284160614, 0.00661373371258378, 0.30242225527763367]\n",
      "Step 1019, loss = [0.2930667996406555, 0.008254284039139748, 0.2922413647174835]\n",
      "Step 1020, loss = [0.2968771755695343, 0.004622417502105236, 0.29641494154930115]\n",
      "Step 1021, loss = [0.29489344358444214, 0.0030356913339346647, 0.2945898771286011]\n",
      "Step 1022, loss = [0.29783695936203003, 0.002925956156104803, 0.29754436016082764]\n",
      "Step 1023, loss = [0.2997346520423889, 0.00773276574909687, 0.2989613711833954]\n",
      "Step 1024, loss = [0.29478058218955994, 0.007907566614449024, 0.2939898371696472]\n",
      "Step 1025, loss = [0.306853711605072, 0.009869735687971115, 0.3058667480945587]\n",
      "Step 1026, loss = [0.29167917370796204, 0.008995402604341507, 0.29077962040901184]\n",
      "Step 1027, loss = [0.3054872155189514, 0.005407512653619051, 0.3049464523792267]\n",
      "Step 1028, loss = [0.29722100496292114, 0.0036188059020787477, 0.29685911536216736]\n",
      "Step 1029, loss = [0.29786500334739685, 0.00895594246685505, 0.2969694137573242]\n",
      "Step 1030, loss = [0.29527223110198975, 0.006473849527537823, 0.29462483525276184]\n",
      "Step 1031, loss = [0.3026374578475952, 0.0024459552951157093, 0.3023928701877594]\n",
      "Step 1032, loss = [0.30824509263038635, 0.004714492708444595, 0.3077736496925354]\n",
      "Step 1033, loss = [0.3087158799171448, 0.007211415562778711, 0.3079947531223297]\n",
      "Step 1034, loss = [0.29802578687667847, 0.010464178398251534, 0.2969793677330017]\n",
      "Step 1035, loss = [0.3017076253890991, 0.007203573361039162, 0.30098727345466614]\n",
      "Step 1036, loss = [0.30176135897636414, 0.00441946554929018, 0.301319420337677]\n",
      "Step 1037, loss = [0.3023260831832886, 0.00917529035359621, 0.3014085590839386]\n",
      "Step 1038, loss = [0.2983373701572418, 0.007005373015999794, 0.29763683676719666]\n",
      "Step 1039, loss = [0.3077547252178192, 0.009385012090206146, 0.3068162202835083]\n",
      "Step 1040, loss = [0.29817700386047363, 0.009362775832414627, 0.2972407341003418]\n",
      "Step 1041, loss = [0.2954552471637726, 0.009094947017729282, 0.2945457398891449]\n",
      "Step 1042, loss = [0.310873806476593, 0.008554065600037575, 0.310018390417099]\n",
      "Step 1043, loss = [0.30660343170166016, 0.003879447467625141, 0.3062154948711395]\n",
      "Step 1044, loss = [0.294935405254364, 0.006995502393692732, 0.29423585534095764]\n",
      "Step 1045, loss = [0.29848453402519226, 0.004705269820988178, 0.2980140149593353]\n",
      "Step 1046, loss = [0.3067369759082794, 0.002433614106848836, 0.3064936101436615]\n",
      "Step 1047, loss = [0.2978983521461487, 0.007361645810306072, 0.29716217517852783]\n",
      "Step 1048, loss = [0.30071282386779785, 0.006657607387751341, 0.3000470697879791]\n",
      "Step 1049, loss = [0.2879733443260193, 0.008310362696647644, 0.2871423065662384]\n",
      "Step 1050, loss = [0.2939186692237854, 0.007322861813008785, 0.29318639636039734]\n",
      "Step 1051, loss = [0.30333489179611206, 0.002815080340951681, 0.3030533790588379]\n",
      "Step 1052, loss = [0.2951551675796509, 0.01150040328502655, 0.2940051257610321]\n",
      "Step 1053, loss = [0.30561453104019165, 0.00529558164998889, 0.3050849735736847]\n",
      "Step 1054, loss = [0.3084408640861511, 0.010607910342514515, 0.3073800802230835]\n",
      "Step 1055, loss = [0.2938937246799469, 0.008045927621424198, 0.2930891215801239]\n",
      "Step 1056, loss = [0.30889132618904114, 0.010325435549020767, 0.30785879492759705]\n",
      "Step 1057, loss = [0.2885773479938507, 0.010989313945174217, 0.28747841715812683]\n",
      "Step 1058, loss = [0.30553510785102844, 0.00727919302880764, 0.304807186126709]\n",
      "Step 1059, loss = [0.2941657602787018, 0.004014907870441675, 0.29376426339149475]\n",
      "Step 1060, loss = [0.30203378200531006, 0.004582117311656475, 0.30157557129859924]\n",
      "Step 1061, loss = [0.3093002736568451, 0.0070682428777217865, 0.3085934519767761]\n",
      "Step 1062, loss = [0.3095496892929077, 0.007169836666435003, 0.30883270502090454]\n",
      "Step 1063, loss = [0.3050385117530823, 0.002873427700251341, 0.3047511577606201]\n",
      "Step 1064, loss = [0.30234041810035706, 0.002222404582425952, 0.302118182182312]\n",
      "Step 1065, loss = [0.30583977699279785, 0.005406138487160206, 0.30529916286468506]\n",
      "Step 1066, loss = [0.29401496052742004, 0.007230008020997047, 0.29329195618629456]\n",
      "Step 1067, loss = [0.2968285083770752, 0.006988479755818844, 0.29612967371940613]\n",
      "Step 1068, loss = [0.3105196952819824, 0.0037008721847087145, 0.310149610042572]\n",
      "Step 1069, loss = [0.3055126368999481, 0.006045485846698284, 0.3049080967903137]\n",
      "Step 1070, loss = [0.2902034521102905, 0.006921341642737389, 0.2895113229751587]\n",
      "Step 1071, loss = [0.3071288466453552, 0.00911732204258442, 0.30621710419654846]\n",
      "Step 1072, loss = [0.2930467426776886, 0.0027778090443462133, 0.2927689552307129]\n",
      "Step 1073, loss = [0.30233636498451233, 0.003488095011562109, 0.30198755860328674]\n",
      "Step 1074, loss = [0.3007413148880005, 0.005882906261831522, 0.3001530170440674]\n",
      "Step 1075, loss = [0.287715882062912, 0.0072530354373157024, 0.28699058294296265]\n",
      "Step 1076, loss = [0.29866451025009155, 0.004714513197541237, 0.2981930673122406]\n",
      "Step 1077, loss = [0.2937224805355072, 0.0076100267469882965, 0.2929614782333374]\n",
      "Step 1078, loss = [0.29123327136039734, 0.005728900898247957, 0.29066038131713867]\n",
      "Step 1079, loss = [0.29780274629592896, 0.006585824303328991, 0.29714417457580566]\n",
      "Step 1080, loss = [0.2983848452568054, 0.0074683246202766895, 0.2976379990577698]\n",
      "Step 1081, loss = [0.304763525724411, 0.00738873053342104, 0.30402466654777527]\n",
      "Step 1082, loss = [0.3020032048225403, 0.009306654334068298, 0.30107253789901733]\n",
      "Step 1083, loss = [0.3061755895614624, 0.007843228057026863, 0.30539125204086304]\n",
      "Step 1084, loss = [0.29091909527778625, 0.005953354761004448, 0.29032376408576965]\n",
      "Step 1085, loss = [0.2965725064277649, 0.004859971813857555, 0.29608651995658875]\n",
      "Step 1086, loss = [0.2920796573162079, 0.007337504997849464, 0.29134589433670044]\n",
      "Step 1087, loss = [0.309429407119751, 0.00890292227268219, 0.30853912234306335]\n",
      "Step 1088, loss = [0.30045202374458313, 0.002294018166139722, 0.30022263526916504]\n",
      "Step 1089, loss = [0.2780153751373291, 0.0038979777600616217, 0.27762559056282043]\n",
      "Step 1090, loss = [0.302720844745636, 0.00419788621366024, 0.3023010492324829]\n",
      "Step 1091, loss = [0.29980093240737915, 0.00876779854297638, 0.29892414808273315]\n",
      "Step 1092, loss = [0.3044998347759247, 0.008860955014824867, 0.3036137521266937]\n",
      "Step 1093, loss = [0.2989192605018616, 0.0035599779803305864, 0.29856327176094055]\n",
      "Step 1094, loss = [0.2999495565891266, 0.005408879369497299, 0.2994086742401123]\n",
      "Step 1095, loss = [0.29952237010002136, 0.0059471409767866135, 0.2989276647567749]\n",
      "Step 1096, loss = [0.3052727282047272, 0.011610480956733227, 0.30411168932914734]\n",
      "Step 1097, loss = [0.3090246021747589, 0.0032716854475438595, 0.3086974322795868]\n",
      "Step 1098, loss = [0.2991753816604614, 0.012809036299586296, 0.2978944778442383]\n",
      "Step 1099, loss = [0.3063681721687317, 0.008935471996665001, 0.3054746389389038]\n",
      "Step 1100, loss = [0.3125966489315033, 0.004539367742836475, 0.3121426999568939]\n",
      "Step 1101, loss = [0.2925565540790558, 0.006591076031327248, 0.2918974459171295]\n",
      "Step 1102, loss = [0.30392199754714966, 0.0052689556032419205, 0.3033950924873352]\n",
      "Step 1103, loss = [0.30515673756599426, 0.006834912113845348, 0.30447325110435486]\n",
      "Step 1104, loss = [0.3073548972606659, 0.006492875050753355, 0.30670562386512756]\n",
      "Step 1105, loss = [0.3108123242855072, 0.004572754725813866, 0.3103550374507904]\n",
      "Step 1106, loss = [0.3062523603439331, 0.008977383375167847, 0.30535462498664856]\n",
      "Step 1107, loss = [0.27560973167419434, 0.009467014111578465, 0.2746630311012268]\n",
      "Step 1108, loss = [0.2988177537918091, 0.00684425700455904, 0.29813331365585327]\n",
      "Step 1109, loss = [0.295711487531662, 0.006080487743020058, 0.29510343074798584]\n",
      "Step 1110, loss = [0.30918940901756287, 0.009434760548174381, 0.3082459270954132]\n",
      "Step 1111, loss = [0.29734960198402405, 0.005484671797603369, 0.2968011200428009]\n",
      "Step 1112, loss = [0.3077477812767029, 0.0060754623264074326, 0.3071402311325073]\n",
      "Step 1113, loss = [0.2768516540527344, 0.004641633480787277, 0.276387482881546]\n",
      "Step 1114, loss = [0.28822723031044006, 0.00688102887943387, 0.28753912448883057]\n",
      "Step 1115, loss = [0.3064086139202118, 0.00890279933810234, 0.30551832914352417]\n",
      "Step 1116, loss = [0.281400203704834, 0.007940671406686306, 0.28060615062713623]\n",
      "Step 1117, loss = [0.30409514904022217, 0.003784600645303726, 0.3037166893482208]\n",
      "Step 1118, loss = [0.30819690227508545, 0.0062807099893689156, 0.30756881833076477]\n",
      "Step 1119, loss = [0.29456061124801636, 0.0058337850496172905, 0.2939772307872772]\n",
      "Step 1120, loss = [0.29726096987724304, 0.003907452803105116, 0.29687023162841797]\n",
      "Step 1121, loss = [0.3001272678375244, 0.0049515715800225735, 0.29963210225105286]\n",
      "Step 1122, loss = [0.30279093980789185, 0.006833371706306934, 0.3021076023578644]\n",
      "Step 1123, loss = [0.29742228984832764, 0.006219988688826561, 0.29680028557777405]\n",
      "Step 1124, loss = [0.30454981327056885, 0.007149366196244955, 0.3038348853588104]\n",
      "Step 1125, loss = [0.28404876589775085, 0.008587787859141827, 0.283189982175827]\n",
      "Step 1126, loss = [0.2847253382205963, 0.007067144848406315, 0.2840186357498169]\n",
      "Step 1127, loss = [0.3014383316040039, 0.010408279486000538, 0.30039751529693604]\n",
      "Step 1128, loss = [0.31167396903038025, 0.010927092283964157, 0.3105812668800354]\n",
      "Step 1129, loss = [0.2828626334667206, 0.005280768498778343, 0.282334566116333]\n",
      "Step 1130, loss = [0.2989368736743927, 0.006954113487154245, 0.2982414662837982]\n",
      "Step 1131, loss = [0.29418689012527466, 0.004014396574348211, 0.2937854528427124]\n",
      "Step 1132, loss = [0.30673474073410034, 0.006586171220988035, 0.3060761094093323]\n",
      "Step 1133, loss = [0.31026536226272583, 0.011604556813836098, 0.30910491943359375]\n",
      "Step 1134, loss = [0.2958689332008362, 0.00832175463438034, 0.29503676295280457]\n",
      "Step 1135, loss = [0.3006035089492798, 0.006946813780814409, 0.2999088168144226]\n",
      "Step 1136, loss = [0.2824209928512573, 0.010228175669908524, 0.2813981771469116]\n",
      "Step 1137, loss = [0.2962137460708618, 0.0019449018873274326, 0.2960192561149597]\n",
      "Step 1138, loss = [0.29579269886016846, 0.00590840820223093, 0.2952018678188324]\n",
      "Step 1139, loss = [0.3013938367366791, 0.007016349118202925, 0.30069220066070557]\n",
      "Step 1140, loss = [0.2805047035217285, 0.004829892888665199, 0.2800217270851135]\n",
      "Step 1141, loss = [0.2835841774940491, 0.009285088628530502, 0.28265565633773804]\n",
      "Step 1142, loss = [0.30270007252693176, 0.011407973244786263, 0.3015592694282532]\n",
      "Step 1143, loss = [0.3111917972564697, 0.010620294138789177, 0.3101297616958618]\n",
      "Step 1144, loss = [0.30506372451782227, 0.008414804004132748, 0.3042222559452057]\n",
      "Step 1145, loss = [0.2924362123012543, 0.00818698201328516, 0.2916175127029419]\n",
      "Step 1146, loss = [0.3005712628364563, 0.007850044406950474, 0.2997862696647644]\n",
      "Step 1147, loss = [0.30801713466644287, 0.005513121373951435, 0.3074658215045929]\n",
      "Step 1148, loss = [0.27140703797340393, 0.006821255199611187, 0.27072492241859436]\n",
      "Step 1149, loss = [0.29385271668434143, 0.0027606207877397537, 0.2935766577720642]\n",
      "Step 1150, loss = [0.29660773277282715, 0.004842624068260193, 0.2961234748363495]\n",
      "Step 1151, loss = [0.3085286319255829, 0.006041203625500202, 0.3079245090484619]\n",
      "Step 1152, loss = [0.29222553968429565, 0.0032522587571293116, 0.29190030694007874]\n",
      "Step 1153, loss = [0.29424527287483215, 0.003269597887992859, 0.29391831159591675]\n",
      "Step 1154, loss = [0.28227099776268005, 0.005670486018061638, 0.2817039489746094]\n",
      "Step 1155, loss = [0.30954474210739136, 0.0046230582520365715, 0.3090824484825134]\n",
      "Step 1156, loss = [0.31482169032096863, 0.00966661237180233, 0.31385502219200134]\n",
      "Step 1157, loss = [0.2868718206882477, 0.009704227559268475, 0.28590139746665955]\n",
      "Step 1158, loss = [0.3120538294315338, 0.007915913127362728, 0.31126224994659424]\n",
      "Step 1159, loss = [0.30123209953308105, 0.007463362067937851, 0.300485759973526]\n",
      "Step 1160, loss = [0.2843324840068817, 0.00768490694463253, 0.2835640013217926]\n",
      "Step 1161, loss = [0.3102511465549469, 0.007295628078281879, 0.3095215857028961]\n",
      "Step 1162, loss = [0.29416555166244507, 0.00516120158135891, 0.29364943504333496]\n",
      "Step 1163, loss = [0.2865687608718872, 0.013686977326869965, 0.2852000594139099]\n",
      "Step 1164, loss = [0.30331024527549744, 0.006407254375517368, 0.3026695251464844]\n",
      "Step 1165, loss = [0.27110880613327026, 0.009873827919363976, 0.27012142539024353]\n",
      "Step 1166, loss = [0.30861905217170715, 0.00319890514947474, 0.30829915404319763]\n",
      "Step 1167, loss = [0.2924152612686157, 0.00602615624666214, 0.2918126583099365]\n",
      "Step 1168, loss = [0.3038535714149475, 0.004061298444867134, 0.3034474551677704]\n",
      "Step 1169, loss = [0.3021705150604248, 0.0050230203196406364, 0.3016682267189026]\n",
      "Step 1170, loss = [0.3000796139240265, 0.005201809108257294, 0.29955944418907166]\n",
      "Step 1171, loss = [0.312122106552124, 0.0067844390869140625, 0.31144365668296814]\n",
      "Step 1172, loss = [0.30209437012672424, 0.011532901786267757, 0.3009410798549652]\n",
      "Step 1173, loss = [0.2988632619380951, 0.010492179542779922, 0.2978140413761139]\n",
      "Step 1174, loss = [0.31100010871887207, 0.010292941704392433, 0.30997082591056824]\n",
      "Step 1175, loss = [0.2854112386703491, 0.0050929514691233635, 0.2849019467830658]\n",
      "Step 1176, loss = [0.30810728669166565, 0.006821226794272661, 0.3074251711368561]\n",
      "Step 1177, loss = [0.3025246858596802, 0.00760764442384243, 0.3017639219760895]\n",
      "Step 1178, loss = [0.29753315448760986, 0.005242242943495512, 0.2970089316368103]\n",
      "Step 1179, loss = [0.28417104482650757, 0.006069289520382881, 0.28356412053108215]\n",
      "Step 1180, loss = [0.3002217710018158, 0.008321957662701607, 0.2993895709514618]\n",
      "Step 1181, loss = [0.3060280680656433, 0.004568502306938171, 0.30557122826576233]\n",
      "Step 1182, loss = [0.2952977418899536, 0.007327455095946789, 0.29456499218940735]\n",
      "Step 1183, loss = [0.30398982763290405, 0.006422561127692461, 0.30334755778312683]\n",
      "Step 1184, loss = [0.30706602334976196, 0.006371635477989912, 0.30642884969711304]\n",
      "Step 1185, loss = [0.30126985907554626, 0.004815594293177128, 0.3007883131504059]\n",
      "Step 1186, loss = [0.29014086723327637, 0.008778449147939682, 0.2892630100250244]\n",
      "Step 1187, loss = [0.3170011043548584, 0.007572903297841549, 0.31624382734298706]\n",
      "Step 1188, loss = [0.29530373215675354, 0.00858355313539505, 0.29444536566734314]\n",
      "Step 1189, loss = [0.3081664741039276, 0.007896346971392632, 0.30737683176994324]\n",
      "Step 1190, loss = [0.2853347659111023, 0.007207359187304974, 0.28461402654647827]\n",
      "Step 1191, loss = [0.30041569471359253, 0.005203493870794773, 0.29989534616470337]\n",
      "Step 1192, loss = [0.3083208501338959, 0.009368267841637135, 0.30738401412963867]\n",
      "Step 1193, loss = [0.2908112406730652, 0.006821039132773876, 0.2901291251182556]\n",
      "Step 1194, loss = [0.3048017621040344, 0.0054702153429389, 0.3042547404766083]\n",
      "Step 1195, loss = [0.28235679864883423, 0.0070199789479374886, 0.28165480494499207]\n",
      "Step 1196, loss = [0.2931179106235504, 0.005952887702733278, 0.2925226092338562]\n",
      "Step 1197, loss = [0.31131136417388916, 0.009296455420553684, 0.3103817105293274]\n",
      "Step 1198, loss = [0.3088913559913635, 0.0025831309612840414, 0.308633029460907]\n",
      "Step 1199, loss = [0.307912677526474, 0.005917057860642672, 0.3073209822177887]\n",
      "Step 1200, loss = [0.3028293251991272, 0.007140359841287136, 0.3021152913570404]\n",
      "Step 1201, loss = [0.30501967668533325, 0.005150135140866041, 0.3045046627521515]\n",
      "Step 1202, loss = [0.31280046701431274, 0.010102121159434319, 0.31179025769233704]\n",
      "Step 1203, loss = [0.29691192507743835, 0.008639675565063953, 0.29604795575141907]\n",
      "Step 1204, loss = [0.30747973918914795, 0.006364383734762669, 0.3068433105945587]\n",
      "Step 1205, loss = [0.2939118444919586, 0.003968154080212116, 0.29351502656936646]\n",
      "Step 1206, loss = [0.29476815462112427, 0.00650717131793499, 0.29411745071411133]\n",
      "Step 1207, loss = [0.30090826749801636, 0.005809691734611988, 0.3003273010253906]\n",
      "Step 1208, loss = [0.3052190840244293, 0.002558543346822262, 0.30496323108673096]\n",
      "Step 1209, loss = [0.2973302900791168, 0.008564291521906853, 0.2964738607406616]\n",
      "Step 1210, loss = [0.3068024814128876, 0.007926631718873978, 0.30600982904434204]\n",
      "Step 1211, loss = [0.294310599565506, 0.005803575739264488, 0.293730229139328]\n",
      "Step 1212, loss = [0.29187431931495667, 0.004851469770073891, 0.2913891673088074]\n",
      "Step 1213, loss = [0.28221383690834045, 0.002568216994404793, 0.2819570004940033]\n",
      "Step 1214, loss = [0.3121930956840515, 0.008755791001021862, 0.311317503452301]\n",
      "Step 1215, loss = [0.3116527795791626, 0.010244728997349739, 0.3106282949447632]\n",
      "Step 1216, loss = [0.3094058036804199, 0.01287238672375679, 0.3081185519695282]\n",
      "Step 1217, loss = [0.2976423501968384, 0.008152139373123646, 0.29682713747024536]\n",
      "Step 1218, loss = [0.29961979389190674, 0.00699307257309556, 0.29892048239707947]\n",
      "Step 1219, loss = [0.3006434142589569, 0.0061081452295184135, 0.3000325858592987]\n",
      "Step 1220, loss = [0.3101782500743866, 0.0036657038144767284, 0.30981168150901794]\n",
      "Step 1221, loss = [0.30616235733032227, 0.005037207156419754, 0.30565863847732544]\n",
      "Step 1222, loss = [0.30476176738739014, 0.006652642972767353, 0.3040964901447296]\n",
      "Step 1223, loss = [0.299258291721344, 0.004577082581818104, 0.29880058765411377]\n",
      "Step 1224, loss = [0.3041338622570038, 0.0034109773114323616, 0.3037927746772766]\n",
      "Step 1225, loss = [0.3016079366207123, 0.004969351459294558, 0.30111101269721985]\n",
      "Step 1226, loss = [0.2886975407600403, 0.005372864194214344, 0.2881602644920349]\n",
      "Step 1227, loss = [0.2898150682449341, 0.006884757895022631, 0.28912660479545593]\n",
      "Step 1228, loss = [0.30899783968925476, 0.007842380553483963, 0.30821359157562256]\n",
      "Step 1229, loss = [0.2827112376689911, 0.008233718574047089, 0.28188785910606384]\n",
      "Step 1230, loss = [0.2966301739215851, 0.011076956987380981, 0.2955224812030792]\n",
      "Step 1231, loss = [0.3076359033584595, 0.004508737474679947, 0.30718502402305603]\n",
      "Step 1232, loss = [0.3089284598827362, 0.00886430311948061, 0.3080420196056366]\n",
      "Step 1233, loss = [0.3008963167667389, 0.004771098494529724, 0.3004192113876343]\n",
      "Step 1234, loss = [0.29832005500793457, 0.008486011065542698, 0.29747146368026733]\n",
      "Step 1235, loss = [0.2837260365486145, 0.009262029081583023, 0.2827998399734497]\n",
      "Step 1236, loss = [0.2992081046104431, 0.010318860411643982, 0.29817622900009155]\n",
      "Step 1237, loss = [0.30155906081199646, 0.008224043995141983, 0.300736665725708]\n",
      "Step 1238, loss = [0.29569175839424133, 0.004683738108724356, 0.2952233850955963]\n",
      "Step 1239, loss = [0.3054647743701935, 0.0085996612906456, 0.30460479855537415]\n",
      "Step 1240, loss = [0.30221104621887207, 0.007728387601673603, 0.30143821239471436]\n",
      "Step 1241, loss = [0.29083749651908875, 0.003404673421755433, 0.2904970347881317]\n",
      "Step 1242, loss = [0.2869875133037567, 0.006018596235662699, 0.2863856554031372]\n",
      "Step 1243, loss = [0.2955726683139801, 0.004009209107607603, 0.29517173767089844]\n",
      "Step 1244, loss = [0.2908559739589691, 0.004185413010418415, 0.2904374301433563]\n",
      "Step 1245, loss = [0.31063270568847656, 0.00798806268721819, 0.30983391404151917]\n",
      "Step 1246, loss = [0.3006886839866638, 0.009639965370297432, 0.2997246980667114]\n",
      "Step 1247, loss = [0.3044637143611908, 0.011180821806192398, 0.30334562063217163]\n",
      "Step 1248, loss = [0.2883968949317932, 0.004491367377340794, 0.28794774413108826]\n",
      "Step 1249, loss = [0.3030856251716614, 0.004078564699739218, 0.30267778038978577]\n",
      "Step 1250, loss = [0.305981308221817, 0.006586569361388683, 0.30532264709472656]\n",
      "Step 1251, loss = [0.30871886014938354, 0.0034115933813154697, 0.3083777129650116]\n",
      "Step 1252, loss = [0.29675978422164917, 0.0045159365981817245, 0.2963081896305084]\n",
      "Step 1253, loss = [0.2983575761318207, 0.0065012117847800255, 0.2977074682712555]\n",
      "Step 1254, loss = [0.2982785403728485, 0.00489326287060976, 0.29778921604156494]\n",
      "Step 1255, loss = [0.2981511056423187, 0.00447009177878499, 0.2977041006088257]\n",
      "Step 1256, loss = [0.3156774044036865, 0.0072258939035236835, 0.31495481729507446]\n",
      "Step 1257, loss = [0.3061962425708771, 0.006756517104804516, 0.30552059412002563]\n",
      "Step 1258, loss = [0.30915695428848267, 0.009299732744693756, 0.30822697281837463]\n",
      "Step 1259, loss = [0.28276365995407104, 0.00971787329763174, 0.2817918658256531]\n",
      "Step 1260, loss = [0.3126493990421295, 0.006575347855687141, 0.3119918704032898]\n",
      "Step 1261, loss = [0.29279062151908875, 0.008683914318680763, 0.2919222414493561]\n",
      "Step 1262, loss = [0.3039325475692749, 0.010008211247622967, 0.3029317259788513]\n",
      "Step 1263, loss = [0.3007432818412781, 0.004394104704260826, 0.3003038763999939]\n",
      "Step 1264, loss = [0.31593817472457886, 0.0061000436544418335, 0.3153281807899475]\n",
      "Step 1265, loss = [0.30103492736816406, 0.004395316820591688, 0.30059540271759033]\n",
      "Step 1266, loss = [0.2983832061290741, 0.0025825381744652987, 0.29812493920326233]\n",
      "Step 1267, loss = [0.29838910698890686, 0.009844768792390823, 0.29740461707115173]\n",
      "Step 1268, loss = [0.29802611470222473, 0.0077962251380085945, 0.2972464859485626]\n",
      "Step 1269, loss = [0.3123176395893097, 0.003331216052174568, 0.31198450922966003]\n",
      "Step 1270, loss = [0.30471083521842957, 0.01010969839990139, 0.3036998510360718]\n",
      "Step 1271, loss = [0.30197253823280334, 0.00893543753772974, 0.30107900500297546]\n",
      "Step 1272, loss = [0.31475627422332764, 0.008872004225850105, 0.31386905908584595]\n",
      "Step 1273, loss = [0.31382986903190613, 0.008905375376343727, 0.3129393458366394]\n",
      "Step 1274, loss = [0.29625204205513, 0.0062870848923921585, 0.2956233322620392]\n",
      "Step 1275, loss = [0.2895149886608124, 0.011930296197533607, 0.28832197189331055]\n",
      "Step 1276, loss = [0.30039432644844055, 0.00470391009002924, 0.29992392659187317]\n",
      "Step 1277, loss = [0.29441192746162415, 0.007837601006031036, 0.29362815618515015]\n",
      "Step 1278, loss = [0.2885688543319702, 0.008175832219421864, 0.2877512574195862]\n",
      "Step 1279, loss = [0.301371693611145, 0.0026914910413324833, 0.30110254883766174]\n",
      "Step 1280, loss = [0.3124741315841675, 0.007646453566849232, 0.3117094933986664]\n",
      "Step 1281, loss = [0.3035685420036316, 0.005235959310084581, 0.3030449450016022]\n",
      "Step 1282, loss = [0.30318138003349304, 0.007435143925249577, 0.3024378716945648]\n",
      "Step 1283, loss = [0.2856176197528839, 0.01235637255012989, 0.2843819856643677]\n",
      "Step 1284, loss = [0.2834685742855072, 0.007437076885253191, 0.28272485733032227]\n",
      "Step 1285, loss = [0.2913236916065216, 0.004733208101242781, 0.29085037112236023]\n",
      "Step 1286, loss = [0.2886565923690796, 0.006554116494953632, 0.2880011796951294]\n",
      "Step 1287, loss = [0.3027348816394806, 0.004420731216669083, 0.3022927939891815]\n",
      "Step 1288, loss = [0.2932497560977936, 0.007243411615490913, 0.29252541065216064]\n",
      "Step 1289, loss = [0.2990804612636566, 0.005234047770500183, 0.2985570430755615]\n",
      "Step 1290, loss = [0.3078925311565399, 0.006591996178030968, 0.3072333335876465]\n",
      "Step 1291, loss = [0.3010704517364502, 0.00930661242455244, 0.30013978481292725]\n",
      "Step 1292, loss = [0.30535897612571716, 0.0029782713390886784, 0.3050611615180969]\n",
      "Step 1293, loss = [0.3098786473274231, 0.006152640096843243, 0.3092633783817291]\n",
      "Step 1294, loss = [0.2929861545562744, 0.005089572165161371, 0.29247719049453735]\n",
      "Step 1295, loss = [0.30095961689949036, 0.0053067817352712154, 0.30042892694473267]\n",
      "Step 1296, loss = [0.2995113134384155, 0.008192863315343857, 0.2986920177936554]\n",
      "Step 1297, loss = [0.3109414279460907, 0.007723727263510227, 0.3101690411567688]\n",
      "Step 1298, loss = [0.30273333191871643, 0.0021053142845630646, 0.30252280831336975]\n",
      "Step 1299, loss = [0.31117552518844604, 0.008266128599643707, 0.31034889817237854]\n",
      "Step 1300, loss = [0.2989529073238373, 0.008001703768968582, 0.29815274477005005]\n",
      "Step 1301, loss = [0.3111759126186371, 0.005739718675613403, 0.31060194969177246]\n",
      "Step 1302, loss = [0.298480361700058, 0.0134591618552804, 0.2971344590187073]\n",
      "Step 1303, loss = [0.2970837652683258, 0.003830383997410536, 0.29670071601867676]\n",
      "Step 1304, loss = [0.3075719475746155, 0.00419717188924551, 0.30715224146842957]\n",
      "Step 1305, loss = [0.29664644598960876, 0.00834597460925579, 0.29581186175346375]\n",
      "Step 1306, loss = [0.30482199788093567, 0.008668809197843075, 0.3039551079273224]\n",
      "Step 1307, loss = [0.3128267824649811, 0.008669299073517323, 0.3119598627090454]\n",
      "Step 1308, loss = [0.3062770962715149, 0.00870622880756855, 0.30540648102760315]\n",
      "Step 1309, loss = [0.2983666658401489, 0.0038295574486255646, 0.29798370599746704]\n",
      "Step 1310, loss = [0.2960566580295563, 0.010900812223553658, 0.29496657848358154]\n",
      "Step 1311, loss = [0.29605281352996826, 0.0017203595489263535, 0.2958807647228241]\n",
      "Step 1312, loss = [0.30735906958580017, 0.007842564024031162, 0.30657482147216797]\n",
      "Step 1313, loss = [0.29789260029792786, 0.00861486978828907, 0.29703110456466675]\n",
      "Step 1314, loss = [0.294854074716568, 0.006822742521762848, 0.2941718101501465]\n",
      "Step 1315, loss = [0.3053736686706543, 0.003505646251142025, 0.30502310395240784]\n",
      "Step 1316, loss = [0.3105302155017853, 0.010399209335446358, 0.30949029326438904]\n",
      "Step 1317, loss = [0.29760491847991943, 0.010870169848203659, 0.29651790857315063]\n",
      "Step 1318, loss = [0.3096064627170563, 0.006098651327192783, 0.3089965879917145]\n",
      "Step 1319, loss = [0.29870516061782837, 0.007717609405517578, 0.2979333996772766]\n",
      "Step 1320, loss = [0.3027690649032593, 0.006677181459963322, 0.30210134387016296]\n",
      "Step 1321, loss = [0.30504631996154785, 0.009042050689458847, 0.3041421175003052]\n",
      "Step 1322, loss = [0.2885292172431946, 0.011166475713253021, 0.2874125838279724]\n",
      "Step 1323, loss = [0.30495938658714294, 0.0055046360939741135, 0.3044089376926422]\n",
      "Step 1324, loss = [0.29998308420181274, 0.006469388492405415, 0.29933613538742065]\n",
      "Step 1325, loss = [0.3150169551372528, 0.009377644397318363, 0.3140791952610016]\n",
      "Step 1326, loss = [0.3037274479866028, 0.005189618095755577, 0.30320850014686584]\n",
      "Step 1327, loss = [0.2891514003276825, 0.008850876241922379, 0.28826630115509033]\n",
      "Step 1328, loss = [0.285697340965271, 0.0021985559724271297, 0.28547748923301697]\n",
      "Step 1329, loss = [0.30049848556518555, 0.004772896878421307, 0.3000212013721466]\n",
      "Step 1330, loss = [0.311923086643219, 0.004106319509446621, 0.31151244044303894]\n",
      "Step 1331, loss = [0.28793227672576904, 0.0071020182222127914, 0.28722208738327026]\n",
      "Step 1332, loss = [0.2934027314186096, 0.0057256221771240234, 0.2928301692008972]\n",
      "Step 1333, loss = [0.3023841083049774, 0.007777085527777672, 0.3016063868999481]\n",
      "Step 1334, loss = [0.3027115762233734, 0.008835257962346077, 0.3018280565738678]\n",
      "Step 1335, loss = [0.2972830533981323, 0.0036962819285690784, 0.29691341519355774]\n",
      "Step 1336, loss = [0.29243722558021545, 0.004994439892470837, 0.29193776845932007]\n",
      "Step 1337, loss = [0.2901841104030609, 0.006500912364572287, 0.2895340323448181]\n",
      "Step 1338, loss = [0.3016755282878876, 0.0083434097468853, 0.30084118247032166]\n",
      "Step 1339, loss = [0.3097539246082306, 0.006794670596718788, 0.3090744614601135]\n",
      "Step 1340, loss = [0.30011945962905884, 0.006566363386809826, 0.29946282505989075]\n",
      "Step 1341, loss = [0.29309558868408203, 0.0060903895646333694, 0.2924865484237671]\n",
      "Step 1342, loss = [0.3129926919937134, 0.004034312441945076, 0.31258925795555115]\n",
      "Step 1343, loss = [0.27754732966423035, 0.007519194856286049, 0.2767954170703888]\n",
      "Step 1344, loss = [0.30086883902549744, 0.006329553201794624, 0.30023589730262756]\n",
      "Step 1345, loss = [0.30480635166168213, 0.007070107385516167, 0.30409935116767883]\n",
      "Step 1346, loss = [0.31358686089515686, 0.007813258096575737, 0.31280553340911865]\n",
      "Step 1347, loss = [0.31193807721138, 0.009279586374759674, 0.31101012229919434]\n",
      "Step 1348, loss = [0.31263506412506104, 0.008119594305753708, 0.3118230998516083]\n",
      "Step 1349, loss = [0.31216374039649963, 0.006674843840301037, 0.3114962577819824]\n",
      "Step 1350, loss = [0.28429973125457764, 0.003989507909864187, 0.28390076756477356]\n",
      "Step 1351, loss = [0.30340883135795593, 0.005994446575641632, 0.30280938744544983]\n",
      "Step 1352, loss = [0.29833757877349854, 0.00827687419950962, 0.2975098788738251]\n",
      "Step 1353, loss = [0.29453375935554504, 0.006446880288422108, 0.2938890755176544]\n",
      "Step 1354, loss = [0.3078572154045105, 0.004160326439887285, 0.30744117498397827]\n",
      "Step 1355, loss = [0.29645267128944397, 0.005461322143673897, 0.29590654373168945]\n",
      "Step 1356, loss = [0.2943105697631836, 0.008597128093242645, 0.29345086216926575]\n",
      "Step 1357, loss = [0.3062003552913666, 0.0096736503764987, 0.3052330017089844]\n",
      "Step 1358, loss = [0.29268231987953186, 0.0015991113614290953, 0.2925224006175995]\n",
      "Step 1359, loss = [0.2893444299697876, 0.007811164483428001, 0.2885633111000061]\n",
      "Step 1360, loss = [0.265295147895813, 0.005074243061244488, 0.2647877335548401]\n",
      "Step 1361, loss = [0.286824494600296, 0.00758602749556303, 0.2860659062862396]\n",
      "Step 1362, loss = [0.3047487139701843, 0.006786428391933441, 0.3040700852870941]\n",
      "Step 1363, loss = [0.3104768395423889, 0.012041959911584854, 0.3092726469039917]\n",
      "Step 1364, loss = [0.30043572187423706, 0.008818332105875015, 0.29955390095710754]\n",
      "Step 1365, loss = [0.3158015012741089, 0.009265553206205368, 0.31487494707107544]\n",
      "Step 1366, loss = [0.3064470887184143, 0.008757185190916061, 0.30557137727737427]\n",
      "Step 1367, loss = [0.2928321063518524, 0.005303698591887951, 0.292301744222641]\n",
      "Step 1368, loss = [0.30542534589767456, 0.006951451301574707, 0.30473020672798157]\n",
      "Step 1369, loss = [0.29517942667007446, 0.010167643427848816, 0.29416266083717346]\n",
      "Step 1370, loss = [0.3037113547325134, 0.006703031249344349, 0.3030410408973694]\n",
      "Step 1371, loss = [0.30539003014564514, 0.008655027486383915, 0.3045245409011841]\n",
      "Step 1372, loss = [0.30953365564346313, 0.007999869994819164, 0.30873367190361023]\n",
      "Step 1373, loss = [0.3097202777862549, 0.007396904285997152, 0.3089805841445923]\n",
      "Step 1374, loss = [0.307399719953537, 0.0090004438534379, 0.3064996898174286]\n",
      "Step 1375, loss = [0.2873633801937103, 0.009117918089032173, 0.2864515781402588]\n",
      "Step 1376, loss = [0.3106987476348877, 0.012662571854889393, 0.3094324767589569]\n",
      "Step 1377, loss = [0.3066498339176178, 0.0034064864739775658, 0.30630919337272644]\n",
      "Step 1378, loss = [0.3091088533401489, 0.007868925109505653, 0.3083219528198242]\n",
      "Step 1379, loss = [0.30971020460128784, 0.004985978826880455, 0.3092116117477417]\n",
      "Step 1380, loss = [0.31080853939056396, 0.002477675210684538, 0.31056076288223267]\n",
      "Step 1381, loss = [0.30814966559410095, 0.0033362535759806633, 0.3078160285949707]\n",
      "Step 1382, loss = [0.2969714403152466, 0.006710066460072994, 0.2963004410266876]\n",
      "Step 1383, loss = [0.28594937920570374, 0.007288428023457527, 0.28522053360939026]\n",
      "Step 1384, loss = [0.2989557385444641, 0.006186056416481733, 0.2983371317386627]\n",
      "Step 1385, loss = [0.29791682958602905, 0.00998922623693943, 0.2969179153442383]\n",
      "Step 1386, loss = [0.30113863945007324, 0.004729279316961765, 0.3006657063961029]\n",
      "Step 1387, loss = [0.29733240604400635, 0.003437163308262825, 0.29698869585990906]\n",
      "Step 1388, loss = [0.29747098684310913, 0.005874369293451309, 0.29688355326652527]\n",
      "Step 1389, loss = [0.30107054114341736, 0.00680801086127758, 0.30038973689079285]\n",
      "Step 1390, loss = [0.30392372608184814, 0.004061507526785135, 0.30351758003234863]\n",
      "Step 1391, loss = [0.28457725048065186, 0.003452681005001068, 0.2842319905757904]\n",
      "Step 1392, loss = [0.3062351644039154, 0.002206617034971714, 0.3060145080089569]\n",
      "Step 1393, loss = [0.31255173683166504, 0.009375328198075294, 0.3116142153739929]\n",
      "Step 1394, loss = [0.29227352142333984, 0.001826377701945603, 0.29209089279174805]\n",
      "Step 1395, loss = [0.28829434514045715, 0.00970417633652687, 0.287323921918869]\n",
      "Step 1396, loss = [0.3113465905189514, 0.011116577312350273, 0.310234934091568]\n",
      "Step 1397, loss = [0.2977714538574219, 0.009977227076888084, 0.2967737317085266]\n",
      "Step 1398, loss = [0.30177658796310425, 0.006291439291089773, 0.3011474311351776]\n",
      "Step 1399, loss = [0.2972664535045624, 0.006205027922987938, 0.2966459393501282]\n",
      "Step 1400, loss = [0.31441715359687805, 0.01105473656207323, 0.3133116662502289]\n",
      "Step 1401, loss = [0.2899169921875, 0.008381631225347519, 0.28907883167266846]\n",
      "Step 1402, loss = [0.29687392711639404, 0.0095291119068861, 0.2959210276603699]\n",
      "Step 1403, loss = [0.30870330333709717, 0.0036778603680431843, 0.3083355128765106]\n",
      "Step 1404, loss = [0.2831823527812958, 0.016917459666728973, 0.28149059414863586]\n",
      "Step 1405, loss = [0.30912742018699646, 0.006458071060478687, 0.3084816038608551]\n",
      "Step 1406, loss = [0.2958921492099762, 0.0046947067603468895, 0.29542267322540283]\n",
      "Step 1407, loss = [0.30035120248794556, 0.005669895559549332, 0.29978421330451965]\n",
      "Step 1408, loss = [0.30150091648101807, 0.004718507174402475, 0.3010290563106537]\n",
      "Step 1409, loss = [0.2867324948310852, 0.004243551287800074, 0.2863081395626068]\n",
      "Step 1410, loss = [0.3124755918979645, 0.008133040741086006, 0.3116622865200043]\n",
      "Step 1411, loss = [0.2940680682659149, 0.00547470897436142, 0.29352059960365295]\n",
      "Step 1412, loss = [0.29935240745544434, 0.010991353541612625, 0.29825326800346375]\n",
      "Step 1413, loss = [0.28691598773002625, 0.008306341245770454, 0.2860853672027588]\n",
      "Step 1414, loss = [0.2985573410987854, 0.012767293490469456, 0.29728060960769653]\n",
      "Step 1415, loss = [0.30992475152015686, 0.007417830638587475, 0.3091829717159271]\n",
      "Step 1416, loss = [0.30797916650772095, 0.00531766377389431, 0.3074474036693573]\n",
      "Step 1417, loss = [0.29662808775901794, 0.007788533344864845, 0.2958492338657379]\n",
      "Step 1418, loss = [0.3150685727596283, 0.006523023825138807, 0.3144162595272064]\n",
      "Step 1419, loss = [0.2967725694179535, 0.012329310178756714, 0.29553964734077454]\n",
      "Step 1420, loss = [0.3032306432723999, 0.008030366152524948, 0.30242761969566345]\n",
      "Step 1421, loss = [0.3005991578102112, 0.004788398742675781, 0.3001203238964081]\n",
      "Step 1422, loss = [0.29897788166999817, 0.006063906475901604, 0.29837149381637573]\n",
      "Step 1423, loss = [0.30295056104660034, 0.006656428799033165, 0.30228492617607117]\n",
      "Step 1424, loss = [0.2961576581001282, 0.010975958779454231, 0.29506006836891174]\n",
      "Step 1425, loss = [0.28057265281677246, 0.009007303044199944, 0.27967193722724915]\n",
      "Step 1426, loss = [0.3007064759731293, 0.007301374338567257, 0.2999763488769531]\n",
      "Step 1427, loss = [0.3040164113044739, 0.008262439630925655, 0.3031901717185974]\n",
      "Step 1428, loss = [0.29921507835388184, 0.005981592461466789, 0.2986169159412384]\n",
      "Step 1429, loss = [0.30983054637908936, 0.007997546344995499, 0.30903080105781555]\n",
      "Step 1430, loss = [0.28112533688545227, 0.009076030924916267, 0.2802177369594574]\n",
      "Step 1431, loss = [0.2855472266674042, 0.006218389142304659, 0.2849254012107849]\n",
      "Step 1432, loss = [0.2910601794719696, 0.006937991827726364, 0.29036638140678406]\n",
      "Step 1433, loss = [0.3009151220321655, 0.009045257233083248, 0.3000105917453766]\n",
      "Step 1434, loss = [0.3005295395851135, 0.011478503234684467, 0.29938170313835144]\n",
      "Step 1435, loss = [0.3044463098049164, 0.004356969613581896, 0.3040105998516083]\n",
      "Step 1436, loss = [0.30459079146385193, 0.006719077937304974, 0.30391889810562134]\n",
      "Step 1437, loss = [0.31396347284317017, 0.00813475251197815, 0.31314998865127563]\n",
      "Step 1438, loss = [0.2900608479976654, 0.008114052936434746, 0.289249449968338]\n",
      "Step 1439, loss = [0.2958931624889374, 0.004230237565934658, 0.29547014832496643]\n",
      "Step 1440, loss = [0.3156357705593109, 0.006319883279502392, 0.31500378251075745]\n",
      "Step 1441, loss = [0.30991360545158386, 0.009800112806260586, 0.3089335858821869]\n",
      "Step 1442, loss = [0.29814401268959045, 0.005032776854932308, 0.29764074087142944]\n",
      "Step 1443, loss = [0.30741631984710693, 0.0076568638905882835, 0.30665063858032227]\n",
      "Step 1444, loss = [0.2999001741409302, 0.006137114018201828, 0.29928645491600037]\n",
      "Step 1445, loss = [0.2985662519931793, 0.0072822654619812965, 0.297838032245636]\n",
      "Step 1446, loss = [0.3007746636867523, 0.009193278849124908, 0.2998553216457367]\n",
      "Step 1447, loss = [0.3098345398902893, 0.01000523567199707, 0.3088340163230896]\n",
      "Step 1448, loss = [0.29846033453941345, 0.004501611925661564, 0.2980101704597473]\n",
      "Step 1449, loss = [0.2977398931980133, 0.0059640295803546906, 0.29714348912239075]\n",
      "Step 1450, loss = [0.3029577434062958, 0.004648634232580662, 0.3024928867816925]\n",
      "Step 1451, loss = [0.29725968837738037, 0.006980699487030506, 0.2965616285800934]\n",
      "Step 1452, loss = [0.2860985994338989, 0.0074433377012610435, 0.28535425662994385]\n",
      "Step 1453, loss = [0.28655120730400085, 0.004801126662641764, 0.2860710918903351]\n",
      "Step 1454, loss = [0.28832554817199707, 0.008314685896039009, 0.28749409317970276]\n",
      "Step 1455, loss = [0.3081513047218323, 0.011864308267831802, 0.3069648742675781]\n",
      "Step 1456, loss = [0.30739712715148926, 0.004223704803735018, 0.30697476863861084]\n",
      "Step 1457, loss = [0.30896613001823425, 0.007097958587110043, 0.3082563281059265]\n",
      "Step 1458, loss = [0.2966054081916809, 0.006570848170667887, 0.295948326587677]\n",
      "Step 1459, loss = [0.30030009150505066, 0.01039794646203518, 0.29926028847694397]\n",
      "Step 1460, loss = [0.29677721858024597, 0.00996667891740799, 0.2957805395126343]\n",
      "Step 1461, loss = [0.30629751086235046, 0.006312445737421513, 0.3056662678718567]\n",
      "Step 1462, loss = [0.29481175541877747, 0.00654921680688858, 0.2941568195819855]\n",
      "Step 1463, loss = [0.299753338098526, 0.007820626720786095, 0.2989712655544281]\n",
      "Step 1464, loss = [0.2983955144882202, 0.010387799702584743, 0.2973567247390747]\n",
      "Step 1465, loss = [0.2906942665576935, 0.005539604462683201, 0.290140300989151]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11754 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1466, loss = [0.31463170051574707, 0.008268252946436405, 0.31380486488342285]\n",
      "Update target distribution epoch 2 step 1467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11754/11754 [1:51:26<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos. rate:0.42549019607843136 Neg. rate:0.5745098039215686\n",
      "delta_label  0.0045091032839884295\n",
      "Step 1467, loss = [0.2961820065975189, 0.01001199334859848, 0.2951807975769043]\n",
      "Step 1468, loss = [0.3052713871002197, 0.009886781685054302, 0.30428269505500793]\n",
      "Step 1469, loss = [0.29353541135787964, 0.012339504435658455, 0.2923014461994171]\n",
      "Epoch 2, loss = [0.29962538 0.0069816  0.29892722]\n",
      "\n",
      "Start of epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11754 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update target distribution epoch 3 step 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11754/11754 [1:51:34<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos. rate:0.42745098039215684 Neg. rate:0.5725490196078431\n",
      "delta_label  0.0016164709885996256\n",
      "Step 0, loss = [0.29211124777793884, 0.004909938666969538, 0.29162025451660156]\n",
      "Step 1, loss = [0.3013148307800293, 0.007271354552358389, 0.3005876839160919]\n",
      "Step 2, loss = [0.30048027634620667, 0.0060056052170693874, 0.2998797297477722]\n",
      "Step 3, loss = [0.3004480302333832, 0.009164947085082531, 0.2995315492153168]\n",
      "Step 4, loss = [0.3122847080230713, 0.006478632800281048, 0.3116368353366852]\n",
      "Step 5, loss = [0.3013949692249298, 0.011627193540334702, 0.30023226141929626]\n",
      "Step 6, loss = [0.29012617468833923, 0.008813679218292236, 0.28924480080604553]\n",
      "Step 7, loss = [0.3018266558647156, 0.0065484559163451195, 0.30117180943489075]\n",
      "Step 8, loss = [0.289486825466156, 0.004722311161458492, 0.289014607667923]\n",
      "Step 9, loss = [0.3040945529937744, 0.0071746292524039745, 0.30337709188461304]\n",
      "Step 10, loss = [0.29733750224113464, 0.004898848477751017, 0.2968476116657257]\n",
      "Step 11, loss = [0.3055398166179657, 0.003712242003530264, 0.30516859889030457]\n",
      "Step 12, loss = [0.30479395389556885, 0.004244515672326088, 0.3043695092201233]\n",
      "Step 13, loss = [0.2919214367866516, 0.008708921261131763, 0.2910505533218384]\n",
      "Step 14, loss = [0.3009813129901886, 0.009784262627363205, 0.3000028729438782]\n",
      "Step 15, loss = [0.3113204538822174, 0.0028990949504077435, 0.3110305368900299]\n",
      "Step 16, loss = [0.29479995369911194, 0.009123834781348705, 0.29388755559921265]\n",
      "Step 17, loss = [0.2971882224082947, 0.007798078004270792, 0.29640841484069824]\n",
      "Step 18, loss = [0.30947116017341614, 0.004524423740804195, 0.30901873111724854]\n",
      "Step 19, loss = [0.3128652274608612, 0.008777163922786713, 0.3119875192642212]\n",
      "Step 20, loss = [0.29369449615478516, 0.005865008570253849, 0.2931079864501953]\n",
      "Step 21, loss = [0.300568550825119, 0.0033245214726775885, 0.3002361059188843]\n",
      "Step 22, loss = [0.3052103817462921, 0.0064431228674948215, 0.30456605553627014]\n",
      "Step 23, loss = [0.31018027663230896, 0.0042095547541975975, 0.30975931882858276]\n",
      "Step 24, loss = [0.2947569787502289, 0.009278394281864166, 0.29382914304733276]\n",
      "Step 25, loss = [0.29106882214546204, 0.004334575496613979, 0.2906353771686554]\n",
      "Step 26, loss = [0.29971811175346375, 0.005851794965565205, 0.29913294315338135]\n",
      "Step 27, loss = [0.31058037281036377, 0.008700806647539139, 0.309710294008255]\n",
      "Step 28, loss = [0.3027626872062683, 0.013129213824868202, 0.3014497756958008]\n",
      "Step 29, loss = [0.2995924651622772, 0.006582220084965229, 0.2989342510700226]\n",
      "Step 30, loss = [0.3018609285354614, 0.0022027408704161644, 0.30164065957069397]\n",
      "Step 31, loss = [0.2915951609611511, 0.010606454685330391, 0.29053452610969543]\n",
      "Step 32, loss = [0.30231696367263794, 0.006250445265322924, 0.3016919195652008]\n",
      "Step 33, loss = [0.30141499638557434, 0.005812468938529491, 0.3008337616920471]\n",
      "Step 34, loss = [0.30155521631240845, 0.010725719854235649, 0.3004826307296753]\n",
      "Step 35, loss = [0.30217498540878296, 0.0049205804243683815, 0.3016829192638397]\n",
      "Step 36, loss = [0.29046109318733215, 0.007804117165505886, 0.28968068957328796]\n",
      "Step 37, loss = [0.30098268389701843, 0.008144174702465534, 0.3001682758331299]\n",
      "Step 38, loss = [0.31142184138298035, 0.011355690658092499, 0.310286283493042]\n",
      "Step 39, loss = [0.30253279209136963, 0.008126405999064445, 0.30172014236450195]\n",
      "Step 40, loss = [0.3078746199607849, 0.005368241108953953, 0.30733779072761536]\n",
      "Step 41, loss = [0.30145028233528137, 0.006369417998939753, 0.30081334710121155]\n",
      "Step 42, loss = [0.3101446032524109, 0.004610355943441391, 0.30968356132507324]\n",
      "Step 43, loss = [0.3113781213760376, 0.003142816014587879, 0.31106382608413696]\n",
      "Step 44, loss = [0.3061392307281494, 0.009694267064332962, 0.3051697909832001]\n",
      "Step 45, loss = [0.3072829246520996, 0.005843201652169228, 0.30669859051704407]\n",
      "Step 46, loss = [0.2841549217700958, 0.008629118092358112, 0.2832919955253601]\n",
      "Step 47, loss = [0.30838969349861145, 0.007575587369501591, 0.3076321482658386]\n",
      "Step 48, loss = [0.3011018931865692, 0.004478303715586662, 0.3006540536880493]\n",
      "Step 49, loss = [0.3171501159667969, 0.00928592961281538, 0.31622153520584106]\n",
      "Step 50, loss = [0.3053489625453949, 0.014171830378472805, 0.3039317727088928]\n",
      "Step 51, loss = [0.31042125821113586, 0.0019724357407540083, 0.3102240264415741]\n",
      "Step 52, loss = [0.30152627825737, 0.007601327262818813, 0.30076614022254944]\n",
      "Step 53, loss = [0.30450722575187683, 0.009954682551324368, 0.30351176857948303]\n",
      "Step 54, loss = [0.310432493686676, 0.008140159770846367, 0.3096184730529785]\n",
      "Step 55, loss = [0.2973545789718628, 0.0060294815339148045, 0.29675161838531494]\n",
      "Step 56, loss = [0.2973012626171112, 0.00907672569155693, 0.29639360308647156]\n",
      "Step 57, loss = [0.28867438435554504, 0.011163877323269844, 0.287557989358902]\n",
      "Step 58, loss = [0.2921155095100403, 0.007245050743222237, 0.2913910150527954]\n",
      "Step 59, loss = [0.2983246445655823, 0.004402329679578543, 0.29788440465927124]\n",
      "Step 60, loss = [0.2993611991405487, 0.01083786878734827, 0.2982774078845978]\n",
      "Step 61, loss = [0.30635786056518555, 0.0074813151732087135, 0.30560973286628723]\n",
      "Step 62, loss = [0.29956087470054626, 0.006864548660814762, 0.2988744080066681]\n",
      "Step 63, loss = [0.2969145178794861, 0.008068704977631569, 0.2961076498031616]\n",
      "Step 64, loss = [0.30096960067749023, 0.005774159915745258, 0.30039218068122864]\n",
      "Step 65, loss = [0.30176421999931335, 0.009541647508740425, 0.3008100688457489]\n",
      "Step 66, loss = [0.3017463684082031, 0.007648021448403597, 0.3009815514087677]\n",
      "Step 67, loss = [0.31277695298194885, 0.006073353812098503, 0.31216961145401]\n",
      "Step 68, loss = [0.2714487314224243, 0.006873166188597679, 0.2707614004611969]\n",
      "Step 69, loss = [0.3179418742656708, 0.008950663730502129, 0.3170468211174011]\n",
      "Step 70, loss = [0.2987249791622162, 0.005547866225242615, 0.29817017912864685]\n",
      "Step 71, loss = [0.28953468799591064, 0.004165905527770519, 0.28911811113357544]\n",
      "Step 72, loss = [0.29128298163414, 0.004860395565629005, 0.2907969355583191]\n",
      "Step 73, loss = [0.30561578273773193, 0.008022228255867958, 0.30481356382369995]\n",
      "Step 74, loss = [0.30397582054138184, 0.009357985109090805, 0.3030400276184082]\n",
      "Step 75, loss = [0.2859448492527008, 0.004812914878129959, 0.2854635715484619]\n",
      "Step 76, loss = [0.30056363344192505, 0.009287163615226746, 0.2996349036693573]\n",
      "Step 77, loss = [0.29493018984794617, 0.00753676425665617, 0.29417651891708374]\n",
      "Step 78, loss = [0.30657756328582764, 0.0036826517898589373, 0.3062092959880829]\n",
      "Step 79, loss = [0.3085916042327881, 0.009547589346766472, 0.3076368570327759]\n",
      "Step 80, loss = [0.29489701986312866, 0.008898936212062836, 0.2940071225166321]\n",
      "Step 81, loss = [0.31215327978134155, 0.004700337536633015, 0.3116832375526428]\n",
      "Step 82, loss = [0.29621392488479614, 0.007355124689638615, 0.2954784035682678]\n",
      "Step 83, loss = [0.28921404480934143, 0.012833116576075554, 0.2879307270050049]\n",
      "Step 84, loss = [0.31283852458000183, 0.006952575873583555, 0.3121432662010193]\n",
      "Step 85, loss = [0.298095703125, 0.007614500354975462, 0.2973342537879944]\n",
      "Step 86, loss = [0.3129490315914154, 0.009687207639217377, 0.31198030710220337]\n",
      "Step 87, loss = [0.30408555269241333, 0.009951738640666008, 0.3030903935432434]\n",
      "Step 88, loss = [0.306161105632782, 0.013797003775835037, 0.30478140711784363]\n",
      "Step 89, loss = [0.3008045554161072, 0.004830699879676104, 0.300321489572525]\n",
      "Step 90, loss = [0.2993488907814026, 0.0101925078779459, 0.2983296513557434]\n",
      "Step 91, loss = [0.2768709063529968, 0.0031294389627873898, 0.27655795216560364]\n",
      "Step 92, loss = [0.3068356513977051, 0.0072485459968447685, 0.30611079931259155]\n",
      "Step 93, loss = [0.302986741065979, 0.004734051413834095, 0.30251333117485046]\n",
      "Step 94, loss = [0.28890475630760193, 0.007356228306889534, 0.28816914558410645]\n",
      "Step 95, loss = [0.29207339882850647, 0.00740707665681839, 0.2913326919078827]\n",
      "Step 96, loss = [0.2952040433883667, 0.009777508676052094, 0.2942262887954712]\n",
      "Step 97, loss = [0.2979930639266968, 0.003714225022122264, 0.29762163758277893]\n",
      "Step 98, loss = [0.3148646950721741, 0.0047993287444114685, 0.3143847584724426]\n",
      "Step 99, loss = [0.30616137385368347, 0.012222791090607643, 0.3049390912055969]\n",
      "Step 100, loss = [0.3063085377216339, 0.005017805844545364, 0.3058067560195923]\n",
      "Step 101, loss = [0.3016727864742279, 0.006220506504178047, 0.30105072259902954]\n",
      "Step 102, loss = [0.2939136326313019, 0.0060912142507731915, 0.2933045029640198]\n",
      "Step 103, loss = [0.2946513891220093, 0.006777429953217506, 0.2939736545085907]\n",
      "Step 104, loss = [0.31130433082580566, 0.00707617774605751, 0.3105967044830322]\n",
      "Step 105, loss = [0.2970350384712219, 0.008244197815656662, 0.2962106168270111]\n",
      "Step 106, loss = [0.2855660915374756, 0.007837526500225067, 0.284782350063324]\n",
      "Step 107, loss = [0.29811570048332214, 0.005263131111860275, 0.29758939146995544]\n",
      "Step 108, loss = [0.31470659375190735, 0.007346341386437416, 0.31397196650505066]\n",
      "Step 109, loss = [0.3158667981624603, 0.009832506999373436, 0.3148835599422455]\n",
      "Step 110, loss = [0.29444143176078796, 0.0030077032279223204, 0.29414066672325134]\n",
      "Step 111, loss = [0.3074859082698822, 0.007189201656728983, 0.30676698684692383]\n",
      "Step 112, loss = [0.3089311420917511, 0.008740700781345367, 0.3080570697784424]\n",
      "Step 113, loss = [0.2943035066127777, 0.007241223938763142, 0.2935793697834015]\n",
      "Step 114, loss = [0.29434454441070557, 0.006048210896551609, 0.2937397360801697]\n",
      "Step 115, loss = [0.3044635057449341, 0.0032426929101347923, 0.30413922667503357]\n",
      "Step 116, loss = [0.29012608528137207, 0.00726877897977829, 0.2893992066383362]\n",
      "Step 117, loss = [0.28939998149871826, 0.004481303505599499, 0.2889518439769745]\n",
      "Step 118, loss = [0.28666266798973083, 0.009602328762412071, 0.2857024371623993]\n",
      "Step 119, loss = [0.307515949010849, 0.010097039863467216, 0.3065062463283539]\n",
      "Step 120, loss = [0.3037893772125244, 0.0032924883998930454, 0.30346012115478516]\n",
      "Step 121, loss = [0.30915117263793945, 0.008641036227345467, 0.3082870543003082]\n",
      "Step 122, loss = [0.2966740131378174, 0.007367962505668402, 0.2959372103214264]\n",
      "Step 123, loss = [0.29001447558403015, 0.0076051210053265095, 0.28925395011901855]\n",
      "Step 124, loss = [0.30784541368484497, 0.006058730650693178, 0.3072395324707031]\n",
      "Step 125, loss = [0.3027331531047821, 0.00856953114271164, 0.3018761873245239]\n",
      "Step 126, loss = [0.29021790623664856, 0.007472208701074123, 0.2894706726074219]\n",
      "Step 127, loss = [0.3005222678184509, 0.008462140336632729, 0.2996760606765747]\n",
      "Step 128, loss = [0.3107541799545288, 0.0076989345252513885, 0.3099842965602875]\n",
      "Step 129, loss = [0.30762970447540283, 0.004959108307957649, 0.3071337938308716]\n",
      "Step 130, loss = [0.30140507221221924, 0.005038915667682886, 0.3009011745452881]\n",
      "Step 131, loss = [0.30790087580680847, 0.004842957481741905, 0.3074165880680084]\n",
      "Step 132, loss = [0.30392810702323914, 0.008144401013851166, 0.3031136691570282]\n",
      "Step 133, loss = [0.29068291187286377, 0.0017752943094819784, 0.29050537943840027]\n",
      "Step 134, loss = [0.3032224774360657, 0.01257521566003561, 0.30196496844291687]\n",
      "Step 135, loss = [0.2856512665748596, 0.0053309183567762375, 0.2851181626319885]\n",
      "Step 136, loss = [0.3065749704837799, 0.009165013208985329, 0.3056584596633911]\n",
      "Step 137, loss = [0.309794157743454, 0.0066263373009860516, 0.30913153290748596]\n",
      "Step 138, loss = [0.29265791177749634, 0.008391045033931732, 0.2918187975883484]\n",
      "Step 139, loss = [0.30924922227859497, 0.009091136045753956, 0.3083401024341583]\n",
      "Step 140, loss = [0.3080517649650574, 0.005255377851426601, 0.30752623081207275]\n",
      "Step 141, loss = [0.294321209192276, 0.007924918085336685, 0.2935287058353424]\n",
      "Step 142, loss = [0.300639808177948, 0.012216568924486637, 0.2994181513786316]\n",
      "Step 143, loss = [0.29756593704223633, 0.0060630436055362225, 0.29695963859558105]\n",
      "Step 144, loss = [0.3068080544471741, 0.005075626075267792, 0.30630049109458923]\n",
      "Step 145, loss = [0.29755041003227234, 0.006464181933552027, 0.2969039976596832]\n",
      "Step 146, loss = [0.28610506653785706, 0.009127708151936531, 0.2851922810077667]\n",
      "Step 147, loss = [0.3034272789955139, 0.010752619244158268, 0.30235201120376587]\n",
      "Step 148, loss = [0.30788934230804443, 0.00462281983345747, 0.3074270486831665]\n",
      "Step 149, loss = [0.2820441722869873, 0.004083119332790375, 0.2816358506679535]\n",
      "Step 150, loss = [0.3014911413192749, 0.008324974216520786, 0.300658643245697]\n",
      "Step 151, loss = [0.30505287647247314, 0.008987398818135262, 0.3041541278362274]\n",
      "Step 152, loss = [0.2918333113193512, 0.009795479476451874, 0.2908537685871124]\n",
      "Step 153, loss = [0.29304149746894836, 0.004986791871488094, 0.29254281520843506]\n",
      "Step 154, loss = [0.27716562151908875, 0.006118063814938068, 0.27655380964279175]\n",
      "Step 155, loss = [0.2850000262260437, 0.0031401703599840403, 0.28468599915504456]\n",
      "Step 156, loss = [0.31331363320350647, 0.008837124332785606, 0.31242993474006653]\n",
      "Step 157, loss = [0.30197077989578247, 0.0077577498741447926, 0.3011949956417084]\n",
      "Step 158, loss = [0.295419305562973, 0.0062524620443582535, 0.2947940528392792]\n",
      "Step 159, loss = [0.30116331577301025, 0.008292670361697674, 0.30033403635025024]\n",
      "Step 160, loss = [0.304846853017807, 0.007618077099323273, 0.30408504605293274]\n",
      "Step 161, loss = [0.2901536822319031, 0.00323684373870492, 0.2898299992084503]\n",
      "Step 162, loss = [0.2872495651245117, 0.00667926948517561, 0.2865816354751587]\n",
      "Step 163, loss = [0.29371169209480286, 0.009168131276965141, 0.2927948832511902]\n",
      "Step 164, loss = [0.3022436499595642, 0.010175663977861404, 0.30122607946395874]\n",
      "Step 165, loss = [0.301228791475296, 0.012889320030808449, 0.2999398708343506]\n",
      "Step 166, loss = [0.3014921545982361, 0.008891751989722252, 0.3006029725074768]\n",
      "Step 167, loss = [0.30260688066482544, 0.007713294588029385, 0.3018355369567871]\n",
      "Step 168, loss = [0.3057246208190918, 0.008062152191996574, 0.30491840839385986]\n",
      "Step 169, loss = [0.29857349395751953, 0.007238536141812801, 0.2978496551513672]\n",
      "Step 170, loss = [0.285330593585968, 0.006544678006321192, 0.28467613458633423]\n",
      "Step 171, loss = [0.2884005010128021, 0.00393763929605484, 0.2880067229270935]\n",
      "Step 172, loss = [0.29924246668815613, 0.01039749477058649, 0.2982027232646942]\n",
      "Step 173, loss = [0.31056615710258484, 0.011038154363632202, 0.3094623386859894]\n",
      "Step 174, loss = [0.29966306686401367, 0.005603286437690258, 0.2991027235984802]\n",
      "Step 175, loss = [0.3043439984321594, 0.0029070316813886166, 0.30405330657958984]\n",
      "Step 176, loss = [0.3009561002254486, 0.005120595917105675, 0.3004440367221832]\n",
      "Step 177, loss = [0.3025658130645752, 0.005379850044846535, 0.3020278215408325]\n",
      "Step 178, loss = [0.2861665189266205, 0.006646656431257725, 0.2855018675327301]\n",
      "Step 179, loss = [0.29771852493286133, 0.007605396211147308, 0.29695799946784973]\n",
      "Step 180, loss = [0.29691267013549805, 0.0040575675666332245, 0.2965069115161896]\n",
      "Step 181, loss = [0.3074069023132324, 0.005150276236236095, 0.30689188838005066]\n",
      "Step 182, loss = [0.2883549928665161, 0.00436970591545105, 0.2879180312156677]\n",
      "Step 183, loss = [0.30841198563575745, 0.006182909943163395, 0.3077937066555023]\n",
      "Step 184, loss = [0.30489885807037354, 0.0105243856087327, 0.30384641885757446]\n",
      "Step 185, loss = [0.2990729510784149, 0.005154050886631012, 0.2985575497150421]\n",
      "Step 186, loss = [0.2919970154762268, 0.0023433943279087543, 0.29176267981529236]\n",
      "Step 187, loss = [0.30207905173301697, 0.00667379517108202, 0.3014116585254669]\n",
      "Step 188, loss = [0.2983854413032532, 0.006134632974863052, 0.29777199029922485]\n",
      "Step 189, loss = [0.285900354385376, 0.006311782170087099, 0.285269170999527]\n",
      "Step 190, loss = [0.310650497674942, 0.006917676422744989, 0.30995872616767883]\n",
      "Step 191, loss = [0.3092988133430481, 0.008492350578308105, 0.30844956636428833]\n",
      "Step 192, loss = [0.30934834480285645, 0.011273443698883057, 0.3082210123538971]\n",
      "Step 193, loss = [0.2875402271747589, 0.0034652764443308115, 0.2871936857700348]\n",
      "Step 194, loss = [0.2940506935119629, 0.004626158624887466, 0.2935880720615387]\n",
      "Step 195, loss = [0.3020760118961334, 0.009569894522428513, 0.30111902952194214]\n",
      "Step 196, loss = [0.28192412853240967, 0.006294489838182926, 0.28129467368125916]\n",
      "Step 197, loss = [0.2980463206768036, 0.005723769776523113, 0.2974739372730255]\n",
      "Step 198, loss = [0.29388126730918884, 0.003994364757090807, 0.29348182678222656]\n",
      "Step 199, loss = [0.3038921654224396, 0.008738425560295582, 0.30301833152770996]\n",
      "Step 200, loss = [0.3053506016731262, 0.010217510163784027, 0.3043288588523865]\n",
      "Step 201, loss = [0.3044488728046417, 0.007194799371063709, 0.303729385137558]\n",
      "Step 202, loss = [0.30324476957321167, 0.00598837761208415, 0.3026459217071533]\n",
      "Step 203, loss = [0.29790130257606506, 0.007654884364455938, 0.2971358001232147]\n",
      "Step 204, loss = [0.28755292296409607, 0.006876763887703419, 0.286865234375]\n",
      "Step 205, loss = [0.30131396651268005, 0.0019330184441059828, 0.30112066864967346]\n",
      "Step 206, loss = [0.30546581745147705, 0.008868508040904999, 0.304578959941864]\n",
      "Step 207, loss = [0.3028241991996765, 0.006038387306034565, 0.3022203743457794]\n",
      "Step 208, loss = [0.3008626103401184, 0.005357278976589441, 0.3003268837928772]\n",
      "Step 209, loss = [0.2943647801876068, 0.0044326819479465485, 0.29392150044441223]\n",
      "Step 210, loss = [0.29636746644973755, 0.007230760063976049, 0.2956444025039673]\n",
      "Step 211, loss = [0.30078497529029846, 0.010520479641854763, 0.29973292350769043]\n",
      "Step 212, loss = [0.3173108696937561, 0.008268598467111588, 0.3164840042591095]\n",
      "Step 213, loss = [0.2956375777721405, 0.005196413025259972, 0.29511794447898865]\n",
      "Step 214, loss = [0.28963810205459595, 0.0043286788277328014, 0.2892052233219147]\n",
      "Step 215, loss = [0.3018527626991272, 0.00683862529695034, 0.30116888880729675]\n",
      "Step 216, loss = [0.3016963005065918, 0.006542750634253025, 0.30104202032089233]\n",
      "Step 217, loss = [0.2971724569797516, 0.006380671169608831, 0.29653438925743103]\n",
      "Step 218, loss = [0.2853959798812866, 0.004830178339034319, 0.28491297364234924]\n",
      "Step 219, loss = [0.28713852167129517, 0.008643457666039467, 0.28627416491508484]\n",
      "Step 220, loss = [0.3070370852947235, 0.006407314445823431, 0.30639636516571045]\n",
      "Step 221, loss = [0.30562829971313477, 0.0053235506638884544, 0.30509594082832336]\n",
      "Step 222, loss = [0.2960677146911621, 0.007626301608979702, 0.295305073261261]\n",
      "Step 223, loss = [0.308619886636734, 0.006829234305769205, 0.30793696641921997]\n",
      "Step 224, loss = [0.3087471127510071, 0.007831613533198833, 0.30796393752098083]\n",
      "Step 225, loss = [0.29979798197746277, 0.005819838959723711, 0.29921600222587585]\n",
      "Step 226, loss = [0.2982484996318817, 0.008817581459879875, 0.297366738319397]\n",
      "Step 227, loss = [0.30714738368988037, 0.00804016925394535, 0.3063433766365051]\n",
      "Step 228, loss = [0.2840677499771118, 0.006085652858018875, 0.2834591865539551]\n",
      "Step 229, loss = [0.2905177175998688, 0.0038408793043345213, 0.29013362526893616]\n",
      "Step 230, loss = [0.2965194880962372, 0.0031087168026715517, 0.29620862007141113]\n",
      "Step 231, loss = [0.2963152527809143, 0.005652761086821556, 0.2957499623298645]\n",
      "Step 232, loss = [0.3021112382411957, 0.005118959583342075, 0.3015993535518646]\n",
      "Step 233, loss = [0.2972629964351654, 0.005823151674121618, 0.2966806888580322]\n",
      "Step 234, loss = [0.2964952290058136, 0.007368745747953653, 0.29575836658477783]\n",
      "Step 235, loss = [0.29637211561203003, 0.006530606187880039, 0.29571905732154846]\n",
      "Step 236, loss = [0.3095777630805969, 0.003985344432294369, 0.3091792166233063]\n",
      "Step 237, loss = [0.3093121349811554, 0.005842247977852821, 0.3087279200553894]\n",
      "Step 238, loss = [0.300053209066391, 0.013802189379930496, 0.29867300391197205]\n",
      "Step 239, loss = [0.28995320200920105, 0.012234997004270554, 0.2887296974658966]\n",
      "Step 240, loss = [0.3061576783657074, 0.012192070484161377, 0.3049384653568268]\n",
      "Step 241, loss = [0.2980908751487732, 0.005917762406170368, 0.2974990904331207]\n",
      "Step 242, loss = [0.30543646216392517, 0.005146996583789587, 0.3049217760562897]\n",
      "Step 243, loss = [0.2886020541191101, 0.009003395214676857, 0.28770172595977783]\n",
      "Step 244, loss = [0.3076702058315277, 0.011610958725214005, 0.3065091073513031]\n",
      "Step 245, loss = [0.28694474697113037, 0.004691004753112793, 0.28647565841674805]\n",
      "Step 246, loss = [0.3123736083507538, 0.00999519880861044, 0.31137409806251526]\n",
      "Step 247, loss = [0.30015814304351807, 0.0019058319739997387, 0.29996755719184875]\n",
      "Step 248, loss = [0.30723094940185547, 0.006616855505853891, 0.30656927824020386]\n",
      "Step 249, loss = [0.3015182316303253, 0.005608594045042992, 0.3009573817253113]\n",
      "Step 250, loss = [0.3036535680294037, 0.0037950347177684307, 0.3032740652561188]\n",
      "Step 251, loss = [0.3087753355503082, 0.014682343229651451, 0.30730709433555603]\n",
      "Step 252, loss = [0.3020165264606476, 0.004893596284091473, 0.3015271723270416]\n",
      "Step 253, loss = [0.30042868852615356, 0.0026377749163657427, 0.3001649081707001]\n",
      "Step 254, loss = [0.2999131679534912, 0.002750156680122018, 0.29963815212249756]\n",
      "Step 255, loss = [0.3050878047943115, 0.009231160394847393, 0.30416467785835266]\n",
      "Step 256, loss = [0.2850433588027954, 0.004250406287610531, 0.2846183180809021]\n",
      "Step 257, loss = [0.30402979254722595, 0.006316088140010834, 0.3033981919288635]\n",
      "Step 258, loss = [0.2990958094596863, 0.004145428538322449, 0.29868125915527344]\n",
      "Step 259, loss = [0.3121982514858246, 0.005690082907676697, 0.3116292357444763]\n",
      "Step 260, loss = [0.2866780757904053, 0.004736004397273064, 0.2862044870853424]\n",
      "Step 261, loss = [0.2949201166629791, 0.004531940445303917, 0.29446691274642944]\n",
      "Step 262, loss = [0.30193281173706055, 0.008298183791339397, 0.30110299587249756]\n",
      "Step 263, loss = [0.29826515913009644, 0.00448461901396513, 0.2978166937828064]\n",
      "Step 264, loss = [0.3067762851715088, 0.006122658960521221, 0.306164026260376]\n",
      "Step 265, loss = [0.30819419026374817, 0.008904917165637016, 0.30730369687080383]\n",
      "Step 266, loss = [0.29145869612693787, 0.0072470773011446, 0.2907339930534363]\n",
      "Step 267, loss = [0.29712826013565063, 0.005465167574584484, 0.2965817451477051]\n",
      "Step 268, loss = [0.29827943444252014, 0.007331765256822109, 0.29754626750946045]\n",
      "Step 269, loss = [0.304827481508255, 0.01589326001703739, 0.3032381534576416]\n",
      "Step 270, loss = [0.2988063097000122, 0.004517241381108761, 0.2983545958995819]\n",
      "Step 271, loss = [0.29930251836776733, 0.00702091958373785, 0.298600435256958]\n",
      "Step 272, loss = [0.3034249246120453, 0.005314359441399574, 0.3028934895992279]\n",
      "Step 273, loss = [0.307003378868103, 0.007276416756212711, 0.30627572536468506]\n",
      "Step 274, loss = [0.30128300189971924, 0.010266903787851334, 0.30025631189346313]\n",
      "Step 275, loss = [0.3091751039028168, 0.0038307367358356714, 0.30879202485084534]\n",
      "Step 276, loss = [0.3121400475502014, 0.002750312676653266, 0.3118650019168854]\n",
      "Step 277, loss = [0.29795947670936584, 0.009537027217447758, 0.2970057725906372]\n",
      "Step 278, loss = [0.29115307331085205, 0.0075206817127764225, 0.29040101170539856]\n",
      "Step 279, loss = [0.2949346601963043, 0.006565853022038937, 0.294278085231781]\n",
      "Step 280, loss = [0.29549309611320496, 0.00984642468392849, 0.2945084571838379]\n",
      "Step 281, loss = [0.29094448685646057, 0.0027052524965256453, 0.29067397117614746]\n",
      "Step 282, loss = [0.30635887384414673, 0.009954527020454407, 0.30536341667175293]\n",
      "Step 283, loss = [0.30121666193008423, 0.007927817292511463, 0.30042389035224915]\n",
      "Step 284, loss = [0.30185505747795105, 0.009319739416241646, 0.30092307925224304]\n",
      "Step 285, loss = [0.2958284616470337, 0.004631640389561653, 0.2953653037548065]\n",
      "Step 286, loss = [0.3036249577999115, 0.005247694905847311, 0.30310019850730896]\n",
      "Step 287, loss = [0.3029072880744934, 0.006041151471436024, 0.30230316519737244]\n",
      "Step 288, loss = [0.29694968461990356, 0.00818725023418665, 0.2961309552192688]\n",
      "Step 289, loss = [0.30029386281967163, 0.0022484855726361275, 0.30006900429725647]\n",
      "Step 290, loss = [0.3001597225666046, 0.00625611562281847, 0.2995341122150421]\n",
      "Step 291, loss = [0.3021658957004547, 0.009340787306427956, 0.30123183131217957]\n",
      "Step 292, loss = [0.2947332262992859, 0.0039779674261808395, 0.29433542490005493]\n",
      "Step 293, loss = [0.29338330030441284, 0.006192965433001518, 0.29276400804519653]\n",
      "Step 294, loss = [0.302619993686676, 0.008000321686267853, 0.30181995034217834]\n",
      "Step 295, loss = [0.29485028982162476, 0.009122302755713463, 0.2939380705356598]\n",
      "Step 296, loss = [0.3061210811138153, 0.004817187320441008, 0.3056393563747406]\n",
      "Step 297, loss = [0.2984766960144043, 0.003288745880126953, 0.2981478273868561]\n",
      "Step 298, loss = [0.29912999272346497, 0.008125592023134232, 0.29831743240356445]\n",
      "Step 299, loss = [0.3058221638202667, 0.005495869554579258, 0.30527257919311523]\n",
      "Step 300, loss = [0.29410994052886963, 0.006053651683032513, 0.2935045659542084]\n",
      "Step 301, loss = [0.3069360554218292, 0.007242920808494091, 0.30621176958084106]\n",
      "Step 302, loss = [0.3131595253944397, 0.009343168698251247, 0.31222522258758545]\n",
      "Step 303, loss = [0.30494213104248047, 0.011764448136091232, 0.3037656843662262]\n",
      "Step 304, loss = [0.28913620114326477, 0.007349037565290928, 0.2884013056755066]\n",
      "Step 305, loss = [0.3096342086791992, 0.00937677826732397, 0.30869653820991516]\n",
      "Step 306, loss = [0.29332587122917175, 0.007342949975281954, 0.29259157180786133]\n",
      "Step 307, loss = [0.29256051778793335, 0.005724404938519001, 0.2919880747795105]\n",
      "Step 308, loss = [0.30062341690063477, 0.007977591827511787, 0.29982566833496094]\n",
      "Step 309, loss = [0.29846951365470886, 0.0031794197857379913, 0.29815158247947693]\n",
      "Step 310, loss = [0.2846318185329437, 0.006499075330793858, 0.28398191928863525]\n",
      "Step 311, loss = [0.31224772334098816, 0.004991186782717705, 0.31174859404563904]\n",
      "Step 312, loss = [0.3022722899913788, 0.008123338222503662, 0.3014599680900574]\n",
      "Step 313, loss = [0.3015970289707184, 0.00925333984196186, 0.30067169666290283]\n",
      "Step 314, loss = [0.3001421093940735, 0.007604895159602165, 0.2993816137313843]\n",
      "Step 315, loss = [0.30393826961517334, 0.007703588809818029, 0.3031679093837738]\n",
      "Step 316, loss = [0.29765722155570984, 0.0034809119533747435, 0.29730913043022156]\n",
      "Step 317, loss = [0.31377995014190674, 0.0025649890303611755, 0.31352344155311584]\n",
      "Step 318, loss = [0.2934870421886444, 0.005160260014235973, 0.29297101497650146]\n",
      "Step 319, loss = [0.3050268888473511, 0.01181080099195242, 0.3038458228111267]\n",
      "Step 320, loss = [0.3048871159553528, 0.009807908907532692, 0.30390632152557373]\n",
      "Step 321, loss = [0.29185840487480164, 0.009808364324271679, 0.2908775806427002]\n",
      "Step 322, loss = [0.3063200116157532, 0.006596025079488754, 0.3056603968143463]\n",
      "Step 323, loss = [0.3078402578830719, 0.008897913619875908, 0.30695047974586487]\n",
      "Step 324, loss = [0.31159308552742004, 0.006154847797006369, 0.31097760796546936]\n",
      "Step 325, loss = [0.3067419230937958, 0.0044993869960308075, 0.30629199743270874]\n",
      "Step 326, loss = [0.30228063464164734, 0.006256566848605871, 0.30165496468544006]\n",
      "Step 327, loss = [0.2944754362106323, 0.00990892481058836, 0.29348453879356384]\n",
      "Step 328, loss = [0.30155184864997864, 0.008718558587133884, 0.3006799817085266]\n",
      "Step 329, loss = [0.30779698491096497, 0.01015072874724865, 0.30678191781044006]\n",
      "Step 330, loss = [0.30016013979911804, 0.003365234937518835, 0.2998236119747162]\n",
      "Step 331, loss = [0.2949737012386322, 0.005762110464274883, 0.2943975031375885]\n",
      "Step 332, loss = [0.3014164865016937, 0.00888086762279272, 0.3005284070968628]\n",
      "Step 333, loss = [0.30169153213500977, 0.006234797649085522, 0.3010680377483368]\n",
      "Step 334, loss = [0.29260799288749695, 0.006955974735319614, 0.29191240668296814]\n",
      "Step 335, loss = [0.2995379567146301, 0.008255695924162865, 0.2987123727798462]\n",
      "Step 336, loss = [0.2912748157978058, 0.014135762117803097, 0.2898612320423126]\n",
      "Step 337, loss = [0.3031493127346039, 0.01029106043279171, 0.3021202087402344]\n",
      "Step 338, loss = [0.296781450510025, 0.011127336882054806, 0.29566872119903564]\n",
      "Step 339, loss = [0.29180988669395447, 0.009345611557364464, 0.29087531566619873]\n",
      "Step 340, loss = [0.2962739169597626, 0.005557528231292963, 0.29571816325187683]\n",
      "Step 341, loss = [0.3037561774253845, 0.006538184359669685, 0.3031023442745209]\n",
      "Step 342, loss = [0.3072465658187866, 0.008933022618293762, 0.30635327100753784]\n",
      "Step 343, loss = [0.29743674397468567, 0.01038614846765995, 0.2963981330394745]\n",
      "Step 344, loss = [0.29237285256385803, 0.010074179619550705, 0.29136544466018677]\n",
      "Step 345, loss = [0.30246156454086304, 0.0058600809425115585, 0.3018755614757538]\n",
      "Step 346, loss = [0.3136266767978668, 0.0037248963490128517, 0.313254177570343]\n",
      "Step 347, loss = [0.28727665543556213, 0.010473838075995445, 0.28622928261756897]\n",
      "Step 348, loss = [0.30374494194984436, 0.00488642044365406, 0.3032563030719757]\n",
      "Step 349, loss = [0.30294159054756165, 0.0025747413747012615, 0.30268412828445435]\n",
      "Step 350, loss = [0.30481985211372375, 0.003739012870937586, 0.30444595217704773]\n",
      "Step 351, loss = [0.3067174553871155, 0.010655852034687996, 0.30565187335014343]\n",
      "Step 352, loss = [0.2914218008518219, 0.006834082305431366, 0.29073840379714966]\n",
      "Step 353, loss = [0.2983509600162506, 0.00945834256708622, 0.2974051237106323]\n",
      "Step 354, loss = [0.2989208698272705, 0.00834971759468317, 0.29808589816093445]\n",
      "Step 355, loss = [0.27957409620285034, 0.010521931573748589, 0.27852189540863037]\n",
      "Step 356, loss = [0.30471983551979065, 0.008254479616880417, 0.30389440059661865]\n",
      "Step 357, loss = [0.3023236095905304, 0.009770677424967289, 0.3013465404510498]\n",
      "Step 358, loss = [0.3023833632469177, 0.006013445556163788, 0.3017820119857788]\n",
      "Step 359, loss = [0.3043766915798187, 0.008829399943351746, 0.3034937381744385]\n",
      "Step 360, loss = [0.2945701479911804, 0.006203068885952234, 0.29394984245300293]\n",
      "Step 361, loss = [0.2993274927139282, 0.005211974494159222, 0.2988063097000122]\n",
      "Step 362, loss = [0.2946734130382538, 0.005734531674534082, 0.29409995675086975]\n",
      "Step 363, loss = [0.30022910237312317, 0.008291845209896564, 0.2993999123573303]\n",
      "Step 364, loss = [0.2921268939971924, 0.01031436212360859, 0.29109546542167664]\n",
      "Step 365, loss = [0.3019876182079315, 0.006235707551240921, 0.3013640344142914]\n",
      "Step 366, loss = [0.3079225718975067, 0.006372116506099701, 0.3072853684425354]\n",
      "Step 367, loss = [0.29964977502822876, 0.010489948093891144, 0.29860079288482666]\n",
      "Step 368, loss = [0.2909010350704193, 0.0038879739586263895, 0.29051223397254944]\n",
      "Step 369, loss = [0.30169162154197693, 0.00462937681004405, 0.30122867226600647]\n",
      "Step 370, loss = [0.30082452297210693, 0.009476954117417336, 0.2998768389225006]\n",
      "Step 371, loss = [0.30007943511009216, 0.008665495552122593, 0.29921287298202515]\n",
      "Step 372, loss = [0.2995965778827667, 0.006594732403755188, 0.2989371120929718]\n",
      "Step 373, loss = [0.2963266372680664, 0.005422440357506275, 0.2957843840122223]\n",
      "Step 374, loss = [0.3110896050930023, 0.008878231048583984, 0.3102017939090729]\n",
      "Step 375, loss = [0.28658947348594666, 0.004720467608422041, 0.28611743450164795]\n",
      "Step 376, loss = [0.2884898781776428, 0.009156276471912861, 0.28757426142692566]\n",
      "Step 377, loss = [0.2915537655353546, 0.011475425213575363, 0.2904062271118164]\n",
      "Step 378, loss = [0.30282145738601685, 0.006651234347373247, 0.30215632915496826]\n",
      "Step 379, loss = [0.3054419159889221, 0.007656589150428772, 0.30467626452445984]\n",
      "Step 380, loss = [0.309278666973114, 0.00822226982563734, 0.3084564507007599]\n",
      "Step 381, loss = [0.2988736629486084, 0.009200811386108398, 0.2979535758495331]\n",
      "Step 382, loss = [0.30218520760536194, 0.014120932668447495, 0.30077311396598816]\n",
      "Step 383, loss = [0.31262853741645813, 0.00697000976651907, 0.3119315505027771]\n",
      "Step 384, loss = [0.2997148931026459, 0.0028255037032067776, 0.29943233728408813]\n",
      "Step 385, loss = [0.2939712703227997, 0.0034148143604397774, 0.29362979531288147]\n",
      "Step 386, loss = [0.3045949637889862, 0.0074761053547263145, 0.3038473427295685]\n",
      "Step 387, loss = [0.298181414604187, 0.0047384873032569885, 0.29770755767822266]\n",
      "Step 388, loss = [0.30939000844955444, 0.008300128392875195, 0.30855998396873474]\n",
      "Step 389, loss = [0.3022574484348297, 0.006440875120460987, 0.30161336064338684]\n",
      "Step 390, loss = [0.28523942828178406, 0.0036058593541383743, 0.28487884998321533]\n",
      "Step 391, loss = [0.29615092277526855, 0.0039079380221664906, 0.2957601249217987]\n",
      "Step 392, loss = [0.29469048976898193, 0.012133768759667873, 0.2934771180152893]\n",
      "Step 393, loss = [0.3008834719657898, 0.002540825866162777, 0.3006293773651123]\n",
      "Step 394, loss = [0.2978591024875641, 0.006726299412548542, 0.2971864640712738]\n",
      "Step 395, loss = [0.29621589183807373, 0.005111382342875004, 0.29570475220680237]\n",
      "Step 396, loss = [0.30730679631233215, 0.005790427792817354, 0.3067277669906616]\n",
      "Step 397, loss = [0.298836350440979, 0.010818611830472946, 0.2977544963359833]\n",
      "Step 398, loss = [0.31172797083854675, 0.007870977744460106, 0.31094086170196533]\n",
      "Step 399, loss = [0.30966857075691223, 0.004877240397036076, 0.3091808557510376]\n",
      "Step 400, loss = [0.2951187491416931, 0.006933366879820824, 0.2944253981113434]\n",
      "Step 401, loss = [0.3064960837364197, 0.00818131398409605, 0.30567795038223267]\n",
      "Step 402, loss = [0.30540040135383606, 0.0053173997439444065, 0.3048686683177948]\n",
      "Step 403, loss = [0.3089510202407837, 0.00857417844235897, 0.3080936074256897]\n",
      "Step 404, loss = [0.2668468952178955, 0.003966168966144323, 0.26645028591156006]\n",
      "Step 405, loss = [0.30912694334983826, 0.003711584722623229, 0.3087557852268219]\n",
      "Step 406, loss = [0.29767879843711853, 0.007212477270513773, 0.2969575524330139]\n",
      "Step 407, loss = [0.301403671503067, 0.010532286018133163, 0.30035045742988586]\n",
      "Step 408, loss = [0.2933717668056488, 0.011799147352576256, 0.29219186305999756]\n",
      "Step 409, loss = [0.2830522954463959, 0.005217291414737701, 0.2825305759906769]\n",
      "Step 410, loss = [0.3082095980644226, 0.010531064122915268, 0.307156503200531]\n",
      "Step 411, loss = [0.2815925180912018, 0.0027398522943258286, 0.2813185453414917]\n",
      "Step 412, loss = [0.3046483099460602, 0.008119131438434124, 0.3038364052772522]\n",
      "Step 413, loss = [0.3000667989253998, 0.0074973455630242825, 0.29931706190109253]\n",
      "Step 414, loss = [0.3069871962070465, 0.007159451022744179, 0.3062712550163269]\n",
      "Step 415, loss = [0.2999033033847809, 0.002603177446871996, 0.29964298009872437]\n",
      "Step 416, loss = [0.2956044673919678, 0.005997614469379187, 0.2950046956539154]\n",
      "Step 417, loss = [0.29461660981178284, 0.008943920955061913, 0.2937222123146057]\n",
      "Step 418, loss = [0.30728721618652344, 0.007167274132370949, 0.30657050013542175]\n",
      "Step 419, loss = [0.30229613184928894, 0.00892479158937931, 0.30140364170074463]\n",
      "Step 420, loss = [0.3057267367839813, 0.006576129235327244, 0.30506911873817444]\n",
      "Step 421, loss = [0.3020438551902771, 0.006691362708806992, 0.30137473344802856]\n",
      "Step 422, loss = [0.2945181727409363, 0.0073396554216742516, 0.2937842011451721]\n",
      "Step 423, loss = [0.28545212745666504, 0.0035047016572207212, 0.28510165214538574]\n",
      "Step 424, loss = [0.30210110545158386, 0.004781806841492653, 0.3016229271888733]\n",
      "Step 425, loss = [0.3114694058895111, 0.007042999379336834, 0.3107651174068451]\n",
      "Step 426, loss = [0.294537752866745, 0.004386249464005232, 0.2940991222858429]\n",
      "Step 427, loss = [0.3088407516479492, 0.003053045831620693, 0.30853545665740967]\n",
      "Step 428, loss = [0.3038290739059448, 0.0036722500808537006, 0.30346184968948364]\n",
      "Step 429, loss = [0.3179032504558563, 0.008041982538998127, 0.31709906458854675]\n",
      "Step 430, loss = [0.29754796624183655, 0.011669178493320942, 0.29638105630874634]\n",
      "Step 431, loss = [0.30453920364379883, 0.010464893653988838, 0.3034927248954773]\n",
      "Step 432, loss = [0.3009287118911743, 0.012332397513091564, 0.2996954619884491]\n",
      "Step 433, loss = [0.3084281086921692, 0.005025551654398441, 0.3079255521297455]\n",
      "Step 434, loss = [0.2958422005176544, 0.007010770030319691, 0.2951411306858063]\n",
      "Step 435, loss = [0.28953030705451965, 0.005662835203111172, 0.28896403312683105]\n",
      "Step 436, loss = [0.29945892095565796, 0.005243710242211819, 0.29893454909324646]\n",
      "Step 437, loss = [0.30095726251602173, 0.002949553309008479, 0.3006623089313507]\n",
      "Step 438, loss = [0.2804211676120758, 0.01055164635181427, 0.27936601638793945]\n",
      "Step 439, loss = [0.27410703897476196, 0.00623707938939333, 0.2734833359718323]\n",
      "Step 440, loss = [0.30374449491500854, 0.0049756597727537155, 0.3032469153404236]\n",
      "Step 441, loss = [0.2903688848018646, 0.005690114572644234, 0.28979986906051636]\n",
      "Step 442, loss = [0.2861005961894989, 0.003974851220846176, 0.2857031226158142]\n",
      "Step 443, loss = [0.2895742356777191, 0.0023906952701509, 0.289335161447525]\n",
      "Step 444, loss = [0.3029104173183441, 0.0041718012653291225, 0.30249324440956116]\n",
      "Step 445, loss = [0.3005465865135193, 0.006569881457835436, 0.29988959431648254]\n",
      "Step 446, loss = [0.3085309863090515, 0.00919798668473959, 0.3076111972332001]\n",
      "Step 447, loss = [0.307354599237442, 0.009561614133417606, 0.3063984513282776]\n",
      "Step 448, loss = [0.29088595509529114, 0.0038307076320052147, 0.2905028760433197]\n",
      "Step 449, loss = [0.3139992356300354, 0.005412101745605469, 0.31345802545547485]\n",
      "Step 450, loss = [0.3080757260322571, 0.00933217816054821, 0.3071424961090088]\n",
      "Step 451, loss = [0.29488620162010193, 0.005364923272281885, 0.29434970021247864]\n",
      "Step 452, loss = [0.29905980825424194, 0.006635516881942749, 0.2983962595462799]\n",
      "Step 453, loss = [0.2822951376438141, 0.0073449816554784775, 0.28156062960624695]\n",
      "Step 454, loss = [0.300276517868042, 0.008639240637421608, 0.2994126081466675]\n",
      "Step 455, loss = [0.3049963712692261, 0.008098579943180084, 0.30418652296066284]\n",
      "Step 456, loss = [0.3018350303173065, 0.005360263865441084, 0.30129900574684143]\n",
      "Step 457, loss = [0.3033217787742615, 0.005276025272905827, 0.3027941882610321]\n",
      "Step 458, loss = [0.2883140444755554, 0.010516812093555927, 0.28726235032081604]\n",
      "Step 459, loss = [0.3087453544139862, 0.007902410812675953, 0.3079551160335541]\n",
      "Step 460, loss = [0.3062712550163269, 0.008038491010665894, 0.3054673969745636]\n",
      "Step 461, loss = [0.30028676986694336, 0.0069353156723082066, 0.2995932400226593]\n",
      "Step 462, loss = [0.2762380540370941, 0.006526693236082792, 0.2755853831768036]\n",
      "Step 463, loss = [0.30818891525268555, 0.024743299931287766, 0.30571457743644714]\n",
      "Step 464, loss = [0.28658753633499146, 0.005477533675730228, 0.2860397696495056]\n",
      "Step 465, loss = [0.313568115234375, 0.0058927577920258045, 0.3129788339138031]\n",
      "Step 466, loss = [0.2987353503704071, 0.008245939388871193, 0.29791074991226196]\n",
      "Step 467, loss = [0.30388274788856506, 0.01059618592262268, 0.30282312631607056]\n",
      "Step 468, loss = [0.29706305265426636, 0.004395931027829647, 0.29662346839904785]\n",
      "Step 469, loss = [0.3005389869213104, 0.0047681801952421665, 0.3000621795654297]\n",
      "Step 470, loss = [0.30919432640075684, 0.008764581754803658, 0.3083178699016571]\n",
      "Step 471, loss = [0.29674670100212097, 0.007652352564036846, 0.2959814667701721]\n",
      "Step 472, loss = [0.292706161737442, 0.00612834095954895, 0.29209333658218384]\n",
      "Step 473, loss = [0.3044096827507019, 0.006373744923621416, 0.30377230048179626]\n",
      "Step 474, loss = [0.310756117105484, 0.010914633050560951, 0.30966466665267944]\n",
      "Step 475, loss = [0.29981642961502075, 0.003974171355366707, 0.29941901564598083]\n",
      "Step 476, loss = [0.3027500808238983, 0.013157995417714119, 0.3014342784881592]\n",
      "Step 477, loss = [0.3043479025363922, 0.004499127622693777, 0.3038979768753052]\n",
      "Step 478, loss = [0.3145226538181305, 0.003905436024069786, 0.31413212418556213]\n",
      "Step 479, loss = [0.2993699014186859, 0.010486936196684837, 0.2983212172985077]\n",
      "Step 480, loss = [0.29283785820007324, 0.009393023326992989, 0.29189854860305786]\n",
      "Step 481, loss = [0.2886248826980591, 0.005879276432096958, 0.28803694248199463]\n",
      "Step 482, loss = [0.2978718876838684, 0.0066834441386163235, 0.29720354080200195]\n",
      "Step 483, loss = [0.2964932918548584, 0.004254229832440615, 0.29606786370277405]\n",
      "Step 484, loss = [0.29334545135498047, 0.007857764139771461, 0.2925596833229065]\n",
      "Step 485, loss = [0.2981792688369751, 0.0032956115901470184, 0.29784971475601196]\n",
      "Step 486, loss = [0.2946934700012207, 0.008311512880027294, 0.29386231303215027]\n",
      "Step 487, loss = [0.29997923970222473, 0.006959558464586735, 0.29928329586982727]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11754 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 488, loss = [0.2931802272796631, 0.003845706582069397, 0.29279565811157227]\n",
      "Update target distribution epoch 3 step 489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11754/11754 [1:51:39<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos. rate:0.4294117647058823 Neg. rate:0.5705882352941176\n",
      "delta_label  0.003998638761272758\n",
      "Step 489, loss = [0.29302090406417847, 0.007259399630129337, 0.2922949492931366]\n",
      "Step 490, loss = [0.2820057272911072, 0.004737743176519871, 0.28153195977211]\n",
      "Step 491, loss = [0.3095548152923584, 0.006209831219166517, 0.308933824300766]\n",
      "Step 492, loss = [0.29890018701553345, 0.005747778341174126, 0.29832541942596436]\n",
      "Step 493, loss = [0.3166971802711487, 0.006378655321896076, 0.31605932116508484]\n",
      "Step 494, loss = [0.2984694540500641, 0.008106589317321777, 0.2976588010787964]\n",
      "Step 495, loss = [0.27958306670188904, 0.009048951789736748, 0.27867817878723145]\n",
      "Step 496, loss = [0.30459704995155334, 0.012540893629193306, 0.3033429682254791]\n",
      "Step 497, loss = [0.30701199173927307, 0.004436110612004995, 0.3065683841705322]\n",
      "Step 498, loss = [0.31263989210128784, 0.008075371384620667, 0.31183236837387085]\n",
      "Step 499, loss = [0.28291159868240356, 0.009492738172411919, 0.2819623351097107]\n",
      "Step 500, loss = [0.31125104427337646, 0.005871590226888657, 0.3106638789176941]\n",
      "Step 501, loss = [0.2991611957550049, 0.006056232377886772, 0.2985555827617645]\n",
      "Step 502, loss = [0.30009201169013977, 0.00604994036257267, 0.29948702454566956]\n",
      "Step 503, loss = [0.2940390110015869, 0.004922118503600359, 0.29354679584503174]\n",
      "Step 504, loss = [0.30697137117385864, 0.003956426866352558, 0.3065757155418396]\n",
      "Step 505, loss = [0.29888641834259033, 0.007472916506230831, 0.29813912510871887]\n",
      "Step 506, loss = [0.2954658567905426, 0.005879242438822985, 0.29487794637680054]\n",
      "Step 507, loss = [0.30877768993377686, 0.005305218510329723, 0.3082471787929535]\n",
      "Step 508, loss = [0.3011331856250763, 0.006835557520389557, 0.3004496395587921]\n",
      "Step 509, loss = [0.3070831596851349, 0.006210075691342354, 0.3064621388912201]\n",
      "Step 510, loss = [0.3037206828594208, 0.00723664416000247, 0.30299702286720276]\n",
      "Step 511, loss = [0.2994120717048645, 0.006737301126122475, 0.2987383306026459]\n",
      "Step 512, loss = [0.2902475595474243, 0.006479684263467789, 0.28959959745407104]\n",
      "Step 513, loss = [0.30607739090919495, 0.010861366987228394, 0.3049912452697754]\n",
      "Step 514, loss = [0.3131740689277649, 0.009212017059326172, 0.31225287914276123]\n",
      "Step 515, loss = [0.2886219918727875, 0.006525018252432346, 0.2879694998264313]\n",
      "Step 516, loss = [0.29578787088394165, 0.010146199725568295, 0.29477325081825256]\n",
      "Step 517, loss = [0.3060053288936615, 0.0066994293592870235, 0.3053353726863861]\n",
      "Step 518, loss = [0.3140963017940521, 0.009976344183087349, 0.313098669052124]\n",
      "Step 519, loss = [0.3082972764968872, 0.008645094931125641, 0.30743277072906494]\n",
      "Step 520, loss = [0.29008615016937256, 0.003520345315337181, 0.2897341251373291]\n",
      "Step 521, loss = [0.29814961552619934, 0.006297805346548557, 0.29751983284950256]\n",
      "Step 522, loss = [0.30418187379837036, 0.010598229244351387, 0.30312204360961914]\n",
      "Step 523, loss = [0.3035047650337219, 0.011802436783909798, 0.3023245334625244]\n",
      "Step 524, loss = [0.287789911031723, 0.005188656039535999, 0.28727105259895325]\n",
      "Step 525, loss = [0.3022915720939636, 0.007442872039973736, 0.3015472888946533]\n",
      "Step 526, loss = [0.2835192084312439, 0.009374088607728481, 0.28258180618286133]\n",
      "Step 527, loss = [0.30003622174263, 0.009242919273674488, 0.299111932516098]\n",
      "Step 528, loss = [0.29385465383529663, 0.008693991228938103, 0.2929852604866028]\n",
      "Step 529, loss = [0.2983044683933258, 0.009296257980167866, 0.2973748445510864]\n",
      "Step 530, loss = [0.2942574918270111, 0.007558105513453484, 0.29350167512893677]\n",
      "Step 531, loss = [0.3045879900455475, 0.005779408849775791, 0.3040100634098053]\n",
      "Step 532, loss = [0.30687427520751953, 0.0069771138951182365, 0.3061765730381012]\n",
      "Step 533, loss = [0.2982789874076843, 0.004782097414135933, 0.29780077934265137]\n",
      "Step 534, loss = [0.30205583572387695, 0.008818181231617928, 0.30117401480674744]\n",
      "Step 535, loss = [0.2764093577861786, 0.006197692826390266, 0.2757895886898041]\n",
      "Step 536, loss = [0.2961461544036865, 0.007117807865142822, 0.2954343855381012]\n",
      "Step 537, loss = [0.31023848056793213, 0.013435973785817623, 0.3088948726654053]\n",
      "Step 538, loss = [0.30269116163253784, 0.005843108054250479, 0.3021068572998047]\n",
      "Step 539, loss = [0.28866836428642273, 0.004924858920276165, 0.28817588090896606]\n",
      "Step 540, loss = [0.29372739791870117, 0.006302312947809696, 0.2930971682071686]\n",
      "Step 541, loss = [0.30719193816185, 0.006659544073045254, 0.30652597546577454]\n",
      "Step 542, loss = [0.28133702278137207, 0.008657021448016167, 0.2804713249206543]\n",
      "Step 543, loss = [0.3003399074077606, 0.004922595806419849, 0.29984766244888306]\n",
      "Step 544, loss = [0.29819849133491516, 0.009259438142180443, 0.29727253317832947]\n",
      "Step 545, loss = [0.2862379848957062, 0.007476563565433025, 0.28549033403396606]\n",
      "Step 546, loss = [0.31382644176483154, 0.008525418117642403, 0.31297388672828674]\n",
      "Step 547, loss = [0.2994263172149658, 0.01025654748082161, 0.2984006702899933]\n",
      "Step 548, loss = [0.30528053641319275, 0.0065269023180007935, 0.30462783575057983]\n",
      "Step 549, loss = [0.3061089515686035, 0.003001113422214985, 0.3058088421821594]\n",
      "Step 550, loss = [0.30095648765563965, 0.006853334605693817, 0.3002711534500122]\n",
      "Step 551, loss = [0.3012450039386749, 0.0037978917825967073, 0.30086520314216614]\n",
      "Step 552, loss = [0.2827933430671692, 0.0083534000441432, 0.2819580137729645]\n",
      "Step 553, loss = [0.29218897223472595, 0.008719170466065407, 0.29131704568862915]\n",
      "Step 554, loss = [0.29207074642181396, 0.007815220393240452, 0.29128921031951904]\n",
      "Step 555, loss = [0.2987855076789856, 0.006822568364441395, 0.2981032431125641]\n",
      "Step 556, loss = [0.31059375405311584, 0.006913164630532265, 0.3099024295806885]\n",
      "Step 557, loss = [0.3078972399234772, 0.0092852134257555, 0.30696871876716614]\n",
      "Step 558, loss = [0.29297298192977905, 0.006717837415635586, 0.292301207780838]\n",
      "Step 559, loss = [0.299260675907135, 0.007594616152346134, 0.29850122332572937]\n",
      "Step 560, loss = [0.31283071637153625, 0.009764345362782478, 0.3118542730808258]\n",
      "Step 561, loss = [0.3007809817790985, 0.008150167763233185, 0.2999659776687622]\n",
      "Step 562, loss = [0.2880595922470093, 0.007301207631826401, 0.28732946515083313]\n",
      "Step 563, loss = [0.29640185832977295, 0.008783668279647827, 0.2955234944820404]\n",
      "Step 564, loss = [0.29762205481529236, 0.00873158685863018, 0.29674890637397766]\n",
      "Step 565, loss = [0.28728771209716797, 0.003577915718778968, 0.2869299352169037]\n",
      "Step 566, loss = [0.28618332743644714, 0.007354888133704662, 0.2854478359222412]\n",
      "Step 567, loss = [0.29003283381462097, 0.004484041593968868, 0.2895844280719757]\n",
      "Step 568, loss = [0.29529857635498047, 0.007309204898774624, 0.29456764459609985]\n",
      "Step 569, loss = [0.29654210805892944, 0.004710858687758446, 0.29607102274894714]\n",
      "Step 570, loss = [0.3014596700668335, 0.006852676626294851, 0.30077439546585083]\n",
      "Step 571, loss = [0.2949451506137848, 0.005124146118760109, 0.29443272948265076]\n",
      "Step 572, loss = [0.29430508613586426, 0.01157347857952118, 0.2931477427482605]\n",
      "Step 573, loss = [0.2849887013435364, 0.005530055612325668, 0.2844356894493103]\n",
      "Step 574, loss = [0.308209091424942, 0.008343669585883617, 0.3073747158050537]\n",
      "Step 575, loss = [0.3116477131843567, 0.004496795125305653, 0.31119802594184875]\n",
      "Step 576, loss = [0.2914164960384369, 0.0068667735904455185, 0.290729820728302]\n",
      "Step 577, loss = [0.3091171681880951, 0.00713327806442976, 0.3084038496017456]\n",
      "Step 578, loss = [0.3135181963443756, 0.005886824801564217, 0.31292951107025146]\n",
      "Step 579, loss = [0.3095167875289917, 0.005695197731256485, 0.30894726514816284]\n",
      "Step 580, loss = [0.3068540394306183, 0.007612261921167374, 0.3060927987098694]\n",
      "Step 581, loss = [0.30656731128692627, 0.007603890728205442, 0.3058069348335266]\n",
      "Step 582, loss = [0.2848784327507019, 0.002885015681385994, 0.2845899164676666]\n",
      "Step 583, loss = [0.29016703367233276, 0.0082066860049963, 0.2893463671207428]\n",
      "Step 584, loss = [0.31249430775642395, 0.007266033906489611, 0.31176769733428955]\n",
      "Step 585, loss = [0.30708828568458557, 0.006695258896797895, 0.3064187467098236]\n",
      "Step 586, loss = [0.30634379386901855, 0.007905859500169754, 0.3055531978607178]\n",
      "Step 587, loss = [0.3094356954097748, 0.004643265623599291, 0.3089713752269745]\n",
      "Step 588, loss = [0.28809911012649536, 0.010450432077050209, 0.28705406188964844]\n",
      "Step 589, loss = [0.30787625908851624, 0.00998480524867773, 0.3068777918815613]\n",
      "Step 590, loss = [0.31029924750328064, 0.006854230538010597, 0.30961382389068604]\n",
      "Step 591, loss = [0.30465126037597656, 0.010382668115198612, 0.30361300706863403]\n",
      "Step 592, loss = [0.3033730983734131, 0.008363693952560425, 0.3025367259979248]\n",
      "Step 593, loss = [0.29607316851615906, 0.0064282650128006935, 0.29543033242225647]\n",
      "Step 594, loss = [0.30265310406684875, 0.010641675442457199, 0.30158892273902893]\n",
      "Step 595, loss = [0.29843634366989136, 0.00626892177388072, 0.2978094518184662]\n",
      "Step 596, loss = [0.2878846526145935, 0.008498297072947025, 0.287034809589386]\n",
      "Step 597, loss = [0.29469379782676697, 0.006027353461831808, 0.2940910756587982]\n",
      "Step 598, loss = [0.303322434425354, 0.011022230610251427, 0.3022202253341675]\n",
      "Step 599, loss = [0.3040207028388977, 0.004731460474431515, 0.30354756116867065]\n",
      "Step 600, loss = [0.30206626653671265, 0.004777945578098297, 0.3015884757041931]\n",
      "Step 601, loss = [0.311901330947876, 0.0039016681257635355, 0.31151115894317627]\n",
      "Step 602, loss = [0.31698301434516907, 0.005088369362056255, 0.31647416949272156]\n",
      "Step 603, loss = [0.29880383610725403, 0.005057412199676037, 0.29829809069633484]\n",
      "Step 604, loss = [0.29489076137542725, 0.006786600220948458, 0.29421210289001465]\n",
      "Step 605, loss = [0.2947447597980499, 0.004403506871312857, 0.29430440068244934]\n",
      "Step 606, loss = [0.3040953576564789, 0.006888027302920818, 0.30340656638145447]\n",
      "Step 607, loss = [0.2971632480621338, 0.006770528852939606, 0.2964861989021301]\n",
      "Step 608, loss = [0.2962176501750946, 0.006933499593287706, 0.2955242991447449]\n",
      "Step 609, loss = [0.29398640990257263, 0.005878209136426449, 0.29339858889579773]\n",
      "Step 610, loss = [0.29368534684181213, 0.008414879441261292, 0.29284384846687317]\n",
      "Step 611, loss = [0.30488646030426025, 0.0035526189021766186, 0.30453118681907654]\n",
      "Step 612, loss = [0.2931903004646301, 0.007221340201795101, 0.2924681603908539]\n",
      "Step 613, loss = [0.3101218342781067, 0.008318373933434486, 0.30928999185562134]\n",
      "Step 614, loss = [0.29275384545326233, 0.0017651744419708848, 0.29257732629776]\n",
      "Step 615, loss = [0.2933944761753082, 0.00945315882563591, 0.29244914650917053]\n",
      "Step 616, loss = [0.3013109862804413, 0.007805428933352232, 0.30053043365478516]\n",
      "Step 617, loss = [0.29138049483299255, 0.008590622805058956, 0.29052144289016724]\n",
      "Step 618, loss = [0.27529361844062805, 0.008887623436748981, 0.2744048535823822]\n",
      "Step 619, loss = [0.29685884714126587, 0.005403505638241768, 0.29631850123405457]\n",
      "Step 620, loss = [0.3024742007255554, 0.005664927884936333, 0.3019077181816101]\n",
      "Step 621, loss = [0.3037635385990143, 0.00787175353616476, 0.3029763698577881]\n",
      "Step 622, loss = [0.30281686782836914, 0.004461043514311314, 0.30237075686454773]\n",
      "Step 623, loss = [0.2969712018966675, 0.010652999393641949, 0.2959058880805969]\n",
      "Step 624, loss = [0.2876366376876831, 0.008719654753804207, 0.2867646813392639]\n",
      "Step 625, loss = [0.3068528473377228, 0.006971942260861397, 0.30615565180778503]\n",
      "Step 626, loss = [0.3043400049209595, 0.006911633536219597, 0.30364882946014404]\n",
      "Step 627, loss = [0.2915205955505371, 0.007400088012218475, 0.29078057408332825]\n",
      "Step 628, loss = [0.29528212547302246, 0.0037621576339006424, 0.2949059009552002]\n",
      "Step 629, loss = [0.31463849544525146, 0.006262161768972874, 0.3140122890472412]\n",
      "Step 630, loss = [0.3024244010448456, 0.005380624905228615, 0.30188634991645813]\n",
      "Step 631, loss = [0.28662270307540894, 0.007478848099708557, 0.2858748137950897]\n",
      "Step 632, loss = [0.30753156542778015, 0.004389963112771511, 0.3070925772190094]\n",
      "Step 633, loss = [0.30198970437049866, 0.007330454885959625, 0.3012566566467285]\n",
      "Step 634, loss = [0.2867840528488159, 0.0038085984997451305, 0.28640317916870117]\n",
      "Step 635, loss = [0.29571768641471863, 0.005646384786814451, 0.29515305161476135]\n",
      "Step 636, loss = [0.29067519307136536, 0.009254825301468372, 0.28974971175193787]\n",
      "Step 637, loss = [0.3029496967792511, 0.007782223168760538, 0.3021714687347412]\n",
      "Step 638, loss = [0.2957281768321991, 0.010757654905319214, 0.29465240240097046]\n",
      "Step 639, loss = [0.3059861361980438, 0.005573328584432602, 0.30542880296707153]\n",
      "Step 640, loss = [0.30852827429771423, 0.007609035354107618, 0.3077673614025116]\n",
      "Step 641, loss = [0.30835428833961487, 0.006286797113716602, 0.30772560834884644]\n",
      "Step 642, loss = [0.31332460045814514, 0.004868441727012396, 0.31283774971961975]\n",
      "Step 643, loss = [0.2984725534915924, 0.006570970173925161, 0.2978154420852661]\n",
      "Step 644, loss = [0.2986297309398651, 0.005720304790884256, 0.2980577051639557]\n",
      "Step 645, loss = [0.30563515424728394, 0.012011384591460228, 0.30443400144577026]\n",
      "Step 646, loss = [0.27109819650650024, 0.009027371183037758, 0.27019545435905457]\n",
      "Step 647, loss = [0.2934797406196594, 0.0024221076164394617, 0.2932375371456146]\n",
      "Step 648, loss = [0.2921111285686493, 0.00958042312413454, 0.29115307331085205]\n",
      "Step 649, loss = [0.30548369884490967, 0.006211549509316683, 0.30486252903938293]\n",
      "Step 650, loss = [0.3046000301837921, 0.005430547520518303, 0.30405697226524353]\n",
      "Step 651, loss = [0.3093492388725281, 0.004023422487080097, 0.3089469075202942]\n",
      "Step 652, loss = [0.30427807569503784, 0.0060269758105278015, 0.3036753833293915]\n",
      "Step 653, loss = [0.29119938611984253, 0.012346763163805008, 0.2899647057056427]\n",
      "Step 654, loss = [0.2947629690170288, 0.00588202616199851, 0.29417476058006287]\n",
      "Step 655, loss = [0.31333282589912415, 0.014183560386300087, 0.31191447377204895]\n",
      "Step 656, loss = [0.30311307311058044, 0.0048896842636168, 0.3026241064071655]\n",
      "Step 657, loss = [0.30236026644706726, 0.007848679088056087, 0.3015753924846649]\n",
      "Step 658, loss = [0.3019709587097168, 0.0076028089970350266, 0.3012106716632843]\n",
      "Step 659, loss = [0.30303332209587097, 0.006004540249705315, 0.3024328649044037]\n",
      "Step 660, loss = [0.29981258511543274, 0.004499536473304033, 0.2993626296520233]\n",
      "Step 661, loss = [0.3049749732017517, 0.006492962595075369, 0.304325670003891]\n",
      "Step 662, loss = [0.30335506796836853, 0.008168892934918404, 0.3025381863117218]\n",
      "Step 663, loss = [0.30584293603897095, 0.005160977132618427, 0.3053268492221832]\n",
      "Step 664, loss = [0.3002930283546448, 0.0062201498076319695, 0.2996710240840912]\n",
      "Step 665, loss = [0.3035432696342468, 0.003331940621137619, 0.3032100796699524]\n",
      "Step 666, loss = [0.2984474003314972, 0.010308530181646347, 0.2974165380001068]\n",
      "Step 667, loss = [0.3023139238357544, 0.008170360699295998, 0.3014968931674957]\n",
      "Step 668, loss = [0.3034166097640991, 0.005422054324299097, 0.3028744161128998]\n",
      "Step 669, loss = [0.30562639236450195, 0.007218507118523121, 0.3049045503139496]\n",
      "Step 670, loss = [0.29966264963150024, 0.008836382068693638, 0.2987790107727051]\n",
      "Step 671, loss = [0.29813021421432495, 0.008074330165982246, 0.2973227798938751]\n",
      "Step 672, loss = [0.29339390993118286, 0.01094311848282814, 0.2922995984554291]\n",
      "Step 673, loss = [0.2922494113445282, 0.00998995453119278, 0.29125040769577026]\n",
      "Step 674, loss = [0.3062400221824646, 0.011328320950269699, 0.3051071763038635]\n",
      "Step 675, loss = [0.31300511956214905, 0.0082295136526227, 0.31218215823173523]\n",
      "Step 676, loss = [0.30145105719566345, 0.00838485173881054, 0.30061256885528564]\n",
      "Step 677, loss = [0.30727139115333557, 0.008873791433870792, 0.30638399720191956]\n",
      "Step 678, loss = [0.3017809987068176, 0.009942127391695976, 0.3007867932319641]\n",
      "Step 679, loss = [0.2738344073295593, 0.009751666337251663, 0.27285924553871155]\n",
      "Step 680, loss = [0.2891481816768646, 0.0036767867859452963, 0.2887805104255676]\n",
      "Step 681, loss = [0.2991221249103546, 0.004740090109407902, 0.2986481189727783]\n",
      "Step 682, loss = [0.2956487536430359, 0.007123986724764109, 0.2949363589286804]\n",
      "Step 683, loss = [0.2997465431690216, 0.009854322299361229, 0.2987610995769501]\n",
      "Step 684, loss = [0.28896307945251465, 0.003695391584187746, 0.2885935306549072]\n",
      "Step 685, loss = [0.2882584035396576, 0.008931586518883705, 0.28736525774002075]\n",
      "Step 686, loss = [0.291104257106781, 0.006797890178859234, 0.2904244661331177]\n",
      "Step 687, loss = [0.28932222723960876, 0.0090390145778656, 0.28841832280158997]\n",
      "Step 688, loss = [0.299003928899765, 0.003955887630581856, 0.29860833287239075]\n",
      "Step 689, loss = [0.28861188888549805, 0.0044363271445035934, 0.2881682515144348]\n",
      "Step 690, loss = [0.29519739747047424, 0.008302469737827778, 0.2943671643733978]\n",
      "Step 691, loss = [0.29870131611824036, 0.005935668013989925, 0.29810774326324463]\n",
      "Step 692, loss = [0.3101021349430084, 0.010586264543235302, 0.3090434968471527]\n",
      "Step 693, loss = [0.29757875204086304, 0.00381299015134573, 0.29719746112823486]\n",
      "Step 694, loss = [0.31195127964019775, 0.006330146919935942, 0.3113182783126831]\n",
      "Step 695, loss = [0.31331369280815125, 0.006600888911634684, 0.3126536011695862]\n",
      "Step 696, loss = [0.3119249939918518, 0.006413057446479797, 0.311283677816391]\n",
      "Step 697, loss = [0.2989464998245239, 0.00812957901507616, 0.2981335520744324]\n",
      "Step 698, loss = [0.30856388807296753, 0.008425617590546608, 0.3077213168144226]\n",
      "Step 699, loss = [0.29881757497787476, 0.008446447551250458, 0.2979729175567627]\n",
      "Step 700, loss = [0.2954268455505371, 0.0063973902724683285, 0.29478710889816284]\n",
      "Step 701, loss = [0.29987841844558716, 0.007515423931181431, 0.29912686347961426]\n",
      "Step 702, loss = [0.28911998867988586, 0.006531773135066032, 0.28846681118011475]\n",
      "Step 703, loss = [0.3155146837234497, 0.00504921842366457, 0.3150097727775574]\n",
      "Step 704, loss = [0.3102432191371918, 0.005913295783102512, 0.3096518814563751]\n",
      "Step 705, loss = [0.2730812728404999, 0.00778551958501339, 0.2723027169704437]\n",
      "Step 706, loss = [0.3029709756374359, 0.006019875407218933, 0.30236899852752686]\n",
      "Step 707, loss = [0.30069512128829956, 0.010937810875475407, 0.29960134625434875]\n",
      "Step 708, loss = [0.3011147677898407, 0.004005523398518562, 0.3007142245769501]\n",
      "Step 709, loss = [0.30070146918296814, 0.006025430280715227, 0.3000989258289337]\n",
      "Step 710, loss = [0.2969827651977539, 0.012829373590648174, 0.2956998348236084]\n",
      "Step 711, loss = [0.2949969470500946, 0.00969248078763485, 0.2940276861190796]\n",
      "Step 712, loss = [0.2920866310596466, 0.003814251162111759, 0.2917051911354065]\n",
      "Step 713, loss = [0.2938563823699951, 0.013069305568933487, 0.2925494611263275]\n",
      "Step 714, loss = [0.2837105691432953, 0.00857152882963419, 0.2828534245491028]\n",
      "Step 715, loss = [0.298064649105072, 0.009146519005298615, 0.29714998602867126]\n",
      "Step 716, loss = [0.2968665063381195, 0.009843348525464535, 0.2958821654319763]\n",
      "Step 717, loss = [0.30366581678390503, 0.004258688539266586, 0.30323994159698486]\n",
      "Step 718, loss = [0.3084758520126343, 0.008573673665523529, 0.30761849880218506]\n",
      "Step 719, loss = [0.31248536705970764, 0.008381928317248821, 0.3116471767425537]\n",
      "Step 720, loss = [0.2985434830188751, 0.003142767585813999, 0.2982292175292969]\n",
      "Step 721, loss = [0.3016567528247833, 0.007472017779946327, 0.30090954899787903]\n",
      "Step 722, loss = [0.2830776274204254, 0.00814024731516838, 0.2822636067867279]\n",
      "Step 723, loss = [0.3079456686973572, 0.010217472910881042, 0.30692392587661743]\n",
      "Step 724, loss = [0.3101904094219208, 0.0034286631271243095, 0.30984753370285034]\n",
      "Step 725, loss = [0.3132232427597046, 0.0047055939212441444, 0.31275269389152527]\n",
      "Step 726, loss = [0.2989749014377594, 0.004603588953614235, 0.29851454496383667]\n",
      "Step 727, loss = [0.30341628193855286, 0.004679957404732704, 0.3029482960700989]\n",
      "Step 728, loss = [0.30860960483551025, 0.005056161433458328, 0.3081039786338806]\n",
      "Step 729, loss = [0.30591142177581787, 0.007726756855845451, 0.3051387369632721]\n",
      "Step 730, loss = [0.30861297249794006, 0.004882374778389931, 0.30812472105026245]\n",
      "Step 731, loss = [0.30654650926589966, 0.002102385275065899, 0.30633628368377686]\n",
      "Step 732, loss = [0.31280630826950073, 0.0073775313794612885, 0.31206855177879333]\n",
      "Step 733, loss = [0.28320130705833435, 0.01014641486108303, 0.2821866571903229]\n",
      "Step 734, loss = [0.30929648876190186, 0.010002091526985168, 0.3082962930202484]\n",
      "Step 735, loss = [0.3097790479660034, 0.00862125027924776, 0.30891692638397217]\n",
      "Step 736, loss = [0.29324549436569214, 0.016642142087221146, 0.29158127307891846]\n",
      "Step 737, loss = [0.3075321614742279, 0.00875523779541254, 0.3066566288471222]\n",
      "Step 738, loss = [0.30103468894958496, 0.0101123983040452, 0.3000234365463257]\n",
      "Step 739, loss = [0.30547034740448, 0.0048134708777070045, 0.3049890100955963]\n",
      "Step 740, loss = [0.296691358089447, 0.005809271242469549, 0.2961104214191437]\n",
      "Step 741, loss = [0.29011380672454834, 0.009052911773324013, 0.2892085015773773]\n",
      "Step 742, loss = [0.3020102083683014, 0.006191213149577379, 0.3013910949230194]\n",
      "Step 743, loss = [0.29905983805656433, 0.009175149723887444, 0.29814231395721436]\n",
      "Step 744, loss = [0.28760457038879395, 0.0038025113753974438, 0.28722432255744934]\n",
      "Step 745, loss = [0.31573912501335144, 0.00485934829339385, 0.31525319814682007]\n",
      "Step 746, loss = [0.30040204524993896, 0.008894341997802258, 0.2995126247406006]\n",
      "Step 747, loss = [0.3040238916873932, 0.006939711049199104, 0.3033299148082733]\n",
      "Step 748, loss = [0.29524797201156616, 0.009036277420818806, 0.29434433579444885]\n",
      "Step 749, loss = [0.29296010732650757, 0.006595735438168049, 0.2923005223274231]\n",
      "Step 750, loss = [0.3087371587753296, 0.006565021816641092, 0.30808064341545105]\n",
      "Step 751, loss = [0.2875969111919403, 0.00869382917881012, 0.28672751784324646]\n",
      "Step 752, loss = [0.2764196991920471, 0.011254698969423771, 0.2752942144870758]\n",
      "Step 753, loss = [0.28235912322998047, 0.005544126033782959, 0.2818047106266022]\n",
      "Step 754, loss = [0.30967259407043457, 0.01080338004976511, 0.3085922598838806]\n",
      "Step 755, loss = [0.2806501090526581, 0.012371491640806198, 0.2794129550457001]\n",
      "Step 756, loss = [0.2987002432346344, 0.01072144228965044, 0.29762810468673706]\n",
      "Step 757, loss = [0.2921198010444641, 0.005607932340353727, 0.29155901074409485]\n",
      "Step 758, loss = [0.310591459274292, 0.024514079093933105, 0.3081400394439697]\n",
      "Step 759, loss = [0.3085845112800598, 0.006041011773049831, 0.30798041820526123]\n",
      "Step 760, loss = [0.3008534014225006, 0.008255600929260254, 0.30002784729003906]\n",
      "Step 761, loss = [0.2860781252384186, 0.006194847170263529, 0.28545865416526794]\n",
      "Step 762, loss = [0.3022901117801666, 0.005016198847442865, 0.30178847908973694]\n",
      "Step 763, loss = [0.3101734220981598, 0.008430873975157738, 0.3093303442001343]\n",
      "Step 764, loss = [0.29532232880592346, 0.004844245500862598, 0.2948378920555115]\n",
      "Step 765, loss = [0.3119106590747833, 0.008222571574151516, 0.3110884130001068]\n",
      "Step 766, loss = [0.2989553213119507, 0.005890959873795509, 0.2983662188053131]\n",
      "Step 767, loss = [0.29523420333862305, 0.00547318160533905, 0.294686883687973]\n",
      "Step 768, loss = [0.2850957214832306, 0.009483527392148972, 0.28414738178253174]\n",
      "Step 769, loss = [0.29670578241348267, 0.0084496159106493, 0.29586082696914673]\n",
      "Step 770, loss = [0.3090147376060486, 0.006156236864626408, 0.30839911103248596]\n",
      "Step 771, loss = [0.28514012694358826, 0.008671633899211884, 0.2842729687690735]\n",
      "Step 772, loss = [0.2955973446369171, 0.006960762664675713, 0.2949012815952301]\n",
      "Step 773, loss = [0.3032180368900299, 0.01263564545661211, 0.3019544780254364]\n",
      "Step 774, loss = [0.2919943332672119, 0.004865242168307304, 0.2915078103542328]\n",
      "Step 775, loss = [0.30711525678634644, 0.008259113878011703, 0.30628934502601624]\n",
      "Step 776, loss = [0.29913821816444397, 0.006470737978816032, 0.29849115014076233]\n",
      "Step 777, loss = [0.2913638949394226, 0.005772298667579889, 0.29078665375709534]\n",
      "Step 778, loss = [0.29561886191368103, 0.008351277559995651, 0.29478374123573303]\n",
      "Step 779, loss = [0.30458611249923706, 0.0074706547893583775, 0.3038390576839447]\n",
      "Step 780, loss = [0.31036821007728577, 0.005172999110072851, 0.30985090136528015]\n",
      "Step 781, loss = [0.30638962984085083, 0.005246592685580254, 0.30586495995521545]\n",
      "Step 782, loss = [0.27736684679985046, 0.006842689588665962, 0.276682585477829]\n",
      "Step 783, loss = [0.30881112813949585, 0.007416111882776022, 0.30806952714920044]\n",
      "Step 784, loss = [0.2952059209346771, 0.008446944877505302, 0.2943612337112427]\n",
      "Step 785, loss = [0.296321302652359, 0.008213507942855358, 0.2954999506473541]\n",
      "Step 786, loss = [0.2929001748561859, 0.0072042858228087425, 0.29217973351478577]\n",
      "Step 787, loss = [0.29804620146751404, 0.0024743624962866306, 0.297798752784729]\n",
      "Step 788, loss = [0.303290456533432, 0.0042437417432665825, 0.3028660714626312]\n",
      "Step 789, loss = [0.29745474457740784, 0.009701509028673172, 0.2964845895767212]\n",
      "Step 790, loss = [0.3086841106414795, 0.002652824390679598, 0.3084188401699066]\n",
      "Step 791, loss = [0.3127387762069702, 0.002509668003767729, 0.31248781085014343]\n",
      "Step 792, loss = [0.30525702238082886, 0.006554355379194021, 0.3046015799045563]\n",
      "Step 793, loss = [0.29726651310920715, 0.0064608752727508545, 0.2966204285621643]\n",
      "Step 794, loss = [0.29211992025375366, 0.011758493259549141, 0.29094406962394714]\n",
      "Step 795, loss = [0.29641881585121155, 0.006590784527361393, 0.29575973749160767]\n",
      "Step 796, loss = [0.28949588537216187, 0.0037902186159044504, 0.28911685943603516]\n",
      "Step 797, loss = [0.27813202142715454, 0.009925181046128273, 0.2771395146846771]\n",
      "Step 798, loss = [0.3031960129737854, 0.005891942884773016, 0.30260682106018066]\n",
      "Step 799, loss = [0.3097946345806122, 0.007725249044597149, 0.30902209877967834]\n",
      "Step 800, loss = [0.30656519532203674, 0.006418850738555193, 0.30592331290245056]\n",
      "Step 801, loss = [0.2880825698375702, 0.007446247152984142, 0.2873379588127136]\n",
      "Step 802, loss = [0.30859559774398804, 0.008033409714698792, 0.3077922463417053]\n",
      "Step 803, loss = [0.3166198432445526, 0.0049635861068964005, 0.31612348556518555]\n",
      "Step 804, loss = [0.30459079146385193, 0.010635806247591972, 0.30352720618247986]\n",
      "Step 805, loss = [0.3117541968822479, 0.006005666684359312, 0.3111536204814911]\n",
      "Step 806, loss = [0.2991752326488495, 0.009615322574973106, 0.2982136905193329]\n",
      "Step 807, loss = [0.28673961758613586, 0.0051939659751951694, 0.2862202227115631]\n",
      "Step 808, loss = [0.2984330654144287, 0.008877556771039963, 0.29754531383514404]\n",
      "Step 809, loss = [0.29567134380340576, 0.007559528108686209, 0.2949153780937195]\n",
      "Step 810, loss = [0.30098679661750793, 0.004134578630328178, 0.30057334899902344]\n",
      "Step 811, loss = [0.3018019497394562, 0.005730651319026947, 0.3012288808822632]\n",
      "Step 812, loss = [0.29636743664741516, 0.006535298191010952, 0.2957139015197754]\n",
      "Step 813, loss = [0.29920855164527893, 0.0034433724358677864, 0.2988642156124115]\n",
      "Step 814, loss = [0.28152990341186523, 0.004981908015906811, 0.28103169798851013]\n",
      "Step 815, loss = [0.3083343505859375, 0.005669810809195042, 0.3077673614025116]\n",
      "Step 816, loss = [0.300628662109375, 0.009281855076551437, 0.29970046877861023]\n",
      "Step 817, loss = [0.283537358045578, 0.00281727546826005, 0.2832556366920471]\n",
      "Step 818, loss = [0.3006594181060791, 0.006857954431325197, 0.29997363686561584]\n",
      "Step 819, loss = [0.3091583251953125, 0.009089055471122265, 0.30824941396713257]\n",
      "Step 820, loss = [0.29653409123420715, 0.007273191120475531, 0.29580676555633545]\n",
      "Step 821, loss = [0.2868427336215973, 0.0022606723941862583, 0.28661665320396423]\n",
      "Step 822, loss = [0.29889070987701416, 0.006831032689660788, 0.2982076108455658]\n",
      "Step 823, loss = [0.2964215874671936, 0.009779166430234909, 0.29544368386268616]\n",
      "Step 824, loss = [0.2973650395870209, 0.009388068690896034, 0.2964262366294861]\n",
      "Step 825, loss = [0.29837632179260254, 0.00923154503107071, 0.2974531650543213]\n",
      "Step 826, loss = [0.2992592453956604, 0.003066282719373703, 0.2989526093006134]\n",
      "Step 827, loss = [0.2921511232852936, 0.011226006783545017, 0.29102852940559387]\n",
      "Step 828, loss = [0.29873692989349365, 0.006017724052071571, 0.2981351613998413]\n",
      "Step 829, loss = [0.2927691340446472, 0.005233996547758579, 0.2922457456588745]\n",
      "Step 830, loss = [0.2986389398574829, 0.007628738880157471, 0.2978760600090027]\n",
      "Step 831, loss = [0.30469581484794617, 0.006333963014185429, 0.3040624260902405]\n",
      "Step 832, loss = [0.3033425807952881, 0.004895421676337719, 0.3028530478477478]\n",
      "Step 833, loss = [0.28213387727737427, 0.010076350532472134, 0.2811262309551239]\n",
      "Step 834, loss = [0.2791330814361572, 0.005430863704532385, 0.27858999371528625]\n",
      "Step 835, loss = [0.30034565925598145, 0.0040494780987501144, 0.29994070529937744]\n",
      "Step 836, loss = [0.3107621669769287, 0.008453612215816975, 0.30991679430007935]\n",
      "Step 837, loss = [0.2920943796634674, 0.00899970531463623, 0.2911944091320038]\n",
      "Step 838, loss = [0.3086966574192047, 0.007439677137881517, 0.3079527020454407]\n",
      "Step 839, loss = [0.3112218677997589, 0.006479253061115742, 0.310573935508728]\n",
      "Step 840, loss = [0.289101243019104, 0.0067448243498802185, 0.2884267568588257]\n",
      "Step 841, loss = [0.3039120137691498, 0.009849261492490768, 0.30292707681655884]\n",
      "Step 842, loss = [0.3032163083553314, 0.009246060624718666, 0.3022916913032532]\n",
      "Step 843, loss = [0.3087921738624573, 0.010250674560666084, 0.3077670931816101]\n",
      "Step 844, loss = [0.3106396496295929, 0.00241781584918499, 0.3103978633880615]\n",
      "Step 845, loss = [0.3081558644771576, 0.011461421847343445, 0.3070097267627716]\n",
      "Step 846, loss = [0.2961827218532562, 0.0027045607566833496, 0.2959122657775879]\n",
      "Step 847, loss = [0.27809441089630127, 0.00910972524434328, 0.2771834433078766]\n",
      "Step 848, loss = [0.30945348739624023, 0.006927108392119408, 0.30876076221466064]\n",
      "Step 849, loss = [0.3017502427101135, 0.0031910217367112637, 0.30143114924430847]\n",
      "Step 850, loss = [0.2940397560596466, 0.00460334587842226, 0.29357942938804626]\n",
      "Step 851, loss = [0.297550767660141, 0.007648176979273558, 0.29678595066070557]\n",
      "Step 852, loss = [0.30959421396255493, 0.007961608469486237, 0.30879804491996765]\n",
      "Step 853, loss = [0.2939300835132599, 0.012566911987960339, 0.29267337918281555]\n",
      "Step 854, loss = [0.30908340215682983, 0.004471872933208942, 0.30863621830940247]\n",
      "Step 855, loss = [0.30085569620132446, 0.006225230172276497, 0.3002331852912903]\n",
      "Step 856, loss = [0.2954886257648468, 0.00712086446583271, 0.2947765290737152]\n",
      "Step 857, loss = [0.3121799826622009, 0.011327192187309265, 0.3110472559928894]\n",
      "Step 858, loss = [0.29179224371910095, 0.005686206743121147, 0.2912236154079437]\n",
      "Step 859, loss = [0.2914584279060364, 0.0069904872216284275, 0.2907593846321106]\n",
      "Step 860, loss = [0.29416245222091675, 0.008535617962479591, 0.29330888390541077]\n",
      "Step 861, loss = [0.3151310384273529, 0.006534435786306858, 0.3144775927066803]\n",
      "Step 862, loss = [0.27452996373176575, 0.005210785195231438, 0.2740088999271393]\n",
      "Step 863, loss = [0.3128523826599121, 0.010967029258608818, 0.3117556869983673]\n",
      "Step 864, loss = [0.3099837601184845, 0.007858136668801308, 0.30919793248176575]\n",
      "Step 865, loss = [0.3026648759841919, 0.00830860249698162, 0.30183401703834534]\n",
      "Step 866, loss = [0.29444971680641174, 0.006390722002834082, 0.29381063580513]\n",
      "Step 867, loss = [0.2896917760372162, 0.007618525996804237, 0.28892990946769714]\n",
      "Step 868, loss = [0.30261746048927307, 0.007328134961426258, 0.30188465118408203]\n",
      "Step 869, loss = [0.3054419159889221, 0.003101370995864272, 0.30513179302215576]\n",
      "Step 870, loss = [0.2989446818828583, 0.005901962053030729, 0.29835447669029236]\n",
      "Step 871, loss = [0.3021414279937744, 0.0048110103234648705, 0.30166032910346985]\n",
      "Step 872, loss = [0.2917688190937042, 0.007239232771098614, 0.2910448908805847]\n",
      "Step 873, loss = [0.3024451434612274, 0.006921025924384594, 0.30175304412841797]\n",
      "Step 874, loss = [0.30066195130348206, 0.0045303828082978725, 0.3002089262008667]\n",
      "Step 875, loss = [0.29625847935676575, 0.00760760810226202, 0.29549771547317505]\n",
      "Step 876, loss = [0.30737772583961487, 0.010349389165639877, 0.3063427805900574]\n",
      "Step 877, loss = [0.29858675599098206, 0.010055478662252426, 0.29758119583129883]\n",
      "Step 878, loss = [0.2986606955528259, 0.008100423961877823, 0.297850638628006]\n",
      "Step 879, loss = [0.30332162976264954, 0.009132293984293938, 0.3024083971977234]\n",
      "Step 880, loss = [0.2937622368335724, 0.0032128035090863705, 0.29344096779823303]\n",
      "Step 881, loss = [0.30057868361473083, 0.006879217457026243, 0.29989075660705566]\n",
      "Step 882, loss = [0.3013840913772583, 0.00871757697314024, 0.3005123436450958]\n",
      "Step 883, loss = [0.3084403872489929, 0.006776384077966213, 0.3077627420425415]\n",
      "Step 884, loss = [0.2913017272949219, 0.0068910024128854275, 0.2906126379966736]\n",
      "Step 885, loss = [0.29461365938186646, 0.0034780276473611593, 0.29426586627960205]\n",
      "Step 886, loss = [0.30238601565361023, 0.003814161289483309, 0.3020046055316925]\n",
      "Step 887, loss = [0.28990235924720764, 0.00877766590565443, 0.28902459144592285]\n",
      "Step 888, loss = [0.29392844438552856, 0.011733983643352985, 0.29275503754615784]\n",
      "Step 889, loss = [0.30422934889793396, 0.007323085330426693, 0.3034970462322235]\n",
      "Step 890, loss = [0.2946230471134186, 0.004506752826273441, 0.29417237639427185]\n",
      "Step 891, loss = [0.30064845085144043, 0.007465634495019913, 0.2999018728733063]\n",
      "Step 892, loss = [0.3104015588760376, 0.012618748471140862, 0.3091396987438202]\n",
      "Step 893, loss = [0.29781290888786316, 0.008604169823229313, 0.296952486038208]\n",
      "Step 894, loss = [0.29423871636390686, 0.004035591147840023, 0.2938351631164551]\n",
      "Step 895, loss = [0.29728710651397705, 0.007434401195496321, 0.2965436577796936]\n",
      "Step 896, loss = [0.30568012595176697, 0.003940113820135593, 0.30528610944747925]\n",
      "Step 897, loss = [0.27752214670181274, 0.007922333665192127, 0.27672991156578064]\n",
      "Step 898, loss = [0.29748964309692383, 0.004589935764670372, 0.29703065752983093]\n",
      "Step 899, loss = [0.30301064252853394, 0.006985545624047518, 0.30231207609176636]\n",
      "Step 900, loss = [0.2937023639678955, 0.0035778896417468786, 0.2933445870876312]\n",
      "Step 901, loss = [0.3028803765773773, 0.013533330522477627, 0.3015270531177521]\n",
      "Step 902, loss = [0.31133466958999634, 0.007165984250605106, 0.3106180727481842]\n",
      "Step 903, loss = [0.2959338128566742, 0.005277452990412712, 0.2954060733318329]\n",
      "Step 904, loss = [0.29755860567092896, 0.009614463895559311, 0.2965971529483795]\n",
      "Step 905, loss = [0.29569512605667114, 0.003925387281924486, 0.2953025996685028]\n",
      "Step 906, loss = [0.29698094725608826, 0.005423881579190493, 0.2964385449886322]\n",
      "Step 907, loss = [0.3060935139656067, 0.003965391777455807, 0.305696964263916]\n",
      "Step 908, loss = [0.2980172038078308, 0.005170144140720367, 0.2975001931190491]\n",
      "Step 909, loss = [0.2891925573348999, 0.012661973014473915, 0.2879263460636139]\n",
      "Step 910, loss = [0.30267348885536194, 0.006338965147733688, 0.30203959345817566]\n",
      "Step 911, loss = [0.2928224503993988, 0.005258065648376942, 0.2922966480255127]\n",
      "Step 912, loss = [0.29441338777542114, 0.011470352299511433, 0.2932663559913635]\n",
      "Step 913, loss = [0.2986741364002228, 0.003990671597421169, 0.29827508330345154]\n",
      "Step 914, loss = [0.28928568959236145, 0.003794532734900713, 0.2889062464237213]\n",
      "Step 915, loss = [0.27398934960365295, 0.0072643267922103405, 0.2732629179954529]\n",
      "Step 916, loss = [0.300348162651062, 0.0084450114518404, 0.2995036542415619]\n",
      "Step 917, loss = [0.2959980070590973, 0.004554504528641701, 0.29554256796836853]\n",
      "Step 918, loss = [0.2929248809814453, 0.009089713916182518, 0.2920159101486206]\n",
      "Step 919, loss = [0.2944248914718628, 0.004181773401796818, 0.29400670528411865]\n",
      "Step 920, loss = [0.3006666898727417, 0.004087022505700588, 0.30025798082351685]\n",
      "Step 921, loss = [0.2988680601119995, 0.005345663987100124, 0.2983334958553314]\n",
      "Step 922, loss = [0.29761195182800293, 0.004452222492545843, 0.29716673493385315]\n",
      "Step 923, loss = [0.2960215210914612, 0.008019881322979927, 0.2952195405960083]\n",
      "Step 924, loss = [0.29053863883018494, 0.006155910901725292, 0.2899230420589447]\n",
      "Step 925, loss = [0.28563687205314636, 0.006329920142889023, 0.2850038707256317]\n",
      "Step 926, loss = [0.3038365840911865, 0.005459010135382414, 0.3032906949520111]\n",
      "Step 927, loss = [0.3059934079647064, 0.005723283160477877, 0.3054210841655731]\n",
      "Step 928, loss = [0.2771269381046295, 0.00864854734390974, 0.2762620747089386]\n",
      "Step 929, loss = [0.3082673251628876, 0.006047810427844524, 0.3076625466346741]\n",
      "Step 930, loss = [0.30728256702423096, 0.008850367739796638, 0.30639752745628357]\n",
      "Step 931, loss = [0.30818307399749756, 0.006052256096154451, 0.30757784843444824]\n",
      "Step 932, loss = [0.29629775881767273, 0.007116225082427263, 0.29558613896369934]\n",
      "Step 933, loss = [0.302681565284729, 0.0070617785677313805, 0.30197539925575256]\n",
      "Step 934, loss = [0.29570549726486206, 0.003954439423978329, 0.29531005024909973]\n",
      "Step 935, loss = [0.30774980783462524, 0.0032844196539372206, 0.30742135643959045]\n",
      "Step 936, loss = [0.3079914152622223, 0.007340597920119762, 0.30725735425949097]\n",
      "Step 937, loss = [0.29979002475738525, 0.006873294245451689, 0.29910269379615784]\n",
      "Step 938, loss = [0.2936268150806427, 0.010798700153827667, 0.29254695773124695]\n",
      "Step 939, loss = [0.30495572090148926, 0.0039369892328977585, 0.3045620322227478]\n",
      "Step 940, loss = [0.3084128201007843, 0.006233568768948317, 0.30778947472572327]\n",
      "Step 941, loss = [0.2957379221916199, 0.008661335334181786, 0.2948717772960663]\n",
      "Step 942, loss = [0.30826887488365173, 0.0031419582664966583, 0.30795466899871826]\n",
      "Step 943, loss = [0.2912472188472748, 0.004059337079524994, 0.290841281414032]\n",
      "Step 944, loss = [0.3010638952255249, 0.006584138609468937, 0.30040547251701355]\n",
      "Step 945, loss = [0.2992681860923767, 0.005626079626381397, 0.2987055778503418]\n",
      "Step 946, loss = [0.29048627614974976, 0.00838562287390232, 0.2896477282047272]\n",
      "Step 947, loss = [0.3040061295032501, 0.008589351549744606, 0.30314719676971436]\n",
      "Step 948, loss = [0.2940789759159088, 0.006295773200690746, 0.29344940185546875]\n",
      "Step 949, loss = [0.30606767535209656, 0.005728536285459995, 0.3054948151111603]\n",
      "Step 950, loss = [0.31240949034690857, 0.0058087920770049095, 0.31182861328125]\n",
      "Step 951, loss = [0.2994841933250427, 0.005474013276398182, 0.29893678426742554]\n",
      "Step 952, loss = [0.30217310786247253, 0.005133762024343014, 0.3016597330570221]\n",
      "Step 953, loss = [0.3088420331478119, 0.007425849791616201, 0.3080994486808777]\n",
      "Step 954, loss = [0.29108357429504395, 0.006599226966500282, 0.2904236614704132]\n",
      "Step 955, loss = [0.3057559132575989, 0.008553902618587017, 0.30490052700042725]\n",
      "Step 956, loss = [0.3135806918144226, 0.004136350005865097, 0.3131670653820038]\n",
      "Step 957, loss = [0.29962652921676636, 0.010114534758031368, 0.29861506819725037]\n",
      "Step 958, loss = [0.30609604716300964, 0.007268640212714672, 0.30536916851997375]\n",
      "Step 959, loss = [0.30049630999565125, 0.011174161918461323, 0.299378901720047]\n",
      "Step 960, loss = [0.3039310872554779, 0.004323235712945461, 0.303498774766922]\n",
      "Step 961, loss = [0.297229528427124, 0.005847569089382887, 0.29664477705955505]\n",
      "Step 962, loss = [0.3033459484577179, 0.004061427898705006, 0.3029398024082184]\n",
      "Step 963, loss = [0.3100290894508362, 0.009420646354556084, 0.30908703804016113]\n",
      "Step 964, loss = [0.30141058564186096, 0.003201241372153163, 0.30109044909477234]\n",
      "Step 965, loss = [0.3019334673881531, 0.006073269061744213, 0.30132612586021423]\n",
      "Step 966, loss = [0.2837372124195099, 0.0067917052656412125, 0.2830580472946167]\n",
      "Step 967, loss = [0.30725833773612976, 0.00391440000385046, 0.3068668842315674]\n",
      "Step 968, loss = [0.3130927085876465, 0.00831905659288168, 0.31226080656051636]\n",
      "Step 969, loss = [0.2848053276538849, 0.005201166495680809, 0.28428521752357483]\n",
      "Step 970, loss = [0.3033931851387024, 0.007877837866544724, 0.30260539054870605]\n",
      "Step 971, loss = [0.2985726594924927, 0.00833466462790966, 0.2977392077445984]\n",
      "Step 972, loss = [0.29471540451049805, 0.008131168782711029, 0.29390227794647217]\n",
      "Step 973, loss = [0.30740150809288025, 0.011234106495976448, 0.3062781095504761]\n",
      "Step 974, loss = [0.30538079142570496, 0.009546959772706032, 0.3044261038303375]\n",
      "Step 975, loss = [0.3013017177581787, 0.004665517248213291, 0.30083516240119934]\n",
      "Step 976, loss = [0.2932550311088562, 0.006673465017229319, 0.2925876975059509]\n",
      "Step 977, loss = [0.2918670177459717, 0.0036768317222595215, 0.2914993464946747]\n",
      "Update target distribution epoch 3 step 978"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11754 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 11754/11754 [1:52:18<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos. rate:0.44509803921568625 Neg. rate:0.5549019607843138\n",
      "delta_label  0.008677896886166412\n",
      "Step 978, loss = [0.3048257529735565, 0.012576408684253693, 0.30356812477111816]\n",
      "Step 979, loss = [0.30603262782096863, 0.004208499565720558, 0.305611789226532]\n",
      "Step 980, loss = [0.3070702850818634, 0.006862865295261145, 0.30638399720191956]\n",
      "Step 981, loss = [0.29435864090919495, 0.01050366647541523, 0.293308287858963]\n",
      "Step 982, loss = [0.3121262192726135, 0.004766902420669794, 0.31164953112602234]\n",
      "Step 983, loss = [0.30701321363449097, 0.010291334241628647, 0.3059840798377991]\n",
      "Step 984, loss = [0.2967810332775116, 0.004121347330510616, 0.29636889696121216]\n",
      "Step 985, loss = [0.30495697259902954, 0.010938793420791626, 0.3038631081581116]\n",
      "Step 986, loss = [0.29762759804725647, 0.008408615365624428, 0.29678672552108765]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Failed to allocate memory for the batch of component 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[1;34m(mode)\u001b[0m\n\u001b[0;32m   1985\u001b[0m       \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1986\u001b[1;33m       \u001b[1;32myield\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1987\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[1;31m# handles execute on the same device as where the resource is placed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 652\u001b[1;33m         ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[0;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6652\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6653\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6654\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Failed to allocate memory for the batch of component 0 [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-4b820df9d727>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mstep_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mpo_neg_rates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mupdate_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'00_Data/q_distrib/q_{0}_{1}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# For Python 3 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 631\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[1;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    659\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 661\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    662\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# Suppress StopIteration *unless* it's the same exception that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[1;34m(mode)\u001b[0m\n\u001b[0;32m   1987\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1988\u001b[0m       \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1989\u001b[1;33m       \u001b[0mexecutor_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1990\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\00_data\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\executor.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mpywrap_tfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mclear_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Failed to allocate memory for the batch of component 0"
     ]
    }
   ],
   "source": [
    "# with tf.device('/GPU:0'):\n",
    "epochs = 200\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    step_losses = []\n",
    "    po_neg_rates = []\n",
    "    for step, (x_batch, idx) in enumerate(ds_train):\n",
    "        if step % update_interval == 0:\n",
    "            if os.path.exists('00_Data/q_distrib/q_{0}_{1}.csv'.format(epoch, step)):\n",
    "                q = pd.read_csv('00_Data/q_distrib/q_{0}_{1}.csv'.format(epoch, step), index_col=0, header=None)\n",
    "                if os.path.exists('00_Data/p_distrib/p_{0}_{1}.csv'.format(epoch, step)):\n",
    "                    p = pd.read_csv('00_Data/p_distrib/p_{0}_{1}.csv'.format(epoch, step), index_col=0, header=None)\n",
    "                else:\n",
    "                    p = target_distribution(q.values)  # update the auxiliary target distribution p\n",
    "                    p = pd.DataFrame(p, index=q.index.to_list())\n",
    "                    p.to_csv('00_Data/p_distrib/p_{0}_{1}.csv'.format(epoch, step), index=True, header=False)\n",
    "                    p = pd.read_csv('00_Data/p_distrib/p_{0}_{1}.csv'.format(epoch, step), index_col=0, header=None)\n",
    "#             if step==0:\n",
    "#                 q = pd.read_csv('00_Data/q_distrib/q_0_0.csv', index_col=0, header=None)\n",
    "#                 p = pd.read_csv('00_Data/p_distrib/p_0_0.csv', index_col=0, header=None)\n",
    "            else:\n",
    "                print('Update target distribution epoch {0} step {1}'.format(epoch, step))\n",
    "                q = {}\n",
    "                with tqdm(total=len(ALL_IDS)) as pbar:\n",
    "                    for i in ALL_IDS:\n",
    "                        x, _ = get_inputs(i)\n",
    "                        x = x.reshape(1,52, 66, 56, 53)\n",
    "                        preds, _ = model.predict(x, batch_size=1, verbose=0)\n",
    "                        q[i] = preds[0]\n",
    "                        pbar.update(1)\n",
    "                q = pd.DataFrame(q).T\n",
    "                q.to_csv('00_Data/q_distrib/q_{0}_{1}.csv'.format(epoch, step), index=True, header=False)\n",
    "                q = pd.read_csv('00_Data/q_distrib/q_{0}_{1}.csv'.format(epoch, step), index_col=0, header=None)\n",
    "                p = target_distribution(q.values)  # update the auxiliary target distribution p\n",
    "                p = pd.DataFrame(p, index=q.index.to_list())\n",
    "                p.to_csv('00_Data/p_distrib/p_{0}_{1}.csv'.format(epoch, step), index=True, header=False)\n",
    "                p = pd.read_csv('00_Data/p_distrib/p_{0}_{1}.csv'.format(epoch, step), index_col=0, header=None)\n",
    "\n",
    "            # evaluate the clustering performance\n",
    "#             display(q)\n",
    "#             print(q.index)\n",
    "            IDS_S2 = pd.read_csv('00_Data/reveal_ID_site2_int.csv', index_col=0, header=None, squeeze=True, dtype=np.int64)\n",
    "            y_pred = np.argmax(q.values, axis=1)\n",
    "            y_pred_s2 = np.argmax(q.loc[IDS_S2].values, axis=1)\n",
    "            pos_rate = len(y_pred_s2[np.where(y_pred_s2==1)]) / len(y_pred_s2)\n",
    "            neg_rate = len(y_pred_s2[np.where(y_pred_s2==0)])/len(y_pred_s2)\n",
    "            po_neg_rates.append([pos_rate, neg_rate])\n",
    "            print('Pos. rate:{0} Neg. rate:{1}'.format(pos_rate, neg_rate))\n",
    "\n",
    "            # check stop criterion\n",
    "            delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "            print('delta_label ', delta_label)\n",
    "            y_pred_last = np.copy(y_pred)\n",
    "            if step > 0 and delta_label < tol:\n",
    "                print('delta_label ', delta_label, '< tol ', tol)\n",
    "                print('Reached tolerance threshold. Stopping training.')\n",
    "                break\n",
    "                \n",
    "        y_probs = p.loc[idx]\n",
    "        y_probs = y_probs.values\n",
    "        loss = model.train_on_batch(x=x_batch, y=[y_probs, x_batch])\n",
    "        step_losses.append(loss)\n",
    "        print('Step {0}, loss = {1}'.format(step, loss))\n",
    "        \n",
    "    epoch_avloss = np.mean(np.array(step_losses), axis=0)\n",
    "    print('Epoch {0}, loss = {1}'.format(epoch, epoch_avloss))\n",
    "    losses.append(epoch_avloss)\n",
    "    model.save_weights('99_Training_checkpoints/mri_clustering/run_05/model_weights_chpt_{0}.h5'.format(epoch))\n",
    "\n",
    "np.savetxt('99_Logs/mri_clustering/run_05/pos_neg_rates.csv', np.array(po_neg_rates), delimiter=',')\n",
    "np.savetxt('99_Logs/mri_clustering/run_05/losses.csv', np.array(losses), delimiter=',')\n",
    "model.save_weights('99_Training_checkpoints/mri_clustering/run_05/model_weights_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 2\n",
    "# for epoch in range(epochs):\n",
    "#     print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "#     # Iterate over the batches of the dataset.\n",
    "#     for step, (x_batch_train, y_batch_train) in enumerate(ds_train):\n",
    "\n",
    "#         # Open a GradientTape to record the operations run\n",
    "#         # during the forward pass, which enables autodifferentiation.\n",
    "#         with tf.GradientTape() as tape:\n",
    "\n",
    "#             # Run the forward pass of the layer.\n",
    "#             # The operations that the layer applies\n",
    "#             # to its inputs are going to be recorded\n",
    "#             # on the GradientTape.\n",
    "#             logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "#             # Compute the loss value for this minibatch.\n",
    "#             loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "#         # Use the gradient tape to automatically retrieve\n",
    "#         # the gradients of the trainable variables with respect to the loss.\n",
    "#         grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "#         # Run one step of gradient descent by updating\n",
    "#         # the value of the variables to minimize the loss.\n",
    "#         optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "#         # Log every 200 batches.\n",
    "#         if step % 200 == 0:\n",
    "#             print(\n",
    "#                 \"Training loss (for one batch) at step %d: %.4f\"\n",
    "#                 % (step, float(loss_value))\n",
    "#             )\n",
    "#             print(\"Seen so far: %s samples\" % ((step + 1) * 64))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
