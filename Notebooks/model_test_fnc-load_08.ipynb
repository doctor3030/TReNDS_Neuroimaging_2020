{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "\n",
    "import scipy.stats as sts\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing as prep\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn import discriminant_analysis as disan\n",
    "from sklearn import calibration as calib\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn import svm\n",
    "from sklearn import gaussian_process as gaup\n",
    "from sklearn import mixture as mix\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble as ens\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# from keras import models as kermdls\n",
    "# from keras import layers as kerlrs\n",
    "# from keras import metrics as kmetrics\n",
    "\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "import pickle\n",
    "\n",
    "import nilearn as nl\n",
    "from nilearn import plotting, image\n",
    "from nilearn import datasets\n",
    "import nibabel as nb\n",
    "import h5py\n",
    "\n",
    "import os\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12543894849852857618\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16055186708134456450\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6589725830\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11014372103137603005\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2219115992518979498\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fnc_10 = pd.read_csv('00_Data/fnc.csv')\n",
    "# fnc_10 = fnc_10.head(5)\n",
    "# for row in fnc_10.iterrows():\n",
    "#     idx = int(row[1][0])\n",
    "#     row = row[1][1:]\n",
    "#     print(row)\n",
    "#     row.to_csv('00_Data/fnc_csv_norm/{0}.csv'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_test'))]\n",
    "TRAIN_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_train'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>age</th>\n",
       "      <th>domain1_var1</th>\n",
       "      <th>domain1_var2</th>\n",
       "      <th>domain2_var1</th>\n",
       "      <th>domain2_var2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>57.436077</td>\n",
       "      <td>30.571975</td>\n",
       "      <td>62.553736</td>\n",
       "      <td>53.325130</td>\n",
       "      <td>51.427998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>59.580851</td>\n",
       "      <td>50.969456</td>\n",
       "      <td>67.470628</td>\n",
       "      <td>60.651856</td>\n",
       "      <td>58.311361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10004</td>\n",
       "      <td>71.413018</td>\n",
       "      <td>53.152498</td>\n",
       "      <td>58.012103</td>\n",
       "      <td>52.418389</td>\n",
       "      <td>62.536641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10005</td>\n",
       "      <td>66.532630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.108977</td>\n",
       "      <td>69.993075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10007</td>\n",
       "      <td>38.617381</td>\n",
       "      <td>49.197021</td>\n",
       "      <td>65.674285</td>\n",
       "      <td>40.151376</td>\n",
       "      <td>34.096421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>21746</td>\n",
       "      <td>14.257265</td>\n",
       "      <td>21.358872</td>\n",
       "      <td>61.165998</td>\n",
       "      <td>51.778483</td>\n",
       "      <td>54.640179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>21747</td>\n",
       "      <td>55.456978</td>\n",
       "      <td>68.169675</td>\n",
       "      <td>29.907995</td>\n",
       "      <td>55.349257</td>\n",
       "      <td>54.019517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>21750</td>\n",
       "      <td>48.948756</td>\n",
       "      <td>55.114811</td>\n",
       "      <td>60.878271</td>\n",
       "      <td>38.617246</td>\n",
       "      <td>50.679885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>21752</td>\n",
       "      <td>66.532630</td>\n",
       "      <td>59.844808</td>\n",
       "      <td>72.303110</td>\n",
       "      <td>55.458281</td>\n",
       "      <td>46.870235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>21754</td>\n",
       "      <td>68.820928</td>\n",
       "      <td>56.594193</td>\n",
       "      <td>34.605868</td>\n",
       "      <td>49.922535</td>\n",
       "      <td>50.383078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5877 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id        age  domain1_var1  domain1_var2  domain2_var1  domain2_var2\n",
       "0     10001  57.436077     30.571975     62.553736     53.325130     51.427998\n",
       "1     10002  59.580851     50.969456     67.470628     60.651856     58.311361\n",
       "2     10004  71.413018     53.152498     58.012103     52.418389     62.536641\n",
       "3     10005  66.532630           NaN           NaN     52.108977     69.993075\n",
       "4     10007  38.617381     49.197021     65.674285     40.151376     34.096421\n",
       "...     ...        ...           ...           ...           ...           ...\n",
       "5872  21746  14.257265     21.358872     61.165998     51.778483     54.640179\n",
       "5873  21747  55.456978     68.169675     29.907995     55.349257     54.019517\n",
       "5874  21750  48.948756     55.114811     60.878271     38.617246     50.679885\n",
       "5875  21752  66.532630     59.844808     72.303110     55.458281     46.870235\n",
       "5876  21754  68.820928     56.594193     34.605868     49.922535     50.383078\n",
       "\n",
       "[5877 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('00_Data/train_scores.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                0\n",
       "age               0\n",
       "domain1_var1    438\n",
       "domain1_var2    438\n",
       "domain2_var1     39\n",
       "domain2_var2     39\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nulls = data.isnull().sum()\n",
    "# l = len(data.index)\n",
    "\n",
    "# nulls['domain1_var1'] / l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length:  5434\n"
     ]
    }
   ],
   "source": [
    "print('Dataset length: ', len(data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_inputs_fnc(idx, labels):\n",
    "#     df = pd.read_csv('00_Data/fnc_csv/{0}.csv'.format(idx), index_col=0)\n",
    "#     X = np.array(df.values).reshape(-1)\n",
    "#     return X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_inputs_loading(idx, labels):\n",
    "#     df = pd.read_csv('00_Data/loading_csv/{0}.csv'.format(idx), index_col=0)\n",
    "#     X = np.array(df.values).reshape(-1)\n",
    "#     return X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(idx, labels):\n",
    "    df_fnc = pd.read_csv('00_Data/fnc_csv/{0}.csv'.format(idx), index_col=0)\n",
    "    X_fnc = np.array(df_fnc.values).reshape(-1)\n",
    "    \n",
    "    df_loading = pd.read_csv('00_Data/loading_csv/{0}.csv'.format(idx), index_col=0)\n",
    "    X_loading = np.array(df_loading.values).reshape(-1)\n",
    "\n",
    "    X = np.concatenate((X_fnc, X_loading))\n",
    "#     print(len(X))\n",
    "    return X, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_decorator(func):\n",
    "    def wrapper(idx, labels):\n",
    "        # Use a tf.py_function to prevent auto-graph from compiling the method\n",
    "        return tf.py_function(func,\n",
    "                              inp=(idx, labels),\n",
    "                              Tout=tf.float64)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_py_function(func, inp, Tout, name=None):\n",
    "    \n",
    "    def wrapped_func(*flat_inp):\n",
    "        reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp,\n",
    "                                                     expand_composites=True)\n",
    "        out = func(*reconstructed_inp)\n",
    "        return tf.nest.flatten(out, expand_composites=True)\n",
    "    \n",
    "    flat_Tout = tf.nest.flatten(Tout, expand_composites=True)\n",
    "    flat_out = tf.py_function(func=wrapped_func, \n",
    "                              inp=tf.nest.flatten(inp, expand_composites=True),\n",
    "                              Tout=[_tensor_spec_to_dtype(v) for v in flat_Tout],\n",
    "                              name=name)\n",
    "    spec_out = tf.nest.map_structure(_dtype_to_tensor_spec, Tout, expand_composites=True)\n",
    "    out = tf.nest.pack_sequence_as(spec_out, flat_out, expand_composites=True)\n",
    "    return out\n",
    "\n",
    "def _dtype_to_tensor_spec(v):\n",
    "    return tf.TensorSpec(None, v) if isinstance(v, tf.dtypes.DType) else v\n",
    "\n",
    "def _tensor_spec_to_dtype(v):\n",
    "    return v.dtype if isinstance(v, tf.TensorSpec) else v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_dataset(data, batch_size):\n",
    "#     data = tf.data.Dataset.from_tensor_slices((data['Id'].values, \n",
    "#                                                data[['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']].values))\n",
    "#     data = data.shuffle(buffer_size=5500, seed=30, reshuffle_each_iteration=True)\n",
    "    \n",
    "#     data_fnc = data.map(map_decorator(get_inputs_fnc), \n",
    "#                      num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "#                      deterministic=True)\n",
    "#     data_loading = data.map(map_decorator(get_inputs_loading), \n",
    "#                      num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "#                      deterministic=True)\n",
    "\n",
    "#     data_fnc = data_fnc.batch(batch_size, drop_remainder=True)\n",
    "#     data_fnc = data_fnc.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "#     data_loading = data_loading.batch(batch_size, drop_remainder=True)\n",
    "#     data_loading = data_loading.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#     return (data_fnc, data_loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data, batch_size):\n",
    "    data = tf.data.Dataset.from_tensor_slices((data['Id'].values, \n",
    "                                               data[['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']].values))\n",
    "    data = data.shuffle(buffer_size=5500, seed=30, reshuffle_each_iteration=True)\n",
    "    \n",
    "    data = data.map(lambda idx, lbl:new_py_function(get_inputs, inp=(idx, lbl), Tout=(tf.float64, tf.float64), name=None), \n",
    "                     num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "                     deterministic=True)\n",
    "    data = data.batch(batch_size, drop_remainder=True)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = model_selection.train_test_split(data, test_size=0.2, shuffle=True, random_state=30)\n",
    "train, val = model_selection.train_test_split(train, test_size=0.2, shuffle=True, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "\n",
    "# ds_train_fnc, ds_train_loading = get_dataset(train, batch_size)\n",
    "# ds_val_fnc, ds_val_loading = get_dataset(val, batch_size)\n",
    "# ds_test_fnc, ds_test_loading = get_dataset(test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "ds_train = get_dataset(train, batch_size)\n",
    "ds_val = get_dataset(val, batch_size)\n",
    "ds_test = get_dataset(test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_time = time.perf_counter()\n",
    "# for f in ds_train.take(1):\n",
    "#     pass\n",
    "# tf.print(\"Execution time:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE_fnc = (1378,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE_loading = (26,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (1404,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input(shape=INPUT_SHAPE, name='inp_fnc')\n",
    "\n",
    "x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                          beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                          moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                          beta_constraint=None, gamma_constraint=None)(inputs)\n",
    "x = keras.layers.Dense(2048,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(x)\n",
    "x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(x)\n",
    "# x = keras.layers.Dropout(rate=0.2, seed=30)(x)\n",
    "\n",
    "x1 = keras.layers.Dense(512,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(x)\n",
    "x1 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(x1)\n",
    "x1 = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                          beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                          moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                          beta_constraint=None, gamma_constraint=None)(x1)\n",
    "\n",
    "x2 = keras.layers.Dense(512,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(x)\n",
    "x2 = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(x2)\n",
    "x2 = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                          beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                          moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                          beta_constraint=None, gamma_constraint=None)(x2)\n",
    "\n",
    "x = keras.layers.concatenate([x1, x2])\n",
    "\n",
    "# x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "#                                           beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "#                                           moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "#                                           beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "x = keras.layers.Dense(256,\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(x)\n",
    "x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(x)\n",
    "# x = keras.layers.Dropout(rate=0.2, seed=30)(x)\n",
    "\n",
    "# output\n",
    "outputs = keras.layers.Dense(5, activation='linear')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='model_combined')\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_combined\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inp_fnc (InputLayer)            [(None, 1404)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1404)         5616        inp_fnc[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 2048)         2877440     batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu (PReLU)                 (None, 2048)         2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          1049088     p_re_lu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          1049088     p_re_lu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_1 (PReLU)               (None, 512)          512         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_2 (PReLU)               (None, 512)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 512)          2048        p_re_lu_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 512)          2048        p_re_lu_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1024)         0           batch_normalization_1[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          262400      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu_3 (PReLU)               (None, 256)          256         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 5)            1285        p_re_lu_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,252,341\n",
      "Trainable params: 5,247,485\n",
      "Non-trainable params: 4,856\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim = keras.optimizers.Adam(lr=0.000001,\n",
    "#                                  beta_1=0.99,\n",
    "#                                  beta_2=0.999,\n",
    "#                                  amsgrad=False)\n",
    "\n",
    "optim = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95)\n",
    "        \n",
    "METRICS = [keras.metrics.RootMeanSquaredError(name='rmse'),\n",
    "           keras.metrics.MeanSquaredError(name='mse'),\n",
    "           keras.metrics.MeanAbsoluteError(name='mae')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weighted_mae(y_true, y_pred):\n",
    "# #     tf.print(y_true)\n",
    "#     W = tf.constant([[0.2, 0.2, 0.2, 0.2, 0.2]])\n",
    "# #     tf.print(W / tf.math.reduce_mean(y_true, axis=0))\n",
    "#     return tf.math.reduce_mean(tf.linalg.matmul(tf.math.abs(y_pred - y_true), tf.transpose(W / tf.math.reduce_mean(y_true, axis=0))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mae', metrics=METRICS, optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint directory to store the checkpoints\n",
    "# Name of the checkpoint files\n",
    "# checkpoint_prefix = os.path.join('./99_Training_checkpoints/fnc-loading', \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./99_Logs/fnc-loading'),\n",
    "#              tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "#                                                 save_weights_only=False),\n",
    "#              tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "#                                                   factor=0.7, \n",
    "#                                                   patience=2, \n",
    "#                                                   verbose=1, \n",
    "#                                                   mode='min',\n",
    "#                                                   min_delta=0.01, \n",
    "#                                                   cooldown=5, \n",
    "#                                                   min_lr=0.00000001),\n",
    "#              tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                               min_delta=0.001, \n",
    "#                                               patience=10, \n",
    "#                                               verbose=1, \n",
    "#                                               mode='min',\n",
    "#                                               baseline=None, \n",
    "#                                               restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./99_Logs/fnc-loading'),\n",
    "#              tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n",
    "#                                                   factor=0.7, \n",
    "#                                                   patience=2, \n",
    "#                                                   verbose=1, \n",
    "#                                                   mode='min',\n",
    "#                                                   min_delta=0.01, \n",
    "#                                                   cooldown=5, \n",
    "#                                                   min_lr=0.00000001),\n",
    "#              tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                               min_delta=0.001, \n",
    "#                                               patience=10, \n",
    "#                                               verbose=1, \n",
    "#                                               mode='min',\n",
    "#                                               baseline=None, \n",
    "#                                               restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./99_Logs/fnc-loading'),\n",
    "             tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                              min_delta=0.001, \n",
    "                                              patience=10, \n",
    "                                              verbose=1, \n",
    "                                              mode='min',\n",
    "                                              baseline=None, \n",
    "                                              restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decay(epoch):\n",
    "#     if epoch < 2:\n",
    "#         return 0.01\n",
    "#     elif epoch >= 2 and epoch < 10:\n",
    "#         return 0.005\n",
    "#     else:\n",
    "#         return 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [tf.keras.callbacks.LearningRateScheduler(decay)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "54/54 [==============================] - 15s 277ms/step - loss: 55.0655 - rmse: 57.1543 - mse: 3266.6145 - mae: 55.0655 - val_loss: 55.9944 - val_rmse: 59.0395 - val_mse: 3485.6606 - val_mae: 55.9944\n",
      "Epoch 2/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 54.9507 - rmse: 57.0462 - mse: 3254.2629 - mae: 54.9507 - val_loss: 55.5004 - val_rmse: 58.0340 - val_mse: 3367.9429 - val_mae: 55.5004\n",
      "Epoch 3/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 54.8412 - rmse: 56.9380 - mse: 3241.9316 - mae: 54.8412 - val_loss: 55.0153 - val_rmse: 57.3479 - val_mse: 3288.7817 - val_mae: 55.0153\n",
      "Epoch 4/200\n",
      "54/54 [==============================] - 15s 271ms/step - loss: 54.7077 - rmse: 56.8096 - mse: 3227.3264 - mae: 54.7077 - val_loss: 54.7154 - val_rmse: 56.9482 - val_mse: 3243.0950 - val_mae: 54.7154\n",
      "Epoch 5/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 54.5809 - rmse: 56.6860 - mse: 3213.3025 - mae: 54.5809 - val_loss: 54.4462 - val_rmse: 56.6349 - val_mse: 3207.5178 - val_mae: 54.4462\n",
      "Epoch 6/200\n",
      "54/54 [==============================] - 14s 265ms/step - loss: 54.4432 - rmse: 56.5566 - mse: 3198.6489 - mae: 54.4432 - val_loss: 54.2882 - val_rmse: 56.4417 - val_mse: 3185.6606 - val_mae: 54.2882\n",
      "Epoch 7/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 54.3102 - rmse: 56.4279 - mse: 3184.1038 - mae: 54.3102 - val_loss: 54.0465 - val_rmse: 56.2154 - val_mse: 3160.1707 - val_mae: 54.0465\n",
      "Epoch 8/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 54.1647 - rmse: 56.2878 - mse: 3168.3152 - mae: 54.1647 - val_loss: 53.9381 - val_rmse: 56.0863 - val_mse: 3145.6760 - val_mae: 53.9381\n",
      "Epoch 9/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 54.0233 - rmse: 56.1544 - mse: 3153.3130 - mae: 54.0233 - val_loss: 53.7842 - val_rmse: 55.9302 - val_mse: 3128.1914 - val_mae: 53.7842\n",
      "Epoch 10/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 53.8572 - rmse: 55.9930 - mse: 3135.2166 - mae: 53.8572 - val_loss: 53.6184 - val_rmse: 55.7858 - val_mse: 3112.0588 - val_mae: 53.6184\n",
      "Epoch 11/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 53.7119 - rmse: 55.8536 - mse: 3119.6216 - mae: 53.7119 - val_loss: 53.4515 - val_rmse: 55.6130 - val_mse: 3092.8052 - val_mae: 53.4515\n",
      "Epoch 12/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 53.5500 - rmse: 55.6995 - mse: 3102.4363 - mae: 53.5500 - val_loss: 53.2874 - val_rmse: 55.4643 - val_mse: 3076.2908 - val_mae: 53.2874\n",
      "Epoch 13/200\n",
      "54/54 [==============================] - 14s 264ms/step - loss: 53.3805 - rmse: 55.5350 - mse: 3084.1377 - mae: 53.3805 - val_loss: 53.1685 - val_rmse: 55.3431 - val_mse: 3062.8628 - val_mae: 53.1685\n",
      "Epoch 14/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 53.2112 - rmse: 55.3685 - mse: 3065.6704 - mae: 53.2112 - val_loss: 52.8972 - val_rmse: 55.0918 - val_mse: 3035.1050 - val_mae: 52.8972\n",
      "Epoch 15/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 53.0373 - rmse: 55.2033 - mse: 3047.4089 - mae: 53.0373 - val_loss: 52.8117 - val_rmse: 54.9861 - val_mse: 3023.4749 - val_mae: 52.8117\n",
      "Epoch 16/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 52.8668 - rmse: 55.0396 - mse: 3029.3518 - mae: 52.8668 - val_loss: 52.6487 - val_rmse: 54.8328 - val_mse: 3006.6382 - val_mae: 52.6487\n",
      "Epoch 17/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 52.6910 - rmse: 54.8691 - mse: 3010.6245 - mae: 52.6910 - val_loss: 52.4006 - val_rmse: 54.6100 - val_mse: 2982.2517 - val_mae: 52.4006\n",
      "Epoch 18/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 52.4927 - rmse: 54.6768 - mse: 2989.5562 - mae: 52.4927 - val_loss: 52.1321 - val_rmse: 54.3537 - val_mse: 2954.3271 - val_mae: 52.1321\n",
      "Epoch 19/200\n",
      "54/54 [==============================] - 14s 265ms/step - loss: 52.3182 - rmse: 54.5122 - mse: 2971.5850 - mae: 52.3182 - val_loss: 51.9808 - val_rmse: 54.2035 - val_mse: 2938.0237 - val_mae: 51.9808\n",
      "Epoch 20/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 52.1087 - rmse: 54.3099 - mse: 2949.5603 - mae: 52.1087 - val_loss: 51.8274 - val_rmse: 54.0540 - val_mse: 2921.8333 - val_mae: 51.8274\n",
      "Epoch 21/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 51.9309 - rmse: 54.1357 - mse: 2930.6768 - mae: 51.9309 - val_loss: 51.6416 - val_rmse: 53.8817 - val_mse: 2903.2366 - val_mae: 51.6416\n",
      "Epoch 22/200\n",
      "54/54 [==============================] - 15s 270ms/step - loss: 51.7257 - rmse: 53.9427 - mse: 2909.8137 - mae: 51.7257 - val_loss: 51.4885 - val_rmse: 53.7378 - val_mse: 2887.7483 - val_mae: 51.4885\n",
      "Epoch 23/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 51.5244 - rmse: 53.7497 - mse: 2889.0330 - mae: 51.5244 - val_loss: 51.1928 - val_rmse: 53.4451 - val_mse: 2856.3752 - val_mae: 51.1928\n",
      "Epoch 24/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 51.3025 - rmse: 53.5378 - mse: 2866.3010 - mae: 51.3025 - val_loss: 50.9266 - val_rmse: 53.1922 - val_mse: 2829.4050 - val_mae: 50.9266\n",
      "Epoch 25/200\n",
      "54/54 [==============================] - 15s 270ms/step - loss: 51.1061 - rmse: 53.3496 - mse: 2846.1799 - mae: 51.1061 - val_loss: 50.8193 - val_rmse: 53.0990 - val_mse: 2819.5010 - val_mae: 50.8193\n",
      "Epoch 26/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 50.8889 - rmse: 53.1407 - mse: 2823.9314 - mae: 50.8889 - val_loss: 50.5669 - val_rmse: 52.8469 - val_mse: 2792.7917 - val_mae: 50.5669\n",
      "Epoch 27/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 50.6896 - rmse: 52.9496 - mse: 2803.6553 - mae: 50.6896 - val_loss: 50.3188 - val_rmse: 52.6020 - val_mse: 2766.9663 - val_mae: 50.3188\n",
      "Epoch 28/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 50.4662 - rmse: 52.7331 - mse: 2780.7800 - mae: 50.4662 - val_loss: 50.1893 - val_rmse: 52.4785 - val_mse: 2753.9934 - val_mae: 50.1893\n",
      "Epoch 29/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 50.2513 - rmse: 52.5272 - mse: 2759.1045 - mae: 50.2513 - val_loss: 49.9207 - val_rmse: 52.2229 - val_mse: 2727.2327 - val_mae: 49.9207\n",
      "Epoch 30/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 50.0242 - rmse: 52.3159 - mse: 2736.9521 - mae: 50.0242 - val_loss: 49.6946 - val_rmse: 52.0268 - val_mse: 2706.7856 - val_mae: 49.6946\n",
      "Epoch 31/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 49.7958 - rmse: 52.0935 - mse: 2713.7344 - mae: 49.7958 - val_loss: 49.4637 - val_rmse: 51.7817 - val_mse: 2681.3474 - val_mae: 49.4637\n",
      "Epoch 32/200\n",
      "54/54 [==============================] - 15s 270ms/step - loss: 49.5684 - rmse: 51.8756 - mse: 2691.0798 - mae: 49.5684 - val_loss: 49.2271 - val_rmse: 51.5692 - val_mse: 2659.3828 - val_mae: 49.2271\n",
      "Epoch 33/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 49.3148 - rmse: 51.6348 - mse: 2666.1543 - mae: 49.3148 - val_loss: 49.0875 - val_rmse: 51.4246 - val_mse: 2644.4880 - val_mae: 49.0875\n",
      "Epoch 34/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 49.0899 - rmse: 51.4173 - mse: 2643.7395 - mae: 49.0899 - val_loss: 48.7625 - val_rmse: 51.1066 - val_mse: 2611.8823 - val_mae: 48.7625\n",
      "Epoch 35/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 48.8575 - rmse: 51.1908 - mse: 2620.5002 - mae: 48.8575 - val_loss: 48.5620 - val_rmse: 50.9300 - val_mse: 2593.8616 - val_mae: 48.5620\n",
      "Epoch 36/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 48.5977 - rmse: 50.9476 - mse: 2595.6567 - mae: 48.5977 - val_loss: 48.2116 - val_rmse: 50.5940 - val_mse: 2559.7542 - val_mae: 48.2116\n",
      "Epoch 37/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 48.3574 - rmse: 50.7194 - mse: 2572.4600 - mae: 48.3574 - val_loss: 48.1787 - val_rmse: 50.5583 - val_mse: 2556.1370 - val_mae: 48.1787\n",
      "Epoch 38/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 48.1137 - rmse: 50.4846 - mse: 2548.6975 - mae: 48.1137 - val_loss: 47.7874 - val_rmse: 50.1796 - val_mse: 2517.9878 - val_mae: 47.7874\n",
      "Epoch 39/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 47.8521 - rmse: 50.2392 - mse: 2523.9780 - mae: 47.8521 - val_loss: 47.5758 - val_rmse: 49.9901 - val_mse: 2499.0083 - val_mae: 47.5758\n",
      "Epoch 40/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 47.6080 - rmse: 50.0051 - mse: 2500.5081 - mae: 47.6080 - val_loss: 47.3452 - val_rmse: 49.7490 - val_mse: 2474.9602 - val_mae: 47.3452\n",
      "Epoch 41/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 47.3342 - rmse: 49.7408 - mse: 2474.1514 - mae: 47.3342 - val_loss: 47.0597 - val_rmse: 49.5043 - val_mse: 2450.6792 - val_mae: 47.0597\n",
      "Epoch 42/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 47.0942 - rmse: 49.5107 - mse: 2451.3064 - mae: 47.0942 - val_loss: 46.7508 - val_rmse: 49.1908 - val_mse: 2419.7300 - val_mae: 46.7508\n",
      "Epoch 43/200\n",
      "54/54 [==============================] - 15s 271ms/step - loss: 46.8348 - rmse: 49.2641 - mse: 2426.9534 - mae: 46.8348 - val_loss: 46.4667 - val_rmse: 48.9262 - val_mse: 2393.7710 - val_mae: 46.4667\n",
      "Epoch 44/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 46.5675 - rmse: 49.0110 - mse: 2402.0779 - mae: 46.5675 - val_loss: 46.2246 - val_rmse: 48.6977 - val_mse: 2371.4702 - val_mae: 46.2246\n",
      "Epoch 45/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 46.2898 - rmse: 48.7487 - mse: 2376.4399 - mae: 46.2898 - val_loss: 45.9670 - val_rmse: 48.4600 - val_mse: 2348.3677 - val_mae: 45.9670\n",
      "Epoch 46/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 46.0253 - rmse: 48.4963 - mse: 2351.8950 - mae: 46.0253 - val_loss: 45.7140 - val_rmse: 48.2005 - val_mse: 2323.2898 - val_mae: 45.7140\n",
      "Epoch 47/200\n",
      "54/54 [==============================] - 14s 265ms/step - loss: 45.7362 - rmse: 48.2247 - mse: 2325.6216 - mae: 45.7362 - val_loss: 45.3569 - val_rmse: 47.8771 - val_mse: 2292.2146 - val_mae: 45.3569\n",
      "Epoch 48/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 45.4678 - rmse: 47.9635 - mse: 2300.4951 - mae: 45.4678 - val_loss: 45.1260 - val_rmse: 47.6427 - val_mse: 2269.8257 - val_mae: 45.1260\n",
      "Epoch 49/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 45.2047 - rmse: 47.7189 - mse: 2277.0964 - mae: 45.2047 - val_loss: 44.8632 - val_rmse: 47.4142 - val_mse: 2248.1084 - val_mae: 44.8632\n",
      "Epoch 50/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 44.9143 - rmse: 47.4405 - mse: 2250.5981 - mae: 44.9143 - val_loss: 44.5699 - val_rmse: 47.1235 - val_mse: 2220.6265 - val_mae: 44.5699\n",
      "Epoch 51/200\n",
      "54/54 [==============================] - 14s 251ms/step - loss: 44.6321 - rmse: 47.1757 - mse: 2225.5496 - mae: 44.6321 - val_loss: 44.3394 - val_rmse: 46.9046 - val_mse: 2200.0425 - val_mae: 44.3394\n",
      "Epoch 52/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 44.3382 - rmse: 46.8962 - mse: 2199.2546 - mae: 44.3382 - val_loss: 44.0081 - val_rmse: 46.5912 - val_mse: 2170.7422 - val_mae: 44.0081\n",
      "Epoch 53/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 44.0523 - rmse: 46.6269 - mse: 2174.0701 - mae: 44.0523 - val_loss: 43.7533 - val_rmse: 46.3318 - val_mse: 2146.6345 - val_mae: 43.7533\n",
      "Epoch 54/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 43.7635 - rmse: 46.3484 - mse: 2148.1729 - mae: 43.7635 - val_loss: 43.4327 - val_rmse: 46.0597 - val_mse: 2121.4988 - val_mae: 43.4327\n",
      "Epoch 55/200\n",
      "54/54 [==============================] - 14s 250ms/step - loss: 43.4657 - rmse: 46.0677 - mse: 2122.2351 - mae: 43.4657 - val_loss: 43.0581 - val_rmse: 45.7052 - val_mse: 2088.9653 - val_mae: 43.0581\n",
      "Epoch 56/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 43.1612 - rmse: 45.7781 - mse: 2095.6311 - mae: 43.1612 - val_loss: 42.8937 - val_rmse: 45.5495 - val_mse: 2074.7563 - val_mae: 42.8937\n",
      "Epoch 57/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 42.8734 - rmse: 45.5046 - mse: 2070.6721 - mae: 42.8734 - val_loss: 42.4894 - val_rmse: 45.1442 - val_mse: 2037.9982 - val_mae: 42.4894\n",
      "Epoch 58/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 42.5633 - rmse: 45.2155 - mse: 2044.4432 - mae: 42.5633 - val_loss: 42.1829 - val_rmse: 44.8596 - val_mse: 2012.3839 - val_mae: 42.1829\n",
      "Epoch 59/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 42.2509 - rmse: 44.9188 - mse: 2017.7036 - mae: 42.2509 - val_loss: 41.9632 - val_rmse: 44.6640 - val_mse: 1994.8765 - val_mae: 41.9632\n",
      "Epoch 60/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 41.9600 - rmse: 44.6442 - mse: 1993.1089 - mae: 41.9600 - val_loss: 41.5714 - val_rmse: 44.2888 - val_mse: 1961.4949 - val_mae: 41.5714\n",
      "Epoch 61/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 41.6508 - rmse: 44.3508 - mse: 1966.9961 - mae: 41.6508 - val_loss: 41.3132 - val_rmse: 44.0422 - val_mse: 1939.7169 - val_mae: 41.3132\n",
      "Epoch 62/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 41.3204 - rmse: 44.0413 - mse: 1939.6368 - mae: 41.3204 - val_loss: 40.9714 - val_rmse: 43.7162 - val_mse: 1911.1040 - val_mae: 40.9714\n",
      "Epoch 63/200\n",
      "54/54 [==============================] - 14s 269ms/step - loss: 41.0172 - rmse: 43.7532 - mse: 1914.3428 - mae: 41.0172 - val_loss: 40.6596 - val_rmse: 43.4231 - val_mse: 1885.5691 - val_mae: 40.6596\n",
      "Epoch 64/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 40.7119 - rmse: 43.4656 - mse: 1889.2573 - mae: 40.7119 - val_loss: 40.3410 - val_rmse: 43.1149 - val_mse: 1858.8934 - val_mae: 40.3410\n",
      "Epoch 65/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 40.3780 - rmse: 43.1497 - mse: 1861.8943 - mae: 40.3780 - val_loss: 39.9742 - val_rmse: 42.7952 - val_mse: 1831.4250 - val_mae: 39.9742\n",
      "Epoch 66/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 40.0642 - rmse: 42.8525 - mse: 1836.3350 - mae: 40.0642 - val_loss: 39.7047 - val_rmse: 42.5240 - val_mse: 1808.2871 - val_mae: 39.7047\n",
      "Epoch 67/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 39.7552 - rmse: 42.5649 - mse: 1811.7704 - mae: 39.7552 - val_loss: 39.3649 - val_rmse: 42.1877 - val_mse: 1779.8014 - val_mae: 39.3649\n",
      "Epoch 68/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 39.4113 - rmse: 42.2413 - mse: 1784.3280 - mae: 39.4113 - val_loss: 39.0455 - val_rmse: 41.8928 - val_mse: 1755.0073 - val_mae: 39.0455\n",
      "Epoch 69/200\n",
      "54/54 [==============================] - 15s 270ms/step - loss: 39.0638 - rmse: 41.9160 - mse: 1756.9490 - mae: 39.0638 - val_loss: 38.7206 - val_rmse: 41.6273 - val_mse: 1732.8318 - val_mae: 38.7206\n",
      "Epoch 70/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 38.7430 - rmse: 41.6121 - mse: 1731.5682 - mae: 38.7430 - val_loss: 38.4142 - val_rmse: 41.3295 - val_mse: 1708.1309 - val_mae: 38.4142\n",
      "Epoch 71/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 38.4178 - rmse: 41.3065 - mse: 1706.2255 - mae: 38.4178 - val_loss: 38.0872 - val_rmse: 41.0179 - val_mse: 1682.4718 - val_mae: 38.0872\n",
      "Epoch 72/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 38.0853 - rmse: 40.9909 - mse: 1680.2526 - mae: 38.0853 - val_loss: 37.8209 - val_rmse: 40.7539 - val_mse: 1660.8789 - val_mae: 37.8209\n",
      "Epoch 73/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 37.7579 - rmse: 40.6809 - mse: 1654.9390 - mae: 37.7579 - val_loss: 37.4991 - val_rmse: 40.4523 - val_mse: 1636.3921 - val_mae: 37.4991\n",
      "Epoch 74/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 37.3998 - rmse: 40.3490 - mse: 1628.0422 - mae: 37.3998 - val_loss: 37.0062 - val_rmse: 39.9889 - val_mse: 1599.1157 - val_mae: 37.0062\n",
      "Epoch 75/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 37.0730 - rmse: 40.0434 - mse: 1603.4706 - mae: 37.0730 - val_loss: 36.7004 - val_rmse: 39.7081 - val_mse: 1576.7307 - val_mae: 36.7004\n",
      "Epoch 76/200\n",
      "54/54 [==============================] - 14s 268ms/step - loss: 36.7198 - rmse: 39.7080 - mse: 1576.7256 - mae: 36.7198 - val_loss: 36.4487 - val_rmse: 39.4554 - val_mse: 1556.7266 - val_mae: 36.4487\n",
      "Epoch 77/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 36.3813 - rmse: 39.3891 - mse: 1551.5032 - mae: 36.3813 - val_loss: 36.1140 - val_rmse: 39.1389 - val_mse: 1531.8522 - val_mae: 36.1140\n",
      "Epoch 78/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 36.0214 - rmse: 39.0525 - mse: 1525.1010 - mae: 36.0214 - val_loss: 35.6442 - val_rmse: 38.7107 - val_mse: 1498.5171 - val_mae: 35.6442loss: 36.1007 - rmse: 39.1065 - mse: 1529\n",
      "Epoch 79/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 35.6906 - rmse: 38.7411 - mse: 1500.8711 - mae: 35.6906 - val_loss: 35.3267 - val_rmse: 38.4101 - val_mse: 1475.3353 - val_mae: 35.3267\n",
      "Epoch 80/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 35.3318 - rmse: 38.4081 - mse: 1475.1825 - mae: 35.3318 - val_loss: 34.9043 - val_rmse: 37.9857 - val_mse: 1442.9167 - val_mae: 34.9043\n",
      "Epoch 81/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 34.9858 - rmse: 38.0747 - mse: 1449.6852 - mae: 34.9858 - val_loss: 34.5590 - val_rmse: 37.6868 - val_mse: 1420.2986 - val_mae: 34.5590\n",
      "Epoch 82/200\n",
      "54/54 [==============================] - 14s 252ms/step - loss: 34.6249 - rmse: 37.7376 - mse: 1424.1299 - mae: 34.6249 - val_loss: 34.2884 - val_rmse: 37.4267 - val_mse: 1400.7566 - val_mae: 34.2884\n",
      "Epoch 83/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 34.2848 - rmse: 37.4184 - mse: 1400.1405 - mae: 34.2848 - val_loss: 33.7964 - val_rmse: 36.9554 - val_mse: 1365.7024 - val_mae: 33.7964\n",
      "Epoch 84/200\n",
      "54/54 [==============================] - 15s 272ms/step - loss: 33.9112 - rmse: 37.0681 - mse: 1374.0413 - mae: 33.9112 - val_loss: 33.5844 - val_rmse: 36.7553 - val_mse: 1350.9513 - val_mae: 33.5844\n",
      "Epoch 85/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 33.5542 - rmse: 36.7307 - mse: 1349.1471 - mae: 33.5542 - val_loss: 33.2097 - val_rmse: 36.4028 - val_mse: 1325.1649 - val_mae: 33.2097\n",
      "Epoch 86/200\n",
      "54/54 [==============================] - 15s 271ms/step - loss: 33.2002 - rmse: 36.4017 - mse: 1325.0865 - mae: 33.2002 - val_loss: 32.8149 - val_rmse: 36.0509 - val_mse: 1299.6708 - val_mae: 32.8149\n",
      "Epoch 87/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 32.8358 - rmse: 36.0571 - mse: 1300.1127 - mae: 32.8358 - val_loss: 32.4590 - val_rmse: 35.7036 - val_mse: 1274.7444 - val_mae: 32.4590\n",
      "Epoch 88/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 32.4888 - rmse: 35.7260 - mse: 1276.3463 - mae: 32.4888 - val_loss: 32.1400 - val_rmse: 35.3978 - val_mse: 1253.0028 - val_mae: 32.1400\n",
      "Epoch 89/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 32.1108 - rmse: 35.3753 - mse: 1251.4086 - mae: 32.1108 - val_loss: 31.7415 - val_rmse: 35.0292 - val_mse: 1227.0465 - val_mae: 31.7415\n",
      "Epoch 90/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 31.7392 - rmse: 35.0238 - mse: 1226.6649 - mae: 31.7392 - val_loss: 31.3670 - val_rmse: 34.6812 - val_mse: 1202.7830 - val_mae: 31.3670\n",
      "Epoch 91/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 31.3659 - rmse: 34.6733 - mse: 1202.2366 - mae: 31.3659 - val_loss: 31.0075 - val_rmse: 34.3414 - val_mse: 1179.3307 - val_mae: 31.0075\n",
      "Epoch 92/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 31.0079 - rmse: 34.3321 - mse: 1178.6937 - mae: 31.0079 - val_loss: 30.6980 - val_rmse: 34.0603 - val_mse: 1160.1047 - val_mae: 30.6980\n",
      "Epoch 93/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 30.6396 - rmse: 33.9852 - mse: 1154.9924 - mae: 30.6396 - val_loss: 30.3076 - val_rmse: 33.6504 - val_mse: 1132.3466 - val_mae: 30.3076\n",
      "Epoch 94/200\n",
      "54/54 [==============================] - 14s 266ms/step - loss: 30.2739 - rmse: 33.6326 - mse: 1131.1549 - mae: 30.2739 - val_loss: 29.9260 - val_rmse: 33.3114 - val_mse: 1109.6476 - val_mae: 29.9260\n",
      "Epoch 95/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 29.8975 - rmse: 33.2776 - mse: 1107.3972 - mae: 29.8975 - val_loss: 29.4822 - val_rmse: 32.9007 - val_mse: 1082.4530 - val_mae: 29.4822\n",
      "Epoch 96/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 29.5283 - rmse: 32.9237 - mse: 1083.9701 - mae: 29.5283 - val_loss: 29.1981 - val_rmse: 32.6322 - val_mse: 1064.8627 - val_mae: 29.1981\n",
      "Epoch 97/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 29.1560 - rmse: 32.5630 - mse: 1060.3508 - mae: 29.1560 - val_loss: 28.8180 - val_rmse: 32.2584 - val_mse: 1040.6053 - val_mae: 28.8180\n",
      "Epoch 98/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 28.8019 - rmse: 32.2263 - mse: 1038.5369 - mae: 28.8019 - val_loss: 28.4551 - val_rmse: 31.9040 - val_mse: 1017.8644 - val_mae: 28.4551\n",
      "Epoch 99/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 28.4125 - rmse: 31.8543 - mse: 1014.6953 - mae: 28.4125 - val_loss: 28.0705 - val_rmse: 31.5526 - val_mse: 995.5635 - val_mae: 28.0705\n",
      "Epoch 100/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 28.0443 - rmse: 31.4983 - mse: 992.1413 - mae: 28.0443 - val_loss: 27.6243 - val_rmse: 31.1078 - val_mse: 967.6938 - val_mae: 27.6243\n",
      "Epoch 101/200\n",
      "54/54 [==============================] - 14s 268ms/step - loss: 27.6837 - rmse: 31.1548 - mse: 970.6220 - mae: 27.6837 - val_loss: 27.2753 - val_rmse: 30.7931 - val_mse: 948.2133 - val_mae: 27.2753\n",
      "Epoch 102/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 27.2984 - rmse: 30.7802 - mse: 947.4216 - mae: 27.2984 - val_loss: 26.8948 - val_rmse: 30.4040 - val_mse: 924.4033 - val_mae: 26.8948\n",
      "Epoch 103/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 26.9409 - rmse: 30.4357 - mse: 926.3289 - mae: 26.9409 - val_loss: 26.5148 - val_rmse: 30.0437 - val_mse: 902.6224 - val_mae: 26.5148\n",
      "Epoch 104/200\n",
      "54/54 [==============================] - 14s 252ms/step - loss: 26.5798 - rmse: 30.0821 - mse: 904.9348 - mae: 26.5798 - val_loss: 26.2095 - val_rmse: 29.7335 - val_mse: 884.0817 - val_mae: 26.2095: 30.0768 - mse: 904. - ETA: 0s - loss: 26.5778 - rmse: 30.0670 - mse: 904.0258 - mae: 26\n",
      "Epoch 105/200\n",
      "54/54 [==============================] - 14s 252ms/step - loss: 26.1883 - rmse: 29.7022 - mse: 882.2177 - mae: 26.1883 - val_loss: 25.8717 - val_rmse: 29.3812 - val_mse: 863.2576 - val_mae: 25.8717\n",
      "Epoch 106/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 25.8194 - rmse: 29.3437 - mse: 861.0539 - mae: 25.8194 - val_loss: 25.3820 - val_rmse: 28.9254 - val_mse: 836.6765 - val_mae: 25.3820\n",
      "Epoch 107/200\n",
      "54/54 [==============================] - 14s 265ms/step - loss: 25.4418 - rmse: 28.9711 - mse: 839.3269 - mae: 25.4418 - val_loss: 25.0740 - val_rmse: 28.6401 - val_mse: 820.2578 - val_mae: 25.0740\n",
      "Epoch 108/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 25.0780 - rmse: 28.6080 - mse: 818.4167 - mae: 25.0780 - val_loss: 24.6125 - val_rmse: 28.1616 - val_mse: 793.0776 - val_mae: 24.6125\n",
      "Epoch 109/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 24.7134 - rmse: 28.2512 - mse: 798.1290 - mae: 24.7134 - val_loss: 24.3068 - val_rmse: 27.8605 - val_mse: 776.2087 - val_mae: 24.3068\n",
      "Epoch 110/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 24.3463 - rmse: 27.8896 - mse: 777.8315 - mae: 24.3463 - val_loss: 24.0019 - val_rmse: 27.5517 - val_mse: 759.0972 - val_mae: 24.0019\n",
      "Epoch 111/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 23.9795 - rmse: 27.5207 - mse: 757.3874 - mae: 23.9795 - val_loss: 23.7116 - val_rmse: 27.2772 - val_mse: 744.0432 - val_mae: 23.7116\n",
      "Epoch 112/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 23.6163 - rmse: 27.1647 - mse: 737.9220 - mae: 23.6163 - val_loss: 23.2055 - val_rmse: 26.7944 - val_mse: 717.9421 - val_mae: 23.2055\n",
      "Epoch 113/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 23.2670 - rmse: 26.8126 - mse: 718.9148 - mae: 23.2670 - val_loss: 22.8616 - val_rmse: 26.4426 - val_mse: 699.2125 - val_mae: 22.8616\n",
      "Epoch 114/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 22.8915 - rmse: 26.4410 - mse: 699.1249 - mae: 22.8915 - val_loss: 22.5900 - val_rmse: 26.1652 - val_mse: 684.6174 - val_mae: 22.5900\n",
      "Epoch 115/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 22.5320 - rmse: 26.0750 - mse: 679.9067 - mae: 22.5320 - val_loss: 22.2045 - val_rmse: 25.7690 - val_mse: 664.0433 - val_mae: 22.2045\n",
      "Epoch 116/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 22.1806 - rmse: 25.7196 - mse: 661.4991 - mae: 22.1806 - val_loss: 21.9030 - val_rmse: 25.4604 - val_mse: 648.2340 - val_mae: 21.9030\n",
      "Epoch 117/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 21.8179 - rmse: 25.3560 - mse: 642.9243 - mae: 21.8179 - val_loss: 21.4873 - val_rmse: 25.0241 - val_mse: 626.2065 - val_mae: 21.4873\n",
      "Epoch 118/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 21.4644 - rmse: 24.9938 - mse: 624.6898 - mae: 21.4644 - val_loss: 21.1448 - val_rmse: 24.7114 - val_mse: 610.6515 - val_mae: 21.1448\n",
      "Epoch 119/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 21.1237 - rmse: 24.6469 - mse: 607.4702 - mae: 21.1237 - val_loss: 20.7213 - val_rmse: 24.2647 - val_mse: 588.7744 - val_mae: 20.7213\n",
      "Epoch 120/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 20.7692 - rmse: 24.2825 - mse: 589.6378 - mae: 20.7692 - val_loss: 20.4306 - val_rmse: 23.9475 - val_mse: 573.4846 - val_mae: 20.4306\n",
      "Epoch 121/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 20.4147 - rmse: 23.9148 - mse: 571.9174 - mae: 20.4147 - val_loss: 20.0463 - val_rmse: 23.5673 - val_mse: 555.4199 - val_mae: 20.0463\n",
      "Epoch 122/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 20.0709 - rmse: 23.5657 - mse: 555.3435 - mae: 20.0709 - val_loss: 19.6897 - val_rmse: 23.1945 - val_mse: 537.9869 - val_mae: 19.6897\n",
      "Epoch 123/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 19.7437 - rmse: 23.2217 - mse: 539.2494 - mae: 19.7437 - val_loss: 19.3603 - val_rmse: 22.8492 - val_mse: 522.0854 - val_mae: 19.3603\n",
      "Epoch 124/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 19.3910 - rmse: 22.8552 - mse: 522.3589 - mae: 19.3910 - val_loss: 19.0596 - val_rmse: 22.5225 - val_mse: 507.2616 - val_mae: 19.0596\n",
      "Epoch 125/200\n",
      "54/54 [==============================] - 14s 264ms/step - loss: 19.0617 - rmse: 22.5162 - mse: 506.9777 - mae: 19.0617 - val_loss: 18.7504 - val_rmse: 22.2151 - val_mse: 493.5128 - val_mae: 18.7504\n",
      "Epoch 126/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 18.7304 - rmse: 22.1726 - mse: 491.6252 - mae: 18.7304 - val_loss: 18.3787 - val_rmse: 21.8222 - val_mse: 476.2103 - val_mae: 18.3787\n",
      "Epoch 127/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 18.3973 - rmse: 21.8156 - mse: 475.9202 - mae: 18.3973 - val_loss: 18.0641 - val_rmse: 21.5041 - val_mse: 462.4278 - val_mae: 18.0641\n",
      "Epoch 128/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 18.0613 - rmse: 21.4646 - mse: 460.7293 - mae: 18.0613 - val_loss: 17.7278 - val_rmse: 21.1488 - val_mse: 447.2716 - val_mae: 17.7278\n",
      "Epoch 129/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 17.7413 - rmse: 21.1236 - mse: 446.2086 - mae: 17.7413 - val_loss: 17.3546 - val_rmse: 20.7235 - val_mse: 429.4615 - val_mae: 17.3546\n",
      "Epoch 130/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 17.4393 - rmse: 20.8009 - mse: 432.6760 - mae: 17.4393 - val_loss: 17.0249 - val_rmse: 20.4073 - val_mse: 416.4577 - val_mae: 17.0249\n",
      "Epoch 131/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 17.1153 - rmse: 20.4598 - mse: 418.6045 - mae: 17.1153 - val_loss: 16.7584 - val_rmse: 20.1169 - val_mse: 404.6898 - val_mae: 16.7584\n",
      "Epoch 132/200\n",
      "54/54 [==============================] - 14s 253ms/step - loss: 16.7883 - rmse: 20.1124 - mse: 404.5089 - mae: 16.7883 - val_loss: 16.4337 - val_rmse: 19.7654 - val_mse: 390.6729 - val_mae: 16.4337\n",
      "Epoch 133/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 16.5006 - rmse: 19.8004 - mse: 392.0544 - mae: 16.5006 - val_loss: 16.1231 - val_rmse: 19.4327 - val_mse: 377.6298 - val_mae: 16.1231\n",
      "Epoch 134/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 16.1953 - rmse: 19.4719 - mse: 379.1544 - mae: 16.1953 - val_loss: 15.8451 - val_rmse: 19.1153 - val_mse: 365.3960 - val_mae: 15.8451\n",
      "Epoch 135/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 15.9039 - rmse: 19.1496 - mse: 366.7053 - mae: 15.9039 - val_loss: 15.5842 - val_rmse: 18.8306 - val_mse: 354.5916 - val_mae: 15.5842\n",
      "Epoch 136/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 15.6044 - rmse: 18.8273 - mse: 354.4679 - mae: 15.6044 - val_loss: 15.2828 - val_rmse: 18.4892 - val_mse: 341.8508 - val_mae: 15.2828\n",
      "Epoch 137/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 15.3162 - rmse: 18.5082 - mse: 342.5530 - mae: 15.3162 - val_loss: 14.9795 - val_rmse: 18.1587 - val_mse: 329.7392 - val_mae: 14.9795\n",
      "Epoch 138/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 15.0356 - rmse: 18.1976 - mse: 331.1512 - mae: 15.0356 - val_loss: 14.7512 - val_rmse: 17.8956 - val_mse: 320.2520 - val_mae: 14.7512\n",
      "Epoch 139/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 14.7638 - rmse: 17.8956 - mse: 320.2537 - mae: 14.7638 - val_loss: 14.4330 - val_rmse: 17.5437 - val_mse: 307.7822 - val_mae: 14.4330\n",
      "Epoch 140/200\n",
      "54/54 [==============================] - 14s 264ms/step - loss: 14.4912 - rmse: 17.5893 - mse: 309.3836 - mae: 14.4912 - val_loss: 14.2587 - val_rmse: 17.3263 - val_mse: 300.2000 - val_mae: 14.2587\n",
      "Epoch 141/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 14.2347 - rmse: 17.2986 - mse: 299.2426 - mae: 14.2347 - val_loss: 13.9862 - val_rmse: 17.0227 - val_mse: 289.7734 - val_mae: 13.9862\n",
      "Epoch 142/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 13.9673 - rmse: 16.9987 - mse: 288.9575 - mae: 13.9673 - val_loss: 13.7104 - val_rmse: 16.7168 - val_mse: 279.4501 - val_mae: 13.7104\n",
      "Epoch 143/200\n",
      "54/54 [==============================] - 15s 276ms/step - loss: 13.7182 - rmse: 16.7173 - mse: 279.4691 - mae: 13.7182 - val_loss: 13.4331 - val_rmse: 16.3877 - val_mse: 268.5554 - val_mae: 13.4331\n",
      "Epoch 144/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 13.4795 - rmse: 16.4420 - mse: 270.3379 - mae: 13.4795 - val_loss: 13.2740 - val_rmse: 16.2117 - val_mse: 262.8198 - val_mae: 13.2740\n",
      "Epoch 145/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 13.2361 - rmse: 16.1698 - mse: 261.4618 - mae: 13.2361 - val_loss: 13.0062 - val_rmse: 15.9055 - val_mse: 252.9863 - val_mae: 13.0062\n",
      "Epoch 146/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 13.0060 - rmse: 15.9062 - mse: 253.0074 - mae: 13.0060 - val_loss: 12.7883 - val_rmse: 15.6376 - val_mse: 244.5349 - val_mae: 12.7883\n",
      "Epoch 147/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 12.7809 - rmse: 15.6474 - mse: 244.8419 - mae: 12.7809 - val_loss: 12.5212 - val_rmse: 15.3525 - val_mse: 235.6988 - val_mae: 12.5212\n",
      "Epoch 148/200\n",
      "54/54 [==============================] - 14s 255ms/step - loss: 12.5701 - rmse: 15.4104 - mse: 237.4812 - mae: 12.5701 - val_loss: 12.3645 - val_rmse: 15.1370 - val_mse: 229.1293 - val_mae: 12.3645\n",
      "Epoch 149/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 12.3609 - rmse: 15.1640 - mse: 229.9478 - mae: 12.3609 - val_loss: 12.1605 - val_rmse: 14.9377 - val_mse: 223.1358 - val_mae: 12.1605\n",
      "Epoch 150/200\n",
      "54/54 [==============================] - 15s 270ms/step - loss: 12.1531 - rmse: 14.9242 - mse: 222.7321 - mae: 12.1531 - val_loss: 12.0127 - val_rmse: 14.7507 - val_mse: 217.5827 - val_mae: 12.0127\n",
      "Epoch 151/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 11.9649 - rmse: 14.7050 - mse: 216.2360 - mae: 11.9649 - val_loss: 11.7767 - val_rmse: 14.4916 - val_mse: 210.0064 - val_mae: 11.7767\n",
      "Epoch 152/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 11.7693 - rmse: 14.4900 - mse: 209.9599 - mae: 11.7693 - val_loss: 11.6072 - val_rmse: 14.2967 - val_mse: 204.3945 - val_mae: 11.6072\n",
      "Epoch 153/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 11.5901 - rmse: 14.2793 - mse: 203.8995 - mae: 11.5901 - val_loss: 11.4017 - val_rmse: 14.0638 - val_mse: 197.7904 - val_mae: 11.4017\n",
      "Epoch 154/200\n",
      "54/54 [==============================] - 14s 268ms/step - loss: 11.4157 - rmse: 14.0770 - mse: 198.1629 - mae: 11.4157 - val_loss: 11.2794 - val_rmse: 13.9231 - val_mse: 193.8539 - val_mae: 11.2794\n",
      "Epoch 155/200\n",
      "54/54 [==============================] - 14s 264ms/step - loss: 11.2474 - rmse: 13.8879 - mse: 192.8727 - mae: 11.2474 - val_loss: 11.1478 - val_rmse: 13.7556 - val_mse: 189.2170 - val_mae: 11.1478\n",
      "Epoch 156/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 11.0959 - rmse: 13.7009 - mse: 187.7134 - mae: 11.0959 - val_loss: 10.9963 - val_rmse: 13.5795 - val_mse: 184.4033 - val_mae: 10.9963\n",
      "Epoch 157/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 10.9561 - rmse: 13.5409 - mse: 183.3570 - mae: 10.9561 - val_loss: 10.8705 - val_rmse: 13.4492 - val_mse: 180.8803 - val_mae: 10.8705\n",
      "Epoch 158/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 10.7995 - rmse: 13.3613 - mse: 178.5242 - mae: 10.7995 - val_loss: 10.7358 - val_rmse: 13.2892 - val_mse: 176.6024 - val_mae: 10.7358.7289 - rmse: 13.2826 - mse: 176.\n",
      "Epoch 159/200\n",
      "54/54 [==============================] - 14s 258ms/step - loss: 10.6779 - rmse: 13.2165 - mse: 174.6758 - mae: 10.6779 - val_loss: 10.6639 - val_rmse: 13.2031 - val_mse: 174.3210 - val_mae: 10.6639\n",
      "Epoch 160/200\n",
      "54/54 [==============================] - 15s 272ms/step - loss: 10.5395 - rmse: 13.0663 - mse: 170.7276 - mae: 10.5395 - val_loss: 10.4987 - val_rmse: 13.0095 - val_mse: 169.2475 - val_mae: 10.4987\n",
      "Epoch 161/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 10.4437 - rmse: 12.9490 - mse: 167.6768 - mae: 10.4437 - val_loss: 10.3636 - val_rmse: 12.8832 - val_mse: 165.9764 - val_mae: 10.3636\n",
      "Epoch 162/200\n",
      "54/54 [==============================] - 15s 272ms/step - loss: 10.3057 - rmse: 12.7873 - mse: 163.5155 - mae: 10.3057 - val_loss: 10.2876 - val_rmse: 12.7755 - val_mse: 163.2142 - val_mae: 10.2876\n",
      "Epoch 163/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 10.2061 - rmse: 12.6765 - mse: 160.6937 - mae: 10.2061 - val_loss: 10.2327 - val_rmse: 12.7203 - val_mse: 161.8062 - val_mae: 10.2327\n",
      "Epoch 164/200\n",
      "54/54 [==============================] - 14s 264ms/step - loss: 10.0974 - rmse: 12.5566 - mse: 157.6669 - mae: 10.0974 - val_loss: 10.1159 - val_rmse: 12.5586 - val_mse: 157.7177 - val_mae: 10.1159\n",
      "Epoch 165/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 9.9899 - rmse: 12.4376 - mse: 154.6944 - mae: 9.9899 - val_loss: 10.0346 - val_rmse: 12.4803 - val_mse: 155.7586 - val_mae: 10.0346\n",
      "Epoch 166/200\n",
      "54/54 [==============================] - 15s 274ms/step - loss: 9.9064 - rmse: 12.3394 - mse: 152.2617 - mae: 9.9064 - val_loss: 9.9645 - val_rmse: 12.3842 - val_mse: 153.3688 - val_mae: 9.9645\n",
      "Epoch 167/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 9.8258 - rmse: 12.2468 - mse: 149.9841 - mae: 9.8258 - val_loss: 9.9217 - val_rmse: 12.3460 - val_mse: 152.4233 - val_mae: 9.9217\n",
      "Epoch 168/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 9.7468 - rmse: 12.1582 - mse: 147.8206 - mae: 9.7468 - val_loss: 9.8702 - val_rmse: 12.2878 - val_mse: 150.9891 - val_mae: 9.8702\n",
      "Epoch 169/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 9.6557 - rmse: 12.0598 - mse: 145.4384 - mae: 9.6557 - val_loss: 9.7462 - val_rmse: 12.1560 - val_mse: 147.7691 - val_mae: 9.7462\n",
      "Epoch 170/200\n",
      "54/54 [==============================] - 15s 279ms/step - loss: 9.5936 - rmse: 11.9868 - mse: 143.6830 - mae: 9.5936 - val_loss: 9.7218 - val_rmse: 12.1168 - val_mse: 146.8165 - val_mae: 9.7218\n",
      "Epoch 171/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 9.5192 - rmse: 11.9054 - mse: 141.7387 - mae: 9.5192 - val_loss: 9.6408 - val_rmse: 12.0201 - val_mse: 144.4835 - val_mae: 9.6408\n",
      "Epoch 172/200\n",
      "54/54 [==============================] - 14s 256ms/step - loss: 9.4598 - rmse: 11.8358 - mse: 140.0866 - mae: 9.4598 - val_loss: 9.6054 - val_rmse: 11.9916 - val_mse: 143.7984 - val_mae: 9.6054\n",
      "Epoch 173/200\n",
      "54/54 [==============================] - 14s 257ms/step - loss: 9.3957 - rmse: 11.7673 - mse: 138.4687 - mae: 9.3957 - val_loss: 9.5638 - val_rmse: 11.9610 - val_mse: 143.0653 - val_mae: 9.5638\n",
      "Epoch 174/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 9.3402 - rmse: 11.7077 - mse: 137.0692 - mae: 9.3402 - val_loss: 9.5275 - val_rmse: 11.9060 - val_mse: 141.7520 - val_mae: 9.5275\n",
      "Epoch 175/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 9.3080 - rmse: 11.6734 - mse: 136.2680 - mae: 9.3080 - val_loss: 9.4480 - val_rmse: 11.8201 - val_mse: 139.7144 - val_mae: 9.4480\n",
      "Epoch 176/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 9.2527 - rmse: 11.6077 - mse: 134.7391 - mae: 9.2527 - val_loss: 9.4453 - val_rmse: 11.8141 - val_mse: 139.5734 - val_mae: 9.4453\n",
      "Epoch 177/200\n",
      "54/54 [==============================] - 15s 270ms/step - loss: 9.2119 - rmse: 11.5681 - mse: 133.8220 - mae: 9.2119 - val_loss: 9.3845 - val_rmse: 11.7679 - val_mse: 138.4843 - val_mae: 9.3845\n",
      "Epoch 178/200\n",
      "54/54 [==============================] - 14s 264ms/step - loss: 9.1748 - rmse: 11.5252 - mse: 132.8298 - mae: 9.1748 - val_loss: 9.3873 - val_rmse: 11.7550 - val_mse: 138.1798 - val_mae: 9.3873\n",
      "Epoch 179/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 9.1245 - rmse: 11.4739 - mse: 131.6499 - mae: 9.1245 - val_loss: 9.3936 - val_rmse: 11.7775 - val_mse: 138.7092 - val_mae: 9.3936\n",
      "Epoch 180/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 9.0831 - rmse: 11.4289 - mse: 130.6195 - mae: 9.0831 - val_loss: 9.3261 - val_rmse: 11.6904 - val_mse: 136.6657 - val_mae: 9.3261\n",
      "Epoch 181/200\n",
      "54/54 [==============================] - 15s 273ms/step - loss: 9.0674 - rmse: 11.4098 - mse: 130.1828 - mae: 9.0674 - val_loss: 9.2726 - val_rmse: 11.6404 - val_mse: 135.4990 - val_mae: 9.2726\n",
      "Epoch 182/200\n",
      "54/54 [==============================] - 14s 259ms/step - loss: 9.0268 - rmse: 11.3638 - mse: 129.1358 - mae: 9.0268 - val_loss: 9.2345 - val_rmse: 11.6065 - val_mse: 134.7104 - val_mae: 9.2345\n",
      "Epoch 183/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 8.9930 - rmse: 11.3332 - mse: 128.4424 - mae: 8.9930 - val_loss: 9.2822 - val_rmse: 11.6446 - val_mse: 135.5979 - val_mae: 9.2822\n",
      "Epoch 184/200\n",
      "54/54 [==============================] - 14s 264ms/step - loss: 8.9654 - rmse: 11.3001 - mse: 127.6927 - mae: 8.9654 - val_loss: 9.1963 - val_rmse: 11.5592 - val_mse: 133.6149 - val_mae: 9.1963\n",
      "Epoch 185/200\n",
      "54/54 [==============================] - 14s 260ms/step - loss: 8.9391 - rmse: 11.2783 - mse: 127.2012 - mae: 8.9391 - val_loss: 9.1663 - val_rmse: 11.5188 - val_mse: 132.6827 - val_mae: 9.1663\n",
      "Epoch 186/200\n",
      "54/54 [==============================] - 15s 277ms/step - loss: 8.9303 - rmse: 11.2677 - mse: 126.9606 - mae: 8.9303 - val_loss: 9.1812 - val_rmse: 11.5502 - val_mse: 133.4070 - val_mae: 9.1812\n",
      "Epoch 187/200\n",
      "54/54 [==============================] - 15s 270ms/step - loss: 8.9045 - rmse: 11.2365 - mse: 126.2593 - mae: 8.9045 - val_loss: 9.1691 - val_rmse: 11.5240 - val_mse: 132.8023 - val_mae: 9.1691\n",
      "Epoch 188/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 8.8798 - rmse: 11.2115 - mse: 125.6971 - mae: 8.8798 - val_loss: 9.1456 - val_rmse: 11.5141 - val_mse: 132.5738 - val_mae: 9.1456\n",
      "Epoch 189/200\n",
      "54/54 [==============================] - 14s 268ms/step - loss: 8.8624 - rmse: 11.2013 - mse: 125.4693 - mae: 8.8624 - val_loss: 9.1027 - val_rmse: 11.4604 - val_mse: 131.3411 - val_mae: 9.1027\n",
      "Epoch 190/200\n",
      "54/54 [==============================] - 14s 263ms/step - loss: 8.8311 - rmse: 11.1676 - mse: 124.7154 - mae: 8.8311 - val_loss: 9.0860 - val_rmse: 11.4291 - val_mse: 130.6251 - val_mae: 9.0860\n",
      "Epoch 191/200\n",
      "54/54 [==============================] - 15s 274ms/step - loss: 8.8135 - rmse: 11.1501 - mse: 124.3251 - mae: 8.8135 - val_loss: 9.1279 - val_rmse: 11.5079 - val_mse: 132.4322 - val_mae: 9.1279\n",
      "Epoch 192/200\n",
      "54/54 [==============================] - 14s 267ms/step - loss: 8.7981 - rmse: 11.1354 - mse: 123.9969 - mae: 8.7981 - val_loss: 9.0793 - val_rmse: 11.4294 - val_mse: 130.6309 - val_mae: 9.0793\n",
      "Epoch 193/200\n",
      "54/54 [==============================] - 15s 272ms/step - loss: 8.7861 - rmse: 11.1194 - mse: 123.6421 - mae: 8.7861 - val_loss: 9.1215 - val_rmse: 11.4856 - val_mse: 131.9192 - val_mae: 9.1215\n",
      "Epoch 194/200\n",
      "54/54 [==============================] - 14s 262ms/step - loss: 8.7807 - rmse: 11.1157 - mse: 123.5590 - mae: 8.7807 - val_loss: 9.0558 - val_rmse: 11.4147 - val_mse: 130.2957 - val_mae: 9.0558\n",
      "Epoch 195/200\n",
      "54/54 [==============================] - 14s 254ms/step - loss: 8.7518 - rmse: 11.0866 - mse: 122.9125 - mae: 8.7518 - val_loss: 9.0732 - val_rmse: 11.4469 - val_mse: 131.0323 - val_mae: 9.073211.0644 - mse: 122.4216 - ma - ETA: 3s - loss: 8.7549 - rmse: 11.0778 - mse: 122.7179 - mae - ETA: 2s - loss: 8.7406 - rmse: 11.0680 - mse: 122.4998 -\n",
      "Epoch 196/200\n",
      "54/54 [==============================] - 14s 261ms/step - loss: 8.7429 - rmse: 11.0826 - mse: 122.8251 - mae: 8.7429 - val_loss: 9.0187 - val_rmse: 11.3978 - val_mse: 129.9090 - val_mae: 9.0187\n",
      "Epoch 197/200\n",
      "54/54 [==============================] - 15s 271ms/step - loss: 8.7171 - rmse: 11.0583 - mse: 122.2853 - mae: 8.7171 - val_loss: 9.0176 - val_rmse: 11.3989 - val_mse: 129.9343 - val_mae: 9.0176\n",
      "Epoch 198/200\n",
      "54/54 [==============================] - 15s 276ms/step - loss: 8.7140 - rmse: 11.0523 - mse: 122.1539 - mae: 8.7140 - val_loss: 9.0548 - val_rmse: 11.4380 - val_mse: 130.8277 - val_mae: 9.0548\n",
      "Epoch 199/200\n",
      "54/54 [==============================] - 15s 269ms/step - loss: 8.6957 - rmse: 11.0383 - mse: 121.8439 - mae: 8.6957 - val_loss: 9.0584 - val_rmse: 11.4193 - val_mse: 130.3997 - val_mae: 9.0584\n",
      "Epoch 200/200\n",
      "54/54 [==============================] - 15s 279ms/step - loss: 8.6886 - rmse: 11.0253 - mse: 121.5567 - mae: 8.6886 - val_loss: 9.0083 - val_rmse: 11.4051 - val_mse: 130.0766 - val_mae: 9.0083\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    hist = model.fit(ds_train,\n",
    "                     validation_data=ds_val,\n",
    "                     callbacks=callbacks,\n",
    "                     epochs=200,\n",
    "                     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 3s 197ms/step - loss: 8.8845 - rmse: 11.2428 - mse: 126.4005 - mae: 8.8845 0s - loss: 8.8692 - rmse: 11.2089 - mse: 125.6403 - mae: 8.86\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    results = model.evaluate(ds_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Metric')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(16,16)\n",
    "\n",
    "    ax=fig.add_subplot(3,2,1)\n",
    "    ax.plot(hist.history['rmse'])\n",
    "    ax.plot(hist.history['mse'])\n",
    "    ax.legend(['Metric', 'Loss'])\n",
    "    ax.set_title('Train')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,2)\n",
    "    ax.plot(hist.history['val_rmse'])\n",
    "    ax.plot(hist.history['val_mse'])\n",
    "    ax.legend(['Metric', 'Loss'])\n",
    "    ax.set_title('Test')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,3)\n",
    "    ax.plot(hist.history['loss'])\n",
    "    ax.plot(hist.history['val_loss'])\n",
    "    ax.legend(['Train', 'Test'])\n",
    "    ax.set_title('Loss')\n",
    "\n",
    "    ax=fig.add_subplot(3,2,4)\n",
    "    ax.plot(hist.history['mse'])\n",
    "    ax.plot(hist.history['val_mse'])\n",
    "    ax.legend(['Train', 'Test'])\n",
    "    ax.set_title('Metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test = pd.read_csv('00_Data/fnc.csv')\n",
    "X1_test = X1_test[X1_test['Id'].isin(TEST_IDS)]\n",
    "X1_test = X1_test.to_numpy()\n",
    "X1_test = X1_test[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_test = pd.read_csv('00_Data/loading.csv')\n",
    "X2_test = X2_test[X2_test['Id'].isin(TEST_IDS)]\n",
    "X2_test = X2_test.to_numpy()\n",
    "X2_test = X2_test[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict([X1_test, X2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = y_preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = []\n",
    "i = 0\n",
    "for idx in TEST_IDS:\n",
    "    df_submission.append(['{0}_age'.format(idx), y_preds[i]])\n",
    "    df_submission.append(['{0}_domain1_var1'.format(idx), y_preds[i+1]])\n",
    "    df_submission.append(['{0}_domain1_var2'.format(idx), y_preds[i+2]])\n",
    "    df_submission.append(['{0}_domain2_var1'.format(idx), y_preds[i+3]])\n",
    "    df_submission.append(['{0}_domain2_var2'.format(idx), y_preds[i+4]])\n",
    "    i += 5\n",
    "\n",
    "df_submission = pd.DataFrame(df_submission, columns=['Id', 'Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv('submission_fnc-load_mae_04.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>age</th>\n",
       "      <th>domain1_var1</th>\n",
       "      <th>domain1_var2</th>\n",
       "      <th>domain2_var1</th>\n",
       "      <th>domain2_var2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>57.436077</td>\n",
       "      <td>30.571975</td>\n",
       "      <td>62.553736</td>\n",
       "      <td>53.325130</td>\n",
       "      <td>51.427998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>59.580851</td>\n",
       "      <td>50.969456</td>\n",
       "      <td>67.470628</td>\n",
       "      <td>60.651856</td>\n",
       "      <td>58.311361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10004</td>\n",
       "      <td>71.413018</td>\n",
       "      <td>53.152498</td>\n",
       "      <td>58.012103</td>\n",
       "      <td>52.418389</td>\n",
       "      <td>62.536641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10005</td>\n",
       "      <td>66.532630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.108977</td>\n",
       "      <td>69.993075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10007</td>\n",
       "      <td>38.617381</td>\n",
       "      <td>49.197021</td>\n",
       "      <td>65.674285</td>\n",
       "      <td>40.151376</td>\n",
       "      <td>34.096421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>21746</td>\n",
       "      <td>14.257265</td>\n",
       "      <td>21.358872</td>\n",
       "      <td>61.165998</td>\n",
       "      <td>51.778483</td>\n",
       "      <td>54.640179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>21747</td>\n",
       "      <td>55.456978</td>\n",
       "      <td>68.169675</td>\n",
       "      <td>29.907995</td>\n",
       "      <td>55.349257</td>\n",
       "      <td>54.019517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>21750</td>\n",
       "      <td>48.948756</td>\n",
       "      <td>55.114811</td>\n",
       "      <td>60.878271</td>\n",
       "      <td>38.617246</td>\n",
       "      <td>50.679885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>21752</td>\n",
       "      <td>66.532630</td>\n",
       "      <td>59.844808</td>\n",
       "      <td>72.303110</td>\n",
       "      <td>55.458281</td>\n",
       "      <td>46.870235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>21754</td>\n",
       "      <td>68.820928</td>\n",
       "      <td>56.594193</td>\n",
       "      <td>34.605868</td>\n",
       "      <td>49.922535</td>\n",
       "      <td>50.383078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5877 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id        age  domain1_var1  domain1_var2  domain2_var1  domain2_var2\n",
       "0     10001  57.436077     30.571975     62.553736     53.325130     51.427998\n",
       "1     10002  59.580851     50.969456     67.470628     60.651856     58.311361\n",
       "2     10004  71.413018     53.152498     58.012103     52.418389     62.536641\n",
       "3     10005  66.532630           NaN           NaN     52.108977     69.993075\n",
       "4     10007  38.617381     49.197021     65.674285     40.151376     34.096421\n",
       "...     ...        ...           ...           ...           ...           ...\n",
       "5872  21746  14.257265     21.358872     61.165998     51.778483     54.640179\n",
       "5873  21747  55.456978     68.169675     29.907995     55.349257     54.019517\n",
       "5874  21750  48.948756     55.114811     60.878271     38.617246     50.679885\n",
       "5875  21752  66.532630     59.844808     72.303110     55.458281     46.870235\n",
       "5876  21754  68.820928     56.594193     34.605868     49.922535     50.383078\n",
       "\n",
       "[5877 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('00_Data/train_scores.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nulls = data[data.isnull().any(axis=1)]\n",
    "NULL_IDS = list(data_nulls['Id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
