{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing as prep\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn import cluster\n",
    "from sklearn import mixture\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_test'))]\n",
    "TRAIN_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_train'))]\n",
    "ALL_IDS = TRAIN_IDS + TEST_IDS\n",
    "REVEAL_IDS_S2 = pd.read_csv('00_Data/reveal_ID_site2.csv', dtype=str).values\n",
    "NOREVEAL_IDS = [i for i in ALL_IDS if i not in REVEAL_IDS_S2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11754 510 11244\n"
     ]
    }
   ],
   "source": [
    "print(len(ALL_IDS), len(REVEAL_IDS_S2), len(NOREVEAL_IDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>age</th>\n",
       "      <th>domain1_var1</th>\n",
       "      <th>domain1_var2</th>\n",
       "      <th>domain2_var1</th>\n",
       "      <th>domain2_var2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>57.436077</td>\n",
       "      <td>30.571975</td>\n",
       "      <td>62.553736</td>\n",
       "      <td>53.325130</td>\n",
       "      <td>51.427998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>59.580851</td>\n",
       "      <td>50.969456</td>\n",
       "      <td>67.470628</td>\n",
       "      <td>60.651856</td>\n",
       "      <td>58.311361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10004</td>\n",
       "      <td>71.413018</td>\n",
       "      <td>53.152498</td>\n",
       "      <td>58.012103</td>\n",
       "      <td>52.418389</td>\n",
       "      <td>62.536641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10007</td>\n",
       "      <td>38.617381</td>\n",
       "      <td>49.197021</td>\n",
       "      <td>65.674285</td>\n",
       "      <td>40.151376</td>\n",
       "      <td>34.096421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10008</td>\n",
       "      <td>35.326582</td>\n",
       "      <td>15.769168</td>\n",
       "      <td>65.782269</td>\n",
       "      <td>44.643805</td>\n",
       "      <td>50.448485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>21654</td>\n",
       "      <td>53.103634</td>\n",
       "      <td>50.951656</td>\n",
       "      <td>62.168022</td>\n",
       "      <td>49.389400</td>\n",
       "      <td>53.020847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>21665</td>\n",
       "      <td>38.246437</td>\n",
       "      <td>48.018227</td>\n",
       "      <td>59.522285</td>\n",
       "      <td>45.697098</td>\n",
       "      <td>53.208160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>21674</td>\n",
       "      <td>69.414169</td>\n",
       "      <td>58.593918</td>\n",
       "      <td>60.298779</td>\n",
       "      <td>49.865669</td>\n",
       "      <td>47.863167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>21693</td>\n",
       "      <td>62.009209</td>\n",
       "      <td>54.272484</td>\n",
       "      <td>60.474388</td>\n",
       "      <td>52.325031</td>\n",
       "      <td>52.989803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>21734</td>\n",
       "      <td>36.072495</td>\n",
       "      <td>46.474880</td>\n",
       "      <td>61.304012</td>\n",
       "      <td>42.742592</td>\n",
       "      <td>53.425110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5877 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id        age  domain1_var1  domain1_var2  domain2_var1  domain2_var2\n",
       "0     10001  57.436077     30.571975     62.553736     53.325130     51.427998\n",
       "1     10002  59.580851     50.969456     67.470628     60.651856     58.311361\n",
       "2     10004  71.413018     53.152498     58.012103     52.418389     62.536641\n",
       "3     10007  38.617381     49.197021     65.674285     40.151376     34.096421\n",
       "4     10008  35.326582     15.769168     65.782269     44.643805     50.448485\n",
       "...     ...        ...           ...           ...           ...           ...\n",
       "5872  21654  53.103634     50.951656     62.168022     49.389400     53.020847\n",
       "5873  21665  38.246437     48.018227     59.522285     45.697098     53.208160\n",
       "5874  21674  69.414169     58.593918     60.298779     49.865669     47.863167\n",
       "5875  21693  62.009209     54.272484     60.474388     52.325031     52.989803\n",
       "5876  21734  36.072495     46.474880     61.304012     42.742592     53.425110\n",
       "\n",
       "[5877 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('00_Data/train_scores_full.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(idx):\n",
    "    #MRI inputs\n",
    "    patient_SM = h5py.File('00_Data/fMRI_all/{0}.mat'.format(idx), mode='r')\n",
    "    patient_SM = np.array(patient_SM.get('SM_feature'))\n",
    "#     print(patient_SM.shape)\n",
    "    k = 1\n",
    "    ki_padding = 3\n",
    "    \n",
    "    arr_regions = []\n",
    "    for i in range(patient_SM.shape[0]):\n",
    "        sample_map = patient_SM[i,:,:,:]\n",
    "        if k > 1:\n",
    "            map_shape = sample_map.shape\n",
    "            shape_pad = ((map_shape[0]//k + 1)*k - map_shape[0],\n",
    "                         (map_shape[1]//k + 1)*k - map_shape[1],\n",
    "                         (map_shape[2]//k + 1)*k - map_shape[2])\n",
    "\n",
    "            npad = (((0 if shape_pad[0]%2==0 else shape_pad[0]//2), (shape_pad[0]//2 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "                    ((0 if shape_pad[1]%2==0 else shape_pad[0]//2), (shape_pad[1]//2 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "                    ((0 if shape_pad[2]%2==0 else shape_pad[0]//2), (shape_pad[2]//2 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "            sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "            sx = sample_map_padded.shape[0] / k\n",
    "            sy = sample_map_padded.shape[1] / k\n",
    "            sz = sample_map_padded.shape[2] / k\n",
    "            for kz in range(k):\n",
    "                for ky in range(k):\n",
    "                    for kx in range(k):\n",
    "                        ki_region = sample_map_padded[int(kx*sx): int(kx*sx + sx - 1), \n",
    "                                                     int(ky*sy): int(ky*sy + sy - 1), \n",
    "                                                     int(kz*sz): int(kz*sz + sz - 1)]\n",
    "                        #padding i-th region by 3 pixels\n",
    "                        ki_region_padded = np.pad(ki_region, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "                        arr_regions.append(ki_region_padded)\n",
    "        else:\n",
    "            map_shape = sample_map.shape\n",
    "            shape_pad = ((map_shape[0]//2 + 1)*2 - map_shape[0],\n",
    "                         (map_shape[1]//2 + 1)*2 - map_shape[1],\n",
    "                         (map_shape[2]//2 + 1)*2 - map_shape[2])\n",
    "\n",
    "            npad = (((0 if shape_pad[0]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "                    ((0 if shape_pad[1]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "                    ((0 if shape_pad[2]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "            sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "            \n",
    "#             sample_map_padded = np.pad(sample_map, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "            arr_regions.append(sample_map_padded)\n",
    "            \n",
    "    X_mri = np.stack(arr_regions, axis=3)\n",
    "#     print(X_mri.shape)\n",
    "    return X_mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_inputs('10002')[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_py_function(func, inp, Tout, name=None):\n",
    "    \n",
    "    def wrapped_func(*flat_inp):\n",
    "        reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp,\n",
    "                                                     expand_composites=True)\n",
    "        out = func(*reconstructed_inp)\n",
    "        return tf.nest.flatten(out, expand_composites=True)\n",
    "    \n",
    "    flat_Tout = tf.nest.flatten(Tout, expand_composites=True)\n",
    "    flat_out = tf.py_function(func=wrapped_func, \n",
    "                              inp=tf.nest.flatten(inp, expand_composites=True),\n",
    "                              Tout=[_tensor_spec_to_dtype(v) for v in flat_Tout],\n",
    "                              name=name)\n",
    "    spec_out = tf.nest.map_structure(_dtype_to_tensor_spec, Tout, expand_composites=True)\n",
    "    out = tf.nest.pack_sequence_as(spec_out, flat_out, expand_composites=True)\n",
    "    return out\n",
    "\n",
    "def _dtype_to_tensor_spec(v):\n",
    "    return tf.TensorSpec(None, v) if isinstance(v, tf.dtypes.DType) else v\n",
    "\n",
    "def _tensor_spec_to_dtype(v):\n",
    "    return v.dtype if isinstance(v, tf.TensorSpec) else v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data, batch_size):\n",
    "    data = np.array([int(i) for i in data])\n",
    "    data = tf.data.Dataset.from_tensor_slices(data)\n",
    "    data = data.shuffle(buffer_size=12000, seed=30, reshuffle_each_iteration=True)\n",
    "    \n",
    "    data = data.map(lambda idx: new_py_function(get_inputs, inp=[idx], \n",
    "                                                    Tout=tf.TensorSpec(shape=(None, 52, 66, 56, 53), dtype=tf.dtypes.float64), \n",
    "                                                name=None), \n",
    "                     num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "                     deterministic=False)\n",
    "#     data = data.map(lambda idx: tf.py_function(get_inputs, inp=[idx], \n",
    "#                                                     Tout=tf.float64, name=None), \n",
    "#                      num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "#                      deterministic=False)\n",
    "    data = data.batch(batch_size, drop_remainder=False)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(ALL_IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['Id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array([int(i) for i in ALL_IDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = model_selection.train_test_split(data, test_size=0.2, shuffle=True, random_state=30)\n",
    "# train, val = model_selection.train_test_split(train, test_size=0.2, shuffle=True, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "ds_train = get_dataset(ALL_IDS, batch_size)\n",
    "ds_reveal_s2 = get_dataset(REVEAL_IDS_S2, batch_size)\n",
    "ds_noreveal = get_dataset(NOREVEAL_IDS, batch_size)\n",
    "# ds_val = get_dataset(val, batch_size)\n",
    "# ds_test = get_dataset(test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ds_train.take(1):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE_mri = (52, 66, 56, 53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, filters=[32, 16, 8, 2]):\n",
    "    \n",
    "    #============================================================================\n",
    "    # ENCODER\n",
    "    #============================================================================\n",
    "    inputs_mri = keras.layers.Input(shape=INPUT_SHAPE_mri, name='inpupt_mri')\n",
    "\n",
    "    # convolution block #1\n",
    "    x = keras.layers.Conv3D(filters[0], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(inputs_mri)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.Conv3D(filters[0], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "#     x, p1_idx = tf.nn.max_pool_with_argmax(x, ksize=[2], strides=[2], padding='SAME', name=\"p1\")\n",
    "    x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "    # convolution block #2\n",
    "    x = keras.layers.Conv3D(filters[1], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.Conv3D(filters[1], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "    x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "    # convolution block #3\n",
    "    x = keras.layers.Conv3D(filters[2], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.Conv3D(filters[2], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "    x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "    # convolution block #4\n",
    "#     x = keras.layers.Conv3D(filters[3], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "#                                   kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                   bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = keras.layers.Conv3D(filters[3], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "#                                   kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                   bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "#     x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "#                                               beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "#                                               moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "#                                               beta_constraint=None, gamma_constraint=None)(x)\n",
    "    \n",
    "\n",
    "    flatten = keras.layers.Flatten(data_format='channels_last')(x)\n",
    "\n",
    "    encoded = keras.layers.Dense(2,\n",
    "                               kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                               bias_initializer=keras.initializers.Constant(5.))(flatten)\n",
    "    encoded = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(encoded)\n",
    "\n",
    "    \n",
    "    #============================================================================\n",
    "    # DECODER\n",
    "    #============================================================================\n",
    "    x = keras.layers.Dense(filters[2]*int(input_shape[0]/8)*int(input_shape[1]/8)*int(input_shape[2]/8),\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(encoded)\n",
    "    \n",
    "    x = keras.layers.Reshape((int(input_shape[0]/8), int(input_shape[1]/8), int(input_shape[2]/8), filters[2]))(x)\n",
    "    \n",
    "    # convolution block #4\n",
    "#     x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "#     x = tf.keras.layers.Conv3DTranspose(filters[2], kernel_size=(1, 1, 2), strides=(1,1,1), padding='valid',\n",
    "#                                         kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                         bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    # convolution block #3\n",
    "    x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[2], kernel_size=(2, 1, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[2], kernel_size=(2, 1, 1), strides=(1,1,1), padding='same',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    # convolution block #2\n",
    "    x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[1], kernel_size=(1, 2, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[1], kernel_size=(1, 2, 1), strides=(1,1,1), padding='same',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    # convolution block #1\n",
    "    x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[0], kernel_size=(1, 1, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[0], kernel_size=(1, 1, 1), strides=(1,1,1), padding='same',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(input_shape[3], kernel_size=(1, 1, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    decoded = x\n",
    "    \n",
    "    #============================================================================\n",
    "    # COMPILE\n",
    "    #============================================================================\n",
    "    autoencoder = keras.Model(inputs=inputs_mri, outputs=decoded, name='autoencoder')\n",
    "    encoder = keras.Model(inputs=inputs_mri, outputs=encoded, name='encoder')\n",
    "\n",
    "    optim = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95)\n",
    "\n",
    "    METRICS = [keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    "\n",
    "    autoencoder.compile(loss='mse', metrics=METRICS, optimizer=optim)\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, encoder = create_model(INPUT_SHAPE_mri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inpupt_mri (InputLayer)      [(None, 52, 66, 56, 53)]  0         \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 52, 66, 56, 32)    45824     \n",
      "_________________________________________________________________\n",
      "p_re_lu (PReLU)              (None, 52, 66, 56, 32)    6150144   \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 52, 66, 56, 32)    27680     \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (None, 52, 66, 56, 32)    6150144   \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 26, 33, 28, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 26, 33, 28, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 26, 33, 28, 16)    13840     \n",
      "_________________________________________________________________\n",
      "p_re_lu_2 (PReLU)            (None, 26, 33, 28, 16)    384384    \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 26, 33, 28, 16)    6928      \n",
      "_________________________________________________________________\n",
      "p_re_lu_3 (PReLU)            (None, 26, 33, 28, 16)    384384    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 13, 16, 14, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 13, 16, 14, 16)    64        \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 13, 16, 14, 8)     3464      \n",
      "_________________________________________________________________\n",
      "p_re_lu_4 (PReLU)            (None, 13, 16, 14, 8)     23296     \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 13, 16, 14, 8)     1736      \n",
      "_________________________________________________________________\n",
      "p_re_lu_5 (PReLU)            (None, 13, 16, 14, 8)     23296     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 6, 8, 7, 8)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 8, 7, 8)        32        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 5378      \n",
      "_________________________________________________________________\n",
      "p_re_lu_6 (PReLU)            (None, 2)                 2         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2688)              8064      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 6, 8, 7, 8)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling3d (UpSampling3D) (None, 12, 16, 14, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_transpose (Conv3DTran (None, 13, 16, 14, 8)     136       \n",
      "_________________________________________________________________\n",
      "p_re_lu_7 (PReLU)            (None, 13, 16, 14, 8)     23296     \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_1 (Conv3DTr (None, 13, 16, 14, 8)     136       \n",
      "_________________________________________________________________\n",
      "p_re_lu_8 (PReLU)            (None, 13, 16, 14, 8)     23296     \n",
      "_________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3 (None, 26, 32, 28, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_2 (Conv3DTr (None, 26, 33, 28, 16)    272       \n",
      "_________________________________________________________________\n",
      "p_re_lu_9 (PReLU)            (None, 26, 33, 28, 16)    384384    \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_3 (Conv3DTr (None, 26, 33, 28, 16)    528       \n",
      "_________________________________________________________________\n",
      "p_re_lu_10 (PReLU)           (None, 26, 33, 28, 16)    384384    \n",
      "_________________________________________________________________\n",
      "up_sampling3d_2 (UpSampling3 (None, 52, 66, 56, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_4 (Conv3DTr (None, 52, 66, 56, 32)    544       \n",
      "_________________________________________________________________\n",
      "p_re_lu_11 (PReLU)           (None, 52, 66, 56, 32)    6150144   \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_5 (Conv3DTr (None, 52, 66, 56, 32)    1056      \n",
      "_________________________________________________________________\n",
      "p_re_lu_12 (PReLU)           (None, 52, 66, 56, 32)    6150144   \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_6 (Conv3DTr (None, 52, 66, 56, 53)    1749      \n",
      "_________________________________________________________________\n",
      "p_re_lu_13 (PReLU)           (None, 52, 66, 56, 53)    10186176  \n",
      "=================================================================\n",
      "Total params: 36,535,033\n",
      "Trainable params: 36,534,921\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### checkpoint_dir = './99_Training_checkpoints/mri-fnc-loading'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                               min_delta=0.001, \n",
    "#                                               patience=10, \n",
    "#                                               verbose=1, \n",
    "#                                               mode='min',\n",
    "#                                               baseline=None, \n",
    "#                                               restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_prefix = os.path.join('./99_Training_checkpoints/mri_clustering', \"ckpt_{epoch}\")\n",
    "\n",
    "# callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./99_Logs/mri_clustering'),\n",
    "#              tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "#                                                 save_weights_only=True),\n",
    "#              tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                               min_delta=0.001, \n",
    "#                                               patience=5, \n",
    "#                                               verbose=1, \n",
    "#                                               mode='min',\n",
    "#                                               baseline=None, \n",
    "#                                               restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1470/1470 [==============================] - 8630s 6s/step - loss: 1.2870 - rmse: 1.1345 - val_loss: 0.7527 - val_rmse: 0.8676\n",
      "Epoch 2/400\n",
      "1470/1470 [==============================] - 8586s 6s/step - loss: 0.6158 - rmse: 0.7847 - val_loss: 0.5071 - val_rmse: 0.7121\n",
      "Epoch 3/400\n",
      "1470/1470 [==============================] - 8569s 6s/step - loss: 0.4421 - rmse: 0.6649 - val_loss: 0.3925 - val_rmse: 0.6265\n",
      "Epoch 4/400\n",
      "1470/1470 [==============================] - 8578s 6s/step - loss: 0.3654 - rmse: 0.6045 - val_loss: 0.3449 - val_rmse: 0.5872\n",
      "Epoch 5/400\n",
      "1470/1470 [==============================] - 8574s 6s/step - loss: 0.3325 - rmse: 0.5766 - val_loss: 0.3227 - val_rmse: 0.5681\n",
      "Epoch 6/400\n",
      "1470/1470 [==============================] - 8629s 6s/step - loss: 0.3165 - rmse: 0.5626 - val_loss: 0.3116 - val_rmse: 0.5582\n",
      "Epoch 7/400\n",
      "1470/1470 [==============================] - 8830s 6s/step - loss: 0.3085 - rmse: 0.5554 - val_loss: 0.3060 - val_rmse: 0.5532\n",
      "Epoch 8/400\n",
      "1470/1470 [==============================] - 8700s 6s/step - loss: 0.3045 - rmse: 0.5518 - val_loss: 0.3032 - val_rmse: 0.5507\n",
      "Epoch 9/400\n",
      "1470/1470 [==============================] - 8703s 6s/step - loss: 0.3024 - rmse: 0.5499 - val_loss: 0.3018 - val_rmse: 0.5493\n",
      "Epoch 10/400\n",
      "1470/1470 [==============================] - 8673s 6s/step - loss: 0.3013 - rmse: 0.5489 - val_loss: 0.3010 - val_rmse: 0.5486\n",
      "Epoch 11/400\n",
      "1470/1470 [==============================] - 8659s 6s/step - loss: 0.3007 - rmse: 0.5483 - val_loss: 0.3004 - val_rmse: 0.5481\n",
      "Epoch 12/400\n",
      "1470/1470 [==============================] - 8618s 6s/step - loss: 0.3002 - rmse: 0.5479 - val_loss: 0.3001 - val_rmse: 0.5478\n",
      "Epoch 13/400\n",
      "1470/1470 [==============================] - 8625s 6s/step - loss: 0.2999 - rmse: 0.5477 - val_loss: 0.2998 - val_rmse: 0.5475\n",
      "Epoch 14/400\n",
      "1470/1470 [==============================] - 8628s 6s/step - loss: 0.2997 - rmse: 0.5474 - val_loss: 0.2996 - val_rmse: 0.5473\n",
      "Epoch 15/400\n",
      "1470/1470 [==============================] - 8620s 6s/step - loss: 0.2995 - rmse: 0.5473 - val_loss: 0.2994 - val_rmse: 0.5472\n",
      "Epoch 16/400\n",
      "1470/1470 [==============================] - 8551s 6s/step - loss: 0.2993 - rmse: 0.5471 - val_loss: 0.2993 - val_rmse: 0.5471\n",
      "Epoch 17/400\n",
      "1470/1470 [==============================] - 8675s 6s/step - loss: 0.2992 - rmse: 0.5470 - val_loss: 0.2992 - val_rmse: 0.5470\n",
      "Epoch 18/400\n",
      "1470/1470 [==============================] - 8658s 6s/step - loss: 0.2991 - rmse: 0.5469 - val_loss: 0.2990 - val_rmse: 0.5469\n",
      "Epoch 19/400\n",
      "1470/1470 [==============================] - 8661s 6s/step - loss: 0.2990 - rmse: 0.5468 - val_loss: 0.2990 - val_rmse: 0.5468\n",
      "Epoch 20/400\n",
      "1470/1470 [==============================] - ETA: 0s - loss: 0.2989 - rmse: 0.5467Restoring model weights from the end of the best epoch.\n",
      "1470/1470 [==============================] - 8682s 6s/step - loss: 0.2989 - rmse: 0.5467 - val_loss: 0.2989 - val_rmse: 0.5467\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "# with tf.device('/GPU:0'):\n",
    "# # with tf.device('/CPU:0'):\n",
    "#     hist = autoencoder.fit(ds_train,\n",
    "#                      validation_data=ds_train,\n",
    "#                      callbacks=callbacks,\n",
    "#                      epochs=400,\n",
    "#                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder.save_weights('./99_Training_checkpoints/mri_clustering/model_weights_02.h5')\n",
    "# tf.keras.models.save_model(autoencoder, \n",
    "#                            filepath='./99_Training_checkpoints/mri_clustering/model_02.h5', overwrite=True, include_optimizer=True, save_format=None,\n",
    "#     signatures=None, options=None\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights('./99_Training_checkpoints/mri_clustering/run_02/model_weights_02.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_reveal_s2_enc = encoder.predict(ds_reveal_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_noreveal_enc = encoder.predict(ds_noreveal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_reveal_s2_enc_mean = np.mean(y_reveal_s2_enc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_noreveal_enc_mean = np.mean(y_noreveal_enc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('y_reveal_s2_enc.csv', y_reveal_s2_enc, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('y_noreveal_enc.csv', y_noreveal_enc, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('y_reveal_s2_enc_mean.csv', y_reveal_s2_enc_mean, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('y_noreveal_enc_mean.csv', y_noreveal_enc_mean, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reveal_s2_enc = np.genfromtxt('y_reveal_s2_enc.csv', delimiter=',')\n",
    "y_noreveal_enc = np.genfromtxt('y_noreveal_enc.csv', delimiter=',')\n",
    "\n",
    "y_reveal_s2_enc_mean = np.genfromtxt('y_reveal_s2_enc_mean.csv', delimiter=',')\n",
    "y_noreveal_enc_mean = np.genfromtxt('y_noreveal_enc_mean.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1474206 , -0.07687219])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_reveal_s2_enc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10948444, -0.10004571])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_noreveal_enc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 2) (11244, 2)\n"
     ]
    }
   ],
   "source": [
    "print(y_reveal_s2_enc.shape, y_noreveal_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_all = np.stack((y_reveal_s2_enc, y_noreveal_enc), axis=0)\n",
    "y_all = np.append(y_reveal_s2_enc, y_noreveal_enc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11754, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = keras.layers.InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = keras.layers.InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.        \n",
    "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "model = keras.Model(inputs=encoder.input, outputs=[clustering_layer, autoencoder.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\00_data\\python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1008: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return self.fit(X, sample_weight=sample_weight).labels_\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.27640564, -0.22242   ],\n",
       "       [ 0.19237857,  0.12753383]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=n_clusters, init=np.array([y_reveal_s2_enc_mean, y_noreveal_enc_mean]))\n",
    "y_pred_km = kmeans.fit_predict(y_all)\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04507672,  0.05673662],\n",
       "       [-0.22917223, -0.21675597]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm = mixture.GaussianMixture(n_components=2, \n",
    "                             covariance_type='full', \n",
    "                             tol=0.0001, \n",
    "                             reg_covar=1e-06, \n",
    "                             max_iter=1000, \n",
    "                             n_init=1, \n",
    "                             init_params='kmeans', \n",
    "                             weights_init=[1-len(y_reveal_s2_enc)/len(y_all),\n",
    "                                           1-len(y_noreveal_enc)/len(y_all)], \n",
    "                             means_init=[y_reveal_s2_enc_mean, y_noreveal_enc_mean], \n",
    "                             precisions_init=None, \n",
    "                             random_state=30, \n",
    "                             verbose=0, \n",
    "                             verbose_interval=10)\n",
    "y_pred_gm = gm.fit_predict(y_all)\n",
    "gm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_centers_km = kmeans.cluster_centers_\n",
    "init_centers_gm = gm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_inputs('10002')[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def input_fn():\n",
    "#     def get_inputs(idx):\n",
    "#         #MRI inputs\n",
    "#         patient_SM = h5py.File('00_Data/fMRI_all/{0}.mat'.format(idx), mode='r')\n",
    "#         patient_SM = np.array(patient_SM.get('SM_feature'))\n",
    "#     #     print(patient_SM.shape)\n",
    "#         k = 1\n",
    "#         ki_padding = 3\n",
    "\n",
    "#         arr_regions = []\n",
    "#         for i in range(patient_SM.shape[0]):\n",
    "#             sample_map = patient_SM[i,:,:,:]\n",
    "#             if k > 1:\n",
    "#                 map_shape = sample_map.shape\n",
    "#                 shape_pad = ((map_shape[0]//k + 1)*k - map_shape[0],\n",
    "#                              (map_shape[1]//k + 1)*k - map_shape[1],\n",
    "#                              (map_shape[2]//k + 1)*k - map_shape[2])\n",
    "\n",
    "#                 npad = (((0 if shape_pad[0]%2==0 else shape_pad[0]//2), (shape_pad[0]//2 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "#                         ((0 if shape_pad[1]%2==0 else shape_pad[0]//2), (shape_pad[1]//2 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "#                         ((0 if shape_pad[2]%2==0 else shape_pad[0]//2), (shape_pad[2]//2 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "#                 sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "#                 sx = sample_map_padded.shape[0] / k\n",
    "#                 sy = sample_map_padded.shape[1] / k\n",
    "#                 sz = sample_map_padded.shape[2] / k\n",
    "#                 for kz in range(k):\n",
    "#                     for ky in range(k):\n",
    "#                         for kx in range(k):\n",
    "#                             ki_region = sample_map_padded[int(kx*sx): int(kx*sx + sx - 1), \n",
    "#                                                          int(ky*sy): int(ky*sy + sy - 1), \n",
    "#                                                          int(kz*sz): int(kz*sz + sz - 1)]\n",
    "#                             #padding i-th region by 3 pixels\n",
    "#                             ki_region_padded = np.pad(ki_region, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "#                             arr_regions.append(ki_region_padded)\n",
    "#             else:\n",
    "#                 map_shape = sample_map.shape\n",
    "#                 shape_pad = ((map_shape[0]//2 + 1)*2 - map_shape[0],\n",
    "#                              (map_shape[1]//2 + 1)*2 - map_shape[1],\n",
    "#                              (map_shape[2]//2 + 1)*2 - map_shape[2])\n",
    "\n",
    "#                 npad = (((0 if shape_pad[0]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "#                         ((0 if shape_pad[1]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "#                         ((0 if shape_pad[2]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "#                 sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "#     #             sample_map_padded = np.pad(sample_map, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "#                 arr_regions.append(sample_map_padded)\n",
    "\n",
    "#         X_mri = np.stack(arr_regions, axis=3)\n",
    "#     #     print(X_mri.shape)\n",
    "#         return X_mri\n",
    "    \n",
    "#     def new_py_function(func, inp, Tout, name=None):\n",
    "#         def wrapped_func(*flat_inp):\n",
    "#             reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp,\n",
    "#                                                          expand_composites=True)\n",
    "#             out = func(*reconstructed_inp)\n",
    "#             return tf.nest.flatten(out, expand_composites=True)\n",
    "\n",
    "#         flat_Tout = tf.nest.flatten(Tout, expand_composites=True)\n",
    "#         flat_out = tf.py_function(func=wrapped_func, \n",
    "#                                   inp=tf.nest.flatten(inp, expand_composites=True),\n",
    "#                                   Tout=[_tensor_spec_to_dtype(v) for v in flat_Tout],\n",
    "#                                   name=name)\n",
    "#         spec_out = tf.nest.map_structure(_dtype_to_tensor_spec, Tout, expand_composites=True)\n",
    "#         out = tf.nest.pack_sequence_as(spec_out, flat_out, expand_composites=True)\n",
    "#         return out\n",
    "\n",
    "#     def _dtype_to_tensor_spec(v):\n",
    "#         return tf.TensorSpec(None, v) if isinstance(v, tf.dtypes.DType) else v\n",
    "\n",
    "#     def _tensor_spec_to_dtype(v):\n",
    "#         return v.dtype if isinstance(v, tf.TensorSpec) else v\n",
    "    \n",
    "#     def get_dataset(data, batch_size):\n",
    "#         data = np.array([int(i) for i in data])\n",
    "#         data = tf.data.Dataset.from_tensor_slices(data)\n",
    "#         data = data.shuffle(buffer_size=4000, seed=30, reshuffle_each_iteration=True)\n",
    "\n",
    "#         data = data.map(lambda idx: new_py_function(get_inputs, inp=[idx], \n",
    "#                                                         Tout=tf.TensorSpec(shape=(None, 52, 66, 56, 53), dtype=tf.dtypes.float32), \n",
    "#                                                     name=None), \n",
    "#                          num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "#                          deterministic=False)\n",
    "#         data = data.batch(batch_size, drop_remainder=False)\n",
    "#         data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#         return data\n",
    "    \n",
    "#     return get_dataset(ALL_IDS, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_points = 100\n",
    "# dimensions = 4\n",
    "# points = np.random.uniform(0, 1000, [num_points, 5,6,5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def input_fn():\n",
    "# #     return tf.compat.v1.train.limit_epochs(tf.convert_to_tensor(points, dtype=tf.float32), num_epochs=1)\n",
    "#     return tf.data.Dataset.from_tensors(tf.convert_to_tensor(points, dtype=tf.float32)).repeat(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans = tf.compat.v1.estimator.experimental.KMeans(num_clusters=n_clusters, \n",
    "#                                                     use_mini_batch=True, \n",
    "#                                                     model_dir='./99_Training_checkpoints/KMeans')\n",
    "\n",
    "# # train\n",
    "# num_iterations = 10\n",
    "# previous_centers = None\n",
    "# for _ in range(num_iterations):\n",
    "#     kmeans.train(input_fn)\n",
    "#     cluster_centers = kmeans.cluster_centers()\n",
    "#     if previous_centers is not None:\n",
    "#         print ('delta:', cluster_centers - previous_centers)\n",
    "#     previous_centers = cluster_centers\n",
    "#     print ('score:', kmeans.score(input_fn))\n",
    "# print ('cluster centers:', cluster_centers)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(name='clustering').set_weights([init_centers_gm])\n",
    "# model.compile(optimizer='adam', loss='kld')\n",
    "optim = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95)\n",
    "# METRICS = [keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    "model.compile(loss=['kld', 'mse'], optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.MeanSquaredError(reduction=losses_utils.ReductionV2.AUTO, name='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # computing an auxiliary target distribution\n",
    "# def target_distribution(q):\n",
    "#     weight = q ** 2 / q.sum(0)\n",
    "#     return (weight.T / weight.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, x, y, training):\n",
    "    # computing an auxiliary target distribution\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "    \n",
    "    # training=training is needed only if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    q, _ = model(x, training=training)\n",
    "    p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "    \n",
    "    return loss_object(y_true=y, y_pred=y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, training=True)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_last = np.copy(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "index = 0\n",
    "maxiter = 1470\n",
    "# update_interval = 140\n",
    "index_array = np.arange(x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.001 # tolerance threshold to stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ite in range(int(maxiter)):\n",
    "    if ite % update_interval == 0:\n",
    "        q = model.predict(x, verbose=0)\n",
    "        p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "        # evaluate the clustering performance\n",
    "        y_pred = q.argmax(1)\n",
    "        if y is not None:\n",
    "            acc = np.round(metrics.acc(y, y_pred), 5)\n",
    "            nmi = np.round(metrics.nmi(y, y_pred), 5)\n",
    "            ari = np.round(metrics.ari(y, y_pred), 5)\n",
    "            loss = np.round(loss, 5)\n",
    "            print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "        # check stop criterion\n",
    "        delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        if ite > 0 and delta_label < tol:\n",
    "            print('delta_label ', delta_label, '< tol ', tol)\n",
    "            print('Reached tolerance threshold. Stopping training.')\n",
    "            break\n",
    "    idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "    loss = model.train_on_batch(x=x[idx], y=p[idx])\n",
    "    index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "model.save_weights(save_dir + '/conv_DEC_model_final.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
