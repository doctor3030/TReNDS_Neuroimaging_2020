{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing as prep\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn import cluster\n",
    "from sklearn import mixture\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "\n",
    "import h5py\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_test'))]\n",
    "TRAIN_IDS = [map_id.split('.')[0] for map_id in sorted(os.listdir('00_Data/fMRI_train'))]\n",
    "ALL_IDS = TRAIN_IDS + TEST_IDS\n",
    "REVEAL_IDS_S2 = pd.read_csv('00_Data/reveal_ID_site2.csv', dtype=str).values\n",
    "NOREVEAL_IDS = [i for i in ALL_IDS if i not in REVEAL_IDS_S2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11754 510 11244\n"
     ]
    }
   ],
   "source": [
    "print(len(ALL_IDS), len(REVEAL_IDS_S2), len(NOREVEAL_IDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>age</th>\n",
       "      <th>domain1_var1</th>\n",
       "      <th>domain1_var2</th>\n",
       "      <th>domain2_var1</th>\n",
       "      <th>domain2_var2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>57.436077</td>\n",
       "      <td>30.571975</td>\n",
       "      <td>62.553736</td>\n",
       "      <td>53.325130</td>\n",
       "      <td>51.427998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10002</td>\n",
       "      <td>59.580851</td>\n",
       "      <td>50.969456</td>\n",
       "      <td>67.470628</td>\n",
       "      <td>60.651856</td>\n",
       "      <td>58.311361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10004</td>\n",
       "      <td>71.413018</td>\n",
       "      <td>53.152498</td>\n",
       "      <td>58.012103</td>\n",
       "      <td>52.418389</td>\n",
       "      <td>62.536641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10007</td>\n",
       "      <td>38.617381</td>\n",
       "      <td>49.197021</td>\n",
       "      <td>65.674285</td>\n",
       "      <td>40.151376</td>\n",
       "      <td>34.096421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10008</td>\n",
       "      <td>35.326582</td>\n",
       "      <td>15.769168</td>\n",
       "      <td>65.782269</td>\n",
       "      <td>44.643805</td>\n",
       "      <td>50.448485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5872</th>\n",
       "      <td>21654</td>\n",
       "      <td>53.103634</td>\n",
       "      <td>50.951656</td>\n",
       "      <td>62.168022</td>\n",
       "      <td>49.389400</td>\n",
       "      <td>53.020847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5873</th>\n",
       "      <td>21665</td>\n",
       "      <td>38.246437</td>\n",
       "      <td>48.018227</td>\n",
       "      <td>59.522285</td>\n",
       "      <td>45.697098</td>\n",
       "      <td>53.208160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5874</th>\n",
       "      <td>21674</td>\n",
       "      <td>69.414169</td>\n",
       "      <td>58.593918</td>\n",
       "      <td>60.298779</td>\n",
       "      <td>49.865669</td>\n",
       "      <td>47.863167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5875</th>\n",
       "      <td>21693</td>\n",
       "      <td>62.009209</td>\n",
       "      <td>54.272484</td>\n",
       "      <td>60.474388</td>\n",
       "      <td>52.325031</td>\n",
       "      <td>52.989803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5876</th>\n",
       "      <td>21734</td>\n",
       "      <td>36.072495</td>\n",
       "      <td>46.474880</td>\n",
       "      <td>61.304012</td>\n",
       "      <td>42.742592</td>\n",
       "      <td>53.425110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5877 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id        age  domain1_var1  domain1_var2  domain2_var1  domain2_var2\n",
       "0     10001  57.436077     30.571975     62.553736     53.325130     51.427998\n",
       "1     10002  59.580851     50.969456     67.470628     60.651856     58.311361\n",
       "2     10004  71.413018     53.152498     58.012103     52.418389     62.536641\n",
       "3     10007  38.617381     49.197021     65.674285     40.151376     34.096421\n",
       "4     10008  35.326582     15.769168     65.782269     44.643805     50.448485\n",
       "...     ...        ...           ...           ...           ...           ...\n",
       "5872  21654  53.103634     50.951656     62.168022     49.389400     53.020847\n",
       "5873  21665  38.246437     48.018227     59.522285     45.697098     53.208160\n",
       "5874  21674  69.414169     58.593918     60.298779     49.865669     47.863167\n",
       "5875  21693  62.009209     54.272484     60.474388     52.325031     52.989803\n",
       "5876  21734  36.072495     46.474880     61.304012     42.742592     53.425110\n",
       "\n",
       "[5877 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('00_Data/train_scores_full.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(idx):\n",
    "    #MRI inputs\n",
    "    patient_SM = h5py.File('00_Data/fMRI_all/{0}.mat'.format(idx), mode='r')\n",
    "    patient_SM = np.array(patient_SM.get('SM_feature'))\n",
    "#     print(patient_SM.shape)\n",
    "    k = 1\n",
    "    ki_padding = 3\n",
    "    \n",
    "    arr_regions = []\n",
    "    for i in range(patient_SM.shape[0]):\n",
    "        sample_map = patient_SM[i,:,:,:]\n",
    "        if k > 1:\n",
    "            map_shape = sample_map.shape\n",
    "            shape_pad = ((map_shape[0]//k + 1)*k - map_shape[0],\n",
    "                         (map_shape[1]//k + 1)*k - map_shape[1],\n",
    "                         (map_shape[2]//k + 1)*k - map_shape[2])\n",
    "\n",
    "            npad = (((0 if shape_pad[0]%2==0 else shape_pad[0]//2), (shape_pad[0]//2 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "                    ((0 if shape_pad[1]%2==0 else shape_pad[0]//2), (shape_pad[1]//2 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "                    ((0 if shape_pad[2]%2==0 else shape_pad[0]//2), (shape_pad[2]//2 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "            sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "            sx = sample_map_padded.shape[0] / k\n",
    "            sy = sample_map_padded.shape[1] / k\n",
    "            sz = sample_map_padded.shape[2] / k\n",
    "            for kz in range(k):\n",
    "                for ky in range(k):\n",
    "                    for kx in range(k):\n",
    "                        ki_region = sample_map_padded[int(kx*sx): int(kx*sx + sx - 1), \n",
    "                                                     int(ky*sy): int(ky*sy + sy - 1), \n",
    "                                                     int(kz*sz): int(kz*sz + sz - 1)]\n",
    "                        #padding i-th region by 3 pixels\n",
    "                        ki_region_padded = np.pad(ki_region, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "                        arr_regions.append(ki_region_padded)\n",
    "        else:\n",
    "            map_shape = sample_map.shape\n",
    "            shape_pad = ((map_shape[0]//2 + 1)*2 - map_shape[0],\n",
    "                         (map_shape[1]//2 + 1)*2 - map_shape[1],\n",
    "                         (map_shape[2]//2 + 1)*2 - map_shape[2])\n",
    "\n",
    "            npad = (((0 if shape_pad[0]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[0]%2==0 else shape_pad[0]//2+1)),    \n",
    "                    ((0 if shape_pad[1]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[1]%2==0 else shape_pad[1]//2+1)),    \n",
    "                    ((0 if shape_pad[2]%2==0 else shape_pad[0]//2+1), (0 if shape_pad[2]%2==0 else shape_pad[2]//2+1)))\n",
    "\n",
    "            sample_map_padded = np.pad(sample_map, pad_width=npad, mode='constant', constant_values=0)\n",
    "            \n",
    "#             sample_map_padded = np.pad(sample_map, pad_width=ki_padding, mode='constant', constant_values=0)\n",
    "            arr_regions.append(sample_map_padded)\n",
    "            \n",
    "    X_mri = np.stack(arr_regions, axis=3)\n",
    "#     print(X_mri.shape)\n",
    "    return X_mri, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_inputs('10002')[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_py_function(func, inp, Tout, name=None):\n",
    "    \n",
    "    def wrapped_func(*flat_inp):\n",
    "        reconstructed_inp = tf.nest.pack_sequence_as(inp, flat_inp,\n",
    "                                                     expand_composites=True)\n",
    "        out = func(*reconstructed_inp)\n",
    "        return tf.nest.flatten(out, expand_composites=True)\n",
    "    \n",
    "    flat_Tout = tf.nest.flatten(Tout, expand_composites=True)\n",
    "    flat_out = tf.py_function(func=wrapped_func, \n",
    "                              inp=tf.nest.flatten(inp, expand_composites=True),\n",
    "                              Tout=[_tensor_spec_to_dtype(v) for v in flat_Tout],\n",
    "                              name=name)\n",
    "    spec_out = tf.nest.map_structure(_dtype_to_tensor_spec, Tout, expand_composites=True)\n",
    "    out = tf.nest.pack_sequence_as(spec_out, flat_out, expand_composites=True)\n",
    "    return out\n",
    "\n",
    "def _dtype_to_tensor_spec(v):\n",
    "    return tf.TensorSpec(None, v) if isinstance(v, tf.dtypes.DType) else v\n",
    "\n",
    "def _tensor_spec_to_dtype(v):\n",
    "    return v.dtype if isinstance(v, tf.TensorSpec) else v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data, batch_size):\n",
    "    data = np.array([int(i) for i in data])\n",
    "    data = tf.data.Dataset.from_tensor_slices(data)\n",
    "    data = data.shuffle(buffer_size=12000, seed=30, reshuffle_each_iteration=True)\n",
    "    \n",
    "    data = data.map(lambda idx: new_py_function(get_inputs, inp=[idx], \n",
    "                                                    Tout=(tf.TensorSpec(shape=(None, 52, 66, 56, 53), dtype=tf.dtypes.float64),\n",
    "                                                          tf.int32), \n",
    "                                                name=None), \n",
    "                     num_parallel_calls=tf.data.experimental.AUTOTUNE, \n",
    "                     deterministic=False)\n",
    "    data = data.batch(batch_size, drop_remainder=False)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "ds_train = get_dataset(ALL_IDS, batch_size)\n",
    "ds_reveal_s2 = get_dataset(REVEAL_IDS_S2, batch_size)\n",
    "ds_noreveal = get_dataset(NOREVEAL_IDS, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ds_train.take(1):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE_mri = (52, 66, 56, 53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, filters=[32, 16, 8, 2]):\n",
    "    \n",
    "    #============================================================================\n",
    "    # ENCODER\n",
    "    #============================================================================\n",
    "    inputs_mri = keras.layers.Input(shape=INPUT_SHAPE_mri, name='inpupt_mri')\n",
    "\n",
    "    # convolution block #1\n",
    "    x = keras.layers.Conv3D(filters[0], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(inputs_mri)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.Conv3D(filters[0], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "#     x, p1_idx = tf.nn.max_pool_with_argmax(x, ksize=[2], strides=[2], padding='SAME', name=\"p1\")\n",
    "    x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "    # convolution block #2\n",
    "    x = keras.layers.Conv3D(filters[1], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.Conv3D(filters[1], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "    x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "    # convolution block #3\n",
    "    x = keras.layers.Conv3D(filters[2], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.Conv3D(filters[2], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "                                  kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                  bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "    x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "                                              beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "                                              moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "                                              beta_constraint=None, gamma_constraint=None)(x)\n",
    "\n",
    "    # convolution block #4\n",
    "#     x = keras.layers.Conv3D(filters[3], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "#                                   kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                   bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = keras.layers.Conv3D(filters[3], kernel_size=(3, 3, 3), strides=(1,1,1), padding='same',\n",
    "#                                   kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                   bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = keras.layers.MaxPooling3D(pool_size=(2, 2, 2), strides=(2,2,2))(x)\n",
    "#     x = keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, \n",
    "#                                               beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros',\n",
    "#                                               moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, \n",
    "#                                               beta_constraint=None, gamma_constraint=None)(x)\n",
    "    \n",
    "\n",
    "    flatten = keras.layers.Flatten(data_format='channels_last')(x)\n",
    "\n",
    "    encoded = keras.layers.Dense(2,\n",
    "                               kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                               bias_initializer=keras.initializers.Constant(5.))(flatten)\n",
    "    encoded = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.5))(encoded)\n",
    "\n",
    "    \n",
    "    #============================================================================\n",
    "    # DECODER\n",
    "    #============================================================================\n",
    "    x = keras.layers.Dense(filters[2]*int(input_shape[0]/8)*int(input_shape[1]/8)*int(input_shape[2]/8),\n",
    "                           kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                           bias_initializer=keras.initializers.Constant(5.))(encoded)\n",
    "    \n",
    "    x = keras.layers.Reshape((int(input_shape[0]/8), int(input_shape[1]/8), int(input_shape[2]/8), filters[2]))(x)\n",
    "    \n",
    "    # convolution block #4\n",
    "#     x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "#     x = tf.keras.layers.Conv3DTranspose(filters[2], kernel_size=(1, 1, 2), strides=(1,1,1), padding='valid',\n",
    "#                                         kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "#                                         bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "#     x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    # convolution block #3\n",
    "    x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[2], kernel_size=(2, 1, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[2], kernel_size=(2, 1, 1), strides=(1,1,1), padding='same',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    # convolution block #2\n",
    "    x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[1], kernel_size=(1, 2, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[1], kernel_size=(1, 2, 1), strides=(1,1,1), padding='same',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    # convolution block #1\n",
    "    x = tf.keras.layers.UpSampling3D(size=(2, 2, 2))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[0], kernel_size=(1, 1, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(filters[0], kernel_size=(1, 1, 1), strides=(1,1,1), padding='same',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.Conv3DTranspose(input_shape[3], kernel_size=(1, 1, 1), strides=(1,1,1), padding='valid',\n",
    "                                        kernel_initializer=keras.initializers.he_normal(seed=30),\n",
    "                                        bias_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    x = tf.keras.layers.PReLU(alpha_initializer=keras.initializers.Constant(0.01))(x)\n",
    "    \n",
    "    decoded = x\n",
    "    \n",
    "    #============================================================================\n",
    "    # COMPILE\n",
    "    #============================================================================\n",
    "    autoencoder = keras.Model(inputs=inputs_mri, outputs=decoded, name='autoencoder')\n",
    "    encoder = keras.Model(inputs=inputs_mri, outputs=encoded, name='encoder')\n",
    "\n",
    "    optim = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95)\n",
    "\n",
    "#     METRICS = [keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    "\n",
    "#     autoencoder.compile(loss='mse', metrics=METRICS, optimizer=optim)\n",
    "    autoencoder.compile(loss='mse', optimizer=optim)\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder, encoder = create_model(INPUT_SHAPE_mri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inpupt_mri (InputLayer)      [(None, 52, 66, 56, 53)]  0         \n",
      "_________________________________________________________________\n",
      "conv3d (Conv3D)              (None, 52, 66, 56, 32)    45824     \n",
      "_________________________________________________________________\n",
      "p_re_lu (PReLU)              (None, 52, 66, 56, 32)    6150144   \n",
      "_________________________________________________________________\n",
      "conv3d_1 (Conv3D)            (None, 52, 66, 56, 32)    27680     \n",
      "_________________________________________________________________\n",
      "p_re_lu_1 (PReLU)            (None, 52, 66, 56, 32)    6150144   \n",
      "_________________________________________________________________\n",
      "max_pooling3d (MaxPooling3D) (None, 26, 33, 28, 32)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 26, 33, 28, 32)    128       \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 26, 33, 28, 16)    13840     \n",
      "_________________________________________________________________\n",
      "p_re_lu_2 (PReLU)            (None, 26, 33, 28, 16)    384384    \n",
      "_________________________________________________________________\n",
      "conv3d_3 (Conv3D)            (None, 26, 33, 28, 16)    6928      \n",
      "_________________________________________________________________\n",
      "p_re_lu_3 (PReLU)            (None, 26, 33, 28, 16)    384384    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 13, 16, 14, 16)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 13, 16, 14, 16)    64        \n",
      "_________________________________________________________________\n",
      "conv3d_4 (Conv3D)            (None, 13, 16, 14, 8)     3464      \n",
      "_________________________________________________________________\n",
      "p_re_lu_4 (PReLU)            (None, 13, 16, 14, 8)     23296     \n",
      "_________________________________________________________________\n",
      "conv3d_5 (Conv3D)            (None, 13, 16, 14, 8)     1736      \n",
      "_________________________________________________________________\n",
      "p_re_lu_5 (PReLU)            (None, 13, 16, 14, 8)     23296     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 6, 8, 7, 8)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 8, 7, 8)        32        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2688)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 5378      \n",
      "_________________________________________________________________\n",
      "p_re_lu_6 (PReLU)            (None, 2)                 2         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2688)              8064      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 6, 8, 7, 8)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling3d (UpSampling3D) (None, 12, 16, 14, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_transpose (Conv3DTran (None, 13, 16, 14, 8)     136       \n",
      "_________________________________________________________________\n",
      "p_re_lu_7 (PReLU)            (None, 13, 16, 14, 8)     23296     \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_1 (Conv3DTr (None, 13, 16, 14, 8)     136       \n",
      "_________________________________________________________________\n",
      "p_re_lu_8 (PReLU)            (None, 13, 16, 14, 8)     23296     \n",
      "_________________________________________________________________\n",
      "up_sampling3d_1 (UpSampling3 (None, 26, 32, 28, 8)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_2 (Conv3DTr (None, 26, 33, 28, 16)    272       \n",
      "_________________________________________________________________\n",
      "p_re_lu_9 (PReLU)            (None, 26, 33, 28, 16)    384384    \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_3 (Conv3DTr (None, 26, 33, 28, 16)    528       \n",
      "_________________________________________________________________\n",
      "p_re_lu_10 (PReLU)           (None, 26, 33, 28, 16)    384384    \n",
      "_________________________________________________________________\n",
      "up_sampling3d_2 (UpSampling3 (None, 52, 66, 56, 16)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_4 (Conv3DTr (None, 52, 66, 56, 32)    544       \n",
      "_________________________________________________________________\n",
      "p_re_lu_11 (PReLU)           (None, 52, 66, 56, 32)    6150144   \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_5 (Conv3DTr (None, 52, 66, 56, 32)    1056      \n",
      "_________________________________________________________________\n",
      "p_re_lu_12 (PReLU)           (None, 52, 66, 56, 32)    6150144   \n",
      "_________________________________________________________________\n",
      "conv3d_transpose_6 (Conv3DTr (None, 52, 66, 56, 53)    1749      \n",
      "_________________________________________________________________\n",
      "p_re_lu_13 (PReLU)           (None, 52, 66, 56, 53)    10186176  \n",
      "=================================================================\n",
      "Total params: 36,535,033\n",
      "Trainable params: 36,534,921\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.load_weights('./99_Training_checkpoints/mri_clustering/run_02/model_weights_02.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reveal_s2_enc = np.genfromtxt('y_reveal_s2_enc.csv', delimiter=',')\n",
    "y_noreveal_enc = np.genfromtxt('y_noreveal_enc.csv', delimiter=',')\n",
    "\n",
    "y_reveal_s2_enc_mean = np.genfromtxt('y_reveal_s2_enc_mean.csv', delimiter=',')\n",
    "y_noreveal_enc_mean = np.genfromtxt('y_noreveal_enc_mean.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1474206 , -0.07687219])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_reveal_s2_enc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10948444, -0.10004571])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_noreveal_enc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(510, 2) (11244, 2)\n"
     ]
    }
   ],
   "source": [
    "print(y_reveal_s2_enc.shape, y_noreveal_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = np.append(y_reveal_s2_enc, y_noreveal_enc, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11754, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = keras.layers.InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = keras.layers.InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.        \n",
    "                 q_ij = 1/(1+dist(x_i, Âµ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "# model = keras.Model(inputs=encoder.input, outputs=clustering_layer)\n",
    "model = keras.Model(inputs=encoder.input, outputs=[clustering_layer, autoencoder.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\00_data\\python38\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1008: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return self.fit(X, sample_weight=sample_weight).labels_\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.27640564, -0.22242   ],\n",
       "       [ 0.19237857,  0.12753383]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=n_clusters, init=np.array([y_reveal_s2_enc_mean, y_noreveal_enc_mean]))\n",
    "# kmeans = cluster.KMeans(n_clusters=n_clusters)\n",
    "y_pred_km = kmeans.fit_predict(y_all)\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array([[-0.27640564, -0.22242   ],\n",
    "#        [ 0.19237857,  0.12753383]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04507672,  0.05673662],\n",
       "       [-0.22917223, -0.21675597]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm = mixture.GaussianMixture(n_components=2, \n",
    "                             covariance_type='full', \n",
    "                             tol=0.0001, \n",
    "                             reg_covar=1e-06, \n",
    "                             max_iter=2000, \n",
    "                             n_init=1, \n",
    "                             init_params='kmeans', \n",
    "                             weights_init=[1-len(y_reveal_s2_enc)/len(y_all),\n",
    "                                           1-len(y_noreveal_enc)/len(y_all)], \n",
    "                             means_init=[y_reveal_s2_enc_mean, y_noreveal_enc_mean], \n",
    "                             precisions_init=None, \n",
    "                             random_state=30, \n",
    "                             verbose=0, \n",
    "                             verbose_interval=10)\n",
    "y_pred_gm = gm.fit_predict(y_all)\n",
    "gm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array([[ 0.04507672,  0.05673662],\n",
    "#        [-0.22917223, -0.21675597]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_centers_km = kmeans.cluster_centers_\n",
    "init_centers_gm = gm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(name='clustering').set_weights([init_centers_gm])\n",
    "# model.get_layer(name='clustering').set_weights([init_centers_km])\n",
    "optim = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95)\n",
    "# model.compile(loss='kld', optimizer=optim)\n",
    "model.compile(loss=['kld', 'mse'], loss_weights=[0.1, 1], optimizer=optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_last = np.copy(y_pred_gm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_last = np.copy(y_pred_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_object = tf.keras.losses.MeanSquaredError(reduction=losses_utils.ReductionV2.AUTO, name='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # computing an auxiliary target distribution\n",
    "# def target_distribution(q):\n",
    "#     weight = q ** 2 / q.sum(0)\n",
    "#     return (weight.T / weight.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss(model, x, y, training):\n",
    "#     # computing an auxiliary target distribution\n",
    "#     def target_distribution(q):\n",
    "#         weight = q ** 2 / q.sum(0)\n",
    "#         return (weight.T / weight.sum(1)).T\n",
    "    \n",
    "#     # training=training is needed only if there are layers with different\n",
    "#     # behavior during training versus inference (e.g. Dropout).\n",
    "#     q, _ = model(x, training=training)\n",
    "#     p = target_distribution(q)  # update the auxiliary target distribution p\n",
    "    \n",
    "#     return loss_object(y_true=y, y_pred=y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def grad(model, inputs, targets):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         loss_value = loss(model, inputs, targets, training=True)\n",
    "#     return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "#     print(weight.shape)\n",
    "    return (weight.T / weight.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = model.predict(ds_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('q.csv', q, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = np.genfromtxt('q.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = {}\n",
    "# with tqdm(total=len(ALL_IDS)) as pbar:\n",
    "#     for i in ALL_IDS:\n",
    "#         x = get_inputs(i)\n",
    "#         x = x.reshape(1,52, 66, 56, 53)\n",
    "#         preds = model.predict(x, batch_size=1, verbose=0)\n",
    "#         q[i] = preds[0]\n",
    "#         pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(q).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = pd.DataFrame(q).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q.to_csv('00_Data/q_distrib/q_0_0.csv', index=True, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = pd.read_csv('00_Data/q_distrib/q_0_0.csv', index_col=0, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [[float(i[0].split('[')[2].split(']')[0].split()[0]), float(i[0].split('[')[2].split(']')[0].split()[1])] for i in q.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = pd.DataFrame(a, index=q.index.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.to_csv('00_Data/q_distrib/q_0_0_0.csv', index=True, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = pd.read_csv('00_Data/q_distrib/q_0_0.csv', index_col=0, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=q.values\n",
    "# np.argmax(q.values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = target_distribution(q.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = pd.DataFrame(p, index=q.index.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.to_csv('00_Data/p_distrib/p_0_0.csv', index=True, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = pd.read_csv('00_Data/p_distrib/p_0_64.csv', index_col=0, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "index = 0\n",
    "# maxiter = 1470\n",
    "update_interval = 489\n",
    "# index_array = np.arange(x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.001 # tolerance threshold to stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.loc[[19612, 21716, 12677, 19759, 13122, 13602, 14409, 15592]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for step, (x_batch, idx) in enumerate(ds_train):\n",
    "#     k = p.loc[idx]\n",
    "#     print(k.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [[0.3019370138645172, 0.0018097225110977888, 0.3017560541629791],\n",
    "#      [0.29576531052589417, 0.005464247427880764, 0.29521888494491577]]\n",
    "# np.mean(np.array(a), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Step 0, loss = [0.3019370138645172, 0.0018097225110977888, 0.3017560541629791]\n",
      "Step 1, loss = [0.29576531052589417, 0.005464247427880764, 0.29521888494491577]\n",
      "Step 2, loss = [0.3003747761249542, 0.0031409231014549732, 0.3000606894493103]\n",
      "Step 3, loss = [0.2931184470653534, 0.006553595885634422, 0.292463093996048]\n",
      "Step 4, loss = [0.2991994321346283, 0.0033951830118894577, 0.29885992407798767]\n",
      "Step 5, loss = [0.31316080689430237, 0.004954854026436806, 0.31266531348228455]\n",
      "Step 6, loss = [0.29874128103256226, 0.005176136270165443, 0.29822367429733276]\n",
      "Step 7, loss = [0.29501959681510925, 0.003706768387928605, 0.2946489155292511]\n",
      "Step 8, loss = [0.2970239222049713, 0.002460853196680546, 0.2967778444290161]\n",
      "Step 9, loss = [0.2979123592376709, 0.002410979475826025, 0.29767125844955444]\n",
      "Step 10, loss = [0.2868533134460449, 0.005539259873330593, 0.28629937767982483]\n",
      "Step 11, loss = [0.2893214821815491, 0.005440941080451012, 0.2887773811817169]\n",
      "Step 12, loss = [0.29792556166648865, 0.005999105051159859, 0.29732564091682434]\n",
      "Step 13, loss = [0.2956854999065399, 0.0059629157185554504, 0.2950892150402069]\n",
      "Step 14, loss = [0.3101060092449188, 0.003935408778488636, 0.3097124695777893]\n",
      "Step 15, loss = [0.3061448037624359, 0.0020208205096423626, 0.30594271421432495]\n",
      "Step 16, loss = [0.30219531059265137, 0.002817706670612097, 0.3019135296344757]\n",
      "Step 17, loss = [0.29859495162963867, 0.006997159216552973, 0.297895222902298]\n",
      "Step 18, loss = [0.29107218980789185, 0.0038798635359853506, 0.29068419337272644]\n",
      "Step 19, loss = [0.2861863374710083, 0.004167834762483835, 0.2857695519924164]\n",
      "Step 20, loss = [0.2940051853656769, 0.00392964668571949, 0.29361221194267273]\n",
      "Step 21, loss = [0.2998500466346741, 0.0033983150497078896, 0.2995102107524872]\n",
      "Step 22, loss = [0.2961832284927368, 0.0038677393458783627, 0.2957964539527893]\n",
      "Step 23, loss = [0.2828347384929657, 0.0038279942236840725, 0.28245192766189575]\n",
      "Step 24, loss = [0.2991696000099182, 0.005208423361182213, 0.29864874482154846]\n",
      "Step 25, loss = [0.30001962184906006, 0.001552191679365933, 0.29986441135406494]\n",
      "Step 26, loss = [0.30027517676353455, 0.0037548409309238195, 0.299899697303772]\n",
      "Step 27, loss = [0.2939642667770386, 0.0023607509210705757, 0.29372820258140564]\n",
      "Step 28, loss = [0.29905378818511963, 0.005310993641614914, 0.2985226809978485]\n",
      "Step 29, loss = [0.3039352297782898, 0.007718532811850309, 0.3031633794307709]\n",
      "Step 30, loss = [0.29579076170921326, 0.0057723005302250385, 0.295213520526886]\n",
      "Step 31, loss = [0.30903008580207825, 0.0028084409423172474, 0.3087492287158966]\n",
      "Step 32, loss = [0.2848922610282898, 0.003847285406664014, 0.28450754284858704]\n",
      "Step 33, loss = [0.3110007345676422, 0.004922461695969105, 0.31050848960876465]\n",
      "Step 34, loss = [0.3021351099014282, 0.004055269528180361, 0.30172958970069885]\n",
      "Step 35, loss = [0.31204527616500854, 0.0059945047833025455, 0.31144583225250244]\n",
      "Step 36, loss = [0.3004158139228821, 0.0034651062451303005, 0.30006930232048035]\n",
      "Step 37, loss = [0.3035235106945038, 0.0024642380885779858, 0.30327707529067993]\n",
      "Step 38, loss = [0.28495773673057556, 0.0014507980085909367, 0.28481265902519226]\n",
      "Step 39, loss = [0.293260395526886, 0.002382159698754549, 0.29302218556404114]\n",
      "Step 40, loss = [0.30786120891571045, 0.0032950120512396097, 0.3075317144393921]\n",
      "Step 41, loss = [0.3009563684463501, 0.003030145075172186, 0.3006533682346344]\n",
      "Step 42, loss = [0.30065271258354187, 0.006356921512633562, 0.30001702904701233]\n",
      "Step 43, loss = [0.30417218804359436, 0.005927719175815582, 0.3035794198513031]\n",
      "Step 44, loss = [0.2877006530761719, 0.00247674947604537, 0.28745296597480774]\n",
      "Step 45, loss = [0.2925039231777191, 0.00361414672806859, 0.29214251041412354]\n",
      "Step 46, loss = [0.29424601793289185, 0.001650618389248848, 0.2940809428691864]\n",
      "Step 47, loss = [0.2965449392795563, 0.0013255999656394124, 0.2964123785495758]\n",
      "Step 48, loss = [0.2968920171260834, 0.004977298900485039, 0.2963942885398865]\n",
      "Step 49, loss = [0.3024176061153412, 0.004171917214989662, 0.30200040340423584]\n",
      "Step 50, loss = [0.28438350558280945, 0.007427921053022146, 0.28364071249961853]\n",
      "Step 51, loss = [0.28852492570877075, 0.0039726500399410725, 0.2881276607513428]\n",
      "Step 52, loss = [0.305553138256073, 0.0054094064980745316, 0.30501219630241394]\n",
      "Step 53, loss = [0.3002946078777313, 0.002154318382963538, 0.3000791668891907]\n",
      "Step 54, loss = [0.28796523809432983, 0.0018159192986786366, 0.2877836525440216]\n",
      "Step 55, loss = [0.2835547924041748, 0.003020514966920018, 0.2832527458667755]\n",
      "Step 56, loss = [0.30861932039260864, 0.004123589023947716, 0.3082069754600525]\n",
      "Step 57, loss = [0.31071463227272034, 0.002431231550872326, 0.3104715049266815]\n",
      "Step 58, loss = [0.31082141399383545, 0.005398058798164129, 0.3102816045284271]\n",
      "Step 59, loss = [0.3087403476238251, 0.0044739264994859695, 0.308292955160141]\n",
      "Step 60, loss = [0.2957318425178528, 0.003552420064806938, 0.29537659883499146]\n",
      "Step 61, loss = [0.27888381481170654, 0.004290582612156868, 0.2784547507762909]\n",
      "Step 62, loss = [0.30878525972366333, 0.0036210217513144016, 0.30842316150665283]\n",
      "Step 63, loss = [0.2935664653778076, 0.007333357818424702, 0.2928331196308136]\n",
      "Step 64, loss = [0.3077287971973419, 0.006085386499762535, 0.30712026357650757]\n",
      "Step 65, loss = [0.3076973855495453, 0.00713465316221118, 0.30698391795158386]\n",
      "Step 66, loss = [0.2917359471321106, 0.004557243548333645, 0.29128021001815796]\n",
      "Step 67, loss = [0.2998923659324646, 0.005297875497490168, 0.29936257004737854]\n",
      "Step 68, loss = [0.29483047127723694, 0.004568694159388542, 0.29437360167503357]\n",
      "Step 69, loss = [0.28486746549606323, 0.0011491371551528573, 0.2847525477409363]\n",
      "Step 70, loss = [0.2891942858695984, 0.003923559561371803, 0.2888019382953644]\n",
      "Step 71, loss = [0.2916776239871979, 0.004259935580193996, 0.29125162959098816]\n",
      "Step 72, loss = [0.2976228892803192, 0.0027498898562043905, 0.29734790325164795]\n",
      "Step 73, loss = [0.2993616759777069, 0.002919071586802602, 0.29906976222991943]\n",
      "Step 74, loss = [0.3037312924861908, 0.00391400046646595, 0.3033398985862732]\n",
      "Step 75, loss = [0.308260977268219, 0.005294859409332275, 0.3077314794063568]\n",
      "Step 76, loss = [0.3015621304512024, 0.004313371144235134, 0.3011308014392853]\n",
      "Step 77, loss = [0.29787805676460266, 0.004336017183959484, 0.2974444627761841]\n",
      "Step 78, loss = [0.30604153871536255, 0.004126071464270353, 0.3056289255619049]\n",
      "Step 79, loss = [0.3027549982070923, 0.004860338289290667, 0.30226895213127136]\n",
      "Step 80, loss = [0.30567464232444763, 0.0010800324380397797, 0.3055666387081146]\n",
      "Step 81, loss = [0.302405446767807, 0.003676047082990408, 0.3020378351211548]\n",
      "Step 82, loss = [0.3006361126899719, 0.0024401263799518347, 0.3003920912742615]\n",
      "Step 83, loss = [0.31054675579071045, 0.004529618192464113, 0.31009379029273987]\n",
      "Step 84, loss = [0.30815890431404114, 0.0024866496678441763, 0.3079102337360382]\n",
      "Step 85, loss = [0.29523974657058716, 0.005471458658576012, 0.29469260573387146]\n",
      "Step 86, loss = [0.309272825717926, 0.0028918138705193996, 0.3089836537837982]\n",
      "Step 87, loss = [0.30307623744010925, 0.003383220639079809, 0.30273792147636414]\n",
      "Step 88, loss = [0.29993772506713867, 0.0041711051017045975, 0.2995206117630005]\n",
      "Step 89, loss = [0.2968752682209015, 0.003403211710974574, 0.2965349555015564]\n",
      "Step 90, loss = [0.3099628984928131, 0.0036703702062368393, 0.30959585309028625]\n",
      "Step 91, loss = [0.29706481099128723, 0.0023892049212008715, 0.2968258857727051]\n",
      "Step 92, loss = [0.2946130633354187, 0.006917716469615698, 0.2939212918281555]\n",
      "Step 93, loss = [0.2876749634742737, 0.0022919257171452045, 0.2874457836151123]\n",
      "Step 94, loss = [0.30123695731163025, 0.0037091700360178947, 0.300866037607193]\n",
      "Step 95, loss = [0.3022744655609131, 0.006196189671754837, 0.3016548454761505]\n",
      "Step 96, loss = [0.3075994551181793, 0.004210329614579678, 0.30717840790748596]\n",
      "Step 97, loss = [0.2859240770339966, 0.00452513387426734, 0.2854715585708618]\n",
      "Step 98, loss = [0.28849345445632935, 0.005653418600559235, 0.28792810440063477]\n",
      "Step 99, loss = [0.31107720732688904, 0.004532695282250643, 0.3106239438056946]\n",
      "Step 100, loss = [0.30049997568130493, 0.0014800262870267034, 0.30035197734832764]\n",
      "Step 101, loss = [0.3096788823604584, 0.005301709286868572, 0.3091486990451813]\n",
      "Step 102, loss = [0.2761607766151428, 0.0023623830638825893, 0.27592453360557556]\n",
      "Step 103, loss = [0.3000454306602478, 0.004872594960033894, 0.299558162689209]\n",
      "Step 104, loss = [0.2927619218826294, 0.0027712550945580006, 0.2924847900867462]\n",
      "Step 105, loss = [0.29574018716812134, 0.003423625137656927, 0.2953978180885315]\n",
      "Step 106, loss = [0.3072241246700287, 0.006373418495059013, 0.30658677220344543]\n",
      "Step 107, loss = [0.2906394600868225, 0.005112881772220135, 0.2901281714439392]\n",
      "Step 108, loss = [0.3005552589893341, 0.0019146176055073738, 0.30036380887031555]\n",
      "Step 109, loss = [0.3080390691757202, 0.004982328042387962, 0.3075408339500427]\n",
      "Step 110, loss = [0.31567466259002686, 0.005514047108590603, 0.3151232600212097]\n",
      "Step 111, loss = [0.3126770257949829, 0.004263678565621376, 0.31225064396858215]\n",
      "Step 112, loss = [0.3088622987270355, 0.005198673345148563, 0.30834242701530457]\n",
      "Step 113, loss = [0.3092554211616516, 0.003862133715301752, 0.30886921286582947]\n",
      "Step 114, loss = [0.31290197372436523, 0.004670565016567707, 0.3124349117279053]\n",
      "Step 115, loss = [0.3019390106201172, 0.008518528193235397, 0.3010871708393097]\n",
      "Step 116, loss = [0.30575892329216003, 0.003808672307059169, 0.3053780496120453]\n",
      "Step 117, loss = [0.2934929132461548, 0.004315423779189587, 0.29306137561798096]\n",
      "Step 118, loss = [0.29877880215644836, 0.005396068096160889, 0.29823920130729675]\n",
      "Step 119, loss = [0.3093979060649872, 0.005908571183681488, 0.30880704522132874]\n",
      "Step 120, loss = [0.303650826215744, 0.005116757936775684, 0.3031391501426697]\n",
      "Step 121, loss = [0.31159430742263794, 0.004523747600615025, 0.3111419379711151]\n",
      "Step 122, loss = [0.2967609167098999, 0.004050903487950563, 0.29635581374168396]\n",
      "Step 123, loss = [0.2916259169578552, 0.004017394967377186, 0.2912241816520691]\n",
      "Step 124, loss = [0.31222712993621826, 0.006654242519289255, 0.3115617036819458]\n",
      "Step 125, loss = [0.3022170960903168, 0.0019321483559906483, 0.30202388763427734]\n",
      "Step 126, loss = [0.28272193670272827, 0.004317352082580328, 0.28229019045829773]\n",
      "Step 127, loss = [0.2917554974555969, 0.0031585271935909986, 0.29143965244293213]\n",
      "Step 128, loss = [0.2977871000766754, 0.004090983420610428, 0.2973780035972595]\n",
      "Step 129, loss = [0.3103499412536621, 0.006462749559432268, 0.30970367789268494]\n",
      "Step 130, loss = [0.3079231083393097, 0.0039558750577270985, 0.3075275123119354]\n",
      "Step 131, loss = [0.2796567380428314, 0.006084018386900425, 0.2790483236312866]\n",
      "Step 132, loss = [0.30772122740745544, 0.0036951196379959583, 0.3073517084121704]\n",
      "Step 133, loss = [0.3023357093334198, 0.004996245726943016, 0.3018360733985901]\n",
      "Step 134, loss = [0.3041560649871826, 0.004135915078222752, 0.3037424683570862]\n",
      "Step 135, loss = [0.3005814850330353, 0.003113738028332591, 0.30027011036872864]\n",
      "Step 136, loss = [0.29339006543159485, 0.004656852222979069, 0.2929243743419647]\n",
      "Step 137, loss = [0.3114291727542877, 0.004936825484037399, 0.31093549728393555]\n",
      "Step 138, loss = [0.29827848076820374, 0.0019253541249781847, 0.2980859577655792]\n",
      "Step 139, loss = [0.2981686294078827, 0.003560516284778714, 0.2978125810623169]\n",
      "Step 140, loss = [0.30987367033958435, 0.004558207932859659, 0.30941784381866455]\n",
      "Step 141, loss = [0.2968143820762634, 0.004985647276043892, 0.2963158190250397]\n",
      "Step 142, loss = [0.29237720370292664, 0.006250561214983463, 0.2917521595954895]\n",
      "Step 143, loss = [0.30626583099365234, 0.0043076700530946255, 0.3058350682258606]\n",
      "Step 144, loss = [0.29317158460617065, 0.003153323195874691, 0.29285624623298645]\n",
      "Step 145, loss = [0.2993197441101074, 0.0033659678883850574, 0.2989831566810608]\n",
      "Step 146, loss = [0.3072775900363922, 0.004626527428627014, 0.30681493878364563]\n",
      "Step 147, loss = [0.30537551641464233, 0.006782154086977243, 0.30469730496406555]\n",
      "Step 148, loss = [0.3127489387989044, 0.004655010066926479, 0.3122834265232086]\n",
      "Step 149, loss = [0.29520338773727417, 0.002041962929069996, 0.2949991822242737]\n",
      "Step 150, loss = [0.3053639829158783, 0.004549702629446983, 0.30490902066230774]\n",
      "Step 151, loss = [0.3082696199417114, 0.0041471077129244804, 0.30785492062568665]\n",
      "Step 152, loss = [0.30092066526412964, 0.005120890215039253, 0.30040857195854187]\n",
      "Step 153, loss = [0.29977136850357056, 0.003952012397348881, 0.29937615990638733]\n",
      "Step 154, loss = [0.31237396597862244, 0.010333988815546036, 0.3113405704498291]\n",
      "Step 155, loss = [0.2945055067539215, 0.0032804994843900204, 0.29417744278907776]\n",
      "Step 156, loss = [0.2922933101654053, 0.00594640476629138, 0.2916986644268036]\n",
      "Step 157, loss = [0.2941019833087921, 0.0037523023784160614, 0.29372674226760864]\n",
      "Step 158, loss = [0.2931918799877167, 0.004623766057193279, 0.2927294969558716]\n",
      "Step 159, loss = [0.30727702379226685, 0.004116589203476906, 0.3068653643131256]\n",
      "Step 160, loss = [0.30055075883865356, 0.006001010537147522, 0.29995065927505493]\n",
      "Step 161, loss = [0.29802024364471436, 0.003804710926488042, 0.29763978719711304]\n",
      "Step 162, loss = [0.3179648220539093, 0.003788675181567669, 0.31758594512939453]\n",
      "Step 163, loss = [0.29296258091926575, 0.007183053996413946, 0.2922442853450775]\n",
      "Step 164, loss = [0.2921679615974426, 0.0030977637507021427, 0.2918581962585449]\n",
      "Step 165, loss = [0.29768383502960205, 0.006303864996880293, 0.2970534563064575]\n",
      "Step 166, loss = [0.3008618652820587, 0.003752301214262843, 0.30048662424087524]\n",
      "Step 167, loss = [0.28857100009918213, 0.0032616746611893177, 0.2882448434829712]\n",
      "Step 168, loss = [0.30351606011390686, 0.0036977066192775965, 0.3031463027000427]\n",
      "Step 169, loss = [0.30882516503334045, 0.0035370984114706516, 0.3084714412689209]\n",
      "Step 170, loss = [0.29868748784065247, 0.004173716530203819, 0.2982701063156128]\n",
      "Step 171, loss = [0.3001954257488251, 0.003239748068153858, 0.29987144470214844]\n",
      "Step 172, loss = [0.30294933915138245, 0.002043504500761628, 0.30274498462677]\n",
      "Step 173, loss = [0.27617889642715454, 0.002014804631471634, 0.27597740292549133]\n",
      "Step 174, loss = [0.30097541213035583, 0.0016446614172309637, 0.30081093311309814]\n",
      "Step 175, loss = [0.316524863243103, 0.008599060587584972, 0.31566494703292847]\n",
      "Step 176, loss = [0.3113136887550354, 0.00625909399241209, 0.310687780380249]\n",
      "Step 177, loss = [0.29246389865875244, 0.0037821754813194275, 0.2920856773853302]\n",
      "Step 178, loss = [0.299947053194046, 0.005884217098355293, 0.29935863614082336]\n",
      "Step 179, loss = [0.2997972369194031, 0.005445149727165699, 0.2992527186870575]\n",
      "Step 180, loss = [0.3079155385494232, 0.004225380253046751, 0.3074930012226105]\n",
      "Step 181, loss = [0.2882536053657532, 0.005190995521843433, 0.2877345085144043]\n",
      "Step 182, loss = [0.30071064829826355, 0.0025965303648263216, 0.30045098066329956]\n",
      "Step 183, loss = [0.29218733310699463, 0.0058342330157756805, 0.2916039228439331]\n",
      "Step 184, loss = [0.29289186000823975, 0.003491172567009926, 0.2925427556037903]\n",
      "Step 185, loss = [0.31362617015838623, 0.0029493027832359076, 0.3133312463760376]\n",
      "Step 186, loss = [0.2927750051021576, 0.0015319767408072948, 0.29262182116508484]\n",
      "Step 187, loss = [0.3110174834728241, 0.002667954657226801, 0.31075069308280945]\n",
      "Step 188, loss = [0.2956192195415497, 0.006842588074505329, 0.2949349582195282]\n",
      "Step 189, loss = [0.3066447377204895, 0.00265638530254364, 0.306379109621048]\n",
      "Step 190, loss = [0.30738019943237305, 0.0038897786289453506, 0.30699121952056885]\n",
      "Step 191, loss = [0.3090863525867462, 0.007264824118465185, 0.30835986137390137]\n",
      "Step 192, loss = [0.30839234590530396, 0.00516972690820694, 0.3078753650188446]\n",
      "Step 193, loss = [0.3042236268520355, 0.005012258887290955, 0.30372241139411926]\n",
      "Step 194, loss = [0.2963407337665558, 0.004879591520875692, 0.29585278034210205]\n",
      "Step 195, loss = [0.3015621602535248, 0.006897370330989361, 0.30087241530418396]\n",
      "Step 196, loss = [0.31075263023376465, 0.008284459821879864, 0.3099241852760315]\n",
      "Step 197, loss = [0.30238404870033264, 0.0038283122703433037, 0.3020012080669403]\n",
      "Step 198, loss = [0.30131834745407104, 0.0033863112330436707, 0.30097970366477966]\n",
      "Step 199, loss = [0.2873808443546295, 0.0017104560974985361, 0.28720980882644653]\n",
      "Step 200, loss = [0.28491026163101196, 0.005093495361506939, 0.28440091013908386]\n",
      "Step 201, loss = [0.29687803983688354, 0.006270141806453466, 0.2962510287761688]\n",
      "Step 202, loss = [0.30350926518440247, 0.002300090156495571, 0.30327925086021423]\n",
      "Step 203, loss = [0.2907029092311859, 0.002736043417826295, 0.2904292941093445]\n",
      "Step 204, loss = [0.3132420778274536, 0.0034549764823168516, 0.31289657950401306]\n",
      "Step 205, loss = [0.30387258529663086, 0.003323716577142477, 0.3035401999950409]\n",
      "Step 206, loss = [0.29617106914520264, 0.0032598753459751606, 0.295845091342926]\n",
      "Step 207, loss = [0.28892800211906433, 0.0030465852469205856, 0.2886233329772949]\n",
      "Step 208, loss = [0.30607327818870544, 0.002966606989502907, 0.3057766258716583]\n",
      "Step 209, loss = [0.29905837774276733, 0.0030844565480947495, 0.2987499237060547]\n",
      "Step 210, loss = [0.30120205879211426, 0.00445970194414258, 0.3007560968399048]\n",
      "Step 211, loss = [0.28067874908447266, 0.0027802898548543453, 0.28040072321891785]\n",
      "Step 212, loss = [0.30701401829719543, 0.004450821317732334, 0.3065689504146576]\n",
      "Step 213, loss = [0.30697500705718994, 0.005898639559745789, 0.3063851296901703]\n",
      "Step 214, loss = [0.2903502583503723, 0.005386171396821737, 0.2898116409778595]\n",
      "Step 215, loss = [0.31266507506370544, 0.002175162546336651, 0.31244754791259766]\n",
      "Step 216, loss = [0.299638032913208, 0.003907995764166117, 0.29924723505973816]\n",
      "Step 217, loss = [0.29671186208724976, 0.005642788950353861, 0.29614758491516113]\n",
      "Step 218, loss = [0.30220767855644226, 0.006032906472682953, 0.30160439014434814]\n",
      "Step 219, loss = [0.2986624240875244, 0.00524792680516839, 0.2981376349925995]\n",
      "Step 220, loss = [0.28963345289230347, 0.005281648598611355, 0.28910529613494873]\n",
      "Step 221, loss = [0.3018215000629425, 0.005056330002844334, 0.30131587386131287]\n",
      "Step 222, loss = [0.2972067892551422, 0.0035330946557223797, 0.2968534827232361]\n",
      "Step 223, loss = [0.3104863464832306, 0.0036710789427161217, 0.31011924147605896]\n",
      "Step 224, loss = [0.2937886416912079, 0.0028614001348614693, 0.2935025095939636]\n",
      "Step 225, loss = [0.28774571418762207, 0.005272815935313702, 0.2872184216976166]\n",
      "Step 226, loss = [0.3087565004825592, 0.0029878581408411264, 0.30845770239830017]\n",
      "Step 227, loss = [0.2916759252548218, 0.0021631144918501377, 0.2914596199989319]\n",
      "Step 228, loss = [0.30244460701942444, 0.00680998619645834, 0.3017635941505432]\n",
      "Step 229, loss = [0.29868680238723755, 0.005566390231251717, 0.2981301546096802]\n",
      "Step 230, loss = [0.30724039673805237, 0.004240215755999088, 0.30681636929512024]\n",
      "Step 231, loss = [0.30478179454803467, 0.0034992736764252186, 0.30443185567855835]\n",
      "Step 232, loss = [0.31061938405036926, 0.004015781916677952, 0.31021779775619507]\n",
      "Step 233, loss = [0.2984793186187744, 0.0029014982283115387, 0.2981891632080078]\n",
      "Step 234, loss = [0.2985894978046417, 0.0031175734475255013, 0.29827773571014404]\n",
      "Step 235, loss = [0.2948787808418274, 0.003409135853871703, 0.29453787207603455]\n",
      "Step 236, loss = [0.3176725506782532, 0.0044807614758610725, 0.3172244727611542]\n",
      "Step 237, loss = [0.2979344129562378, 0.003992384299635887, 0.2975351810455322]\n",
      "Step 238, loss = [0.29955169558525085, 0.004044906236231327, 0.29914721846580505]\n",
      "Step 239, loss = [0.30036473274230957, 0.0014758850447833538, 0.3002171516418457]\n",
      "Step 240, loss = [0.29280635714530945, 0.003259682795032859, 0.29248037934303284]\n",
      "Step 241, loss = [0.3095223307609558, 0.0023119929246604443, 0.30929112434387207]\n",
      "Step 242, loss = [0.2860545814037323, 0.00342991529032588, 0.2857115864753723]\n",
      "Step 243, loss = [0.29637911915779114, 0.003314131870865822, 0.29604771733283997]\n",
      "Step 244, loss = [0.3021318018436432, 0.005098296329379082, 0.3016219735145569]\n",
      "Step 245, loss = [0.29932087659835815, 0.00399369653314352, 0.29892149567604065]\n",
      "Step 246, loss = [0.2852872312068939, 0.0069144852459430695, 0.284595787525177]\n",
      "Step 247, loss = [0.306378573179245, 0.005633275490254164, 0.3058152496814728]\n",
      "Step 248, loss = [0.29916611313819885, 0.009459465742111206, 0.298220157623291]\n",
      "Step 249, loss = [0.3002946078777313, 0.0047090728767216206, 0.29982370138168335]\n",
      "Step 250, loss = [0.2881174683570862, 0.0013578608632087708, 0.28798168897628784]\n",
      "Step 251, loss = [0.3011929392814636, 0.0027801396790891886, 0.3009149134159088]\n",
      "Step 252, loss = [0.2907724976539612, 0.004391718190163374, 0.2903333306312561]\n",
      "Step 253, loss = [0.30994856357574463, 0.004916280508041382, 0.3094569444656372]\n",
      "Step 254, loss = [0.2956698536872864, 0.004388552159070969, 0.2952309846878052]\n",
      "Step 255, loss = [0.3112923502922058, 0.005351522006094456, 0.31075718998908997]\n",
      "Step 256, loss = [0.30001068115234375, 0.005185502115637064, 0.29949212074279785]\n",
      "Step 257, loss = [0.30307549238204956, 0.006454833783209324, 0.30243000388145447]\n",
      "Step 258, loss = [0.29504647850990295, 0.0037286952137947083, 0.2946736216545105]\n",
      "Step 259, loss = [0.29055869579315186, 0.0055122473277151585, 0.29000747203826904]\n",
      "Step 260, loss = [0.3047322630882263, 0.0036933294031769037, 0.3043629229068756]\n",
      "Step 261, loss = [0.29002025723457336, 0.0043103620409965515, 0.2895892262458801]\n",
      "Step 262, loss = [0.27816104888916016, 0.006913679651916027, 0.2774696946144104]\n",
      "Step 263, loss = [0.28803178668022156, 0.0044148163869977, 0.28759029507637024]\n",
      "Step 264, loss = [0.29236000776290894, 0.0036907086614519358, 0.2919909358024597]\n",
      "Step 265, loss = [0.29211100935935974, 0.0031985347159206867, 0.2917911410331726]\n",
      "Step 266, loss = [0.2992963492870331, 0.0054322206415236, 0.29875311255455017]\n",
      "Step 267, loss = [0.2829740047454834, 0.003592998953536153, 0.28261470794677734]\n",
      "Step 268, loss = [0.30516868829727173, 0.0029442780651152134, 0.3048742711544037]\n",
      "Step 269, loss = [0.29605093598365784, 0.0033161623869091272, 0.29571932554244995]\n",
      "Step 270, loss = [0.29895344376564026, 0.002340627135708928, 0.2987193763256073]\n",
      "Step 271, loss = [0.3038976788520813, 0.005113129038363695, 0.3033863604068756]\n",
      "Step 272, loss = [0.3095509111881256, 0.003439920023083687, 0.30920693278312683]\n",
      "Step 273, loss = [0.3097679615020752, 0.004698783624917269, 0.3092980682849884]\n",
      "Step 274, loss = [0.3049655556678772, 0.0024848985485732555, 0.3047170639038086]\n",
      "Step 275, loss = [0.30507221817970276, 0.0025129320565611124, 0.3048209249973297]\n",
      "Step 276, loss = [0.3012579679489136, 0.0033565275371074677, 0.30092230439186096]\n",
      "Step 277, loss = [0.3001024127006531, 0.003941809292882681, 0.29970821738243103]\n",
      "Step 278, loss = [0.316061794757843, 0.0029493020847439766, 0.3157668709754944]\n",
      "Step 279, loss = [0.297248512506485, 0.0047684526070952415, 0.29677167534828186]\n",
      "Step 280, loss = [0.2975342571735382, 0.0031071207486093044, 0.2972235381603241]\n",
      "Step 281, loss = [0.30696260929107666, 0.0038890657015144825, 0.30657368898391724]\n",
      "Step 282, loss = [0.3029631972312927, 0.0018639503978192806, 0.3027768135070801]\n",
      "Step 283, loss = [0.2939809560775757, 0.0029887324199080467, 0.2936820685863495]\n",
      "Step 284, loss = [0.3038111925125122, 0.0019496565219014883, 0.3036162257194519]\n",
      "Step 285, loss = [0.30418160557746887, 0.00368704367429018, 0.3038128912448883]\n",
      "Step 286, loss = [0.30381259322166443, 0.003861156990751624, 0.30342647433280945]\n",
      "Step 287, loss = [0.3005686402320862, 0.002822349313646555, 0.3002864122390747]\n",
      "Step 288, loss = [0.30081918835639954, 0.005692992825061083, 0.3002498745918274]\n",
      "Step 289, loss = [0.3086642324924469, 0.008860519155859947, 0.30777817964553833]\n",
      "Step 290, loss = [0.26747456192970276, 0.0035151734482496977, 0.2671230435371399]\n",
      "Step 291, loss = [0.3007318079471588, 0.0051285987719893456, 0.30021893978118896]\n",
      "Step 292, loss = [0.30037954449653625, 0.004224731121212244, 0.2999570667743683]\n",
      "Step 293, loss = [0.29777073860168457, 0.0029878008645027876, 0.2974719703197479]\n",
      "Step 294, loss = [0.28980782628059387, 0.00391997117549181, 0.2894158363342285]\n",
      "Step 295, loss = [0.3043760061264038, 0.0038600596599280834, 0.3039900064468384]\n",
      "Step 296, loss = [0.28042298555374146, 0.0021359689999371767, 0.28020939230918884]\n",
      "Step 297, loss = [0.2999591529369354, 0.0027363670524209738, 0.2996855080127716]\n",
      "Step 298, loss = [0.30148667097091675, 0.005727448500692844, 0.30091392993927]\n",
      "Step 299, loss = [0.2991282343864441, 0.002566972281783819, 0.29887154698371887]\n",
      "Step 300, loss = [0.2909854054450989, 0.00433740671724081, 0.29055166244506836]\n",
      "Step 301, loss = [0.2886769473552704, 0.004100651014596224, 0.2882668673992157]\n",
      "Step 302, loss = [0.30313628911972046, 0.00467695901170373, 0.30266860127449036]\n",
      "Step 303, loss = [0.29883965849876404, 0.00276938290335238, 0.2985627055168152]\n",
      "Step 304, loss = [0.28488776087760925, 0.006405307445675135, 0.2842472195625305]\n",
      "Step 305, loss = [0.29974794387817383, 0.0052358657121658325, 0.2992243468761444]\n",
      "Step 306, loss = [0.31031838059425354, 0.005254869349300861, 0.3097929060459137]\n",
      "Step 307, loss = [0.30043941736221313, 0.0032862562220543623, 0.300110787153244]\n",
      "Step 308, loss = [0.3056372106075287, 0.0027077593840658665, 0.3053664267063141]\n",
      "Step 309, loss = [0.3071659207344055, 0.006005415227264166, 0.30656537413597107]\n",
      "Step 310, loss = [0.30296850204467773, 0.0015191682614386082, 0.30281659960746765]\n",
      "Step 311, loss = [0.3070406913757324, 0.0061574713326990604, 0.30642494559288025]\n",
      "Step 312, loss = [0.3076176643371582, 0.003742043860256672, 0.3072434663772583]\n",
      "Step 313, loss = [0.30031606554985046, 0.003328296123072505, 0.2999832332134247]\n",
      "Step 314, loss = [0.29610493779182434, 0.0066025275737047195, 0.29544469714164734]\n",
      "Step 315, loss = [0.30228424072265625, 0.0036290602292865515, 0.3019213378429413]\n",
      "Step 316, loss = [0.2967171370983124, 0.005762127228081226, 0.2961409389972687]\n",
      "Step 317, loss = [0.3042997717857361, 0.004970333073288202, 0.3038027286529541]\n",
      "Step 318, loss = [0.31050944328308105, 0.0072863539680838585, 0.3097808063030243]\n",
      "Step 319, loss = [0.28619876503944397, 0.002141297794878483, 0.2859846353530884]\n",
      "Step 320, loss = [0.2953130006790161, 0.002401187317445874, 0.29507288336753845]\n",
      "Step 321, loss = [0.29742544889450073, 0.004277996718883514, 0.29699763655662537]\n",
      "Step 322, loss = [0.2913780212402344, 0.003960510715842247, 0.2909819781780243]\n",
      "Step 323, loss = [0.3085910379886627, 0.004917437210679054, 0.30809929966926575]\n",
      "Step 324, loss = [0.3091484010219574, 0.003893207758665085, 0.30875909328460693]\n",
      "Step 325, loss = [0.30808255076408386, 0.003222040832042694, 0.3077603578567505]\n",
      "Step 326, loss = [0.31292232871055603, 0.004191636107861996, 0.3125031590461731]\n",
      "Step 327, loss = [0.3132634162902832, 0.0038153419736772776, 0.3128818869590759]\n",
      "Step 328, loss = [0.2917324900627136, 0.0069900075905025005, 0.29103347659111023]\n",
      "Step 329, loss = [0.299490749835968, 0.006352681666612625, 0.2988554835319519]\n",
      "Step 330, loss = [0.30345994234085083, 0.006995183415710926, 0.30276042222976685]\n",
      "Step 331, loss = [0.29644379019737244, 0.0025552755687385798, 0.29618826508522034]\n",
      "Step 332, loss = [0.30082517862319946, 0.002775022992864251, 0.30054768919944763]\n",
      "Step 333, loss = [0.2878437936306, 0.003698762971907854, 0.2874739170074463]\n",
      "Step 334, loss = [0.2995912432670593, 0.002460619667544961, 0.2993451952934265]\n",
      "Step 335, loss = [0.3056319057941437, 0.0023980806581676006, 0.3053920865058899]\n",
      "Step 336, loss = [0.3025813400745392, 0.003102575894445181, 0.3022710680961609]\n",
      "Step 337, loss = [0.29303911328315735, 0.004455366171896458, 0.2925935685634613]\n",
      "Step 338, loss = [0.3048796057701111, 0.005040050484240055, 0.3043755888938904]\n",
      "Step 339, loss = [0.30958059430122375, 0.003653007559478283, 0.3092153072357178]\n",
      "Step 340, loss = [0.29969555139541626, 0.006436252035200596, 0.2990519404411316]\n",
      "Step 341, loss = [0.29790976643562317, 0.005076410248875618, 0.29740211367607117]\n",
      "Step 342, loss = [0.3012169599533081, 0.0048857154324650764, 0.30072838068008423]\n",
      "Step 343, loss = [0.3017267882823944, 0.00640510069206357, 0.30108627676963806]\n",
      "Step 344, loss = [0.2971659004688263, 0.004473458975553513, 0.296718567609787]\n",
      "Step 345, loss = [0.2869426906108856, 0.003229292342439294, 0.28661975264549255]\n",
      "Step 346, loss = [0.2994256317615509, 0.004761283285915852, 0.2989495098590851]\n",
      "Step 347, loss = [0.28857094049453735, 0.005039967130869627, 0.28806695342063904]\n",
      "Step 348, loss = [0.29659610986709595, 0.006780887953937054, 0.2959180176258087]\n",
      "Step 349, loss = [0.2846204936504364, 0.004975898656994104, 0.28412291407585144]\n",
      "Step 350, loss = [0.29958418011665344, 0.004981322679668665, 0.2990860342979431]\n",
      "Step 351, loss = [0.2996577024459839, 0.0036136701237410307, 0.2992963492870331]\n",
      "Step 352, loss = [0.29737749695777893, 0.004296513739973307, 0.2969478368759155]\n",
      "Step 353, loss = [0.3078906238079071, 0.004660089034587145, 0.3074246048927307]\n",
      "Step 354, loss = [0.2881450653076172, 0.004589482676237822, 0.2876861095428467]\n",
      "Step 355, loss = [0.3058491051197052, 0.004183624405413866, 0.30543074011802673]\n",
      "Step 356, loss = [0.2710997760295868, 0.00659551378339529, 0.2704402208328247]\n",
      "Step 357, loss = [0.2998090088367462, 0.005831516347825527, 0.2992258667945862]\n",
      "Step 358, loss = [0.2933382987976074, 0.0031889607198536396, 0.2930194139480591]\n",
      "Step 359, loss = [0.30060166120529175, 0.006212678737938404, 0.29998040199279785]\n",
      "Step 360, loss = [0.2992289960384369, 0.004743015393614769, 0.2987546920776367]\n",
      "Step 361, loss = [0.300960510969162, 0.004356508143246174, 0.30052486062049866]\n",
      "Step 362, loss = [0.30973300337791443, 0.0034360233694314957, 0.3093894124031067]\n",
      "Step 363, loss = [0.29626861214637756, 0.00081944081466645, 0.2961866557598114]\n",
      "Step 364, loss = [0.28354451060295105, 0.00479329377412796, 0.28306517004966736]\n",
      "Step 365, loss = [0.28861576318740845, 0.0029717758297920227, 0.28831857442855835]\n",
      "Step 366, loss = [0.29598402976989746, 0.004155255854129791, 0.2955684959888458]\n",
      "Step 367, loss = [0.2898293733596802, 0.003751502139493823, 0.28945422172546387]\n",
      "Step 368, loss = [0.30712881684303284, 0.002975892275571823, 0.3068312406539917]\n",
      "Step 369, loss = [0.29653260111808777, 0.002477762522175908, 0.29628482460975647]\n",
      "Step 370, loss = [0.28727421164512634, 0.005128759890794754, 0.2867613434791565]\n",
      "Step 371, loss = [0.3088566064834595, 0.005681707989424467, 0.30828842520713806]\n",
      "Step 372, loss = [0.29793649911880493, 0.0033320002257823944, 0.2976033091545105]\n",
      "Step 373, loss = [0.30539125204086304, 0.005543636158108711, 0.3048368990421295]\n",
      "Step 374, loss = [0.3042854070663452, 0.004687447566539049, 0.30381667613983154]\n",
      "Step 375, loss = [0.2815995514392853, 0.005908479448407888, 0.28100869059562683]\n",
      "Step 376, loss = [0.2914547920227051, 0.0023919472005218267, 0.29121559858322144]\n",
      "Step 377, loss = [0.2967935800552368, 0.004477258771657944, 0.29634585976600647]\n",
      "Step 378, loss = [0.2965632975101471, 0.003531874157488346, 0.2962101101875305]\n",
      "Step 379, loss = [0.31513962149620056, 0.003999331966042519, 0.3147396743297577]\n",
      "Step 380, loss = [0.29723888635635376, 0.0023223189637064934, 0.29700666666030884]\n",
      "Step 381, loss = [0.3030009865760803, 0.0033039862755686045, 0.30267059803009033]\n",
      "Step 382, loss = [0.29549431800842285, 0.0036191774997860193, 0.2951323986053467]\n",
      "Step 383, loss = [0.30320867896080017, 0.0029769460670650005, 0.3029109835624695]\n",
      "Step 384, loss = [0.30083218216896057, 0.004543107934296131, 0.30037787556648254]\n",
      "Step 385, loss = [0.30861082673072815, 0.004652225412428379, 0.3081456124782562]\n",
      "Step 386, loss = [0.30156776309013367, 0.003073466010391712, 0.30126041173934937]\n",
      "Step 387, loss = [0.3110119700431824, 0.006214577704668045, 0.31039050221443176]\n",
      "Step 388, loss = [0.2923782467842102, 0.003840286284685135, 0.29199421405792236]\n",
      "Step 389, loss = [0.2952604591846466, 0.0034706084989011288, 0.2949134111404419]\n",
      "Step 390, loss = [0.2997295558452606, 0.0028844072949141264, 0.2994411289691925]\n",
      "Step 391, loss = [0.28821995854377747, 0.005030419677495956, 0.28771692514419556]\n",
      "Step 392, loss = [0.2879880368709564, 0.004405953921377659, 0.28754743933677673]\n",
      "Step 393, loss = [0.29096516966819763, 0.004763434175401926, 0.2904888391494751]\n",
      "Step 394, loss = [0.29769858717918396, 0.0030335159972310066, 0.2973952293395996]\n",
      "Step 395, loss = [0.30826592445373535, 0.003676199121400714, 0.30789831280708313]\n",
      "Step 396, loss = [0.30412548780441284, 0.00781495962291956, 0.3033439815044403]\n",
      "Step 397, loss = [0.3035437762737274, 0.005630452185869217, 0.3029807209968567]\n",
      "Step 398, loss = [0.29782646894454956, 0.005495171062648296, 0.29727694392204285]\n",
      "Step 399, loss = [0.3038008213043213, 0.004515472333878279, 0.3033492863178253]\n",
      "Step 400, loss = [0.30150386691093445, 0.0031500374898314476, 0.3011888563632965]\n",
      "Step 401, loss = [0.2898394763469696, 0.003439840395003557, 0.2894954979419708]\n",
      "Step 402, loss = [0.29815733432769775, 0.0043717920780181885, 0.29772016406059265]\n",
      "Step 403, loss = [0.301388144493103, 0.0041114771738648415, 0.3009769916534424]\n",
      "Step 404, loss = [0.28937846422195435, 0.003641737625002861, 0.2890142798423767]\n",
      "Step 405, loss = [0.2969916760921478, 0.0057435412891209126, 0.29641732573509216]\n",
      "Step 406, loss = [0.3052089512348175, 0.002362308092415333, 0.30497270822525024]\n",
      "Step 407, loss = [0.30355212092399597, 0.004819983616471291, 0.3030701279640198]\n",
      "Step 408, loss = [0.3032350540161133, 0.001984037458896637, 0.3030366599559784]\n",
      "Step 409, loss = [0.3052985966205597, 0.0031495275907218456, 0.30498364567756653]\n",
      "Step 410, loss = [0.3138846158981323, 0.004088497254997492, 0.31347575783729553]\n",
      "Step 411, loss = [0.3060692846775055, 0.005898869596421719, 0.30547940731048584]\n",
      "Step 412, loss = [0.3152274489402771, 0.006588566582649946, 0.31456857919692993]\n",
      "Step 413, loss = [0.2941783368587494, 0.003301088698208332, 0.2938482165336609]\n",
      "Step 414, loss = [0.27775058150291443, 0.005604808684438467, 0.27719008922576904]\n",
      "Step 415, loss = [0.3066556751728058, 0.005270153749734163, 0.3061286509037018]\n",
      "Step 416, loss = [0.300396591424942, 0.004393521696329117, 0.2999572455883026]\n",
      "Step 417, loss = [0.2955944538116455, 0.00309394346550107, 0.29528504610061646]\n",
      "Step 418, loss = [0.28634142875671387, 0.00813455879688263, 0.2855279743671417]\n",
      "Step 419, loss = [0.2996408939361572, 0.006564847659319639, 0.2989844083786011]\n",
      "Step 420, loss = [0.29770851135253906, 0.002815388375893235, 0.2974269688129425]\n",
      "Step 421, loss = [0.2905089557170868, 0.004656922072172165, 0.29004326462745667]\n",
      "Step 422, loss = [0.30316853523254395, 0.0041578407399356365, 0.3027527630329132]\n",
      "Step 423, loss = [0.3111141324043274, 0.005464464426040649, 0.3105676770210266]\n",
      "Step 424, loss = [0.30074208974838257, 0.004390746355056763, 0.30030301213264465]\n",
      "Step 425, loss = [0.3109815716743469, 0.003362051211297512, 0.31064537167549133]\n",
      "Step 426, loss = [0.2789095640182495, 0.004009971860796213, 0.27850857377052307]\n",
      "Step 427, loss = [0.3105449676513672, 0.0034158662892878056, 0.3102033734321594]\n",
      "Step 428, loss = [0.29249516129493713, 0.004545680712908506, 0.2920405864715576]\n",
      "Step 429, loss = [0.3065880239009857, 0.004160474054515362, 0.3061719834804535]\n",
      "Step 430, loss = [0.2995193898677826, 0.0038977237418293953, 0.2991296052932739]\n",
      "Step 431, loss = [0.2704281210899353, 0.004591817501932383, 0.2699689269065857]\n",
      "Step 432, loss = [0.30368632078170776, 0.00246364064514637, 0.3034399449825287]\n",
      "Step 433, loss = [0.2936108410358429, 0.002236379776149988, 0.29338720440864563]\n",
      "Step 434, loss = [0.2916316092014313, 0.005118697416037321, 0.2911197543144226]\n",
      "Step 435, loss = [0.3043748736381531, 0.003989548422396183, 0.303975909948349]\n",
      "Step 436, loss = [0.29073792695999146, 0.004361790604889393, 0.29030174016952515]\n",
      "Step 437, loss = [0.2941896915435791, 0.0036769001744687557, 0.2938219904899597]\n",
      "Step 438, loss = [0.2985709309577942, 0.006662220228463411, 0.29790470004081726]\n",
      "Step 439, loss = [0.29586413502693176, 0.004769326653331518, 0.2953872084617615]\n",
      "Step 440, loss = [0.3034985363483429, 0.0022725556045770645, 0.3032712936401367]\n",
      "Step 441, loss = [0.3076648712158203, 0.00487684877589345, 0.30717718601226807]\n",
      "Step 442, loss = [0.310271680355072, 0.004313930869102478, 0.30984029173851013]\n",
      "Step 443, loss = [0.2936561107635498, 0.004584498703479767, 0.2931976616382599]\n",
      "Step 444, loss = [0.3147619962692261, 0.004273223225027323, 0.3143346607685089]\n",
      "Step 445, loss = [0.2945540249347687, 0.0052520474418997765, 0.2940288186073303]\n",
      "Step 446, loss = [0.2960171699523926, 0.004625005181878805, 0.29555466771125793]\n",
      "Step 447, loss = [0.2906201481819153, 0.007390433456748724, 0.2898811101913452]\n",
      "Step 448, loss = [0.30474767088890076, 0.004196583293378353, 0.3043280243873596]\n",
      "Step 449, loss = [0.30283769965171814, 0.0026607662439346313, 0.3025716245174408]\n",
      "Step 450, loss = [0.29288405179977417, 0.0035921731032431126, 0.2925248444080353]\n",
      "Step 451, loss = [0.30707523226737976, 0.0024723191745579243, 0.30682799220085144]\n",
      "Step 452, loss = [0.30256903171539307, 0.005421623587608337, 0.3020268678665161]\n",
      "Step 453, loss = [0.30241140723228455, 0.00402380945160985, 0.3020090162754059]\n",
      "Step 454, loss = [0.31016793847084045, 0.002609929535537958, 0.3099069595336914]\n",
      "Step 455, loss = [0.30853816866874695, 0.0033729367423802614, 0.308200865983963]\n",
      "Step 456, loss = [0.283537358045578, 0.0035198256373405457, 0.28318536281585693]\n",
      "Step 457, loss = [0.2970042824745178, 0.002747856779024005, 0.29672950506210327]\n",
      "Step 458, loss = [0.30608946084976196, 0.0063974615186452866, 0.3054497241973877]\n",
      "Step 459, loss = [0.2966342270374298, 0.002878613071516156, 0.29634636640548706]\n",
      "Step 460, loss = [0.3116278648376465, 0.005843562074005604, 0.31104350090026855]\n",
      "Step 461, loss = [0.29267418384552, 0.0035328748635947704, 0.2923209071159363]\n",
      "Step 462, loss = [0.2940434515476227, 0.004576265346258879, 0.2935858368873596]\n",
      "Step 463, loss = [0.30272161960601807, 0.0037934118881821632, 0.3023422658443451]\n",
      "Step 464, loss = [0.2981109023094177, 0.003369661048054695, 0.29777392745018005]\n",
      "Step 465, loss = [0.2868455648422241, 0.004970085807144642, 0.2863485515117645]\n",
      "Step 466, loss = [0.2934934198856354, 0.00426968839019537, 0.29306644201278687]\n",
      "Step 467, loss = [0.30247247219085693, 0.006320445332676172, 0.3018404245376587]\n",
      "Step 468, loss = [0.2959519326686859, 0.002982179168611765, 0.29565370082855225]\n",
      "Step 469, loss = [0.29716756939888, 0.004820630885660648, 0.29668551683425903]\n",
      "Step 470, loss = [0.30417460203170776, 0.0027621183544397354, 0.3038983941078186]\n",
      "Step 471, loss = [0.30035609006881714, 0.002736015012487769, 0.3000824749469757]\n",
      "Step 472, loss = [0.29432913661003113, 0.002494768239557743, 0.29407966136932373]\n",
      "Step 473, loss = [0.2870332896709442, 0.004338928498327732, 0.28659939765930176]\n",
      "Step 474, loss = [0.29919734597206116, 0.0032650167122483253, 0.29887083172798157]\n",
      "Step 475, loss = [0.29611530900001526, 0.0051141949370503426, 0.2956039011478424]\n",
      "Step 476, loss = [0.3049587309360504, 0.0052332752384245396, 0.3044354021549225]\n",
      "Step 477, loss = [0.2892453372478485, 0.0069439597427845, 0.2885509431362152]\n",
      "Step 478, loss = [0.3028348386287689, 0.0036245055962353945, 0.30247238278388977]\n",
      "Step 479, loss = [0.29630929231643677, 0.004549201112240553, 0.2958543598651886]\n",
      "Step 480, loss = [0.29976460337638855, 0.006405861582607031, 0.29912400245666504]\n",
      "Step 481, loss = [0.29025670886039734, 0.005806250497698784, 0.28967607021331787]\n",
      "Step 482, loss = [0.30332767963409424, 0.0053531029261648655, 0.30279237031936646]\n",
      "Step 483, loss = [0.29344797134399414, 0.006035088561475277, 0.2928444743156433]\n",
      "Step 484, loss = [0.31050238013267517, 0.005757134407758713, 0.3099266588687897]\n",
      "Step 485, loss = [0.3066253662109375, 0.002975856652483344, 0.30632779002189636]\n",
      "Step 486, loss = [0.296536922454834, 0.0026731956750154495, 0.29626959562301636]\n",
      "Step 487, loss = [0.3032743036746979, 0.0041395556181669235, 0.3028603494167328]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11754 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 488, loss = [0.30603745579719543, 0.005741010420024395, 0.30546334385871887]\n",
      "Update target distribution epoch 0 step 489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11754/11754 [1:50:55<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 489, loss = [0.2983538508415222, 0.00496960524469614, 0.2978568971157074]\n",
      "Step 490, loss = [0.28919580578804016, 0.0024704851675778627, 0.28894874453544617]\n",
      "Step 491, loss = [0.31238576769828796, 0.004637500271201134, 0.31192201375961304]\n",
      "Step 492, loss = [0.3024507462978363, 0.005389467813074589, 0.3019118010997772]\n",
      "Step 493, loss = [0.3002834618091583, 0.003679153509438038, 0.2999155521392822]\n",
      "Step 494, loss = [0.31019651889801025, 0.0019634452182799578, 0.3100001811981201]\n",
      "Step 495, loss = [0.3106688857078552, 0.0016718209953978658, 0.31050169467926025]\n",
      "Step 496, loss = [0.3036741614341736, 0.007019944489002228, 0.3029721677303314]\n",
      "Step 497, loss = [0.28775691986083984, 0.0038130339235067368, 0.28737562894821167]\n",
      "Step 498, loss = [0.2853708267211914, 0.004503350704908371, 0.28492048382759094]\n",
      "Step 499, loss = [0.30325964093208313, 0.002132064662873745, 0.30304643511772156]\n",
      "Step 500, loss = [0.3009243309497833, 0.006176600232720375, 0.30030667781829834]\n",
      "Step 501, loss = [0.3018699884414673, 0.004099015146493912, 0.3014600872993469]\n",
      "Step 502, loss = [0.29380279779434204, 0.005590943153947592, 0.2932437062263489]\n",
      "Step 503, loss = [0.3069920837879181, 0.0021636197343468666, 0.3067757189273834]\n",
      "Step 504, loss = [0.28540167212486267, 0.00358014483936131, 0.2850436568260193]\n",
      "Step 505, loss = [0.3132082223892212, 0.002007474657148123, 0.3130074739456177]\n",
      "Step 506, loss = [0.2839290201663971, 0.006114691961556673, 0.28331753611564636]\n",
      "Step 507, loss = [0.2935139238834381, 0.003675204236060381, 0.29314640164375305]\n",
      "Step 508, loss = [0.3023793697357178, 0.004259621724486351, 0.30195340514183044]\n",
      "Step 509, loss = [0.30875957012176514, 0.005851868074387312, 0.30817437171936035]\n",
      "Step 510, loss = [0.29620736837387085, 0.004044930450618267, 0.29580286145210266]\n",
      "Step 511, loss = [0.28185200691223145, 0.004005633294582367, 0.28145143389701843]\n",
      "Step 512, loss = [0.31438565254211426, 0.005213246680796146, 0.3138643205165863]\n",
      "Step 513, loss = [0.2967508137226105, 0.002768926089629531, 0.2964739203453064]\n",
      "Step 514, loss = [0.3039492070674896, 0.004711333196610212, 0.30347806215286255]\n",
      "Step 515, loss = [0.2899646759033203, 0.005885250400751829, 0.2893761396408081]\n",
      "Step 516, loss = [0.29802176356315613, 0.0019882326014339924, 0.2978229522705078]\n",
      "Step 517, loss = [0.29141682386398315, 0.0037745400331914425, 0.291039377450943]\n",
      "Step 518, loss = [0.3104817569255829, 0.0014842927921563387, 0.31033334136009216]\n",
      "Step 519, loss = [0.3079584836959839, 0.003426115959882736, 0.30761587619781494]\n",
      "Step 520, loss = [0.3011338710784912, 0.006050318479537964, 0.3005288541316986]\n",
      "Step 521, loss = [0.30496349930763245, 0.006655515171587467, 0.30429795384407043]\n",
      "Step 522, loss = [0.3038754463195801, 0.0029234124813228846, 0.3035831153392792]\n",
      "Step 523, loss = [0.30656394362449646, 0.0026676710695028305, 0.3062971830368042]\n",
      "Step 524, loss = [0.28497692942619324, 0.004759545437991619, 0.28450098633766174]\n",
      "Step 525, loss = [0.30043932795524597, 0.0016556595219299197, 0.3002737760543823]\n",
      "Step 526, loss = [0.30014339089393616, 0.006385410204529762, 0.2995048463344574]\n",
      "Step 527, loss = [0.2995857298374176, 0.002446783008053899, 0.2993410527706146]\n",
      "Step 528, loss = [0.30153390765190125, 0.005174500867724419, 0.3010164499282837]\n",
      "Step 529, loss = [0.2985749840736389, 0.0011293409625068307, 0.29846206307411194]\n",
      "Step 530, loss = [0.3048619031906128, 0.006550790276378393, 0.30420681834220886]\n",
      "Step 531, loss = [0.3040256202220917, 0.004666432738304138, 0.30355897545814514]\n",
      "Step 532, loss = [0.28405365347862244, 0.002640411490574479, 0.28378960490226746]\n",
      "Step 533, loss = [0.2928852140903473, 0.006444492377340794, 0.29224076867103577]\n",
      "Step 534, loss = [0.30868232250213623, 0.003180650994181633, 0.30836427211761475]\n",
      "Step 535, loss = [0.2907305061817169, 0.0032077659852802753, 0.29040974378585815]\n",
      "Step 536, loss = [0.31454673409461975, 0.002447703154757619, 0.3143019676208496]\n",
      "Step 537, loss = [0.30519577860832214, 0.004767802078276873, 0.3047190010547638]\n",
      "Step 538, loss = [0.29787757992744446, 0.004430143162608147, 0.29743456840515137]\n",
      "Step 539, loss = [0.3052594065666199, 0.004203310701996088, 0.3048390746116638]\n",
      "Step 540, loss = [0.2992573380470276, 0.0011901621473953128, 0.29913830757141113]\n",
      "Step 541, loss = [0.30069032311439514, 0.002384910127148032, 0.3004518449306488]\n",
      "Step 542, loss = [0.3097923696041107, 0.005453269928693771, 0.30924704670906067]\n",
      "Step 543, loss = [0.3057462275028229, 0.0058263204991817474, 0.30516359210014343]\n",
      "Step 544, loss = [0.31006157398223877, 0.003385636257007718, 0.30972301959991455]\n",
      "Step 545, loss = [0.29980334639549255, 0.004890250042080879, 0.29931432008743286]\n",
      "Step 546, loss = [0.29872167110443115, 0.0038629178889095783, 0.29833537340164185]\n",
      "Step 547, loss = [0.30487143993377686, 0.0035295970737934113, 0.3045184910297394]\n",
      "Step 548, loss = [0.30088838934898376, 0.0035683747846633196, 0.3005315661430359]\n",
      "Step 549, loss = [0.2787337005138397, 0.0035914944019168615, 0.2783745527267456]\n",
      "Step 550, loss = [0.31256839632987976, 0.006889489945024252, 0.3118794560432434]\n",
      "Step 551, loss = [0.2897251844406128, 0.0026293150149285793, 0.28946223855018616]\n",
      "Step 552, loss = [0.3069503605365753, 0.00855029933154583, 0.30609533190727234]\n",
      "Step 553, loss = [0.29475149512290955, 0.003728554118424654, 0.2943786382675171]\n",
      "Step 554, loss = [0.2975791096687317, 0.003364162053912878, 0.2972427010536194]\n",
      "Step 555, loss = [0.2781356871128082, 0.00428042933344841, 0.27770763635635376]\n",
      "Step 556, loss = [0.29413124918937683, 0.004284032620489597, 0.2937028408050537]\n",
      "Step 557, loss = [0.3077641725540161, 0.0031992322765290737, 0.3074442446231842]\n",
      "Step 558, loss = [0.31627148389816284, 0.003375464817509055, 0.3159339427947998]\n",
      "Step 559, loss = [0.3049720227718353, 0.0034986522514373064, 0.3046221435070038]\n",
      "Step 560, loss = [0.29108041524887085, 0.002428937703371048, 0.29083752632141113]\n",
      "Step 561, loss = [0.2957768142223358, 0.0033747348934412003, 0.29543933272361755]\n",
      "Step 562, loss = [0.2998027801513672, 0.002510175807401538, 0.29955175518989563]\n",
      "Step 563, loss = [0.29815661907196045, 0.0024220349732786417, 0.29791441559791565]\n",
      "Step 564, loss = [0.29229477047920227, 0.0031431824900209904, 0.29198044538497925]\n",
      "Step 565, loss = [0.30686166882514954, 0.006505503784865141, 0.30621111392974854]\n",
      "Step 566, loss = [0.3012840747833252, 0.0027716585900634527, 0.30100691318511963]\n",
      "Step 567, loss = [0.30899694561958313, 0.00241097342222929, 0.3087558448314667]\n",
      "Step 568, loss = [0.3119685649871826, 0.003983632195740938, 0.3115701973438263]\n",
      "Step 569, loss = [0.2907438576221466, 0.0042261225171387196, 0.2903212308883667]\n",
      "Step 570, loss = [0.3041641414165497, 0.004792310763150454, 0.30368492007255554]\n",
      "Step 571, loss = [0.30869540572166443, 0.005904097110033035, 0.3081049919128418]\n",
      "Step 572, loss = [0.28150999546051025, 0.005849705543369055, 0.28092503547668457]\n",
      "Step 573, loss = [0.30544596910476685, 0.0051846434362232685, 0.3049274981021881]\n",
      "Step 574, loss = [0.2918417751789093, 0.005112062208354473, 0.29133057594299316]\n",
      "Step 575, loss = [0.3079896867275238, 0.005050698295235634, 0.30748462677001953]\n",
      "Step 576, loss = [0.3120293915271759, 0.004707585088908672, 0.31155863404273987]\n",
      "Step 577, loss = [0.3032956123352051, 0.006006372161209583, 0.30269497632980347]\n",
      "Step 578, loss = [0.300987184047699, 0.002989941742271185, 0.3006881773471832]\n",
      "Step 579, loss = [0.2975662350654602, 0.0035186661407351494, 0.2972143590450287]\n",
      "Step 580, loss = [0.2962341904640198, 0.0034238798543810844, 0.29589179158210754]\n",
      "Step 581, loss = [0.28426313400268555, 0.0035225923638790846, 0.283910870552063]\n",
      "Step 582, loss = [0.29629260301589966, 0.0038188856560736895, 0.29591071605682373]\n",
      "Step 583, loss = [0.293409138917923, 0.004397048614919186, 0.2929694354534149]\n",
      "Step 584, loss = [0.29957184195518494, 0.007574946619570255, 0.2988143563270569]\n",
      "Step 585, loss = [0.2942635118961334, 0.004407917149364948, 0.293822705745697]\n",
      "Step 586, loss = [0.3069708049297333, 0.004346916452050209, 0.30653610825538635]\n",
      "Step 587, loss = [0.28816932439804077, 0.005378947593271732, 0.28763142228126526]\n",
      "Step 588, loss = [0.29939448833465576, 0.005699533969163895, 0.2988245487213135]\n",
      "Step 589, loss = [0.2818572521209717, 0.003976566717028618, 0.28145959973335266]\n",
      "Step 590, loss = [0.29917022585868835, 0.007057652808725834, 0.29846444725990295]\n",
      "Step 591, loss = [0.30574968457221985, 0.004362919367849827, 0.305313378572464]\n",
      "Step 592, loss = [0.30639031529426575, 0.0059081255458295345, 0.3057995140552521]\n",
      "Step 593, loss = [0.3117836117744446, 0.0022676996886730194, 0.3115568459033966]\n",
      "Step 594, loss = [0.31607335805892944, 0.005350313149392605, 0.31553831696510315]\n",
      "Step 595, loss = [0.2989943325519562, 0.0016528342384845018, 0.298829048871994]\n",
      "Step 596, loss = [0.29268118739128113, 0.004501368384808302, 0.2922310531139374]\n",
      "Step 597, loss = [0.30609846115112305, 0.0022411122918128967, 0.3058743476867676]\n",
      "Step 598, loss = [0.29032665491104126, 0.0021628064569085836, 0.29011037945747375]\n",
      "Step 599, loss = [0.2900910973548889, 0.0075188493356108665, 0.28933921456336975]\n",
      "Step 600, loss = [0.3052152693271637, 0.0044881850481033325, 0.304766446352005]\n",
      "Step 601, loss = [0.30344581604003906, 0.005658388137817383, 0.3028799891471863]\n",
      "Step 602, loss = [0.31306350231170654, 0.006119015160948038, 0.3124516010284424]\n",
      "Step 603, loss = [0.29751697182655334, 0.004387309774756432, 0.2970782518386841]\n",
      "Step 604, loss = [0.3066287934780121, 0.003768008202314377, 0.30625200271606445]\n",
      "Step 605, loss = [0.27912062406539917, 0.002697545103728771, 0.27885088324546814]\n",
      "Step 606, loss = [0.2673913836479187, 0.004210600163787603, 0.26697033643722534]\n",
      "Step 607, loss = [0.30107545852661133, 0.005982544273138046, 0.30047720670700073]\n",
      "Step 608, loss = [0.30184006690979004, 0.0012505208142101765, 0.30171501636505127]\n",
      "Step 609, loss = [0.29652732610702515, 0.005578312091529369, 0.29596948623657227]\n",
      "Step 610, loss = [0.296459436416626, 0.0036085883621126413, 0.29609858989715576]\n",
      "Step 611, loss = [0.3049362599849701, 0.00436415895819664, 0.3044998347759247]\n",
      "Step 612, loss = [0.29761365056037903, 0.006952287629246712, 0.29691842198371887]\n",
      "Step 613, loss = [0.29912763833999634, 0.004379337653517723, 0.29868969321250916]\n",
      "Step 614, loss = [0.3013870418071747, 0.006401756778359413, 0.3007468581199646]\n",
      "Step 615, loss = [0.29088082909584045, 0.005741986911743879, 0.29030662775039673]\n",
      "Step 616, loss = [0.30469340085983276, 0.00459138210862875, 0.30423426628112793]\n",
      "Step 617, loss = [0.2979315519332886, 0.008075602352619171, 0.2971239984035492]\n",
      "Step 618, loss = [0.30297529697418213, 0.0032274937257170677, 0.3026525378227234]\n",
      "Step 619, loss = [0.30095168948173523, 0.003049176651984453, 0.3006467819213867]\n",
      "Step 620, loss = [0.2924923002719879, 0.005430220626294613, 0.2919492721557617]\n",
      "Step 621, loss = [0.3119819760322571, 0.0037309639155864716, 0.3116088807582855]\n",
      "Step 622, loss = [0.3100070059299469, 0.0029422789812088013, 0.3097127676010132]\n",
      "Step 623, loss = [0.30136194825172424, 0.0031101906206458807, 0.30105093121528625]\n",
      "Step 624, loss = [0.30497801303863525, 0.004567374475300312, 0.30452126264572144]\n",
      "Step 625, loss = [0.3075677752494812, 0.003915721084922552, 0.3071762025356293]\n",
      "Step 626, loss = [0.2990191578865051, 0.0022532546427100897, 0.29879382252693176]\n",
      "Step 627, loss = [0.31085672974586487, 0.004173276945948601, 0.31043940782546997]\n",
      "Step 628, loss = [0.3142266571521759, 0.0017001873347908258, 0.3140566349029541]\n",
      "Step 629, loss = [0.28426289558410645, 0.0025542047806084156, 0.2840074896812439]\n",
      "Step 630, loss = [0.3050296902656555, 0.002786659635603428, 0.30475103855133057]\n",
      "Step 631, loss = [0.29805225133895874, 0.0038778246380388737, 0.29766446352005005]\n",
      "Step 632, loss = [0.30208200216293335, 0.007792928256094456, 0.3013027012348175]\n",
      "Step 633, loss = [0.2976732552051544, 0.0033033096697181463, 0.2973429262638092]\n",
      "Step 634, loss = [0.2931884825229645, 0.006946133449673653, 0.29249387979507446]\n",
      "Step 635, loss = [0.30527636408805847, 0.002916914876550436, 0.3049846589565277]\n",
      "Step 636, loss = [0.2958604395389557, 0.004244891926646233, 0.29543596506118774]\n",
      "Step 637, loss = [0.3074635863304138, 0.003014865331351757, 0.3071621060371399]\n",
      "Step 638, loss = [0.3071281611919403, 0.005954552907496691, 0.30653271079063416]\n",
      "Step 639, loss = [0.30526894330978394, 0.003414871171116829, 0.3049274682998657]\n",
      "Step 640, loss = [0.3125312924385071, 0.0039045033045113087, 0.3121408522129059]\n",
      "Step 641, loss = [0.30666470527648926, 0.003593895584344864, 0.30630531907081604]\n",
      "Step 642, loss = [0.2948416471481323, 0.004803916439414024, 0.29436126351356506]\n",
      "Step 643, loss = [0.2950323522090912, 0.0030631665140390396, 0.29472604393959045]\n",
      "Step 644, loss = [0.3137851059436798, 0.0037934458814561367, 0.31340575218200684]\n",
      "Step 645, loss = [0.295526385307312, 0.004479915369302034, 0.2950783967971802]\n",
      "Step 646, loss = [0.3076821565628052, 0.0025807302445173264, 0.3074240982532501]\n",
      "Step 647, loss = [0.3062010407447815, 0.0038680187426507473, 0.3058142364025116]\n",
      "Step 648, loss = [0.3080650866031647, 0.0050031524151563644, 0.30756476521492004]\n",
      "Step 649, loss = [0.3136095702648163, 0.004441727884113789, 0.3131653964519501]\n",
      "Step 650, loss = [0.30852028727531433, 0.002392164431512356, 0.3082810640335083]\n",
      "Step 651, loss = [0.2880510985851288, 0.006286579184234142, 0.28742244839668274]\n",
      "Step 652, loss = [0.28467461466789246, 0.00454237824305892, 0.2842203676700592]\n",
      "Step 653, loss = [0.3134908378124237, 0.0028948946855962276, 0.31320133805274963]\n",
      "Step 654, loss = [0.30713337659835815, 0.004378884565085173, 0.30669549107551575]\n",
      "Step 655, loss = [0.2920977771282196, 0.005216335412114859, 0.2915761470794678]\n",
      "Step 656, loss = [0.30360573530197144, 0.002935970202088356, 0.30331215262413025]\n",
      "Step 657, loss = [0.2864347994327545, 0.0037191659212112427, 0.28606289625167847]\n",
      "Step 658, loss = [0.2946755588054657, 0.0031424800399690866, 0.29436132311820984]\n",
      "Step 659, loss = [0.3177332580089569, 0.005927260033786297, 0.31714051961898804]\n",
      "Step 660, loss = [0.30158522725105286, 0.0036457241512835026, 0.3012206554412842]\n",
      "Step 661, loss = [0.2965582609176636, 0.005759917199611664, 0.2959822714328766]\n",
      "Step 662, loss = [0.29609814286231995, 0.006237286143004894, 0.29547441005706787]\n",
      "Step 663, loss = [0.31591638922691345, 0.004230884835124016, 0.31549331545829773]\n",
      "Step 664, loss = [0.3072660565376282, 0.003954744897782803, 0.30687057971954346]\n",
      "Step 665, loss = [0.30141666531562805, 0.003853782080113888, 0.30103129148483276]\n",
      "Step 666, loss = [0.31363457441329956, 0.0027860510163009167, 0.3133559823036194]\n",
      "Step 667, loss = [0.301120400428772, 0.005320968106389046, 0.30058830976486206]\n",
      "Step 668, loss = [0.30366501212120056, 0.003389311023056507, 0.3033260703086853]\n",
      "Step 669, loss = [0.30338340997695923, 0.00452943192794919, 0.30293047428131104]\n",
      "Step 670, loss = [0.2891761362552643, 0.0037282537668943405, 0.2888033092021942]\n",
      "Step 671, loss = [0.2969862222671509, 0.0018008837942034006, 0.29680612683296204]\n",
      "Step 672, loss = [0.3050461709499359, 0.0036792249884456396, 0.3046782612800598]\n",
      "Step 673, loss = [0.29462623596191406, 0.0024930769577622414, 0.294376939535141]\n",
      "Step 674, loss = [0.303679496049881, 0.002981017343699932, 0.30338138341903687]\n",
      "Step 675, loss = [0.3044707775115967, 0.005276659037917852, 0.30394309759140015]\n",
      "Step 676, loss = [0.3015783131122589, 0.006638607010245323, 0.300914466381073]\n",
      "Step 677, loss = [0.311425119638443, 0.005078388378024101, 0.31091728806495667]\n",
      "Step 678, loss = [0.2913183271884918, 0.0042097847908735275, 0.29089733958244324]\n",
      "Step 679, loss = [0.29296496510505676, 0.0024833022616803646, 0.2927166223526001]\n",
      "Step 680, loss = [0.29929792881011963, 0.005176027305424213, 0.29878032207489014]\n",
      "Step 681, loss = [0.30626869201660156, 0.0030640442855656147, 0.30596229434013367]\n",
      "Step 682, loss = [0.31198233366012573, 0.0021149124950170517, 0.31177085638046265]\n",
      "Step 683, loss = [0.29820573329925537, 0.0033181942999362946, 0.29787391424179077]\n",
      "Step 684, loss = [0.29664894938468933, 0.004434411413967609, 0.2962055206298828]\n",
      "Step 685, loss = [0.30016645789146423, 0.0023234167601913214, 0.29993411898612976]\n",
      "Step 686, loss = [0.294714093208313, 0.005468599498271942, 0.2941672205924988]\n",
      "Step 687, loss = [0.2787379026412964, 0.005719478242099285, 0.2781659662723541]\n",
      "Step 688, loss = [0.3104223608970642, 0.0012333914637565613, 0.31029900908470154]\n",
      "Step 689, loss = [0.2965490221977234, 0.004389921203255653, 0.29611003398895264]\n",
      "Step 690, loss = [0.3069310784339905, 0.004880599677562714, 0.3064430058002472]\n",
      "Step 691, loss = [0.3073026239871979, 0.0037583874072879553, 0.30692678689956665]\n",
      "Step 692, loss = [0.2964940369129181, 0.005537074990570545, 0.2959403395652771]\n",
      "Step 693, loss = [0.30067119002342224, 0.004277972504496574, 0.30024340748786926]\n",
      "Step 694, loss = [0.29741135239601135, 0.0029159807600080967, 0.29711976647377014]\n",
      "Step 695, loss = [0.30015960335731506, 0.0021520243026316166, 0.2999444007873535]\n",
      "Step 696, loss = [0.31069010496139526, 0.005805008113384247, 0.31010961532592773]\n",
      "Step 697, loss = [0.2838372588157654, 0.006388900335878134, 0.28319835662841797]\n",
      "Step 698, loss = [0.2873985767364502, 0.004994330927729607, 0.2868991494178772]\n",
      "Step 699, loss = [0.30241960287094116, 0.004837043583393097, 0.30193591117858887]\n",
      "Step 700, loss = [0.2944299280643463, 0.0036835381761193275, 0.2940615713596344]\n",
      "Step 701, loss = [0.3082996904850006, 0.0017854372272267938, 0.3081211447715759]\n",
      "Step 702, loss = [0.30272939801216125, 0.0036177197471261024, 0.302367627620697]\n",
      "Step 703, loss = [0.30613309144973755, 0.001543487305752933, 0.3059787452220917]\n",
      "Step 704, loss = [0.30827829241752625, 0.0032074032351374626, 0.30795755982398987]\n",
      "Step 705, loss = [0.3101995587348938, 0.007151448633521795, 0.30948442220687866]\n",
      "Step 706, loss = [0.2940615713596344, 0.00696918647736311, 0.29336464405059814]\n",
      "Step 707, loss = [0.2992928922176361, 0.0021061478182673454, 0.29908227920532227]\n",
      "Step 708, loss = [0.30322709679603577, 0.0015779691748321056, 0.3030692934989929]\n",
      "Step 709, loss = [0.2906835675239563, 0.0020275444258004427, 0.2904808223247528]\n",
      "Step 710, loss = [0.3039833605289459, 0.003510349430143833, 0.30363231897354126]\n",
      "Step 711, loss = [0.29851701855659485, 0.006525376345962286, 0.29786446690559387]\n",
      "Step 712, loss = [0.3171246349811554, 0.0027934687677770853, 0.3168452978134155]\n",
      "Step 713, loss = [0.30036693811416626, 0.00489407405257225, 0.2998775243759155]\n",
      "Step 714, loss = [0.3022444546222687, 0.006053396500647068, 0.3016391098499298]\n",
      "Step 715, loss = [0.290368914604187, 0.005598846357315779, 0.2898090183734894]\n",
      "Step 716, loss = [0.30144891142845154, 0.004764021374285221, 0.30097252130508423]\n",
      "Step 717, loss = [0.2915089726448059, 0.004911402706056833, 0.2910178303718567]\n",
      "Step 718, loss = [0.2895595133304596, 0.003979706205427647, 0.2891615331172943]\n",
      "Step 719, loss = [0.3031022548675537, 0.004685676656663418, 0.302633672952652]\n",
      "Step 720, loss = [0.30301138758659363, 0.005551324225962162, 0.302456259727478]\n",
      "Step 721, loss = [0.29450997710227966, 0.007233989890664816, 0.29378658533096313]\n",
      "Step 722, loss = [0.3087523877620697, 0.003782129380851984, 0.30837416648864746]\n",
      "Step 723, loss = [0.3067173361778259, 0.00523475231602788, 0.30619385838508606]\n",
      "Step 724, loss = [0.2849821150302887, 0.0026860025245696306, 0.2847135066986084]\n",
      "Step 725, loss = [0.30547353625297546, 0.004271477460861206, 0.3050463795661926]\n",
      "Step 726, loss = [0.2847943902015686, 0.006618449464440346, 0.28413254022598267]\n",
      "Step 727, loss = [0.2972944378852844, 0.00517185078933835, 0.29677724838256836]\n",
      "Step 728, loss = [0.28744176030158997, 0.004151824861764908, 0.287026584148407]\n",
      "Step 729, loss = [0.2992836534976959, 0.004161934368312359, 0.29886746406555176]\n",
      "Step 730, loss = [0.2995036840438843, 0.005492744036018848, 0.29895439743995667]\n",
      "Step 731, loss = [0.29548677802085876, 0.0021288199350237846, 0.29527390003204346]\n",
      "Step 732, loss = [0.3031954765319824, 0.004661730490624905, 0.3027293086051941]\n",
      "Step 733, loss = [0.2906281054019928, 0.0037141551729291677, 0.29025667905807495]\n",
      "Step 734, loss = [0.29349496960639954, 0.0037856069393455982, 0.29311642050743103]\n",
      "Step 735, loss = [0.30296462774276733, 0.0013120835646986961, 0.3028334081172943]\n",
      "Step 736, loss = [0.29994088411331177, 0.0036437958478927612, 0.2995764911174774]\n",
      "Step 737, loss = [0.30009713768959045, 0.006081560626626015, 0.29948899149894714]\n",
      "Step 738, loss = [0.28194454312324524, 0.005350266583263874, 0.28140950202941895]\n",
      "Step 739, loss = [0.28571373224258423, 0.0034663749393075705, 0.28536710143089294]\n",
      "Step 740, loss = [0.3050924837589264, 0.005135247949510813, 0.304578959941864]\n",
      "Step 741, loss = [0.2867974638938904, 0.0036031315103173256, 0.28643715381622314]\n",
      "Step 742, loss = [0.3077120780944824, 0.0031984192319214344, 0.3073922395706177]\n",
      "Step 743, loss = [0.30290675163269043, 0.0045911273919045925, 0.302447646856308]\n",
      "Step 744, loss = [0.3132612407207489, 0.0017112160567194223, 0.31309011578559875]\n",
      "Step 745, loss = [0.3093249797821045, 0.0044160401448607445, 0.3088833689689636]\n",
      "Step 746, loss = [0.30628493428230286, 0.010124308988451958, 0.30527248978614807]\n",
      "Step 747, loss = [0.3003706634044647, 0.005170570686459541, 0.2998535931110382]\n",
      "Step 748, loss = [0.3055700957775116, 0.005378616973757744, 0.30503222346305847]\n",
      "Step 749, loss = [0.2933003604412079, 0.004140526056289673, 0.29288631677627563]\n",
      "Step 750, loss = [0.3057890236377716, 0.0037289061583578587, 0.30541613698005676]\n",
      "Step 751, loss = [0.2879479229450226, 0.004823545925319195, 0.28746557235717773]\n",
      "Step 752, loss = [0.3026987314224243, 0.003413969185203314, 0.30235734581947327]\n",
      "Step 753, loss = [0.31024810671806335, 0.005485076457262039, 0.3096995949745178]\n",
      "Step 754, loss = [0.3082132041454315, 0.002235288731753826, 0.3079896867275238]\n",
      "Step 755, loss = [0.3046001195907593, 0.0027774865739047527, 0.30432236194610596]\n",
      "Step 756, loss = [0.31361666321754456, 0.004294508136808872, 0.31318721175193787]\n",
      "Step 757, loss = [0.299561083316803, 0.005124175921082497, 0.29904866218566895]\n",
      "Step 758, loss = [0.30208295583724976, 0.0036140973679721355, 0.3017215430736542]\n",
      "Step 759, loss = [0.3059341013431549, 0.004126441664993763, 0.3055214583873749]\n",
      "Step 760, loss = [0.30821099877357483, 0.006233998108655214, 0.307587593793869]\n",
      "Step 761, loss = [0.30526113510131836, 0.006251448765397072, 0.30463600158691406]\n",
      "Step 762, loss = [0.3091500699520111, 0.0034271422773599625, 0.3088073432445526]\n",
      "Step 763, loss = [0.2976115942001343, 0.0035985226277261972, 0.29725173115730286]\n",
      "Step 764, loss = [0.2958287298679352, 0.004675809293985367, 0.29536116123199463]\n",
      "Step 765, loss = [0.2975230813026428, 0.0039150407537817955, 0.29713156819343567]\n",
      "Step 766, loss = [0.2897772490978241, 0.0038218244444578886, 0.2893950641155243]\n",
      "Step 767, loss = [0.3112001121044159, 0.0007328445790335536, 0.31112682819366455]\n",
      "Step 768, loss = [0.29620471596717834, 0.004755622707307339, 0.2957291603088379]\n",
      "Step 769, loss = [0.2882554531097412, 0.001995021477341652, 0.288055956363678]\n",
      "Step 770, loss = [0.3067238926887512, 0.00504024513065815, 0.3062198758125305]\n",
      "Step 771, loss = [0.28993621468544006, 0.005013931542634964, 0.2894348204135895]\n",
      "Step 772, loss = [0.29759839177131653, 0.0015264502726495266, 0.29744574427604675]\n",
      "Step 773, loss = [0.2891194820404053, 0.0034410934895277023, 0.28877538442611694]\n",
      "Step 774, loss = [0.30405473709106445, 0.00563511997461319, 0.3034912347793579]\n",
      "Step 775, loss = [0.3041674494743347, 0.003985084127634764, 0.30376893281936646]\n",
      "Step 776, loss = [0.29939109086990356, 0.0034723831340670586, 0.2990438640117645]\n",
      "Step 777, loss = [0.30738401412963867, 0.006153371185064316, 0.3067686855792999]\n",
      "Step 778, loss = [0.3133181631565094, 0.0030505689792335033, 0.31301310658454895]\n",
      "Step 779, loss = [0.29220956563949585, 0.004907622002065182, 0.29171881079673767]\n",
      "Step 780, loss = [0.3049934506416321, 0.0010704654268920422, 0.3048864006996155]\n",
      "Step 781, loss = [0.306253582239151, 0.003735160455107689, 0.305880069732666]\n",
      "Step 782, loss = [0.3083156645298004, 0.005614598747342825, 0.3077542185783386]\n",
      "Step 783, loss = [0.28470584750175476, 0.0024770908057689667, 0.28445813059806824]\n",
      "Step 784, loss = [0.30019301176071167, 0.0026997551321983337, 0.29992303252220154]\n",
      "Step 785, loss = [0.293245792388916, 0.004121031612157822, 0.29283368587493896]\n",
      "Step 786, loss = [0.30478978157043457, 0.002350530819967389, 0.3045547306537628]\n",
      "Step 787, loss = [0.3019377589225769, 0.003460971172899008, 0.3015916645526886]\n",
      "Step 788, loss = [0.303913414478302, 0.002309504896402359, 0.30368247628211975]\n",
      "Step 789, loss = [0.3004348576068878, 0.00439018290489912, 0.2999958395957947]\n",
      "Step 790, loss = [0.2811752259731293, 0.005663999356329441, 0.2806088328361511]\n",
      "Step 791, loss = [0.30560141801834106, 0.003653519321233034, 0.3052360713481903]\n",
      "Step 792, loss = [0.2918108105659485, 0.003330029547214508, 0.2914777994155884]\n",
      "Step 793, loss = [0.298618346452713, 0.004796142224222422, 0.29813873767852783]\n",
      "Step 794, loss = [0.30404067039489746, 0.004783032927662134, 0.30356237292289734]\n",
      "Step 795, loss = [0.30919376015663147, 0.004869983531534672, 0.30870676040649414]\n",
      "Step 796, loss = [0.3020536005496979, 0.0022368039935827255, 0.3018299341201782]\n",
      "Step 797, loss = [0.3130095303058624, 0.0055305990390479565, 0.3124564588069916]\n",
      "Step 798, loss = [0.3148132562637329, 0.0042661079205572605, 0.31438663601875305]\n",
      "Step 799, loss = [0.29232168197631836, 0.0026271273382008076, 0.2920589745044708]\n",
      "Step 800, loss = [0.30776846408843994, 0.005394584033638239, 0.30722901225090027]\n",
      "Step 801, loss = [0.3000642955303192, 0.0033653099089860916, 0.29972776770591736]\n",
      "Step 802, loss = [0.30958881974220276, 0.003544287057593465, 0.3092343807220459]\n",
      "Step 803, loss = [0.2923097014427185, 0.0034680969547480345, 0.2919628918170929]\n",
      "Step 804, loss = [0.3006210923194885, 0.002130442298948765, 0.3004080355167389]\n",
      "Step 805, loss = [0.28996679186820984, 0.00429779477417469, 0.2895370125770569]\n",
      "Step 806, loss = [0.3054206371307373, 0.00572693208232522, 0.30484795570373535]\n",
      "Step 807, loss = [0.30893203616142273, 0.0045574866235256195, 0.3084762990474701]\n",
      "Step 808, loss = [0.30133920907974243, 0.0036920495331287384, 0.30097001791000366]\n",
      "Step 809, loss = [0.28145334124565125, 0.005026868544518948, 0.280950665473938]\n",
      "Step 810, loss = [0.3076173663139343, 0.004577599465847015, 0.3071596026420593]\n",
      "Step 811, loss = [0.30571499466896057, 0.003482257714495063, 0.30536675453186035]\n",
      "Step 812, loss = [0.2884543538093567, 0.0036454773508012295, 0.2880898118019104]\n",
      "Step 813, loss = [0.3055638372898102, 0.004269980359822512, 0.3051368296146393]\n",
      "Step 814, loss = [0.30232492089271545, 0.005429072771221399, 0.3017820119857788]\n",
      "Step 815, loss = [0.3054003119468689, 0.0021547493524849415, 0.30518484115600586]\n",
      "Step 816, loss = [0.3003835678100586, 0.006285947281867266, 0.2997549772262573]\n",
      "Step 817, loss = [0.3002367615699768, 0.003049854887649417, 0.29993176460266113]\n",
      "Step 818, loss = [0.2930138409137726, 0.005935785360634327, 0.29242026805877686]\n",
      "Step 819, loss = [0.29369819164276123, 0.00381492986343801, 0.29331669211387634]\n",
      "Step 820, loss = [0.28647947311401367, 0.002775191329419613, 0.28620195388793945]\n",
      "Step 821, loss = [0.3080804646015167, 0.005408961791545153, 0.30753958225250244]\n",
      "Step 822, loss = [0.29046690464019775, 0.0030064769089221954, 0.2901662588119507]\n",
      "Step 823, loss = [0.30362576246261597, 0.0020354758016765118, 0.303422212600708]\n",
      "Step 824, loss = [0.29273682832717896, 0.004784294869750738, 0.2922584116458893]\n",
      "Step 825, loss = [0.31335368752479553, 0.0015085169579833746, 0.313202828168869]\n",
      "Step 826, loss = [0.29617732763290405, 0.005353189073503017, 0.29564201831817627]\n",
      "Step 827, loss = [0.2844320237636566, 0.0034203226678073406, 0.28408998250961304]\n",
      "Step 828, loss = [0.30069422721862793, 0.006234269589185715, 0.30007079243659973]\n",
      "Step 829, loss = [0.2978193759918213, 0.0014054073253646493, 0.2976788282394409]\n",
      "Step 830, loss = [0.3119898736476898, 0.006798752583563328, 0.3113099932670593]\n",
      "Step 831, loss = [0.2914593815803528, 0.0032782736234366894, 0.29113155603408813]\n",
      "Step 832, loss = [0.28517699241638184, 0.004037277773022652, 0.2847732603549957]\n",
      "Step 833, loss = [0.3078646659851074, 0.0034461645409464836, 0.3075200617313385]\n",
      "Step 834, loss = [0.27625834941864014, 0.0024696295149624348, 0.2760113775730133]\n",
      "Step 835, loss = [0.30480748414993286, 0.003339032642543316, 0.3044735789299011]\n",
      "Step 836, loss = [0.3099861443042755, 0.001510709640569985, 0.3098350763320923]\n",
      "Step 837, loss = [0.31022390723228455, 0.007124273106455803, 0.3095114827156067]\n",
      "Step 838, loss = [0.305879145860672, 0.0031233911868184805, 0.30556681752204895]\n",
      "Step 839, loss = [0.2992233633995056, 0.004018185660243034, 0.2988215386867523]\n",
      "Step 840, loss = [0.28903135657310486, 0.005022261757403612, 0.2885291278362274]\n",
      "Step 841, loss = [0.30964165925979614, 0.004334434866905212, 0.3092082142829895]\n",
      "Step 842, loss = [0.30793651938438416, 0.005252286791801453, 0.3074112832546234]\n",
      "Step 843, loss = [0.300931453704834, 0.004259498789906502, 0.30050548911094666]\n",
      "Step 844, loss = [0.2851993143558502, 0.0050023701041936874, 0.28469908237457275]\n",
      "Step 845, loss = [0.29855531454086304, 0.005407298915088177, 0.2980145812034607]\n",
      "Step 846, loss = [0.310191810131073, 0.00597044313326478, 0.3095947802066803]\n",
      "Step 847, loss = [0.31463953852653503, 0.006749526597559452, 0.3139645755290985]\n",
      "Step 848, loss = [0.30688393115997314, 0.0061299605295062065, 0.30627092719078064]\n",
      "Step 849, loss = [0.30229777097702026, 0.007419697009027004, 0.3015558123588562]\n",
      "Step 850, loss = [0.2971422076225281, 0.0026167926844209433, 0.2968805432319641]\n",
      "Step 851, loss = [0.2978941798210144, 0.005340554751455784, 0.2973601222038269]\n",
      "Step 852, loss = [0.3074515163898468, 0.0028275838121771812, 0.30716875195503235]\n",
      "Step 853, loss = [0.2973994016647339, 0.0018000040436163545, 0.2972193956375122]\n",
      "Step 854, loss = [0.27901023626327515, 0.0030898761469870806, 0.2787012457847595]\n",
      "Step 855, loss = [0.2989582121372223, 0.004310023970901966, 0.29852721095085144]\n",
      "Step 856, loss = [0.3108980357646942, 0.0033726089168339968, 0.31056076288223267]\n",
      "Step 857, loss = [0.29247331619262695, 0.006633874960243702, 0.29180991649627686]\n",
      "Step 858, loss = [0.3044177293777466, 0.004100849851965904, 0.3040076494216919]\n",
      "Step 859, loss = [0.30660489201545715, 0.003857771400362253, 0.30621910095214844]\n",
      "Step 860, loss = [0.28869685530662537, 0.00495289359241724, 0.28820157051086426]\n",
      "Step 861, loss = [0.3056403398513794, 0.0034966827370226383, 0.30529066920280457]\n",
      "Step 862, loss = [0.291873574256897, 0.00696980208158493, 0.29117658734321594]\n",
      "Step 863, loss = [0.2877552807331085, 0.0073489174246788025, 0.28702038526535034]\n",
      "Step 864, loss = [0.29362013936042786, 0.004406478721648455, 0.2931794822216034]\n",
      "Step 865, loss = [0.3045984208583832, 0.0024446132592856884, 0.3043539524078369]\n",
      "Step 866, loss = [0.3045920431613922, 0.004351666197180748, 0.3041568696498871]\n",
      "Step 867, loss = [0.3027927875518799, 0.0054227798245847225, 0.3022505044937134]\n",
      "Step 868, loss = [0.3001261353492737, 0.0043396297842264175, 0.29969218373298645]\n",
      "Step 869, loss = [0.30146753787994385, 0.004936672747135162, 0.3009738624095917]\n",
      "Step 870, loss = [0.30418235063552856, 0.004443028010427952, 0.3037380576133728]\n",
      "Step 871, loss = [0.2963391840457916, 0.00582478940486908, 0.2957566976547241]\n",
      "Step 872, loss = [0.2926526963710785, 0.003921830095350742, 0.2922605276107788]\n",
      "Step 873, loss = [0.3032267093658447, 0.0036959522403776646, 0.30285710096359253]\n",
      "Step 874, loss = [0.3063259720802307, 0.00752849318087101, 0.30557313561439514]\n",
      "Step 875, loss = [0.30899298191070557, 0.0032750368118286133, 0.3086654841899872]\n",
      "Step 876, loss = [0.29955074191093445, 0.003828340442851186, 0.2991679012775421]\n",
      "Step 877, loss = [0.3045184314250946, 0.00599688570946455, 0.3039187490940094]\n",
      "Step 878, loss = [0.2987104058265686, 0.0018953755497932434, 0.29852086305618286]\n",
      "Step 879, loss = [0.30014219880104065, 0.00396377220749855, 0.2997458279132843]\n",
      "Step 880, loss = [0.3049405813217163, 0.004433051683008671, 0.30449727177619934]\n",
      "Step 881, loss = [0.2865713834762573, 0.0050203558057546616, 0.2860693335533142]\n",
      "Step 882, loss = [0.2981395125389099, 0.003336999798193574, 0.2978058159351349]\n",
      "Step 883, loss = [0.3065003454685211, 0.002263001399114728, 0.30627405643463135]\n",
      "Step 884, loss = [0.2964989244937897, 0.0026401355862617493, 0.2962349057197571]\n",
      "Step 885, loss = [0.29003509879112244, 0.005020718090236187, 0.28953301906585693]\n",
      "Step 886, loss = [0.3027365505695343, 0.0049059269949793816, 0.30224594473838806]\n",
      "Step 887, loss = [0.2919526994228363, 0.005002068355679512, 0.2914524972438812]\n",
      "Step 888, loss = [0.2979193329811096, 0.005386136006563902, 0.2973807156085968]\n",
      "Step 889, loss = [0.3060077428817749, 0.004925599321722984, 0.3055151700973511]\n",
      "Step 890, loss = [0.297193706035614, 0.004850985482335091, 0.2967086136341095]\n",
      "Step 891, loss = [0.3087708652019501, 0.004795148968696594, 0.30829134583473206]\n",
      "Step 892, loss = [0.2872508764266968, 0.007056950591504574, 0.28654518723487854]\n",
      "Step 893, loss = [0.3016909062862396, 0.003051543142646551, 0.301385760307312]\n",
      "Step 894, loss = [0.29480990767478943, 0.004372788593173027, 0.2943726181983948]\n",
      "Step 895, loss = [0.2923794388771057, 0.002128356834873557, 0.2921665906906128]\n",
      "Step 896, loss = [0.30579623579978943, 0.0055030519142746925, 0.30524593591690063]\n",
      "Step 897, loss = [0.29207736253738403, 0.00366030540317297, 0.29171133041381836]\n",
      "Step 898, loss = [0.30036476254463196, 0.004286282695829868, 0.2999361455440521]\n",
      "Step 899, loss = [0.2963920533657074, 0.006137480027973652, 0.2957783043384552]\n",
      "Step 900, loss = [0.30268511176109314, 0.0035036683548241854, 0.3023347556591034]\n",
      "Step 901, loss = [0.29456019401550293, 0.003965309821069241, 0.29416367411613464]\n",
      "Step 902, loss = [0.2797063887119293, 0.0045819878578186035, 0.2792481780052185]\n",
      "Step 903, loss = [0.28321585059165955, 0.007218972779810429, 0.2824939489364624]\n",
      "Step 904, loss = [0.30327698588371277, 0.007606387138366699, 0.3025163412094116]\n",
      "Step 905, loss = [0.29207590222358704, 0.005210531875491142, 0.29155483841896057]\n",
      "Step 906, loss = [0.31495359539985657, 0.0039870236068964005, 0.314554899930954]\n",
      "Step 907, loss = [0.30737099051475525, 0.006438299082219601, 0.30672717094421387]\n",
      "Step 908, loss = [0.29539504647254944, 0.0038767410442233086, 0.2950073778629303]\n",
      "Step 909, loss = [0.2990710735321045, 0.004048582632094622, 0.29866620898246765]\n",
      "Step 910, loss = [0.29355868697166443, 0.0037707225419580936, 0.2931816279888153]\n",
      "Step 911, loss = [0.31467488408088684, 0.0013761756708845496, 0.31453725695610046]\n",
      "Step 912, loss = [0.31277698278427124, 0.005406218580901623, 0.31223636865615845]\n",
      "Step 913, loss = [0.304535835981369, 0.004709398373961449, 0.30406489968299866]\n",
      "Step 914, loss = [0.29571443796157837, 0.0037563052028417587, 0.29533880949020386]\n",
      "Step 915, loss = [0.30173423886299133, 0.004861894063651562, 0.30124804377555847]\n",
      "Step 916, loss = [0.3092697560787201, 0.003489935304969549, 0.3089207708835602]\n",
      "Step 917, loss = [0.28923091292381287, 0.004513122141361237, 0.2887795865535736]\n",
      "Step 918, loss = [0.2875917851924896, 0.004862823523581028, 0.2871055006980896]\n",
      "Step 919, loss = [0.2898698151111603, 0.006678981706500053, 0.28920191526412964]\n",
      "Step 920, loss = [0.3064836859703064, 0.004770240746438503, 0.30600666999816895]\n",
      "Step 921, loss = [0.31053298711776733, 0.003813154762610793, 0.3101516664028168]\n",
      "Step 922, loss = [0.3016255795955658, 0.0036567251663655043, 0.3012599050998688]\n",
      "Step 923, loss = [0.29587289690971375, 0.004173142835497856, 0.29545557498931885]\n",
      "Step 924, loss = [0.2947416305541992, 0.002796947956085205, 0.2944619357585907]\n",
      "Step 925, loss = [0.3030679523944855, 0.003491540439426899, 0.30271878838539124]\n",
      "Step 926, loss = [0.29707205295562744, 0.002987147308886051, 0.29677334427833557]\n",
      "Step 927, loss = [0.2926246225833893, 0.005328347906470299, 0.2920917868614197]\n",
      "Step 928, loss = [0.298331081867218, 0.003669130150228739, 0.2979641556739807]\n",
      "Step 929, loss = [0.3004447817802429, 0.004357234109193087, 0.3000090718269348]\n",
      "Step 930, loss = [0.29305484890937805, 0.005517208948731422, 0.29250311851501465]\n",
      "Step 931, loss = [0.2988828420639038, 0.005788019392639399, 0.2983040511608124]\n",
      "Step 932, loss = [0.2845640182495117, 0.004335595760494471, 0.2841304540634155]\n",
      "Step 933, loss = [0.2967948317527771, 0.0030874672811478376, 0.2964860796928406]\n",
      "Step 934, loss = [0.2992335259914398, 0.0038684469182044268, 0.29884669184684753]\n",
      "Step 935, loss = [0.303523987531662, 0.002267231233417988, 0.3032972514629364]\n",
      "Step 936, loss = [0.309468537569046, 0.0030412657652050257, 0.3091644048690796]\n",
      "Step 937, loss = [0.3086438775062561, 0.0031852256506681442, 0.3083253502845764]\n",
      "Step 938, loss = [0.2982669174671173, 0.0063764434307813644, 0.2976292669773102]\n",
      "Step 939, loss = [0.3025246262550354, 0.005720893386751413, 0.3019525408744812]\n",
      "Step 940, loss = [0.2969137132167816, 0.004076569341123104, 0.29650604724884033]\n",
      "Step 941, loss = [0.28127798438072205, 0.005039081908762455, 0.2807740867137909]\n",
      "Step 942, loss = [0.29695141315460205, 0.006070443894714117, 0.2963443696498871]\n",
      "Step 943, loss = [0.3011265993118286, 0.018257373943924904, 0.2993008494377136]\n",
      "Step 944, loss = [0.30128729343414307, 0.004614110104739666, 0.30082589387893677]\n",
      "Step 945, loss = [0.31327059864997864, 0.004476298112422228, 0.31282296776771545]\n",
      "Step 946, loss = [0.29364213347435, 0.0028563032392412424, 0.2933565080165863]\n",
      "Step 947, loss = [0.2905181646347046, 0.0040717963129282, 0.2901109755039215]\n",
      "Step 948, loss = [0.3066418766975403, 0.007370198145508766, 0.3059048652648926]\n",
      "Step 949, loss = [0.3074830174446106, 0.003351649036630988, 0.3071478605270386]\n",
      "Step 950, loss = [0.2958168089389801, 0.003975165542215109, 0.295419305562973]\n",
      "Step 951, loss = [0.286577433347702, 0.004380754195153713, 0.2861393690109253]\n",
      "Step 952, loss = [0.2997516095638275, 0.002070970833301544, 0.2995445132255554]\n",
      "Step 953, loss = [0.3082493245601654, 0.004048495553433895, 0.30784448981285095]\n",
      "Step 954, loss = [0.28437238931655884, 0.0022488103713840246, 0.2841475009918213]\n",
      "Step 955, loss = [0.29677221179008484, 0.0013045934028923512, 0.2966417670249939]\n",
      "Step 956, loss = [0.3117848336696625, 0.0059193577617406845, 0.31119289994239807]\n",
      "Step 957, loss = [0.2971815764904022, 0.0035489301662892103, 0.29682669043540955]\n",
      "Step 958, loss = [0.28568732738494873, 0.006034424528479576, 0.2850838899612427]\n",
      "Step 959, loss = [0.2976302206516266, 0.004235345870256424, 0.29720669984817505]\n",
      "Step 960, loss = [0.2951138913631439, 0.004389187321066856, 0.29467496275901794]\n",
      "Step 961, loss = [0.3013845384120941, 0.0019232011400163174, 0.3011922240257263]\n",
      "Step 962, loss = [0.2843807339668274, 0.003248349530622363, 0.2840558886528015]\n",
      "Step 963, loss = [0.29131293296813965, 0.005231243558228016, 0.29078981280326843]\n",
      "Step 964, loss = [0.30618783831596375, 0.0016483123181387782, 0.3060230016708374]\n",
      "Step 965, loss = [0.30039459466934204, 0.0019925320520997047, 0.3001953363418579]\n",
      "Step 966, loss = [0.29496631026268005, 0.0033752084709703922, 0.2946287989616394]\n",
      "Step 967, loss = [0.30441224575042725, 0.006806172430515289, 0.30373162031173706]\n",
      "Step 968, loss = [0.30938300490379333, 0.0026470120064914227, 0.3091183006763458]\n",
      "Step 969, loss = [0.29461392760276794, 0.011063285171985626, 0.2935076057910919]\n",
      "Step 970, loss = [0.3090551197528839, 0.00693078339099884, 0.30836203694343567]\n",
      "Step 971, loss = [0.2973234951496124, 0.002105877036228776, 0.29711291193962097]\n",
      "Step 972, loss = [0.31129059195518494, 0.005502982996404171, 0.31074029207229614]\n",
      "Step 973, loss = [0.3038517236709595, 0.00598655641078949, 0.30325305461883545]\n",
      "Step 974, loss = [0.29717668890953064, 0.0030899387784302235, 0.296867698431015]\n",
      "Step 975, loss = [0.2957756519317627, 0.0019289606716483831, 0.29558274149894714]\n",
      "Step 976, loss = [0.2839948236942291, 0.003992812242358923, 0.2835955321788788]\n",
      "Step 977, loss = [0.29289668798446655, 0.0024354769848287106, 0.2926531434059143]\n",
      "Update target distribution epoch 0 step 978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11754/11754 [1:50:40<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 978, loss = [0.2788509130477905, 0.006115457974374294, 0.278239369392395]\n",
      "Step 979, loss = [0.29964083433151245, 0.0038301837630569935, 0.2992578148841858]\n",
      "Step 980, loss = [0.278536856174469, 0.004040088504552841, 0.2781328558921814]\n",
      "Step 981, loss = [0.2939717769622803, 0.005215039011090994, 0.293450266122818]\n",
      "Step 982, loss = [0.28610673546791077, 0.003079922404140234, 0.28579872846603394]\n",
      "Step 983, loss = [0.29527994990348816, 0.0036189518868923187, 0.2949180603027344]\n",
      "Step 984, loss = [0.31070610880851746, 0.005502746440470219, 0.31015583872795105]\n",
      "Step 985, loss = [0.3026113212108612, 0.004015602171421051, 0.3022097647190094]\n",
      "Step 986, loss = [0.3024245500564575, 0.0048388270661234856, 0.3019406795501709]\n",
      "Step 987, loss = [0.30607524514198303, 0.006786600686609745, 0.30539658665657043]\n",
      "Step 988, loss = [0.3024629056453705, 0.004465560428798199, 0.30201634764671326]\n",
      "Step 989, loss = [0.30607765913009644, 0.0027787950821220875, 0.30579978227615356]\n",
      "Step 990, loss = [0.29890090227127075, 0.0037875892594456673, 0.29852214455604553]\n",
      "Step 991, loss = [0.3008280396461487, 0.0025311256758868694, 0.30057492852211]\n",
      "Step 992, loss = [0.30239641666412354, 0.003576842835173011, 0.3020387291908264]\n",
      "Step 993, loss = [0.3117768466472626, 0.004726024344563484, 0.3113042414188385]\n",
      "Step 994, loss = [0.30403029918670654, 0.0026896558701992035, 0.3037613332271576]\n",
      "Step 995, loss = [0.28427231311798096, 0.003513728268444538, 0.28392094373703003]\n",
      "Step 996, loss = [0.29605376720428467, 0.006234436761587858, 0.29543033242225647]\n",
      "Step 997, loss = [0.28794538974761963, 0.00165219372138381, 0.28778016567230225]\n",
      "Step 998, loss = [0.3055965304374695, 0.005720993038266897, 0.3050244450569153]\n",
      "Step 999, loss = [0.30749598145484924, 0.0075515820644795895, 0.30674082040786743]\n",
      "Step 1000, loss = [0.3000618815422058, 0.005443168804049492, 0.29951757192611694]\n",
      "Step 1001, loss = [0.300039142370224, 0.006227400153875351, 0.2994163930416107]\n",
      "Step 1002, loss = [0.2921803295612335, 0.005004191305488348, 0.29167991876602173]\n",
      "Step 1003, loss = [0.2954862117767334, 0.004578306805342436, 0.2950283885002136]\n",
      "Step 1004, loss = [0.30267080664634705, 0.0030446425080299377, 0.30236634612083435]\n",
      "Step 1005, loss = [0.309785932302475, 0.0064206235110759735, 0.30914387106895447]\n",
      "Step 1006, loss = [0.307817280292511, 0.004185158759355545, 0.3073987662792206]\n",
      "Step 1007, loss = [0.3026079833507538, 0.005165832117199898, 0.3020913898944855]\n",
      "Step 1008, loss = [0.2893654406070709, 0.006789695005863905, 0.28868648409843445]\n",
      "Step 1009, loss = [0.2929520606994629, 0.007124003488570452, 0.2922396659851074]\n",
      "Step 1010, loss = [0.30147066712379456, 0.005089727230370045, 0.3009617030620575]\n",
      "Step 1011, loss = [0.3026649057865143, 0.0036621764302253723, 0.3022986948490143]\n",
      "Step 1012, loss = [0.31383100152015686, 0.005049142986536026, 0.3133260905742645]\n",
      "Step 1013, loss = [0.3051821291446686, 0.006840330548584461, 0.3044981062412262]\n",
      "Step 1014, loss = [0.2842176556587219, 0.005385132972151041, 0.28367912769317627]\n",
      "Step 1015, loss = [0.2864782512187958, 0.005526208318769932, 0.28592562675476074]\n",
      "Step 1016, loss = [0.3061411380767822, 0.005523744039237499, 0.3055887520313263]\n",
      "Step 1017, loss = [0.30341529846191406, 0.0028005968779325485, 0.3031352460384369]\n",
      "Step 1018, loss = [0.2901838421821594, 0.003080870723351836, 0.28987574577331543]\n",
      "Step 1019, loss = [0.2962128221988678, 0.005192052572965622, 0.29569360613822937]\n",
      "Step 1020, loss = [0.2949840724468231, 0.006501887459307909, 0.29433387517929077]\n",
      "Step 1021, loss = [0.31107860803604126, 0.004367533139884472, 0.3106418550014496]\n",
      "Step 1022, loss = [0.2891941964626312, 0.0028326911851763725, 0.2889109253883362]\n",
      "Step 1023, loss = [0.3057311177253723, 0.005281554535031319, 0.3052029609680176]\n",
      "Step 1024, loss = [0.3021456301212311, 0.005494571756571531, 0.30159616470336914]\n",
      "Step 1025, loss = [0.3082931339740753, 0.004481606185436249, 0.30784496665000916]\n",
      "Step 1026, loss = [0.29578056931495667, 0.006525441538542509, 0.2951280176639557]\n",
      "Step 1027, loss = [0.2956942915916443, 0.003865528153255582, 0.2953077256679535]\n",
      "Step 1028, loss = [0.2989733815193176, 0.004570099990814924, 0.2985163629055023]\n",
      "Step 1029, loss = [0.28310051560401917, 0.004143517930060625, 0.28268617391586304]\n",
      "Step 1030, loss = [0.29652148485183716, 0.004664510954171419, 0.29605501890182495]\n",
      "Step 1031, loss = [0.28556686639785767, 0.006062224041670561, 0.28496065735816956]\n",
      "Step 1032, loss = [0.2869478464126587, 0.0019481935305520892, 0.2867530286312103]\n",
      "Step 1033, loss = [0.29267916083335876, 0.002798347733914852, 0.2923993170261383]\n",
      "Step 1034, loss = [0.2960764467716217, 0.00391406798735261, 0.2956850528717041]\n",
      "Step 1035, loss = [0.2837785482406616, 0.004432159475982189, 0.2833353281021118]\n",
      "Step 1036, loss = [0.29728955030441284, 0.006766954902559519, 0.29661285877227783]\n",
      "Step 1037, loss = [0.29839909076690674, 0.00456314068287611, 0.29794278740882874]\n",
      "Step 1038, loss = [0.2989598214626312, 0.003992890007793903, 0.2985605299472809]\n",
      "Step 1039, loss = [0.3031308054924011, 0.002208931837230921, 0.3029099106788635]\n",
      "Step 1040, loss = [0.30277353525161743, 0.007977512665092945, 0.3019757866859436]\n",
      "Step 1041, loss = [0.3011724352836609, 0.0036070665810257196, 0.3008117377758026]\n",
      "Step 1042, loss = [0.308962345123291, 0.004531245678663254, 0.3085092306137085]\n",
      "Step 1043, loss = [0.30441373586654663, 0.0027046562172472477, 0.3041432797908783]\n",
      "Step 1044, loss = [0.2972281277179718, 0.008609022945165634, 0.29636722803115845]\n",
      "Step 1045, loss = [0.29913872480392456, 0.004539732821285725, 0.2986847460269928]\n",
      "Step 1046, loss = [0.3055625259876251, 0.001420123502612114, 0.30542051792144775]\n",
      "Step 1047, loss = [0.3040353059768677, 0.007109177298843861, 0.3033244013786316]\n",
      "Step 1048, loss = [0.3040926456451416, 0.0034042757470160723, 0.30375221371650696]\n",
      "Step 1049, loss = [0.3127768933773041, 0.0032319987658411264, 0.3124536871910095]\n",
      "Step 1050, loss = [0.30579495429992676, 0.0033312577288597822, 0.3054618239402771]\n",
      "Step 1051, loss = [0.30246996879577637, 0.006819477304816246, 0.3017880320549011]\n",
      "Step 1052, loss = [0.3069564700126648, 0.0026340207550674677, 0.30669307708740234]\n",
      "Step 1053, loss = [0.3045285940170288, 0.005747206509113312, 0.3039538860321045]\n",
      "Step 1054, loss = [0.3040180504322052, 0.002055621473118663, 0.3038124740123749]\n",
      "Step 1055, loss = [0.26729273796081543, 0.005392922088503838, 0.2667534351348877]\n",
      "Step 1056, loss = [0.30437979102134705, 0.003850631881505251, 0.30399471521377563]\n",
      "Step 1057, loss = [0.29910939931869507, 0.003876654664054513, 0.2987217307090759]\n",
      "Step 1058, loss = [0.3135533332824707, 0.002860799664631486, 0.3132672607898712]\n",
      "Step 1059, loss = [0.29845669865608215, 0.005816054530441761, 0.2978751063346863]\n",
      "Step 1060, loss = [0.2899700701236725, 0.003457650076597929, 0.28962430357933044]\n",
      "Step 1061, loss = [0.30500173568725586, 0.0026343949139118195, 0.30473828315734863]\n",
      "Step 1062, loss = [0.30553966760635376, 0.0053345514461398125, 0.305006206035614]\n",
      "Step 1063, loss = [0.301322340965271, 0.003421013243496418, 0.30098024010658264]\n",
      "Step 1064, loss = [0.31078413128852844, 0.0028988858684897423, 0.31049424409866333]\n",
      "Step 1065, loss = [0.3002946078777313, 0.0034365069586783648, 0.2999509572982788]\n",
      "Step 1066, loss = [0.31285110116004944, 0.0056720394641160965, 0.3122839033603668]\n",
      "Step 1067, loss = [0.3015667200088501, 0.005482001695781946, 0.30101850628852844]\n",
      "Step 1068, loss = [0.3010949492454529, 0.00571027398109436, 0.30052393674850464]\n",
      "Step 1069, loss = [0.29388073086738586, 0.005247571039944887, 0.2933559715747833]\n",
      "Step 1070, loss = [0.30032631754875183, 0.003321116091683507, 0.29999420046806335]\n",
      "Step 1071, loss = [0.3050193786621094, 0.006008943542838097, 0.3044184744358063]\n",
      "Step 1072, loss = [0.3035199046134949, 0.00427359389141202, 0.3030925393104553]\n",
      "Step 1073, loss = [0.29021674394607544, 0.006211733911186457, 0.2895955741405487]\n",
      "Step 1074, loss = [0.30497831106185913, 0.0027714637108147144, 0.30470117926597595]\n",
      "Step 1075, loss = [0.3082359731197357, 0.004250315949320793, 0.3078109323978424]\n",
      "Step 1076, loss = [0.2819696068763733, 0.006361581385135651, 0.28133344650268555]\n",
      "Step 1077, loss = [0.30138954520225525, 0.00391619186848402, 0.30099791288375854]\n",
      "Step 1078, loss = [0.30303341150283813, 0.003653386840596795, 0.3026680648326874]\n",
      "Step 1079, loss = [0.30550888180732727, 0.0045061903074383736, 0.3050582706928253]\n",
      "Step 1080, loss = [0.29540514945983887, 0.0033716519828885794, 0.29506799578666687]\n",
      "Step 1081, loss = [0.3025614321231842, 0.005433280952274799, 0.30201810598373413]\n",
      "Step 1082, loss = [0.2892474830150604, 0.005344187840819359, 0.2887130677700043]\n",
      "Step 1083, loss = [0.29533669352531433, 0.0041729602962732315, 0.2949194014072418]\n",
      "Step 1084, loss = [0.2989158630371094, 0.00567503459751606, 0.2983483672142029]\n",
      "Step 1085, loss = [0.29499372839927673, 0.003909119870513678, 0.29460281133651733]\n",
      "Step 1086, loss = [0.3027823567390442, 0.005084959324449301, 0.30227386951446533]\n",
      "Step 1087, loss = [0.30587121844291687, 0.004490772262215614, 0.3054221272468567]\n",
      "Step 1088, loss = [0.29155832529067993, 0.004574822727590799, 0.2911008298397064]\n",
      "Step 1089, loss = [0.3007679283618927, 0.005133595317602158, 0.30025458335876465]\n",
      "Step 1090, loss = [0.3020533323287964, 0.004075265489518642, 0.30164581537246704]\n",
      "Step 1091, loss = [0.30447548627853394, 0.006903195753693581, 0.30378517508506775]\n",
      "Step 1092, loss = [0.29026558995246887, 0.002869419287890196, 0.28997865319252014]\n",
      "Step 1093, loss = [0.29839468002319336, 0.0060769664123654366, 0.29778698086738586]\n",
      "Step 1094, loss = [0.3042140603065491, 0.0025917766615748405, 0.3039548695087433]\n",
      "Step 1095, loss = [0.3005066514015198, 0.0036300900392234325, 0.30014362931251526]\n",
      "Step 1096, loss = [0.30223512649536133, 0.00520070968195796, 0.30171504616737366]\n",
      "Step 1097, loss = [0.3003879189491272, 0.0033489728812128305, 0.30005303025245667]\n",
      "Step 1098, loss = [0.3127405345439911, 0.0038192737847566605, 0.3123586177825928]\n",
      "Step 1099, loss = [0.30494433641433716, 0.0021575363352894783, 0.30472859740257263]\n",
      "Step 1100, loss = [0.2950802445411682, 0.004449769854545593, 0.29463526606559753]\n",
      "Step 1101, loss = [0.2962222397327423, 0.001493477146141231, 0.29607290029525757]\n",
      "Step 1102, loss = [0.296257346868515, 0.00226208521053195, 0.2960311472415924]\n",
      "Step 1103, loss = [0.301913321018219, 0.005605478771030903, 0.30135276913642883]\n",
      "Step 1104, loss = [0.2957358956336975, 0.0021113394759595394, 0.2955247759819031]\n",
      "Step 1105, loss = [0.29579660296440125, 0.004379166290163994, 0.29535868763923645]\n",
      "Step 1106, loss = [0.2790638208389282, 0.002413511974737048, 0.27882248163223267]\n",
      "Step 1107, loss = [0.3035307824611664, 0.005236985627561808, 0.3030070960521698]\n",
      "Step 1108, loss = [0.31212589144706726, 0.004476620815694332, 0.3116782307624817]\n",
      "Step 1109, loss = [0.29929742217063904, 0.005008803214877844, 0.29879653453826904]\n",
      "Step 1110, loss = [0.2906216084957123, 0.004771095700562, 0.29014450311660767]\n",
      "Step 1111, loss = [0.3008252680301666, 0.0013700248673558235, 0.3006882667541504]\n",
      "Step 1112, loss = [0.30342793464660645, 0.0032479637302458286, 0.30310314893722534]\n",
      "Step 1113, loss = [0.29310518503189087, 0.006559549365192652, 0.2924492359161377]\n",
      "Step 1114, loss = [0.302173376083374, 0.004739829339087009, 0.3016993999481201]\n",
      "Step 1115, loss = [0.31327134370803833, 0.00949865859001875, 0.3123214840888977]\n",
      "Step 1116, loss = [0.270501971244812, 0.003706168383359909, 0.27013134956359863]\n",
      "Step 1117, loss = [0.2975149154663086, 0.004421281162649393, 0.29707279801368713]\n",
      "Step 1118, loss = [0.3049268126487732, 0.0035042748786509037, 0.30457639694213867]\n",
      "Step 1119, loss = [0.2827545702457428, 0.004211214371025562, 0.28233346343040466]\n",
      "Step 1120, loss = [0.2933898866176605, 0.004390455782413483, 0.292950838804245]\n",
      "Step 1121, loss = [0.29992055892944336, 0.008999411948025227, 0.2990206182003021]\n",
      "Step 1122, loss = [0.29975712299346924, 0.003153730882331729, 0.29944175481796265]\n",
      "Step 1123, loss = [0.3027516007423401, 0.006965286564081907, 0.3020550608634949]\n",
      "Step 1124, loss = [0.31015175580978394, 0.004276283551007509, 0.3097241222858429]\n",
      "Step 1125, loss = [0.29974111914634705, 0.00319536286406219, 0.2994215786457062]\n",
      "Step 1126, loss = [0.3068119287490845, 0.0031235506758093834, 0.30649957060813904]\n",
      "Step 1127, loss = [0.2944643497467041, 0.005781408399343491, 0.2938862144947052]\n",
      "Step 1128, loss = [0.30332669615745544, 0.005348297767341137, 0.30279186367988586]\n",
      "Step 1129, loss = [0.3041940927505493, 0.002832159399986267, 0.30391088128089905]\n",
      "Step 1130, loss = [0.28952497243881226, 0.003443382680416107, 0.2891806364059448]\n",
      "Step 1131, loss = [0.29466065764427185, 0.001087447628378868, 0.29455190896987915]\n",
      "Step 1132, loss = [0.3029109537601471, 0.0016621123068034649, 0.3027447462081909]\n",
      "Step 1133, loss = [0.3012332320213318, 0.004562266170978546, 0.30077701807022095]\n",
      "Step 1134, loss = [0.3023602068424225, 0.008361895568668842, 0.30152401328086853]\n",
      "Step 1135, loss = [0.30513420701026917, 0.003657691180706024, 0.304768443107605]\n",
      "Step 1136, loss = [0.306558758020401, 0.004578252788633108, 0.3061009347438812]\n",
      "Step 1137, loss = [0.30066755414009094, 0.003785757115110755, 0.30028897523880005]\n",
      "Step 1138, loss = [0.2994287312030792, 0.0033568046055734158, 0.2990930378437042]\n",
      "Step 1139, loss = [0.2924748957157135, 0.004125582054257393, 0.29206234216690063]\n",
      "Step 1140, loss = [0.2873549163341522, 0.005749061703681946, 0.2867799997329712]\n",
      "Step 1141, loss = [0.2910802960395813, 0.002266409806907177, 0.2908536493778229]\n",
      "Step 1142, loss = [0.31070956587791443, 0.00439884327352047, 0.31026968359947205]\n",
      "Step 1143, loss = [0.3016141951084137, 0.001321010640822351, 0.30148208141326904]\n",
      "Step 1144, loss = [0.30382460355758667, 0.0032098465599119663, 0.3035036325454712]\n",
      "Step 1145, loss = [0.3151763081550598, 0.0049113743007183075, 0.3146851658821106]\n",
      "Step 1146, loss = [0.2904183864593506, 0.0038941067177802324, 0.29002898931503296]\n",
      "Step 1147, loss = [0.30161184072494507, 0.0031356681138277054, 0.30129826068878174]\n",
      "Step 1148, loss = [0.30305635929107666, 0.00569161307066679, 0.30248719453811646]\n",
      "Step 1149, loss = [0.3099800646305084, 0.003648353274911642, 0.30961522459983826]\n",
      "Step 1150, loss = [0.2916938066482544, 0.003532212460413575, 0.29134058952331543]\n",
      "Step 1151, loss = [0.30967792868614197, 0.004056180827319622, 0.30927231907844543]\n",
      "Step 1152, loss = [0.3061211109161377, 0.0042011309415102005, 0.30570098757743835]\n",
      "Step 1153, loss = [0.30959033966064453, 0.005463284905999899, 0.3090440034866333]\n",
      "Step 1154, loss = [0.3038032650947571, 0.0029486985877156258, 0.3035084009170532]\n",
      "Step 1155, loss = [0.3087538480758667, 0.003747939597815275, 0.30837905406951904]\n",
      "Step 1156, loss = [0.3012387454509735, 0.002929507289081812, 0.30094578862190247]\n",
      "Step 1157, loss = [0.30255183577537537, 0.004308076109737158, 0.30212101340293884]\n",
      "Step 1158, loss = [0.31096962094306946, 0.005503317806869745, 0.3104192912578583]\n",
      "Step 1159, loss = [0.3005503714084625, 0.005412078928202391, 0.300009161233902]\n",
      "Step 1160, loss = [0.30523717403411865, 0.0040826876647770405, 0.3048289120197296]\n",
      "Step 1161, loss = [0.2946912348270416, 0.00520923500880599, 0.2941703200340271]\n",
      "Step 1162, loss = [0.3014794886112213, 0.006603259593248367, 0.30081915855407715]\n",
      "Step 1163, loss = [0.29707667231559753, 0.0038717524148523808, 0.296689510345459]\n",
      "Step 1164, loss = [0.30705612897872925, 0.006981994025409222, 0.3063579201698303]\n",
      "Step 1165, loss = [0.3168919086456299, 0.0046576871536672115, 0.3164261281490326]\n",
      "Step 1166, loss = [0.3159266710281372, 0.004202930256724358, 0.31550636887550354]\n",
      "Step 1167, loss = [0.3101389706134796, 0.002500328700989485, 0.30988892912864685]\n",
      "Step 1168, loss = [0.29087576270103455, 0.005878634285181761, 0.29028791189193726]\n",
      "Step 1169, loss = [0.3010229170322418, 0.0013918490149080753, 0.3008837401866913]\n",
      "Step 1170, loss = [0.2965743839740753, 0.004818074870854616, 0.29609256982803345]\n",
      "Step 1171, loss = [0.3011280298233032, 0.0018419690895825624, 0.3009438216686249]\n",
      "Step 1172, loss = [0.2906859219074249, 0.006149474997073412, 0.2900709807872772]\n",
      "Step 1173, loss = [0.28875720500946045, 0.003381365444511175, 0.28841906785964966]\n",
      "Step 1174, loss = [0.31330162286758423, 0.0074336109682917595, 0.31255826354026794]\n",
      "Step 1175, loss = [0.30116352438926697, 0.005472471006214619, 0.3006162643432617]\n",
      "Step 1176, loss = [0.28410470485687256, 0.003896198933944106, 0.2837150990962982]\n",
      "Step 1177, loss = [0.30616289377212524, 0.0054426491260528564, 0.30561861395835876]\n",
      "Step 1178, loss = [0.3139178156852722, 0.005634305998682976, 0.31335437297821045]\n",
      "Step 1179, loss = [0.2981950342655182, 0.005450369790196419, 0.297650009393692]\n",
      "Step 1180, loss = [0.3015297055244446, 0.004015770740807056, 0.3011281192302704]\n",
      "Step 1181, loss = [0.2980465292930603, 0.008172594010829926, 0.29722926020622253]\n",
      "Step 1182, loss = [0.298112154006958, 0.002590540796518326, 0.29785311222076416]\n",
      "Step 1183, loss = [0.3142324984073639, 0.0037231831811368465, 0.3138601779937744]\n",
      "Step 1184, loss = [0.3169301450252533, 0.005834157578647137, 0.3163467347621918]\n",
      "Step 1185, loss = [0.3040366768836975, 0.006569779012352228, 0.30337968468666077]\n",
      "Step 1186, loss = [0.2988947033882141, 0.0022821472957730293, 0.29866647720336914]\n",
      "Step 1187, loss = [0.30317264795303345, 0.0012100182939320803, 0.3030516505241394]\n",
      "Step 1188, loss = [0.3063246011734009, 0.003992127254605293, 0.3059253990650177]\n",
      "Step 1189, loss = [0.28718408942222595, 0.004831457510590553, 0.28670093417167664]\n",
      "Step 1190, loss = [0.2946421205997467, 0.002547935349866748, 0.2943873405456543]\n",
      "Step 1191, loss = [0.2935097813606262, 0.0031657142098993063, 0.2931932210922241]\n",
      "Step 1192, loss = [0.29792264103889465, 0.003515547374263406, 0.2975710928440094]\n",
      "Step 1193, loss = [0.30621078610420227, 0.0038446749094873667, 0.3058263063430786]\n",
      "Step 1194, loss = [0.3038707375526428, 0.0037310863845050335, 0.30349764227867126]\n",
      "Step 1195, loss = [0.2952461540699005, 0.006160652730613947, 0.2946300804615021]\n",
      "Step 1196, loss = [0.2963544726371765, 0.00356087414547801, 0.29599839448928833]\n",
      "Step 1197, loss = [0.3015174865722656, 0.0051398565992712975, 0.30100351572036743]\n",
      "Step 1198, loss = [0.291384756565094, 0.0042650047689676285, 0.2909582555294037]\n",
      "Step 1199, loss = [0.3070473372936249, 0.003824916435405612, 0.3066648542881012]\n",
      "Step 1200, loss = [0.2985006868839264, 0.004646830260753632, 0.29803600907325745]\n",
      "Step 1201, loss = [0.3136396110057831, 0.007478625047951937, 0.31289175152778625]\n",
      "Step 1202, loss = [0.30246639251708984, 0.004080229438841343, 0.3020583689212799]\n",
      "Step 1203, loss = [0.30626624822616577, 0.0031241653487086296, 0.30595383048057556]\n",
      "Step 1204, loss = [0.29710614681243896, 0.0036732640583068132, 0.2967388331890106]\n",
      "Step 1205, loss = [0.3090556859970093, 0.005876126699149609, 0.3084680736064911]\n",
      "Step 1206, loss = [0.3089621365070343, 0.005914073437452316, 0.3083707392215729]\n",
      "Step 1207, loss = [0.285014808177948, 0.002199924550950527, 0.28479480743408203]\n",
      "Step 1208, loss = [0.3048618733882904, 0.0034504516515880823, 0.30451682209968567]\n",
      "Step 1209, loss = [0.30329617857933044, 0.006568208336830139, 0.302639365196228]\n",
      "Step 1210, loss = [0.29830580949783325, 0.004932044539600611, 0.2978126108646393]\n",
      "Step 1211, loss = [0.29856133460998535, 0.004498599097132683, 0.2981114685535431]\n",
      "Step 1212, loss = [0.31174150109291077, 0.00372037710621953, 0.3113694489002228]\n",
      "Step 1213, loss = [0.2988564670085907, 0.0047695874236524105, 0.298379510641098]\n",
      "Step 1214, loss = [0.29378315806388855, 0.001368039520457387, 0.29364636540412903]\n",
      "Step 1215, loss = [0.3018319010734558, 0.005737766157835722, 0.3012581169605255]\n",
      "Step 1216, loss = [0.30810031294822693, 0.0020829627756029367, 0.3078920245170593]\n",
      "Step 1217, loss = [0.31188279390335083, 0.0050810775719583035, 0.311374694108963]\n",
      "Step 1218, loss = [0.30180656909942627, 0.003686379874125123, 0.30143794417381287]\n",
      "Step 1219, loss = [0.2954822778701782, 0.002980942605063319, 0.2951841950416565]\n",
      "Step 1220, loss = [0.3050103187561035, 0.005701809655874968, 0.30444014072418213]\n",
      "Step 1221, loss = [0.2967292368412018, 0.002405070001259446, 0.2964887320995331]\n",
      "Step 1222, loss = [0.29007044434547424, 0.004137945827096701, 0.2896566390991211]\n",
      "Step 1223, loss = [0.30531251430511475, 0.004404878243803978, 0.3048720359802246]\n",
      "Step 1224, loss = [0.28350499272346497, 0.0059033227153122425, 0.2829146683216095]\n",
      "Step 1225, loss = [0.30017125606536865, 0.004814324900507927, 0.2996898293495178]\n",
      "Step 1226, loss = [0.29902684688568115, 0.0021409159526228905, 0.29881274700164795]\n",
      "Step 1227, loss = [0.30701932311058044, 0.0040125856176018715, 0.3066180646419525]\n",
      "Step 1228, loss = [0.30961188673973083, 0.0010793479159474373, 0.3095039427280426]\n",
      "Step 1229, loss = [0.3112204372882843, 0.004749934654682875, 0.3107454478740692]\n",
      "Step 1230, loss = [0.29842084646224976, 0.002208117861300707, 0.2982000410556793]\n",
      "Step 1231, loss = [0.2886843979358673, 0.003287296276539564, 0.28835567831993103]\n",
      "Step 1232, loss = [0.2922895848751068, 0.004715138114988804, 0.2918180823326111]\n",
      "Step 1233, loss = [0.3079754710197449, 0.003178811864927411, 0.3076575994491577]\n",
      "Step 1234, loss = [0.29691264033317566, 0.00440433993935585, 0.2964721918106079]\n",
      "Step 1235, loss = [0.3115187883377075, 0.005020527634769678, 0.3110167384147644]\n",
      "Step 1236, loss = [0.29714372754096985, 0.0047049084678292274, 0.2966732382774353]\n",
      "Step 1237, loss = [0.3079304099082947, 0.007728220894932747, 0.30715757608413696]\n",
      "Step 1238, loss = [0.30884867906570435, 0.004952403716742992, 0.3083534240722656]\n",
      "Step 1239, loss = [0.2899915277957916, 0.0046232002787292, 0.2895292043685913]\n",
      "Step 1240, loss = [0.27802059054374695, 0.0057868813164532185, 0.2774418890476227]\n",
      "Step 1241, loss = [0.30140507221221924, 0.002880547195672989, 0.3011170029640198]\n",
      "Step 1242, loss = [0.2878580689430237, 0.0031847741920500994, 0.28753960132598877]\n",
      "Step 1243, loss = [0.3052004873752594, 0.003387602511793375, 0.30486172437667847]\n",
      "Step 1244, loss = [0.30236339569091797, 0.004027876071631908, 0.30196061730384827]\n",
      "Step 1245, loss = [0.29956236481666565, 0.0034453272819519043, 0.2992178201675415]\n",
      "Step 1246, loss = [0.3097444772720337, 0.008157874457538128, 0.3089286983013153]\n",
      "Step 1247, loss = [0.307104229927063, 0.0031777885742485523, 0.306786447763443]\n",
      "Step 1248, loss = [0.2865496277809143, 0.0041794851422309875, 0.28613168001174927]\n",
      "Step 1249, loss = [0.30074888467788696, 0.004698088392615318, 0.30027908086776733]\n",
      "Step 1250, loss = [0.3021945059299469, 0.00525284418836236, 0.3016692101955414]\n",
      "Step 1251, loss = [0.30090245604515076, 0.002417712938040495, 0.3006606996059418]\n",
      "Step 1252, loss = [0.3080865144729614, 0.0036920870188623667, 0.30771729350090027]\n",
      "Step 1253, loss = [0.3110460042953491, 0.002519386587664485, 0.31079405546188354]\n",
      "Step 1254, loss = [0.298401802778244, 0.0031500528566539288, 0.2980867922306061]\n",
      "Step 1255, loss = [0.30007997155189514, 0.0011278283782303333, 0.2999671995639801]\n",
      "Step 1256, loss = [0.3096191883087158, 0.002880810759961605, 0.30933111906051636]\n",
      "Step 1257, loss = [0.29323092103004456, 0.0055234599858522415, 0.292678564786911]\n",
      "Step 1258, loss = [0.285257488489151, 0.007106526754796505, 0.284546822309494]\n",
      "Step 1259, loss = [0.3008619546890259, 0.004394518677145243, 0.3004224896430969]\n",
      "Step 1260, loss = [0.29935187101364136, 0.006254017353057861, 0.29872646927833557]\n",
      "Step 1261, loss = [0.30243709683418274, 0.004609357565641403, 0.30197617411613464]\n",
      "Step 1262, loss = [0.30906346440315247, 0.0062271598726511, 0.3084407448768616]\n",
      "Step 1263, loss = [0.30005475878715515, 0.005431037396192551, 0.2995116412639618]\n",
      "Step 1264, loss = [0.3112064301967621, 0.0027706415858119726, 0.3109293580055237]\n",
      "Step 1265, loss = [0.3088650107383728, 0.005706561263650656, 0.3082943558692932]\n",
      "Step 1266, loss = [0.2961430847644806, 0.004374642390757799, 0.2957056164741516]\n",
      "Step 1267, loss = [0.297294944524765, 0.002270823810249567, 0.2970678508281708]\n",
      "Step 1268, loss = [0.29186388850212097, 0.00322271347977221, 0.29154160618782043]\n",
      "Step 1269, loss = [0.31056514382362366, 0.0066015226766467094, 0.3099049925804138]\n",
      "Step 1270, loss = [0.29465773701667786, 0.0029167388565838337, 0.2943660616874695]\n",
      "Step 1271, loss = [0.2977651059627533, 0.0026726974174380302, 0.29749783873558044]\n",
      "Step 1272, loss = [0.28183743357658386, 0.005225019529461861, 0.2813149392604828]\n",
      "Step 1273, loss = [0.28467079997062683, 0.004522309172898531, 0.28421857953071594]\n",
      "Step 1274, loss = [0.30409520864486694, 0.004247544799000025, 0.3036704659461975]\n",
      "Step 1275, loss = [0.2985069155693054, 0.0028010436799377203, 0.29822680354118347]\n",
      "Step 1276, loss = [0.3047407865524292, 0.002619068371132016, 0.30447888374328613]\n",
      "Step 1277, loss = [0.31050416827201843, 0.0029301238246262074, 0.3102111518383026]\n",
      "Step 1278, loss = [0.29894545674324036, 0.003835859941318631, 0.29856187105178833]\n",
      "Step 1279, loss = [0.30102255940437317, 0.0034843964967876673, 0.30067411065101624]\n",
      "Step 1280, loss = [0.2863262891769409, 0.00358791695907712, 0.28596749901771545]\n",
      "Step 1281, loss = [0.2778075039386749, 0.003135227831080556, 0.27749398350715637]\n",
      "Step 1282, loss = [0.30771487951278687, 0.0033116061240434647, 0.3073837161064148]\n",
      "Step 1283, loss = [0.29403427243232727, 0.002541545545682311, 0.293780118227005]\n",
      "Step 1284, loss = [0.2873836159706116, 0.0057260701432824135, 0.2868109941482544]\n",
      "Step 1285, loss = [0.309315025806427, 0.005472992081195116, 0.30876773595809937]\n",
      "Step 1286, loss = [0.3062531352043152, 0.0034316526725888252, 0.3059099614620209]\n",
      "Step 1287, loss = [0.3123285472393036, 0.0024118097499012947, 0.31208735704421997]\n",
      "Step 1288, loss = [0.29714277386665344, 0.005655305460095406, 0.29657724499702454]\n",
      "Step 1289, loss = [0.2883032560348511, 0.005389753729104996, 0.2877642810344696]\n",
      "Step 1290, loss = [0.30336102843284607, 0.003357783192768693, 0.3030252456665039]\n",
      "Step 1291, loss = [0.2990303337574005, 0.004229980055242777, 0.29860734939575195]\n",
      "Step 1292, loss = [0.2999010682106018, 0.00226547010242939, 0.29967451095581055]\n",
      "Step 1293, loss = [0.29835379123687744, 0.007874000817537308, 0.29756638407707214]\n",
      "Step 1294, loss = [0.2758050560951233, 0.0028040807228535414, 0.27552464604377747]\n",
      "Step 1295, loss = [0.28837889432907104, 0.005436936393380165, 0.2878352105617523]\n",
      "Step 1296, loss = [0.30317944288253784, 0.0038979416713118553, 0.3027896583080292]\n",
      "Step 1297, loss = [0.2997282147407532, 0.001648423494771123, 0.29956337809562683]\n",
      "Step 1298, loss = [0.30787214636802673, 0.004151638597249985, 0.30745697021484375]\n",
      "Step 1299, loss = [0.29888734221458435, 0.004146626219153404, 0.29847267270088196]\n",
      "Step 1300, loss = [0.3133522570133209, 0.003125573508441448, 0.3130396902561188]\n",
      "Step 1301, loss = [0.2986491322517395, 0.0028450298123061657, 0.29836463928222656]\n",
      "Step 1302, loss = [0.30060485005378723, 0.005720348097383976, 0.3000328242778778]\n",
      "Step 1303, loss = [0.31650814414024353, 0.0035533031914383173, 0.31615281105041504]\n",
      "Step 1304, loss = [0.30464962124824524, 0.002017165534198284, 0.3044479191303253]\n",
      "Step 1305, loss = [0.3024863600730896, 0.007292899768799543, 0.3017570674419403]\n",
      "Step 1306, loss = [0.30845773220062256, 0.00413054134696722, 0.3080446720123291]\n",
      "Step 1307, loss = [0.29653701186180115, 0.007713761180639267, 0.29576563835144043]\n",
      "Step 1308, loss = [0.2786025106906891, 0.0031980108469724655, 0.27828270196914673]\n",
      "Step 1309, loss = [0.29948335886001587, 0.005020820535719395, 0.29898127913475037]\n",
      "Step 1310, loss = [0.30054551362991333, 0.006209964863955975, 0.2999245226383209]\n",
      "Step 1311, loss = [0.2961452901363373, 0.004042408894747496, 0.2957410514354706]\n",
      "Step 1312, loss = [0.30598950386047363, 0.005022059194743633, 0.3054873049259186]\n",
      "Step 1313, loss = [0.30743294954299927, 0.002317313803359866, 0.30720120668411255]\n",
      "Step 1314, loss = [0.2967122793197632, 0.005167773924767971, 0.29619550704956055]\n",
      "Step 1315, loss = [0.3121579885482788, 0.0028285409789532423, 0.3118751347064972]\n",
      "Step 1316, loss = [0.30910998582839966, 0.003398272907361388, 0.30877014994621277]\n",
      "Step 1317, loss = [0.28207361698150635, 0.005253982730209827, 0.28154823184013367]\n",
      "Step 1318, loss = [0.27149707078933716, 0.005516950972378254, 0.27094537019729614]\n",
      "Step 1319, loss = [0.30992791056632996, 0.0026419248897582293, 0.30966371297836304]\n",
      "Step 1320, loss = [0.29618650674819946, 0.002287803217768669, 0.2959577143192291]\n",
      "Step 1321, loss = [0.300293892621994, 0.003795437514781952, 0.2999143600463867]\n",
      "Step 1322, loss = [0.3084152340888977, 0.003167832503095269, 0.3080984652042389]\n",
      "Step 1323, loss = [0.3126415014266968, 0.005555902607738972, 0.312085896730423]\n",
      "Step 1324, loss = [0.30046260356903076, 0.003667866811156273, 0.3000958263874054]\n",
      "Step 1325, loss = [0.31060805916786194, 0.0046628802083432674, 0.31014177203178406]\n",
      "Step 1326, loss = [0.2881260812282562, 0.0052327378652989864, 0.28760281205177307]\n",
      "Step 1327, loss = [0.287556916475296, 0.005185606889426708, 0.2870383560657501]\n",
      "Step 1328, loss = [0.308540940284729, 0.007283816114068031, 0.30781257152557373]\n",
      "Step 1329, loss = [0.2991861402988434, 0.0037840097211301327, 0.2988077402114868]\n",
      "Step 1330, loss = [0.30015838146209717, 0.004037145059555769, 0.29975467920303345]\n",
      "Step 1331, loss = [0.30696070194244385, 0.002887823386117816, 0.3066719174385071]\n",
      "Step 1332, loss = [0.2981138229370117, 0.008034901693463326, 0.29731032252311707]\n",
      "Step 1333, loss = [0.2824209928512573, 0.00404928345233202, 0.2820160686969757]\n",
      "Step 1334, loss = [0.28452107310295105, 0.002656854223459959, 0.28425538539886475]\n",
      "Step 1335, loss = [0.2998235821723938, 0.004226040095090866, 0.2994009852409363]\n",
      "Step 1336, loss = [0.30790168046951294, 0.006135052070021629, 0.30728816986083984]\n",
      "Step 1337, loss = [0.3001640737056732, 0.0017268570372834802, 0.2999913990497589]\n",
      "Step 1338, loss = [0.28577476739883423, 0.0036967815831303596, 0.28540509939193726]\n",
      "Step 1339, loss = [0.29710307717323303, 0.004344027489423752, 0.29666867852211]\n",
      "Step 1340, loss = [0.2905057966709137, 0.005526602268218994, 0.2899531424045563]\n",
      "Step 1341, loss = [0.306692898273468, 0.009213162586092949, 0.3057715892791748]\n",
      "Step 1342, loss = [0.3010917603969574, 0.00637237261980772, 0.3004545271396637]\n",
      "Step 1343, loss = [0.2908482849597931, 0.0034972820430994034, 0.2904985547065735]\n",
      "Step 1344, loss = [0.29328468441963196, 0.005122904200106859, 0.2927723824977875]\n",
      "Step 1345, loss = [0.3030797839164734, 0.005203478969633579, 0.30255943536758423]\n",
      "Step 1346, loss = [0.30533367395401, 0.006386535707861185, 0.3046950101852417]\n",
      "Step 1347, loss = [0.3090106248855591, 0.005263049621134996, 0.3084843158721924]\n",
      "Step 1348, loss = [0.29414668679237366, 0.004427558276802301, 0.29370394349098206]\n",
      "Step 1349, loss = [0.29833483695983887, 0.004428623244166374, 0.2978919744491577]\n",
      "Step 1350, loss = [0.30367445945739746, 0.005580183118581772, 0.30311644077301025]\n",
      "Step 1351, loss = [0.30542007088661194, 0.0028315989766269922, 0.30513691902160645]\n",
      "Step 1352, loss = [0.3030398488044739, 0.0033939070999622345, 0.3027004599571228]\n",
      "Step 1353, loss = [0.3119794726371765, 0.004672775976359844, 0.31151220202445984]\n",
      "Step 1354, loss = [0.2988937199115753, 0.0032339792232960463, 0.29857033491134644]\n",
      "Step 1355, loss = [0.3021242618560791, 0.004195129033178091, 0.3017047345638275]\n",
      "Step 1356, loss = [0.2878364324569702, 0.0023271609097719193, 0.2876037061214447]\n",
      "Step 1357, loss = [0.3029199242591858, 0.004010661505162716, 0.3025188446044922]\n",
      "Step 1358, loss = [0.2916291356086731, 0.005785469431430101, 0.29105058312416077]\n",
      "Step 1359, loss = [0.28316640853881836, 0.0034531974233686924, 0.28282108902931213]\n",
      "Step 1360, loss = [0.3040262460708618, 0.0045311772264540195, 0.3035731315612793]\n",
      "Step 1361, loss = [0.3095647692680359, 0.00339156249538064, 0.3092256188392639]\n",
      "Step 1362, loss = [0.30459994077682495, 0.00385115179233253, 0.30421483516693115]\n",
      "Step 1363, loss = [0.2996504008769989, 0.004672852344810963, 0.2991831302642822]\n",
      "Step 1364, loss = [0.2945516109466553, 0.005351540632545948, 0.29401645064353943]\n",
      "Step 1365, loss = [0.29980307817459106, 0.0022375069092959166, 0.29957932233810425]\n",
      "Step 1366, loss = [0.3122343420982361, 0.006274930201470852, 0.31160685420036316]\n",
      "Step 1367, loss = [0.3010982275009155, 0.005667118355631828, 0.3005315065383911]\n",
      "Step 1368, loss = [0.3038739562034607, 0.005621039774268866, 0.30331185460090637]\n",
      "Step 1369, loss = [0.2889850437641144, 0.0014368077972903848, 0.2888413667678833]\n",
      "Step 1370, loss = [0.29206159710884094, 0.004536187741905451, 0.29160797595977783]\n",
      "Step 1371, loss = [0.29551663994789124, 0.0049930959939956665, 0.2950173318386078]\n",
      "Step 1372, loss = [0.3022119402885437, 0.005053461994975805, 0.30170658230781555]\n",
      "Step 1373, loss = [0.2997182309627533, 0.003314403584226966, 0.29938679933547974]\n",
      "Step 1374, loss = [0.30631381273269653, 0.0037764732260257006, 0.30593615770339966]\n",
      "Step 1375, loss = [0.3058268427848816, 0.0060927290469408035, 0.30521756410598755]\n",
      "Step 1376, loss = [0.3022252023220062, 0.00455402722582221, 0.30176979303359985]\n",
      "Step 1377, loss = [0.28937938809394836, 0.004486407153308392, 0.288930743932724]\n",
      "Step 1378, loss = [0.30812960863113403, 0.006535880267620087, 0.3074760138988495]\n",
      "Step 1379, loss = [0.30742448568344116, 0.005156723316758871, 0.30690881609916687]\n",
      "Step 1380, loss = [0.30668535828590393, 0.00190601812209934, 0.30649474263191223]\n",
      "Step 1381, loss = [0.29931965470314026, 0.003978312481194735, 0.2989218235015869]\n",
      "Step 1382, loss = [0.2971549332141876, 0.0029421690851449966, 0.2968607246875763]\n",
      "Step 1383, loss = [0.3112412691116333, 0.003450238611549139, 0.31089624762535095]\n",
      "Step 1384, loss = [0.2738007605075836, 0.0033797500655055046, 0.27346277236938477]\n",
      "Step 1385, loss = [0.31422555446624756, 0.0058783069252967834, 0.31363773345947266]\n",
      "Step 1386, loss = [0.29991400241851807, 0.00629015825688839, 0.29928499460220337]\n",
      "Step 1387, loss = [0.30201536417007446, 0.004766557831317186, 0.30153870582580566]\n",
      "Step 1388, loss = [0.30733758211135864, 0.0025136135518550873, 0.3070862293243408]\n",
      "Step 1389, loss = [0.2938827574253082, 0.004397297278046608, 0.2934430241584778]\n",
      "Step 1390, loss = [0.3057292401790619, 0.002726869424805045, 0.3054565489292145]\n",
      "Step 1391, loss = [0.29465025663375854, 0.0036153283435851336, 0.2942887246608734]\n",
      "Step 1392, loss = [0.2960391342639923, 0.004392706323415041, 0.29559987783432007]\n",
      "Step 1393, loss = [0.3114713132381439, 0.003671862417832017, 0.3111041188240051]\n",
      "Step 1394, loss = [0.29001832008361816, 0.002620427869260311, 0.28975626826286316]\n",
      "Step 1395, loss = [0.2994692921638489, 0.0031468458473682404, 0.2991546094417572]\n",
      "Step 1396, loss = [0.30874934792518616, 0.003589026629924774, 0.30839043855667114]\n",
      "Step 1397, loss = [0.2983398139476776, 0.004876409191638231, 0.29785215854644775]\n",
      "Step 1398, loss = [0.30042970180511475, 0.005938215646892786, 0.2998358905315399]\n",
      "Step 1399, loss = [0.3085983097553253, 0.006277843378484249, 0.3079705238342285]\n",
      "Step 1400, loss = [0.315187007188797, 0.002995395101606846, 0.31488746404647827]\n",
      "Step 1401, loss = [0.2743036150932312, 0.0024392209015786648, 0.2740596830844879]\n",
      "Step 1402, loss = [0.3040800988674164, 0.0039712851867079735, 0.30368298292160034]\n",
      "Step 1403, loss = [0.2928767800331116, 0.0063130962662398815, 0.292245477437973]\n",
      "Step 1404, loss = [0.2972140610218048, 0.0032488498836755753, 0.29688918590545654]\n",
      "Step 1405, loss = [0.3082752227783203, 0.00666312500834465, 0.3076089024543762]\n",
      "Step 1406, loss = [0.2953820526599884, 0.0038442977238446474, 0.2949976325035095]\n",
      "Step 1407, loss = [0.30832505226135254, 0.004653571173548698, 0.3078596889972687]\n",
      "Step 1408, loss = [0.3010987639427185, 0.002845852170139551, 0.3008141815662384]\n",
      "Step 1409, loss = [0.2972908318042755, 0.006135785020887852, 0.29667726159095764]\n",
      "Step 1410, loss = [0.29252687096595764, 0.004039086867123842, 0.2921229600906372]\n",
      "Step 1411, loss = [0.3100132942199707, 0.006321900524199009, 0.3093810975551605]\n",
      "Step 1412, loss = [0.30853942036628723, 0.004550249315798283, 0.3080843985080719]\n",
      "Step 1413, loss = [0.3048451542854309, 0.002949764719232917, 0.3045501708984375]\n",
      "Step 1414, loss = [0.2949470281600952, 0.0049217697232961655, 0.2944548428058624]\n",
      "Step 1415, loss = [0.2962487041950226, 0.004634648561477661, 0.29578524827957153]\n",
      "Step 1416, loss = [0.30262279510498047, 0.002663734368979931, 0.30235642194747925]\n",
      "Step 1417, loss = [0.3040933609008789, 0.0027125170454382896, 0.3038221001625061]\n",
      "Step 1418, loss = [0.2919941246509552, 0.0036380263045430183, 0.2916303277015686]\n",
      "Step 1419, loss = [0.3119659721851349, 0.006127180997282267, 0.31135326623916626]\n",
      "Step 1420, loss = [0.30518749356269836, 0.003159161889925599, 0.3048715889453888]\n",
      "Step 1421, loss = [0.28706255555152893, 0.006200707983225584, 0.28644248843193054]\n",
      "Step 1422, loss = [0.2902075946331024, 0.0028068553656339645, 0.2899269163608551]\n",
      "Step 1423, loss = [0.3050418496131897, 0.0038153536152094603, 0.3046603202819824]\n",
      "Step 1424, loss = [0.30603352189064026, 0.0028640006203204393, 0.3057471215724945]\n",
      "Step 1425, loss = [0.30597594380378723, 0.003930605016648769, 0.3055828809738159]\n",
      "Step 1426, loss = [0.3023938238620758, 0.003262806683778763, 0.3020675480365753]\n",
      "Step 1427, loss = [0.28646865487098694, 0.005003718659281731, 0.28596827387809753]\n",
      "Step 1428, loss = [0.30454835295677185, 0.010396486148238182, 0.3035086989402771]\n",
      "Step 1429, loss = [0.30169960856437683, 0.0021484019234776497, 0.30148476362228394]\n",
      "Step 1430, loss = [0.3044171929359436, 0.0025310295168310404, 0.3041640818119049]\n",
      "Step 1431, loss = [0.2953382432460785, 0.002780282637104392, 0.2950602173805237]\n",
      "Step 1432, loss = [0.2998792231082916, 0.003230650909245014, 0.299556165933609]\n",
      "Step 1433, loss = [0.30730709433555603, 0.0025101692881435156, 0.3070560693740845]\n",
      "Step 1434, loss = [0.29761645197868347, 0.008804642595350742, 0.2967360019683838]\n",
      "Step 1435, loss = [0.290500670671463, 0.005464525427669287, 0.28995421528816223]\n",
      "Step 1436, loss = [0.30556970834732056, 0.004161878488957882, 0.3051535189151764]\n",
      "Step 1437, loss = [0.30069831013679504, 0.005470368079841137, 0.3001512587070465]\n",
      "Step 1438, loss = [0.30189165472984314, 0.004017443396151066, 0.301489919424057]\n",
      "Step 1439, loss = [0.2993866205215454, 0.0038246356416493654, 0.2990041673183441]\n",
      "Step 1440, loss = [0.2915330231189728, 0.006393728777766228, 0.29089364409446716]\n",
      "Step 1441, loss = [0.31234613060951233, 0.0023744632489979267, 0.31210869550704956]\n",
      "Step 1442, loss = [0.3063772916793823, 0.005561095662415028, 0.30582118034362793]\n",
      "Step 1443, loss = [0.30565208196640015, 0.003981380723416805, 0.3052539527416229]\n",
      "Step 1444, loss = [0.2979685068130493, 0.00589135242626071, 0.29737937450408936]\n",
      "Step 1445, loss = [0.3025575876235962, 0.004232338164001703, 0.30213436484336853]\n",
      "Step 1446, loss = [0.29070496559143066, 0.006093762814998627, 0.29009559750556946]\n",
      "Step 1447, loss = [0.30449169874191284, 0.00556397857144475, 0.30393528938293457]\n",
      "Step 1448, loss = [0.3089517056941986, 0.0036152340471744537, 0.3085901737213135]\n",
      "Step 1449, loss = [0.29628124833106995, 0.004469291307032108, 0.29583433270454407]\n",
      "Step 1450, loss = [0.29817870259284973, 0.005492838099598885, 0.2976294159889221]\n",
      "Step 1451, loss = [0.31170037388801575, 0.0027183699421584606, 0.3114285469055176]\n",
      "Step 1452, loss = [0.30503979325294495, 0.0030587301589548588, 0.30473393201828003]\n",
      "Step 1453, loss = [0.3081350028514862, 0.00678657041862607, 0.3074563443660736]\n",
      "Step 1454, loss = [0.28105124831199646, 0.002336586359888315, 0.28081759810447693]\n",
      "Step 1455, loss = [0.2939453125, 0.003508981317281723, 0.2935944199562073]\n",
      "Step 1456, loss = [0.3011023700237274, 0.003848655615001917, 0.3007175028324127]\n",
      "Step 1457, loss = [0.30126065015792847, 0.002806231379508972, 0.3009800314903259]\n",
      "Step 1458, loss = [0.3096742331981659, 0.0037545240484178066, 0.3092987835407257]\n",
      "Step 1459, loss = [0.2898816168308258, 0.007448918651789427, 0.28913673758506775]\n",
      "Step 1460, loss = [0.2922908663749695, 0.00209356052801013, 0.2920815050601959]\n",
      "Step 1461, loss = [0.3054380714893341, 0.007517989259213209, 0.3046862781047821]\n",
      "Step 1462, loss = [0.28769463300704956, 0.004689981695264578, 0.2872256338596344]\n",
      "Step 1463, loss = [0.29032841324806213, 0.006220914423465729, 0.2897063195705414]\n",
      "Step 1464, loss = [0.2967727482318878, 0.0023559783585369587, 0.2965371608734131]\n",
      "Step 1465, loss = [0.3062530755996704, 0.006240289658308029, 0.30562904477119446]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11754 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1466, loss = [0.3123961389064789, 0.004116897936910391, 0.31198444962501526]\n",
      "Update target distribution epoch 0 step 1467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11754/11754 [1:50:41<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1467, loss = [0.30373647809028625, 0.00410443264991045, 0.3033260405063629]\n",
      "Step 1468, loss = [0.30134350061416626, 0.0039012697525322437, 0.30095338821411133]\n",
      "Step 1469, loss = [0.31440141797065735, 0.003148617222905159, 0.31408655643463135]\n",
      "Epoch 0, loss = [0.29978147 0.00428172 0.2993533 ]\n",
      "\n",
      "Start of epoch 1\n",
      "Step 0, loss = [0.30372411012649536, 0.004205648321658373, 0.3033035397529602]\n",
      "Step 1, loss = [0.29304128885269165, 0.007213727571070194, 0.2923199236392975]\n",
      "Step 2, loss = [0.29901009798049927, 0.0015503860777243972, 0.2988550662994385]\n",
      "Step 3, loss = [0.29494404792785645, 0.0030487559270113707, 0.2946391701698303]\n",
      "Step 4, loss = [0.3084055483341217, 0.000959952303674072, 0.30830955505371094]\n",
      "Step 5, loss = [0.29407981038093567, 0.004647247493267059, 0.29361507296562195]\n",
      "Step 6, loss = [0.2966693043708801, 0.002481105737388134, 0.29642120003700256]\n",
      "Step 7, loss = [0.30762141942977905, 0.0023216153495013714, 0.3073892593383789]\n",
      "Step 8, loss = [0.30898159742355347, 0.002732159337028861, 0.3087083697319031]\n",
      "Step 9, loss = [0.30095618963241577, 0.0035342653281986713, 0.3006027638912201]\n",
      "Step 10, loss = [0.29314902424812317, 0.0027930575888603926, 0.2928697168827057]\n",
      "Step 11, loss = [0.29417452216148376, 0.0038788761012256145, 0.2937866449356079]\n",
      "Step 12, loss = [0.3096778392791748, 0.002490636892616749, 0.30942878127098083]\n",
      "Step 13, loss = [0.3007514774799347, 0.003513186238706112, 0.30040016770362854]\n",
      "Step 14, loss = [0.30797481536865234, 0.0033159046433866024, 0.30764323472976685]\n",
      "Step 15, loss = [0.2933204472064972, 0.0053673419170081615, 0.2927837073802948]\n",
      "Step 16, loss = [0.31138351559638977, 0.005839294753968716, 0.31079959869384766]\n",
      "Step 17, loss = [0.3035575747489929, 0.004070233553647995, 0.30315056443214417]\n",
      "Step 18, loss = [0.2886679768562317, 0.004177804104983807, 0.288250207901001]\n",
      "Step 19, loss = [0.29515719413757324, 0.0035703612957149744, 0.29480016231536865]\n",
      "Step 20, loss = [0.28508877754211426, 0.00422838656231761, 0.28466594219207764]\n",
      "Step 21, loss = [0.2942284047603607, 0.003910386934876442, 0.29383736848831177]\n",
      "Step 22, loss = [0.287168025970459, 0.005435660015791655, 0.2866244614124298]\n",
      "Step 23, loss = [0.29738953709602356, 0.006711744237691164, 0.2967183589935303]\n",
      "Step 24, loss = [0.2953566312789917, 0.004987156018614769, 0.294857919216156]\n",
      "Step 25, loss = [0.2890692949295044, 0.005856402218341827, 0.2884836494922638]\n",
      "Step 26, loss = [0.30164748430252075, 0.005938168615102768, 0.3010536730289459]\n",
      "Step 27, loss = [0.2885999381542206, 0.0050225392915308475, 0.28809767961502075]\n",
      "Step 28, loss = [0.3037370443344116, 0.0034914121497422457, 0.30338791012763977]\n",
      "Step 29, loss = [0.298178493976593, 0.005837286822497845, 0.29759475588798523]\n",
      "Step 30, loss = [0.30003821849823, 0.0065388442017138, 0.29938432574272156]\n",
      "Step 31, loss = [0.3048987090587616, 0.0024647824466228485, 0.30465224385261536]\n",
      "Step 32, loss = [0.3096526861190796, 0.0029677245765924454, 0.3093559145927429]\n",
      "Step 33, loss = [0.2995414733886719, 0.0033349208533763885, 0.29920798540115356]\n",
      "Step 34, loss = [0.30419644713401794, 0.0039915102533996105, 0.30379730463027954]\n",
      "Step 35, loss = [0.31346556544303894, 0.006318907719105482, 0.31283366680145264]\n",
      "Step 36, loss = [0.30377307534217834, 0.003410779871046543, 0.30343198776245117]\n",
      "Step 37, loss = [0.30475884675979614, 0.002055198885500431, 0.3045533299446106]\n",
      "Step 38, loss = [0.29628244042396545, 0.0018598658498376608, 0.29609644412994385]\n",
      "Step 39, loss = [0.3029245436191559, 0.006520711351186037, 0.3022724688053131]\n",
      "Step 40, loss = [0.30740949511528015, 0.00436005461961031, 0.30697348713874817]\n",
      "Step 41, loss = [0.2969904839992523, 0.0018757828511297703, 0.29680290818214417]\n",
      "Step 42, loss = [0.2828441858291626, 0.006355315446853638, 0.282208651304245]\n",
      "Step 43, loss = [0.30712684988975525, 0.004635362885892391, 0.30666330456733704]\n",
      "Step 44, loss = [0.2979850471019745, 0.004817206412553787, 0.2975033223628998]\n",
      "Step 45, loss = [0.3054538071155548, 0.004785980097949505, 0.3049752116203308]\n",
      "Step 46, loss = [0.30437999963760376, 0.003624198492616415, 0.304017573595047]\n",
      "Step 47, loss = [0.30800968408584595, 0.005427082534879446, 0.307466983795166]\n",
      "Step 48, loss = [0.30212339758872986, 0.004441094119101763, 0.3016792833805084]\n",
      "Step 49, loss = [0.3001359701156616, 0.004649092443287373, 0.2996710538864136]\n",
      "Step 50, loss = [0.302510529756546, 0.009028475731611252, 0.3016076683998108]\n",
      "Step 51, loss = [0.2904566526412964, 0.0039986502379179, 0.2900567948818207]\n",
      "Step 52, loss = [0.29270514845848083, 0.002962166676297784, 0.29240894317626953]\n",
      "Step 53, loss = [0.30828413367271423, 0.0017468864098191261, 0.30810943245887756]\n",
      "Step 54, loss = [0.29093077778816223, 0.003261905163526535, 0.2906045913696289]\n",
      "Step 55, loss = [0.2996521294116974, 0.003063833573833108, 0.2993457317352295]\n",
      "Step 56, loss = [0.30823713541030884, 0.007738275919109583, 0.30746331810951233]\n",
      "Step 57, loss = [0.2936953604221344, 0.0035519988741725683, 0.29334014654159546]\n",
      "Step 58, loss = [0.2934654653072357, 0.007632746361196041, 0.29270219802856445]\n",
      "Step 59, loss = [0.29866844415664673, 0.002307567745447159, 0.2984376847743988]\n",
      "Step 60, loss = [0.3024766445159912, 0.0025562874507158995, 0.30222102999687195]\n",
      "Step 61, loss = [0.3102636933326721, 0.0027524158358573914, 0.30998843908309937]\n",
      "Step 62, loss = [0.29682818055152893, 0.0035745962522923946, 0.2964707314968109]\n",
      "Step 63, loss = [0.30332985520362854, 0.003932536579668522, 0.3029366135597229]\n",
      "Step 64, loss = [0.30827587842941284, 0.007054569199681282, 0.3075704276561737]\n",
      "Step 65, loss = [0.30525514483451843, 0.0031670010648667812, 0.3049384355545044]\n",
      "Step 66, loss = [0.3017781972885132, 0.0057739149779081345, 0.301200807094574]\n",
      "Step 67, loss = [0.30427753925323486, 0.005099118687212467, 0.3037676215171814]\n",
      "Step 68, loss = [0.29488417506217957, 0.0032284834887832403, 0.29456132650375366]\n",
      "Step 69, loss = [0.28861045837402344, 0.005532023496925831, 0.28805726766586304]\n",
      "Step 70, loss = [0.3031274974346161, 0.0038835499435663223, 0.30273914337158203]\n",
      "Step 71, loss = [0.30225783586502075, 0.00315484800375998, 0.3019423484802246]\n",
      "Step 72, loss = [0.3047809898853302, 0.006417778320610523, 0.30413922667503357]\n",
      "Step 73, loss = [0.30176982283592224, 0.004291248507797718, 0.3013406991958618]\n",
      "Step 74, loss = [0.2861684560775757, 0.0057599847204983234, 0.2855924665927887]\n",
      "Step 75, loss = [0.29802513122558594, 0.0036897582467645407, 0.2976561486721039]\n",
      "Step 76, loss = [0.2956334352493286, 0.006922530941665173, 0.2949411869049072]\n",
      "Step 77, loss = [0.3060915768146515, 0.003975451923906803, 0.305694043636322]\n",
      "Step 78, loss = [0.30186331272125244, 0.0034020054154098034, 0.3015231192111969]\n",
      "Step 79, loss = [0.30279478430747986, 0.00383859034627676, 0.30241093039512634]\n",
      "Step 80, loss = [0.298384428024292, 0.003003150224685669, 0.2980841100215912]\n",
      "Step 81, loss = [0.3018690049648285, 0.0021576392464339733, 0.3016532361507416]\n",
      "Step 82, loss = [0.30079734325408936, 0.004009774420410395, 0.3003963530063629]\n",
      "Step 83, loss = [0.30670052766799927, 0.004166979342699051, 0.3062838315963745]\n",
      "Step 84, loss = [0.3019833564758301, 0.006683296989649534, 0.301315039396286]\n",
      "Step 85, loss = [0.30365532636642456, 0.0038558291271328926, 0.30326974391937256]\n",
      "Step 86, loss = [0.3098553717136383, 0.004736798815429211, 0.3093816936016083]\n",
      "Step 87, loss = [0.28392288088798523, 0.002170028630644083, 0.2837058901786804]\n",
      "Step 88, loss = [0.30650150775909424, 0.0019111472647637129, 0.30631038546562195]\n",
      "Step 89, loss = [0.3016640841960907, 0.0023510954342782497, 0.30142897367477417]\n",
      "Step 90, loss = [0.3052922189235687, 0.004699344746768475, 0.30482229590415955]\n",
      "Step 91, loss = [0.28853535652160645, 0.001603629905730486, 0.28837499022483826]\n",
      "Step 92, loss = [0.2893267571926117, 0.0040297494269907475, 0.2889237701892853]\n",
      "Step 93, loss = [0.30278104543685913, 0.0013576715718954802, 0.3026452660560608]\n",
      "Step 94, loss = [0.3096490800380707, 0.004607453942298889, 0.3091883361339569]\n",
      "Step 95, loss = [0.3098900616168976, 0.0039868042804300785, 0.3094913959503174]\n",
      "Step 96, loss = [0.3019542098045349, 0.004694395698606968, 0.30148476362228394]\n",
      "Step 97, loss = [0.3035447299480438, 0.007012884598225355, 0.30284345149993896]\n",
      "Step 98, loss = [0.30991724133491516, 0.0024196140002459288, 0.30967527627944946]\n",
      "Step 99, loss = [0.3057869076728821, 0.0046213469468057156, 0.3053247630596161]\n",
      "Step 100, loss = [0.2959795594215393, 0.004114766605198383, 0.2955680787563324]\n",
      "Step 101, loss = [0.29154279828071594, 0.008673718199133873, 0.29067543148994446]\n",
      "Step 102, loss = [0.2965559959411621, 0.004074650816619396, 0.29614853858947754]\n",
      "Step 103, loss = [0.3174300789833069, 0.004690860863775015, 0.31696099042892456]\n",
      "Step 104, loss = [0.2739975154399872, 0.005446398630738258, 0.27345287799835205]\n",
      "Step 105, loss = [0.2978267967700958, 0.0019336853874847293, 0.29763343930244446]\n",
      "Step 106, loss = [0.30748656392097473, 0.007115909829735756, 0.30677497386932373]\n",
      "Step 107, loss = [0.3029043972492218, 0.004329827148467302, 0.30247142910957336]\n",
      "Step 108, loss = [0.30496636033058167, 0.004137988202273846, 0.3045525550842285]\n",
      "Step 109, loss = [0.30018818378448486, 0.005883718840777874, 0.299599826335907]\n",
      "Step 110, loss = [0.3091316521167755, 0.002869677497074008, 0.3088446855545044]\n",
      "Step 111, loss = [0.2992403209209442, 0.007318338844925165, 0.29850849509239197]\n",
      "Step 112, loss = [0.30417147278785706, 0.0029547023586928844, 0.30387601256370544]\n",
      "Step 113, loss = [0.29464176297187805, 0.0024188586976379156, 0.2943998873233795]\n",
      "Step 114, loss = [0.2822887599468231, 0.0048921722918748856, 0.2817995548248291]\n",
      "Step 115, loss = [0.294452965259552, 0.005052721127867699, 0.293947696685791]\n",
      "Step 116, loss = [0.29610347747802734, 0.002330085262656212, 0.29587048292160034]\n",
      "Step 117, loss = [0.3069327175617218, 0.006637223064899445, 0.30626899003982544]\n",
      "Step 118, loss = [0.2814018130302429, 0.007183614186942577, 0.2806834578514099]\n",
      "Step 119, loss = [0.28964757919311523, 0.0020517874509096146, 0.28944239020347595]\n",
      "Step 120, loss = [0.28437864780426025, 0.003127374919131398, 0.2840659022331238]\n",
      "Step 121, loss = [0.30352357029914856, 0.004219189286231995, 0.30310165882110596]\n",
      "Step 122, loss = [0.30244529247283936, 0.006680639926344156, 0.3017772138118744]\n",
      "Step 123, loss = [0.30565235018730164, 0.0033209682442247868, 0.30532026290893555]\n",
      "Step 124, loss = [0.2877037525177002, 0.0019429964013397694, 0.2875094413757324]\n",
      "Step 125, loss = [0.3011718690395355, 0.005066797137260437, 0.3006651997566223]\n",
      "Step 126, loss = [0.30168071389198303, 0.00292977481149137, 0.3013877272605896]\n",
      "Step 127, loss = [0.31150421500205994, 0.0036282027140259743, 0.31114140152931213]\n",
      "Step 128, loss = [0.2996651828289032, 0.0012189189437776804, 0.2995432913303375]\n",
      "Step 129, loss = [0.29802796244621277, 0.0035578319802880287, 0.29767218232154846]\n",
      "Step 130, loss = [0.30721989274024963, 0.003827696666121483, 0.3068371117115021]\n",
      "Step 131, loss = [0.311297744512558, 0.0035191348288208246, 0.3109458386898041]\n",
      "Step 132, loss = [0.29761892557144165, 0.002034407574683428, 0.29741549491882324]\n",
      "Step 133, loss = [0.30046892166137695, 0.0038211499340832233, 0.3000867962837219]\n",
      "Step 134, loss = [0.28708699345588684, 0.004475616849958897, 0.28663942217826843]\n",
      "Step 135, loss = [0.29399970173835754, 0.0024449899792671204, 0.2937552034854889]\n",
      "Step 136, loss = [0.304266095161438, 0.005308855790644884, 0.3037351965904236]\n",
      "Step 137, loss = [0.3122415244579315, 0.0036046705208718777, 0.31188106536865234]\n",
      "Step 138, loss = [0.28761711716651917, 0.004169616848230362, 0.2872001528739929]\n",
      "Step 139, loss = [0.29077115654945374, 0.006901400163769722, 0.2900810241699219]\n",
      "Step 140, loss = [0.28584024310112, 0.004185691475868225, 0.2854216694831848]\n",
      "Step 141, loss = [0.2994488477706909, 0.004408721346408129, 0.29900798201560974]\n",
      "Step 142, loss = [0.3106721043586731, 0.0044822185300290585, 0.31022387742996216]\n",
      "Step 143, loss = [0.29606109857559204, 0.004102697130292654, 0.295650839805603]\n",
      "Step 144, loss = [0.3042837381362915, 0.005536211188882589, 0.3037301301956177]\n",
      "Step 145, loss = [0.3065146803855896, 0.005163759458810091, 0.305998295545578]\n",
      "Step 146, loss = [0.2898557484149933, 0.004493895918130875, 0.28940635919570923]\n",
      "Step 147, loss = [0.309916228055954, 0.005171114578843117, 0.3093991279602051]\n",
      "Step 148, loss = [0.30843275785446167, 0.0021426030434668064, 0.30821850895881653]\n",
      "Step 149, loss = [0.3063473701477051, 0.0016963259549811482, 0.3061777353286743]\n",
      "Step 150, loss = [0.3080920875072479, 0.00209514400921762, 0.3078825771808624]\n",
      "Step 151, loss = [0.2980044186115265, 0.0038763773627579212, 0.29761677980422974]\n",
      "Step 152, loss = [0.2927383482456207, 0.006627826020121574, 0.29207557439804077]\n",
      "Step 153, loss = [0.2935274839401245, 0.00598643533885479, 0.2929288446903229]\n",
      "Step 154, loss = [0.2914488613605499, 0.0040097786113619804, 0.2910478711128235]\n",
      "Step 155, loss = [0.31010398268699646, 0.005793042480945587, 0.30952468514442444]\n",
      "Step 156, loss = [0.3134976923465729, 0.0025447390507906675, 0.31324321031570435]\n",
      "Step 157, loss = [0.3035382330417633, 0.0020733606070280075, 0.3033308982849121]\n",
      "Step 158, loss = [0.2806379497051239, 0.005087009631097317, 0.28012925386428833]\n",
      "Step 159, loss = [0.30685245990753174, 0.005465018097311258, 0.3063059449195862]\n",
      "Step 160, loss = [0.28970953822135925, 0.003844956401735544, 0.2893250286579132]\n",
      "Step 161, loss = [0.302348256111145, 0.0034628526773303747, 0.3020019829273224]\n",
      "Step 162, loss = [0.3080037236213684, 0.004840217065066099, 0.30751970410346985]\n",
      "Step 163, loss = [0.2919480800628662, 0.00319215701892972, 0.2916288673877716]\n",
      "Step 164, loss = [0.29471448063850403, 0.004936802200973034, 0.29422080516815186]\n",
      "Step 165, loss = [0.2972624897956848, 0.005420194938778877, 0.2967204749584198]\n",
      "Step 166, loss = [0.3057310879230499, 0.0017948850290849805, 0.30555158853530884]\n",
      "Step 167, loss = [0.3021697402000427, 0.004525480326265097, 0.30171719193458557]\n",
      "Step 168, loss = [0.2864416241645813, 0.0015797482337802649, 0.2862836420536041]\n",
      "Step 169, loss = [0.31273213028907776, 0.0048800278455019, 0.31224411725997925]\n",
      "Step 170, loss = [0.30487290024757385, 0.004593697376549244, 0.3044135272502899]\n",
      "Step 171, loss = [0.2926456332206726, 0.0018752999603748322, 0.29245811700820923]\n",
      "Step 172, loss = [0.2970326840877533, 0.005322863347828388, 0.29650038480758667]\n",
      "Step 173, loss = [0.2826528549194336, 0.00329039990901947, 0.28232380747795105]\n",
      "Step 174, loss = [0.3122223913669586, 0.002530815312638879, 0.3119693100452423]\n",
      "Step 175, loss = [0.2950367033481598, 0.005397914908826351, 0.29449692368507385]\n",
      "Step 176, loss = [0.30544599890708923, 0.003771189833059907, 0.30506888031959534]\n",
      "Step 177, loss = [0.3016030192375183, 0.006725712213665247, 0.3009304404258728]\n",
      "Step 178, loss = [0.3027125597000122, 0.005107628181576729, 0.3022018074989319]\n",
      "Step 179, loss = [0.3111492097377777, 0.0021306490525603294, 0.3109361529350281]\n",
      "Step 180, loss = [0.29726362228393555, 0.0032148906029760838, 0.2969421446323395]\n",
      "Step 181, loss = [0.29413264989852905, 0.004075851757079363, 0.29372507333755493]\n",
      "Step 182, loss = [0.29432475566864014, 0.003752104938030243, 0.29394954442977905]\n",
      "Step 183, loss = [0.2956920564174652, 0.0030000563710927963, 0.2953920364379883]\n",
      "Step 184, loss = [0.29483431577682495, 0.002959325211122632, 0.29453837871551514]\n",
      "Step 185, loss = [0.29768261313438416, 0.004540282301604748, 0.2972285747528076]\n",
      "Step 186, loss = [0.3037823736667633, 0.0025828913785517216, 0.30352407693862915]\n",
      "Step 187, loss = [0.3096102178096771, 0.004810435697436333, 0.30912917852401733]\n",
      "Step 188, loss = [0.297698050737381, 0.0030213019344955683, 0.2973959147930145]\n",
      "Step 189, loss = [0.29847365617752075, 0.003765585832297802, 0.2980971038341522]\n",
      "Step 190, loss = [0.2982650101184845, 0.0032753944396972656, 0.2979374825954437]\n",
      "Step 191, loss = [0.3025070130825043, 0.00451920460909605, 0.30205509066581726]\n",
      "Step 192, loss = [0.3092059791088104, 0.005037181079387665, 0.3087022602558136]\n",
      "Step 193, loss = [0.2965947687625885, 0.0056342026218771935, 0.2960313558578491]\n",
      "Step 194, loss = [0.30345842242240906, 0.003206927329301834, 0.30313771963119507]\n",
      "Step 195, loss = [0.3055044710636139, 0.004317514598369598, 0.30507272481918335]\n",
      "Step 196, loss = [0.29175660014152527, 0.004033773206174374, 0.2913532257080078]\n",
      "Step 197, loss = [0.29269614815711975, 0.0025061608757823706, 0.2924455404281616]\n",
      "Step 198, loss = [0.30331337451934814, 0.005942886229604483, 0.3027190864086151]\n",
      "Step 199, loss = [0.31108468770980835, 0.0037711835466325283, 0.31070756912231445]\n",
      "Step 200, loss = [0.2992706894874573, 0.0051374612376093864, 0.2987569570541382]\n",
      "Step 201, loss = [0.3049759566783905, 0.002600672421976924, 0.3047159016132355]\n",
      "Step 202, loss = [0.2966853380203247, 0.003815686795860529, 0.29630377888679504]\n",
      "Step 203, loss = [0.30850327014923096, 0.004103526007384062, 0.3080929219722748]\n",
      "Step 204, loss = [0.2798958718776703, 0.002008776878938079, 0.2796950042247772]\n",
      "Step 205, loss = [0.27549684047698975, 0.006192993372678757, 0.27487754821777344]\n",
      "Step 206, loss = [0.2966567277908325, 0.004578130319714546, 0.29619890451431274]\n",
      "Step 207, loss = [0.30012667179107666, 0.002719317562878132, 0.29985472559928894]\n",
      "Step 208, loss = [0.30956968665122986, 0.006428086198866367, 0.30892688035964966]\n",
      "Step 209, loss = [0.3003799319267273, 0.004299166612327099, 0.2999500036239624]\n",
      "Step 210, loss = [0.29830682277679443, 0.0024388516321778297, 0.2980629503726959]\n",
      "Step 211, loss = [0.295419305562973, 0.006169809028506279, 0.29480233788490295]\n",
      "Step 212, loss = [0.3118108808994293, 0.004183105193078518, 0.31139257550239563]\n",
      "Step 213, loss = [0.29392075538635254, 0.003598840907216072, 0.29356086254119873]\n",
      "Step 214, loss = [0.297529935836792, 0.0031438139267265797, 0.2972155511379242]\n",
      "Step 215, loss = [0.3050173223018646, 0.003215639153495431, 0.3046957552433014]\n",
      "Step 216, loss = [0.2954903841018677, 0.005172389093786478, 0.29497313499450684]\n",
      "Step 217, loss = [0.31471318006515503, 0.006084225606173277, 0.31410476565361023]\n",
      "Step 218, loss = [0.29609981179237366, 0.0023431675508618355, 0.2958655059337616]\n",
      "Step 219, loss = [0.3018210530281067, 0.004212645813822746, 0.3013997972011566]\n",
      "Step 220, loss = [0.2928391098976135, 0.004329999443143606, 0.2924061119556427]\n",
      "Step 221, loss = [0.29047858715057373, 0.0038497927598655224, 0.2900936007499695]\n",
      "Step 222, loss = [0.30389320850372314, 0.003756826277822256, 0.30351752042770386]\n",
      "Step 223, loss = [0.30051517486572266, 0.003491651965305209, 0.3001660108566284]\n",
      "Step 224, loss = [0.3063230514526367, 0.007914555259048939, 0.3055315911769867]\n",
      "Step 225, loss = [0.3027283549308777, 0.004675399512052536, 0.3022608160972595]\n",
      "Step 226, loss = [0.2891031503677368, 0.005030608735978603, 0.2886000871658325]\n",
      "Step 227, loss = [0.2977341115474701, 0.004211029969155788, 0.29731300473213196]\n",
      "Step 228, loss = [0.2971039414405823, 0.0022362410090863705, 0.296880304813385]\n",
      "Step 229, loss = [0.29784396290779114, 0.0053351400420069695, 0.2973104417324066]\n",
      "Step 230, loss = [0.3071088492870331, 0.005301803350448608, 0.306578665971756]\n",
      "Step 231, loss = [0.2940714657306671, 0.005314341746270657, 0.29354003071784973]\n",
      "Step 232, loss = [0.3024676740169525, 0.001070910133421421, 0.3023605942726135]\n",
      "Step 233, loss = [0.2911793887615204, 0.001968028489500284, 0.29098257422447205]\n",
      "Step 234, loss = [0.2932380437850952, 0.003537984099239111, 0.2928842306137085]\n",
      "Step 235, loss = [0.3021255433559418, 0.003062276868149638, 0.3018193244934082]\n",
      "Step 236, loss = [0.3015361726284027, 0.002685819286853075, 0.3012675940990448]\n",
      "Step 237, loss = [0.29820728302001953, 0.002564468653872609, 0.2979508340358734]\n",
      "Step 238, loss = [0.30296483635902405, 0.0028980663046240807, 0.3026750385761261]\n",
      "Step 239, loss = [0.299591600894928, 0.004401293583214283, 0.2991514801979065]\n",
      "Step 240, loss = [0.30698883533477783, 0.0037140939384698868, 0.3066174387931824]\n",
      "Step 241, loss = [0.2778494954109192, 0.00463348301127553, 0.2773861587047577]\n",
      "Step 242, loss = [0.3020436763763428, 0.0037797994446009398, 0.30166569352149963]\n",
      "Step 243, loss = [0.29814401268959045, 0.0037038696464151144, 0.2977736294269562]\n",
      "Step 244, loss = [0.30113571882247925, 0.004646659828722477, 0.3006710410118103]\n",
      "Step 245, loss = [0.2891637980937958, 0.003977488726377487, 0.2887660562992096]\n",
      "Step 246, loss = [0.30076128244400024, 0.0039850627072155476, 0.300362765789032]\n",
      "Step 247, loss = [0.3004889488220215, 0.0015681309159845114, 0.30033212900161743]\n",
      "Step 248, loss = [0.2992604672908783, 0.005555350333452225, 0.29870492219924927]\n",
      "Step 249, loss = [0.2954283654689789, 0.0029007685370743275, 0.29513829946517944]\n",
      "Step 250, loss = [0.30524981021881104, 0.004873873665928841, 0.30476242303848267]\n",
      "Step 251, loss = [0.2910306453704834, 0.004572541918605566, 0.290573388338089]\n",
      "Step 252, loss = [0.3060535490512848, 0.0027383272536098957, 0.30577972531318665]\n",
      "Step 253, loss = [0.30554017424583435, 0.0037182851228863, 0.30516836047172546]\n",
      "Step 254, loss = [0.2902551591396332, 0.004996964707970619, 0.2897554636001587]\n",
      "Step 255, loss = [0.3115033805370331, 0.002200492424890399, 0.31128332018852234]\n",
      "Step 256, loss = [0.30103564262390137, 0.004085320979356766, 0.30062711238861084]\n",
      "Step 257, loss = [0.2981666326522827, 0.004610997159034014, 0.2977055311203003]\n",
      "Step 258, loss = [0.3039599359035492, 0.004118320532143116, 0.30354809761047363]\n",
      "Step 259, loss = [0.29499539732933044, 0.0030877171084284782, 0.29468661546707153]\n",
      "Step 260, loss = [0.3040429949760437, 0.0030669996049255133, 0.30373629927635193]\n",
      "Step 261, loss = [0.30479028820991516, 0.0020718511659651995, 0.3045831024646759]\n",
      "Step 262, loss = [0.3107468783855438, 0.004237146582454443, 0.31032314896583557]\n",
      "Step 263, loss = [0.31127068400382996, 0.003337870817631483, 0.31093689799308777]\n",
      "Step 264, loss = [0.28691723942756653, 0.0022209114395081997, 0.2866951525211334]\n",
      "Step 265, loss = [0.307062566280365, 0.005702988710254431, 0.30649226903915405]\n",
      "Step 266, loss = [0.3018213212490082, 0.0032882196828722954, 0.30149251222610474]\n",
      "Step 267, loss = [0.2974066436290741, 0.002465002704411745, 0.29716014862060547]\n",
      "Step 268, loss = [0.30516862869262695, 0.008936303667724133, 0.3042750060558319]\n",
      "Step 269, loss = [0.28493720293045044, 0.007177160121500492, 0.2842194736003876]\n",
      "Step 270, loss = [0.30902135372161865, 0.005303584039211273, 0.3084909915924072]\n",
      "Step 271, loss = [0.2928254306316376, 0.0012453589588403702, 0.2927008867263794]\n",
      "Step 272, loss = [0.2922115623950958, 0.002422234509140253, 0.29196932911872864]\n",
      "Step 273, loss = [0.2990932762622833, 0.004666414111852646, 0.2986266314983368]\n",
      "Step 274, loss = [0.306563138961792, 0.007029505912214518, 0.3058601915836334]\n",
      "Step 275, loss = [0.2839619219303131, 0.004886814393103123, 0.28347325325012207]\n",
      "Step 276, loss = [0.304479718208313, 0.004407346248626709, 0.30403897166252136]\n",
      "Step 277, loss = [0.29958364367485046, 0.005708681419491768, 0.29901278018951416]\n",
      "Step 278, loss = [0.30900877714157104, 0.003521168138831854, 0.3086566627025604]\n",
      "Step 279, loss = [0.3097016513347626, 0.004634464625269175, 0.3092381954193115]\n",
      "Step 280, loss = [0.2964809834957123, 0.005384494550526142, 0.2959425449371338]\n",
      "Step 281, loss = [0.2804720401763916, 0.0037279592361301184, 0.2800992429256439]\n",
      "Step 282, loss = [0.2933471202850342, 0.0028560860082507133, 0.2930615246295929]\n",
      "Step 283, loss = [0.3112376034259796, 0.005627954378724098, 0.3106748163700104]\n",
      "Step 284, loss = [0.3044965863227844, 0.003604381112381816, 0.30413615703582764]\n",
      "Step 285, loss = [0.2866761386394501, 0.003972250036895275, 0.2862789034843445]\n",
      "Step 286, loss = [0.28989994525909424, 0.00649581104516983, 0.28925037384033203]\n",
      "Step 287, loss = [0.2936294376850128, 0.004579796455800533, 0.2931714653968811]\n",
      "Step 288, loss = [0.28618526458740234, 0.003156947670504451, 0.2858695685863495]\n",
      "Step 289, loss = [0.3077658414840698, 0.004769868683069944, 0.30728885531425476]\n",
      "Step 290, loss = [0.30809107422828674, 0.002344744047150016, 0.30785658955574036]\n",
      "Step 291, loss = [0.2952377200126648, 0.004866168834269047, 0.2947511076927185]\n",
      "Step 292, loss = [0.29564133286476135, 0.003713545622304082, 0.2952699661254883]\n",
      "Step 293, loss = [0.29667791724205017, 0.0036664381623268127, 0.29631125926971436]\n",
      "Step 294, loss = [0.2997010350227356, 0.003712364938110113, 0.2993297874927521]\n",
      "Step 295, loss = [0.31216320395469666, 0.006005001720041037, 0.311562716960907]\n",
      "Step 296, loss = [0.2939507067203522, 0.004277476575225592, 0.2935229539871216]\n",
      "Step 297, loss = [0.29752117395401, 0.003308144398033619, 0.2971903681755066]\n",
      "Step 298, loss = [0.2884622812271118, 0.0056817433796823025, 0.2878940999507904]\n",
      "Step 299, loss = [0.31062009930610657, 0.0031108506955206394, 0.3103090226650238]\n",
      "Step 300, loss = [0.3038775324821472, 0.00512812752276659, 0.30336472392082214]\n",
      "Step 301, loss = [0.29873982071876526, 0.0049492791295051575, 0.2982448935508728]\n",
      "Step 302, loss = [0.28873729705810547, 0.0044287629425525665, 0.2882944345474243]\n",
      "Step 303, loss = [0.29245883226394653, 0.004874984733760357, 0.2919713258743286]\n",
      "Step 304, loss = [0.3035728931427002, 0.0034351046197116375, 0.3032293915748596]\n",
      "Step 305, loss = [0.2983170747756958, 0.006823169998824596, 0.2976347506046295]\n",
      "Step 306, loss = [0.3013859689235687, 0.004433522000908852, 0.30094262957572937]\n",
      "Step 307, loss = [0.30554142594337463, 0.003753283992409706, 0.305166095495224]\n",
      "Step 308, loss = [0.30223318934440613, 0.0018211679998785257, 0.3020510673522949]\n",
      "Step 309, loss = [0.2868165671825409, 0.0019959418568760157, 0.2866169810295105]\n",
      "Step 310, loss = [0.3068704903125763, 0.0030996515415608883, 0.3065605163574219]\n",
      "Step 311, loss = [0.2904926538467407, 0.006206991616636515, 0.2898719608783722]\n",
      "Step 312, loss = [0.3081803321838379, 0.005738463718444109, 0.3076064884662628]\n",
      "Step 313, loss = [0.2829436957836151, 0.007070548832416534, 0.28223663568496704]\n",
      "Step 314, loss = [0.2909148335456848, 0.0019218167290091515, 0.29072263836860657]\n",
      "Step 315, loss = [0.30363020300865173, 0.005978638306260109, 0.3030323386192322]\n",
      "Step 316, loss = [0.30866333842277527, 0.004973902832716703, 0.30816593766212463]\n",
      "Step 317, loss = [0.2947240471839905, 0.00500528747215867, 0.29422351717948914]\n",
      "Step 318, loss = [0.29583585262298584, 0.0051115467213094234, 0.2953246831893921]\n",
      "Step 319, loss = [0.31052708625793457, 0.002936827251687646, 0.3102334141731262]\n",
      "Step 320, loss = [0.3042523264884949, 0.003835474606603384, 0.30386877059936523]\n",
      "Step 321, loss = [0.3037070333957672, 0.0049086520448327065, 0.3032161593437195]\n",
      "Step 322, loss = [0.30070504546165466, 0.0036825304850935936, 0.3003367781639099]\n",
      "Step 323, loss = [0.30460307002067566, 0.005355185829102993, 0.30406755208969116]\n",
      "Step 324, loss = [0.28485387563705444, 0.005564126651734114, 0.28429746627807617]\n",
      "Step 325, loss = [0.2902225852012634, 0.00474591413512826, 0.2897479832172394]\n",
      "Step 326, loss = [0.29589420557022095, 0.0057798102498054504, 0.295316219329834]\n",
      "Step 327, loss = [0.29916074872016907, 0.003059009788557887, 0.29885485768318176]\n",
      "Step 328, loss = [0.28930139541625977, 0.0026894770562648773, 0.2890324592590332]\n",
      "Step 329, loss = [0.2958027720451355, 0.003138961736112833, 0.2954888641834259]\n",
      "Step 330, loss = [0.29879552125930786, 0.003968363627791405, 0.2983986735343933]\n",
      "Step 331, loss = [0.2956339716911316, 0.002758617978543043, 0.2953581213951111]\n",
      "Step 332, loss = [0.31259679794311523, 0.005338638089597225, 0.31206294894218445]\n",
      "Step 333, loss = [0.2980175316333771, 0.0019330392824485898, 0.2978242337703705]\n",
      "Step 334, loss = [0.30289170145988464, 0.003995360340923071, 0.3024921715259552]\n",
      "Step 335, loss = [0.29777631163597107, 0.002173017244786024, 0.2975590229034424]\n",
      "Step 336, loss = [0.3064842224121094, 0.002298955572769046, 0.3062543272972107]\n",
      "Step 337, loss = [0.29432740807533264, 0.003521538805216551, 0.29397526383399963]\n",
      "Step 338, loss = [0.30060532689094543, 0.0025537125766277313, 0.3003499507904053]\n",
      "Step 339, loss = [0.2946523427963257, 0.003132420592010021, 0.2943390905857086]\n",
      "Step 340, loss = [0.28851041197776794, 0.005085793323814869, 0.2880018353462219]\n",
      "Step 341, loss = [0.31225040555000305, 0.004752687644213438, 0.3117751479148865]\n",
      "Step 342, loss = [0.29386571049690247, 0.002104001585394144, 0.29365530610084534]\n",
      "Step 343, loss = [0.2823267877101898, 0.001829155022278428, 0.28214386105537415]\n",
      "Step 344, loss = [0.3126722574234009, 0.0041280388832092285, 0.3122594654560089]\n",
      "Step 345, loss = [0.29916661977767944, 0.003370539518073201, 0.2988295555114746]\n",
      "Step 346, loss = [0.3059121370315552, 0.003669215366244316, 0.30554521083831787]\n",
      "Step 347, loss = [0.29738157987594604, 0.0033804054837673903, 0.2970435321331024]\n",
      "Step 348, loss = [0.2898283004760742, 0.0037129251286387444, 0.2894569933414459]\n",
      "Step 349, loss = [0.3001795709133148, 0.003396345768123865, 0.29983994364738464]\n",
      "Step 350, loss = [0.3025053143501282, 0.0057502202689647675, 0.3019302785396576]\n",
      "Step 351, loss = [0.29293379187583923, 0.005144000053405762, 0.2924194037914276]\n",
      "Step 352, loss = [0.3055015802383423, 0.004557667300105095, 0.30504581332206726]\n",
      "Step 353, loss = [0.29905959963798523, 0.00548055674880743, 0.2985115349292755]\n",
      "Step 354, loss = [0.29613274335861206, 0.0038536549545824528, 0.2957473695278168]\n",
      "Step 355, loss = [0.3039708733558655, 0.005020299926400185, 0.30346885323524475]\n",
      "Step 356, loss = [0.30841198563575745, 0.003997126594185829, 0.3080122768878937]\n",
      "Step 357, loss = [0.2874734699726105, 0.004353843629360199, 0.28703808784484863]\n",
      "Step 358, loss = [0.2853958308696747, 0.0035988767631351948, 0.2850359380245209]\n",
      "Step 359, loss = [0.29636019468307495, 0.002905155997723341, 0.2960696816444397]\n",
      "Step 360, loss = [0.3120109438896179, 0.003182697109878063, 0.3116926848888397]\n",
      "Step 361, loss = [0.3112623691558838, 0.003936579450964928, 0.3108687102794647]\n",
      "Step 362, loss = [0.31070077419281006, 0.005404759664088488, 0.3101603090763092]\n",
      "Step 363, loss = [0.303032785654068, 0.003930459730327129, 0.30263975262641907]\n",
      "Step 364, loss = [0.29426246881484985, 0.003494173288345337, 0.2939130365848541]\n",
      "Step 365, loss = [0.2998199760913849, 0.007738125044852495, 0.2990461587905884]\n",
      "Step 366, loss = [0.3005474805831909, 0.00506126182153821, 0.3000413477420807]\n",
      "Step 367, loss = [0.29057252407073975, 0.005947990342974663, 0.2899777293205261]\n",
      "Step 368, loss = [0.3009302318096161, 0.006754525937139988, 0.30025479197502136]\n",
      "Step 369, loss = [0.3015987277030945, 0.0032376577146351337, 0.30127495527267456]\n",
      "Step 370, loss = [0.29185667634010315, 0.0024077226407825947, 0.29161590337753296]\n",
      "Step 371, loss = [0.29115357995033264, 0.008730452507734299, 0.2902805209159851]\n",
      "Step 372, loss = [0.29175856709480286, 0.003511803923174739, 0.29140737652778625]\n",
      "Step 373, loss = [0.29998448491096497, 0.005027398467063904, 0.29948174953460693]\n",
      "Step 374, loss = [0.30282530188560486, 0.003987990319728851, 0.3024265170097351]\n",
      "Step 375, loss = [0.3056284189224243, 0.0021875454112887383, 0.30540966987609863]\n",
      "Step 376, loss = [0.292525976896286, 0.0029510599561035633, 0.29223087430000305]\n",
      "Step 377, loss = [0.29809272289276123, 0.0036422195844352245, 0.2977285087108612]\n",
      "Step 378, loss = [0.30659982562065125, 0.0027004871517419815, 0.30632978677749634]\n",
      "Step 379, loss = [0.31514158844947815, 0.002843699883669615, 0.31485721468925476]\n",
      "Step 380, loss = [0.3115796148777008, 0.004325333051383495, 0.3111470937728882]\n",
      "Step 381, loss = [0.29880157113075256, 0.006217916961759329, 0.2981797754764557]\n",
      "Step 382, loss = [0.3075856864452362, 0.0037245634011924267, 0.3072132170200348]\n",
      "Step 383, loss = [0.30066508054733276, 0.004455759655684233, 0.30021950602531433]\n",
      "Step 384, loss = [0.31673088669776917, 0.005986110307276249, 0.3161322772502899]\n",
      "Step 385, loss = [0.2954205572605133, 0.004488794133067131, 0.29497167468070984]\n",
      "Step 386, loss = [0.29311177134513855, 0.0043818773701786995, 0.29267358779907227]\n",
      "Step 387, loss = [0.3029254674911499, 0.00454715546220541, 0.30247074365615845]\n",
      "Step 388, loss = [0.29361462593078613, 0.0054865144193172455, 0.29306596517562866]\n",
      "Step 389, loss = [0.3038351237773895, 0.0021422063000500202, 0.3036209046840668]\n",
      "Step 390, loss = [0.2923498749732971, 0.003984224051237106, 0.291951447725296]\n",
      "Step 391, loss = [0.30111318826675415, 0.0025510452687740326, 0.3008580803871155]\n",
      "Step 392, loss = [0.31316307187080383, 0.005200676620006561, 0.31264299154281616]\n",
      "Step 393, loss = [0.3038935959339142, 0.004434057045727968, 0.30345019698143005]\n",
      "Step 394, loss = [0.28855642676353455, 0.0036729956045746803, 0.2881891131401062]\n",
      "Step 395, loss = [0.297690749168396, 0.0017121108248829842, 0.2975195348262787]\n",
      "Step 396, loss = [0.28842225670814514, 0.0035379999317228794, 0.2880684435367584]\n",
      "Step 397, loss = [0.3112609386444092, 0.004693357273936272, 0.31079161167144775]\n",
      "Step 398, loss = [0.30275410413742065, 0.004347248002886772, 0.30231937766075134]\n",
      "Step 399, loss = [0.28160515427589417, 0.005088247358798981, 0.28109633922576904]\n",
      "Step 400, loss = [0.3128271996974945, 0.005238689947873354, 0.3123033344745636]\n",
      "Step 401, loss = [0.3083966076374054, 0.0057572536170482635, 0.3078208863735199]\n",
      "Step 402, loss = [0.31154826283454895, 0.004508714657276869, 0.3110973834991455]\n",
      "Step 403, loss = [0.2936883568763733, 0.0036668460816144943, 0.2933216691017151]\n",
      "Step 404, loss = [0.29775354266166687, 0.005521288141608238, 0.2972014248371124]\n",
      "Step 405, loss = [0.29749661684036255, 0.005945105105638504, 0.2969021201133728]\n",
      "Step 406, loss = [0.30745452642440796, 0.004468611441552639, 0.30700767040252686]\n",
      "Step 407, loss = [0.29766568541526794, 0.0051353694871068, 0.29715216159820557]\n",
      "Step 408, loss = [0.2939271628856659, 0.0034006114583462477, 0.2935870885848999]\n",
      "Step 409, loss = [0.3035280704498291, 0.005380289629101753, 0.30299004912376404]\n",
      "Step 410, loss = [0.29556575417518616, 0.005747701041400433, 0.29499098658561707]\n",
      "Step 411, loss = [0.28606757521629333, 0.0023880843073129654, 0.28582876920700073]\n",
      "Step 412, loss = [0.2903831899166107, 0.0038571078330278397, 0.28999748826026917]\n",
      "Step 413, loss = [0.3058186173439026, 0.0038292482495307922, 0.3054356873035431]\n",
      "Step 414, loss = [0.29142382740974426, 0.004193610046058893, 0.291004478931427]\n",
      "Step 415, loss = [0.3068189322948456, 0.0029985224828124046, 0.306519091129303]\n",
      "Step 416, loss = [0.2957583963871002, 0.003545456100255251, 0.2954038381576538]\n",
      "Step 417, loss = [0.31063729524612427, 0.005418997723609209, 0.3100953996181488]\n",
      "Step 418, loss = [0.3071174621582031, 0.004938853904604912, 0.30662357807159424]\n",
      "Step 419, loss = [0.29941725730895996, 0.004978703334927559, 0.2989193797111511]\n",
      "Step 420, loss = [0.30218830704689026, 0.0044416747987270355, 0.30174413323402405]\n",
      "Step 421, loss = [0.30735689401626587, 0.003467862494289875, 0.30701011419296265]\n",
      "Step 422, loss = [0.3101128339767456, 0.0025923666544258595, 0.30985358357429504]\n",
      "Step 423, loss = [0.3046457767486572, 0.003362484509125352, 0.30430951714515686]\n",
      "Step 424, loss = [0.3060896098613739, 0.0028004255145788193, 0.30580955743789673]\n",
      "Step 425, loss = [0.3112797737121582, 0.007626010570675135, 0.31051716208457947]\n",
      "Step 426, loss = [0.30264797806739807, 0.001661213580518961, 0.30248185992240906]\n",
      "Step 427, loss = [0.3038429915904999, 0.0052591064013540745, 0.3033170700073242]\n",
      "Step 428, loss = [0.29308170080184937, 0.005734845995903015, 0.29250821471214294]\n",
      "Step 429, loss = [0.30049601197242737, 0.006094899028539658, 0.2998865246772766]\n",
      "Step 430, loss = [0.30063968896865845, 0.0016358931316062808, 0.3004761040210724]\n",
      "Step 431, loss = [0.2995743155479431, 0.0035271435044705868, 0.29922160506248474]\n",
      "Step 432, loss = [0.31041890382766724, 0.003960087429732084, 0.31002289056777954]\n",
      "Step 433, loss = [0.29628264904022217, 0.0027389274910092354, 0.29600876569747925]\n",
      "Step 434, loss = [0.30132412910461426, 0.0062402887269854546, 0.3007000982761383]\n",
      "Step 435, loss = [0.30108633637428284, 0.004598061554133892, 0.3006265163421631]\n",
      "Step 436, loss = [0.3026129901409149, 0.003986907657235861, 0.30221429467201233]\n",
      "Step 437, loss = [0.31684303283691406, 0.004489712417125702, 0.31639406085014343]\n",
      "Step 438, loss = [0.3006715774536133, 0.003083455841988325, 0.3003632426261902]\n",
      "Step 439, loss = [0.3110681474208832, 0.00400605658069253, 0.3106675446033478]\n",
      "Step 440, loss = [0.2976623475551605, 0.00600056629627943, 0.2970622777938843]\n",
      "Step 441, loss = [0.3021351993083954, 0.007748930715024471, 0.3013603091239929]\n",
      "Step 442, loss = [0.3073848485946655, 0.002663330640643835, 0.3071185052394867]\n",
      "Step 443, loss = [0.30997806787490845, 0.005903298035264015, 0.309387743473053]\n",
      "Step 444, loss = [0.31190136075019836, 0.0035788416862487793, 0.31154346466064453]\n",
      "Step 445, loss = [0.2961198389530182, 0.0032967408187687397, 0.2957901656627655]\n",
      "Step 446, loss = [0.30047816038131714, 0.006977696903049946, 0.29978039860725403]\n",
      "Step 447, loss = [0.29262274503707886, 0.003915412817150354, 0.2922312021255493]\n",
      "Step 448, loss = [0.2952837347984314, 0.0038272819947451353, 0.2949010133743286]\n",
      "Step 449, loss = [0.2908685505390167, 0.005037801805883646, 0.2903647720813751]\n",
      "Step 450, loss = [0.30614379048347473, 0.0035123522393405437, 0.30579257011413574]\n",
      "Step 451, loss = [0.29312101006507874, 0.004700580611824989, 0.2926509380340576]\n",
      "Step 452, loss = [0.29587915539741516, 0.0028655952773988247, 0.29559260606765747]\n",
      "Step 453, loss = [0.29444968700408936, 0.004090975038707256, 0.29404059052467346]\n",
      "Step 454, loss = [0.3068922162055969, 0.004332626238465309, 0.3064589500427246]\n",
      "Step 455, loss = [0.2931717336177826, 0.002294085454195738, 0.2929423153400421]\n",
      "Step 456, loss = [0.2985398769378662, 0.0019696643576025963, 0.29834291338920593]\n",
      "Step 457, loss = [0.3137640953063965, 0.003431143704801798, 0.31342098116874695]\n",
      "Step 458, loss = [0.2880636155605316, 0.0036882467102259398, 0.2876947820186615]\n",
      "Step 459, loss = [0.3107775151729584, 0.003518748562783003, 0.31042563915252686]\n",
      "Step 460, loss = [0.29239538311958313, 0.0029189763590693474, 0.29210349917411804]\n",
      "Step 461, loss = [0.2953413128852844, 0.003290555439889431, 0.2950122654438019]\n",
      "Step 462, loss = [0.2899942696094513, 0.005369032267481089, 0.28945738077163696]\n",
      "Step 463, loss = [0.2921425700187683, 0.0033731383737176657, 0.2918052673339844]\n",
      "Step 464, loss = [0.31182214617729187, 0.004187406972050667, 0.31140339374542236]\n",
      "Step 465, loss = [0.2970673739910126, 0.0043781958520412445, 0.29662954807281494]\n",
      "Step 466, loss = [0.3015861511230469, 0.005715767852962017, 0.30101457238197327]\n",
      "Step 467, loss = [0.2872207462787628, 0.006629342213273048, 0.2865578234195709]\n",
      "Step 468, loss = [0.30454233288764954, 0.00739290751516819, 0.30380305647850037]\n",
      "Step 469, loss = [0.2996313273906708, 0.0040558865293860435, 0.29922574758529663]\n",
      "Step 470, loss = [0.301010400056839, 0.0024772752076387405, 0.30076268315315247]\n",
      "Step 471, loss = [0.2765909731388092, 0.005555768497288227, 0.2760353982448578]\n",
      "Step 472, loss = [0.3033061921596527, 0.0016010527033358812, 0.303146094083786]\n",
      "Step 473, loss = [0.31338199973106384, 0.0048535289242863655, 0.31289663910865784]\n",
      "Step 474, loss = [0.2791661024093628, 0.005044626537710428, 0.2786616384983063]\n",
      "Step 475, loss = [0.30648425221443176, 0.0053193224593997, 0.3059523105621338]\n",
      "Step 476, loss = [0.3013349771499634, 0.003915451932698488, 0.30094343423843384]\n",
      "Step 477, loss = [0.29852789640426636, 0.004155617672950029, 0.29811233282089233]\n",
      "Step 478, loss = [0.30368417501449585, 0.006047741509974003, 0.30307939648628235]\n",
      "Step 479, loss = [0.30084720253944397, 0.005148765631020069, 0.30033233761787415]\n",
      "Step 480, loss = [0.29774296283721924, 0.0022900591138750315, 0.2975139617919922]\n",
      "Step 481, loss = [0.286824494600296, 0.0027993773110210896, 0.2865445613861084]\n",
      "Step 482, loss = [0.3029676079750061, 0.007710661739110947, 0.30219653248786926]\n",
      "Step 483, loss = [0.29178473353385925, 0.0035442186053842306, 0.2914303243160248]\n",
      "Step 484, loss = [0.3091968595981598, 0.0043441178277134895, 0.30876246094703674]\n",
      "Step 485, loss = [0.29814577102661133, 0.007151369471102953, 0.2974306344985962]\n",
      "Step 486, loss = [0.28169381618499756, 0.006154983304440975, 0.2810783088207245]\n",
      "Step 487, loss = [0.3152543008327484, 0.002929757349193096, 0.314961314201355]\n",
      "Step 488, loss = [0.28144267201423645, 0.0027979116421192884, 0.28116288781166077]\n",
      "Update target distribution epoch 1 step 489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11754/11754 [1:51:19<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 489, loss = [0.2963223159313202, 0.0018998601008206606, 0.29613232612609863]\n",
      "Step 490, loss = [0.30768153071403503, 0.0018829831387847662, 0.3074932396411896]\n",
      "Step 491, loss = [0.28650107979774475, 0.0035318532027304173, 0.2861478924751282]\n",
      "Step 492, loss = [0.30724775791168213, 0.0047815036959946156, 0.30676960945129395]\n",
      "Step 493, loss = [0.28845012187957764, 0.0033781235106289387, 0.2881123125553131]\n",
      "Step 494, loss = [0.3030017018318176, 0.005884954705834389, 0.3024131953716278]\n",
      "Step 495, loss = [0.2900097966194153, 0.005271796137094498, 0.28948262333869934]\n",
      "Step 496, loss = [0.30310696363449097, 0.004032939672470093, 0.3027036786079407]\n",
      "Step 497, loss = [0.3008517622947693, 0.0017129897605627775, 0.3006804585456848]\n",
      "Step 498, loss = [0.30080485343933105, 0.003685125382617116, 0.3004363477230072]\n",
      "Step 499, loss = [0.3007209897041321, 0.003771367482841015, 0.3003438413143158]\n",
      "Step 500, loss = [0.29975226521492004, 0.005575054325163364, 0.2991947531700134]\n",
      "Step 501, loss = [0.3013703525066376, 0.007069068029522896, 0.30066344141960144]\n",
      "Step 502, loss = [0.30655068159103394, 0.004601682536303997, 0.30609050393104553]\n",
      "Step 503, loss = [0.29938554763793945, 0.0012635659659281373, 0.2992591857910156]\n",
      "Step 504, loss = [0.30008241534233093, 0.00514595303684473, 0.2995678186416626]\n",
      "Step 505, loss = [0.29194194078445435, 0.00574881536886096, 0.2913670539855957]\n",
      "Step 506, loss = [0.30879175662994385, 0.0035705971531569958, 0.30843469500541687]\n",
      "Step 507, loss = [0.30571743845939636, 0.004192462656646967, 0.30529817938804626]\n",
      "Step 508, loss = [0.2771024703979492, 0.004865492694079876, 0.2766159176826477]\n",
      "Step 509, loss = [0.29789289832115173, 0.004026537295430899, 0.2974902391433716]\n",
      "Step 510, loss = [0.31177210807800293, 0.006875059567391872, 0.3110845983028412]\n",
      "Step 511, loss = [0.31705623865127563, 0.0043816519901156425, 0.31661808490753174]\n",
      "Step 512, loss = [0.31007835268974304, 0.008248511701822281, 0.3092535138130188]\n",
      "Step 513, loss = [0.2935275137424469, 0.0041379183530807495, 0.29311370849609375]\n",
      "Step 514, loss = [0.29180991649627686, 0.0014536632224917412, 0.2916645407676697]\n",
      "Step 515, loss = [0.28993722796440125, 0.006402383092790842, 0.2892969846725464]\n",
      "Step 516, loss = [0.3079136610031128, 0.0038324191700667143, 0.3075304329395294]\n",
      "Step 517, loss = [0.29229483008384705, 0.002018305938690901, 0.2920930087566376]\n",
      "Step 518, loss = [0.3117598295211792, 0.0025001787580549717, 0.3115098178386688]\n",
      "Step 519, loss = [0.30226266384124756, 0.007012093905359507, 0.3015614449977875]\n",
      "Step 520, loss = [0.3116223216056824, 0.005889104679226875, 0.3110333979129791]\n",
      "Step 521, loss = [0.3030334413051605, 0.003270496614277363, 0.30270639061927795]\n",
      "Step 522, loss = [0.3051607310771942, 0.004834478255361319, 0.304677277803421]\n",
      "Step 523, loss = [0.3055800199508667, 0.0022797901183366776, 0.30535203218460083]\n",
      "Step 524, loss = [0.27689406275749207, 0.004428278189152479, 0.2764512300491333]\n",
      "Step 525, loss = [0.2980799973011017, 0.004884608089923859, 0.29759153723716736]\n",
      "Step 526, loss = [0.29089924693107605, 0.0022825971245765686, 0.2906709909439087]\n",
      "Step 527, loss = [0.3043907880783081, 0.00414494052529335, 0.30397629737854004]\n",
      "Step 528, loss = [0.2877333164215088, 0.005555899813771248, 0.287177711725235]\n",
      "Step 529, loss = [0.29461121559143066, 0.005444491282105446, 0.29406675696372986]\n",
      "Step 530, loss = [0.30932900309562683, 0.007523851934820414, 0.3085766136646271]\n",
      "Step 531, loss = [0.307088702917099, 0.00309972302056849, 0.3067787289619446]\n",
      "Step 532, loss = [0.30470752716064453, 0.005811525508761406, 0.3041263818740845]\n",
      "Step 533, loss = [0.3020554780960083, 0.005583317019045353, 0.30149713158607483]\n",
      "Step 534, loss = [0.3016537129878998, 0.0030742015223950148, 0.3013463020324707]\n",
      "Step 535, loss = [0.3091598153114319, 0.005077867768704891, 0.30865204334259033]\n",
      "Step 536, loss = [0.2969237267971039, 0.003643819596618414, 0.29655933380126953]\n",
      "Step 537, loss = [0.30324047803878784, 0.005013841670006514, 0.30273908376693726]\n",
      "Step 538, loss = [0.295514851808548, 0.0023496546782553196, 0.2952798902988434]\n",
      "Step 539, loss = [0.3042467534542084, 0.002681001089513302, 0.30397865176200867]\n",
      "Step 540, loss = [0.3006851077079773, 0.005101514980196953, 0.3001749515533447]\n",
      "Step 541, loss = [0.30143260955810547, 0.0034760760609060526, 0.3010849952697754]\n",
      "Step 542, loss = [0.30971941351890564, 0.004251711070537567, 0.3092942535877228]\n",
      "Step 543, loss = [0.2996990978717804, 0.0027080224826931953, 0.2994282841682434]\n",
      "Step 544, loss = [0.29627057909965515, 0.00436710799112916, 0.29583385586738586]\n",
      "Step 545, loss = [0.30168402194976807, 0.0035249292850494385, 0.3013315200805664]\n",
      "Step 546, loss = [0.29552772641181946, 0.003550819121301174, 0.29517263174057007]\n",
      "Step 547, loss = [0.3002033531665802, 0.002217090455815196, 0.29998165369033813]\n",
      "Step 548, loss = [0.2903904318809509, 0.005420940928161144, 0.28984832763671875]\n",
      "Step 549, loss = [0.27906525135040283, 0.004314431454986334, 0.27863380312919617]\n",
      "Step 550, loss = [0.28866803646087646, 0.004272120539098978, 0.28824082016944885]\n",
      "Step 551, loss = [0.30073094367980957, 0.0042093535885214806, 0.30031001567840576]\n",
      "Step 552, loss = [0.2981297969818115, 0.004061183892190456, 0.2977236807346344]\n",
      "Step 553, loss = [0.29652664065361023, 0.0061822556890547276, 0.2959084212779999]\n",
      "Step 554, loss = [0.2972618639469147, 0.003620108589529991, 0.29689985513687134]\n",
      "Step 555, loss = [0.3061389625072479, 0.0041451770812273026, 0.30572444200515747]\n",
      "Step 556, loss = [0.2984878420829773, 0.00525051262229681, 0.2979627847671509]\n",
      "Step 557, loss = [0.30061835050582886, 0.005095326341688633, 0.30010882019996643]\n",
      "Step 558, loss = [0.2865883708000183, 0.0031702755950391293, 0.286271333694458]\n",
      "Step 559, loss = [0.3061288297176361, 0.003928096499294043, 0.3057360053062439]\n",
      "Step 560, loss = [0.3083789348602295, 0.003960954956710339, 0.30798283219337463]\n",
      "Step 561, loss = [0.2865894138813019, 0.004210580140352249, 0.2861683666706085]\n",
      "Step 562, loss = [0.30959266424179077, 0.0036456307861953974, 0.3092280924320221]\n",
      "Step 563, loss = [0.3124755918979645, 0.006528307683765888, 0.311822772026062]\n",
      "Step 564, loss = [0.3018248677253723, 0.006826708093285561, 0.3011421859264374]\n",
      "Step 565, loss = [0.2904827296733856, 0.007624676451086998, 0.2897202670574188]\n",
      "Step 566, loss = [0.2912405729293823, 0.0032196431420743465, 0.29091861844062805]\n",
      "Step 567, loss = [0.3040241003036499, 0.004478850401937962, 0.30357620120048523]\n",
      "Step 568, loss = [0.2974410057067871, 0.0022801898885518312, 0.29721298813819885]\n",
      "Step 569, loss = [0.301381915807724, 0.004850882105529308, 0.3008968234062195]\n",
      "Step 570, loss = [0.3118303716182709, 0.0022253335919231176, 0.31160783767700195]\n",
      "Step 571, loss = [0.30931219458580017, 0.0044381823390722275, 0.3088683784008026]\n",
      "Step 572, loss = [0.2883962392807007, 0.005696199834346771, 0.28782662749290466]\n",
      "Step 573, loss = [0.29752522706985474, 0.003154253587126732, 0.29720979928970337]\n",
      "Step 574, loss = [0.30375951528549194, 0.004901940934360027, 0.30326932668685913]\n",
      "Step 575, loss = [0.30736976861953735, 0.0023039253428578377, 0.3071393668651581]\n",
      "Step 576, loss = [0.3004283010959625, 0.004030071664601564, 0.3000252842903137]\n",
      "Step 577, loss = [0.29251065850257874, 0.00470762001350522, 0.2920399010181427]\n",
      "Step 578, loss = [0.31412842869758606, 0.0035071277525275946, 0.31377771496772766]\n",
      "Step 579, loss = [0.286744087934494, 0.0028897852171212435, 0.28645509481430054]\n",
      "Step 580, loss = [0.3001351058483124, 0.008175328373908997, 0.2993175685405731]\n",
      "Step 581, loss = [0.30936840176582336, 0.00564595079049468, 0.3088037967681885]\n",
      "Step 582, loss = [0.2935040593147278, 0.003886445891112089, 0.29311540722846985]\n",
      "Step 583, loss = [0.29695266485214233, 0.006124322302639484, 0.2963402271270752]\n",
      "Step 584, loss = [0.30728137493133545, 0.0035799373872578144, 0.30692338943481445]\n",
      "Step 585, loss = [0.309234082698822, 0.004583702888339758, 0.30877572298049927]\n",
      "Step 586, loss = [0.29728490114212036, 0.004803653806447983, 0.2968045473098755]\n",
      "Step 587, loss = [0.3070957064628601, 0.002969498746097088, 0.3067987561225891]\n",
      "Step 588, loss = [0.3040410876274109, 0.003326047444716096, 0.3037084937095642]\n",
      "Step 589, loss = [0.3013579547405243, 0.004154853988438845, 0.30094248056411743]\n",
      "Step 590, loss = [0.30949026346206665, 0.0038195159286260605, 0.30910831689834595]\n",
      "Step 591, loss = [0.30458804965019226, 0.0051010772585868835, 0.30407795310020447]\n",
      "Step 592, loss = [0.2968357503414154, 0.0027346741408109665, 0.2965622842311859]\n",
      "Step 593, loss = [0.29865169525146484, 0.007243984844535589, 0.29792729020118713]\n",
      "Step 594, loss = [0.29626771807670593, 0.0024639598559588194, 0.29602131247520447]\n",
      "Step 595, loss = [0.2942943871021271, 0.003967838361859322, 0.2938975989818573]\n",
      "Step 596, loss = [0.2793996036052704, 0.0018499813741073012, 0.2792145907878876]\n",
      "Step 597, loss = [0.30511608719825745, 0.0016738737467676401, 0.30494868755340576]\n",
      "Step 598, loss = [0.2936331033706665, 0.007684297859668732, 0.29286468029022217]\n",
      "Step 599, loss = [0.29664498567581177, 0.004847478121519089, 0.2961602509021759]\n",
      "Step 600, loss = [0.28482669591903687, 0.0056460704654455185, 0.284262090921402]\n",
      "Step 601, loss = [0.2843407392501831, 0.0028564096428453922, 0.28405508399009705]\n",
      "Step 602, loss = [0.2897999882698059, 0.004770880565047264, 0.2893229126930237]\n",
      "Step 603, loss = [0.2976507842540741, 0.0024537595454603434, 0.2974054217338562]\n",
      "Step 604, loss = [0.3070251941680908, 0.005471360869705677, 0.3064780533313751]\n",
      "Step 605, loss = [0.3029802739620209, 0.00439131585881114, 0.3025411367416382]\n",
      "Step 606, loss = [0.30858945846557617, 0.005207556299865246, 0.3080686926841736]\n",
      "Step 607, loss = [0.2822854816913605, 0.005629951134324074, 0.2817224860191345]\n",
      "Step 608, loss = [0.29380935430526733, 0.004505752585828304, 0.29335877299308777]\n",
      "Step 609, loss = [0.29766276478767395, 0.0036227775271981955, 0.2973004877567291]\n",
      "Step 610, loss = [0.3136666417121887, 0.004620563238859177, 0.3132045865058899]\n",
      "Step 611, loss = [0.3079262673854828, 0.006415914744138718, 0.3072846829891205]\n",
      "Step 612, loss = [0.3074881434440613, 0.006190510932356119, 0.3068690896034241]\n",
      "Step 613, loss = [0.29811859130859375, 0.0031266307923942804, 0.29780593514442444]\n",
      "Step 614, loss = [0.30670127272605896, 0.0029724109917879105, 0.3064040243625641]\n",
      "Step 615, loss = [0.2972766160964966, 0.008455226197838783, 0.2964310944080353]\n",
      "Step 616, loss = [0.30430641770362854, 0.004561481066048145, 0.3038502633571625]\n",
      "Step 617, loss = [0.2987234890460968, 0.0032928315922617912, 0.29839420318603516]\n",
      "Step 618, loss = [0.3123777508735657, 0.006484844721853733, 0.3117292523384094]\n",
      "Step 619, loss = [0.3030106723308563, 0.00646918173879385, 0.3023637533187866]\n",
      "Step 620, loss = [0.28712528944015503, 0.004590028431266546, 0.28666627407073975]\n",
      "Step 621, loss = [0.2842516303062439, 0.0026150329504162073, 0.28399011492729187]\n",
      "Step 622, loss = [0.3050920367240906, 0.005614674650132656, 0.3045305609703064]\n",
      "Step 623, loss = [0.2969330847263336, 0.0027759852819144726, 0.29665547609329224]\n",
      "Step 624, loss = [0.3010975420475006, 0.0033956451807171106, 0.3007579743862152]\n",
      "Step 625, loss = [0.30270448327064514, 0.0025680065155029297, 0.30244767665863037]\n",
      "Step 626, loss = [0.3074405789375305, 0.007466139737516642, 0.306693971157074]\n",
      "Step 627, loss = [0.29287829995155334, 0.0032001156359910965, 0.2925582826137543]\n",
      "Step 628, loss = [0.3024260103702545, 0.0014894194900989532, 0.3022770583629608]\n",
      "Step 629, loss = [0.3089407682418823, 0.00613951962441206, 0.3083268105983734]\n",
      "Step 630, loss = [0.3016718029975891, 0.007311543449759483, 0.3009406626224518]\n",
      "Step 631, loss = [0.2979391813278198, 0.004904826171696186, 0.29744869470596313]\n",
      "Step 632, loss = [0.29909253120422363, 0.0032882371451705694, 0.2987637221813202]\n",
      "Step 633, loss = [0.29733431339263916, 0.002690293826162815, 0.29706528782844543]\n",
      "Step 634, loss = [0.3017674386501312, 0.003604212310165167, 0.30140700936317444]\n",
      "Step 635, loss = [0.294474720954895, 0.005446989089250565, 0.2939300239086151]\n",
      "Step 636, loss = [0.28700894117355347, 0.00651779118925333, 0.28635716438293457]\n",
      "Step 637, loss = [0.2852379083633423, 0.0010496131144464016, 0.2851329445838928]\n",
      "Step 638, loss = [0.3082984983921051, 0.003656682325527072, 0.3079328238964081]\n",
      "Step 639, loss = [0.3104901611804962, 0.005546895787119865, 0.30993548035621643]\n",
      "Step 640, loss = [0.29329952597618103, 0.003676225431263447, 0.2929319143295288]\n",
      "Step 641, loss = [0.31511083245277405, 0.004916074685752392, 0.3146192133426666]\n",
      "Step 642, loss = [0.2998977601528168, 0.005308523774147034, 0.29936692118644714]\n",
      "Step 643, loss = [0.29790300130844116, 0.0025414987467229366, 0.2976488471031189]\n",
      "Step 644, loss = [0.2981604039669037, 0.006017065607011318, 0.2975586950778961]\n",
      "Step 645, loss = [0.2851710319519043, 0.002586917020380497, 0.2849123477935791]\n",
      "Step 646, loss = [0.30662602186203003, 0.004039453808218241, 0.3062220811843872]\n",
      "Step 647, loss = [0.30202168226242065, 0.004122079350054264, 0.30160948634147644]\n",
      "Step 648, loss = [0.30718994140625, 0.005183561239391565, 0.3066715896129608]\n",
      "Step 649, loss = [0.30276355147361755, 0.004576626233756542, 0.3023058772087097]\n",
      "Step 650, loss = [0.29828014969825745, 0.002141584176570177, 0.29806599020957947]\n",
      "Step 651, loss = [0.29766157269477844, 0.005609466694295406, 0.29710063338279724]\n",
      "Step 652, loss = [0.3116239011287689, 0.004004973452538252, 0.31122341752052307]\n",
      "Step 653, loss = [0.3072386384010315, 0.0017419705400243402, 0.3070644438266754]\n",
      "Step 654, loss = [0.2968340814113617, 0.003937244415283203, 0.29644036293029785]\n",
      "Step 655, loss = [0.30626925826072693, 0.0044908542186021805, 0.30582016706466675]\n",
      "Step 656, loss = [0.3062024414539337, 0.003018304705619812, 0.30590060353279114]\n",
      "Step 657, loss = [0.3058522343635559, 0.003654118627309799, 0.3054868280887604]\n",
      "Step 658, loss = [0.30657827854156494, 0.00603502057492733, 0.3059747815132141]\n",
      "Step 659, loss = [0.31463417410850525, 0.0018952714744955301, 0.3144446611404419]\n",
      "Step 660, loss = [0.2925202250480652, 0.0049813613295555115, 0.29202207922935486]\n",
      "Step 661, loss = [0.30189400911331177, 0.00532364659011364, 0.30136165022850037]\n",
      "Step 662, loss = [0.30532607436180115, 0.003917764872312546, 0.3049342930316925]\n",
      "Step 663, loss = [0.30572041869163513, 0.001788447960279882, 0.3055415749549866]\n",
      "Step 664, loss = [0.30061405897140503, 0.0044684456661343575, 0.3001672029495239]\n",
      "Step 665, loss = [0.3007315397262573, 0.004012634977698326, 0.3003302812576294]\n",
      "Step 666, loss = [0.3064945638179779, 0.004161497578024864, 0.30607840418815613]\n",
      "Step 667, loss = [0.3033399283885956, 0.0034229476004838943, 0.3029976189136505]\n",
      "Step 668, loss = [0.3046758770942688, 0.003534742398187518, 0.30432239174842834]\n",
      "Step 669, loss = [0.2857336103916168, 0.0034747766330838203, 0.2853861451148987]\n",
      "Step 670, loss = [0.28640246391296387, 0.0052564446814358234, 0.2858768105506897]\n",
      "Step 671, loss = [0.2892906665802002, 0.002305437345057726, 0.289060115814209]\n",
      "Step 672, loss = [0.3047536313533783, 0.0030356021597981453, 0.30445006489753723]\n",
      "Step 673, loss = [0.2804020643234253, 0.007134903687983751, 0.2796885669231415]\n",
      "Step 674, loss = [0.2806203365325928, 0.005899110808968544, 0.28003042936325073]\n",
      "Step 675, loss = [0.2890138328075409, 0.007204180117696524, 0.28829342126846313]\n",
      "Step 676, loss = [0.29916971921920776, 0.005120743066072464, 0.2986576557159424]\n",
      "Step 677, loss = [0.3064258396625519, 0.004373306408524513, 0.30598852038383484]\n",
      "Step 678, loss = [0.27892032265663147, 0.004595918580889702, 0.2784607410430908]\n",
      "Step 679, loss = [0.2995493710041046, 0.0037797256372869015, 0.2991713881492615]\n",
      "Step 680, loss = [0.3024282157421112, 0.0023876891937106848, 0.302189439535141]\n",
      "Step 681, loss = [0.2887875437736511, 0.00689968466758728, 0.2880975902080536]\n",
      "Step 682, loss = [0.2953949272632599, 0.0036401839461177588, 0.2950309216976166]\n",
      "Step 683, loss = [0.29409685730934143, 0.004481341689825058, 0.29364871978759766]\n",
      "Step 684, loss = [0.2903428077697754, 0.0035111343022435904, 0.28999170660972595]\n",
      "Step 685, loss = [0.2825409173965454, 0.005319529213011265, 0.28200897574424744]\n",
      "Step 686, loss = [0.30749014019966125, 0.005507852882146835, 0.30693936347961426]\n",
      "Step 687, loss = [0.2986772060394287, 0.0043599773198366165, 0.29824119806289673]\n",
      "Step 688, loss = [0.29893165826797485, 0.0025186557322740555, 0.29867979884147644]\n",
      "Step 689, loss = [0.28766366839408875, 0.004630034323781729, 0.2872006595134735]\n",
      "Step 690, loss = [0.2856478691101074, 0.0012967827497050166, 0.28551819920539856]\n",
      "Step 691, loss = [0.29157504439353943, 0.002918075770139694, 0.2912832498550415]\n",
      "Step 692, loss = [0.3067503869533539, 0.0019168523140251637, 0.3065586984157562]\n",
      "Step 693, loss = [0.3075437545776367, 0.002043606247752905, 0.3073394000530243]\n",
      "Step 694, loss = [0.30195170640945435, 0.003996815532445908, 0.30155202746391296]\n",
      "Step 695, loss = [0.3132212162017822, 0.015621460974216461, 0.3116590678691864]\n",
      "Step 696, loss = [0.31743597984313965, 0.003890021936967969, 0.31704697012901306]\n",
      "Step 697, loss = [0.3110218942165375, 0.004586413968354464, 0.31056326627731323]\n",
      "Step 698, loss = [0.2954561114311218, 0.0020703542977571487, 0.2952490746974945]\n",
      "Step 699, loss = [0.3032110929489136, 0.005224049557000399, 0.30268868803977966]\n",
      "Step 700, loss = [0.29378506541252136, 0.004690667614340782, 0.2933160066604614]\n",
      "Step 701, loss = [0.29094937443733215, 0.004743065685033798, 0.290475070476532]\n",
      "Step 702, loss = [0.2811092138290405, 0.006461408920586109, 0.2804630696773529]\n",
      "Step 703, loss = [0.29182273149490356, 0.0028431573882699013, 0.29153841733932495]\n",
      "Step 704, loss = [0.30506476759910583, 0.003592622932046652, 0.30470550060272217]\n",
      "Step 705, loss = [0.29917922616004944, 0.00377395236864686, 0.29880183935165405]\n",
      "Step 706, loss = [0.29698115587234497, 0.004307346418499947, 0.2965504229068756]\n",
      "Step 707, loss = [0.30482128262519836, 0.00514598935842514, 0.30430668592453003]\n",
      "Step 708, loss = [0.30412086844444275, 0.0045411428436636925, 0.30366674065589905]\n",
      "Step 709, loss = [0.30276253819465637, 0.005380302667617798, 0.3022245168685913]\n",
      "Step 710, loss = [0.28919297456741333, 0.005077220965176821, 0.28868526220321655]\n",
      "Step 711, loss = [0.2944553792476654, 0.005509378854185343, 0.29390445351600647]\n",
      "Step 712, loss = [0.28614386916160583, 0.002622053027153015, 0.2858816683292389]\n",
      "Step 713, loss = [0.30694109201431274, 0.006928117945790291, 0.306248277425766]\n",
      "Step 714, loss = [0.3059399724006653, 0.002867709845304489, 0.3056532144546509]\n",
      "Step 715, loss = [0.29004809260368347, 0.0043703531846404076, 0.2896110713481903]\n",
      "Step 716, loss = [0.2990041971206665, 0.00350914872251451, 0.2986532747745514]\n",
      "Step 717, loss = [0.3093191981315613, 0.004944865591824055, 0.30882471799850464]\n",
      "Step 718, loss = [0.29389435052871704, 0.003452697303146124, 0.2935490906238556]\n",
      "Step 719, loss = [0.30753806233406067, 0.0032452684827148914, 0.30721354484558105]\n",
      "Step 720, loss = [0.2694943845272064, 0.00718667870387435, 0.26877573132514954]\n",
      "Step 721, loss = [0.29954251646995544, 0.0044407472014427185, 0.2990984320640564]\n",
      "Step 722, loss = [0.3078208565711975, 0.0025588087737560272, 0.30756497383117676]\n",
      "Step 723, loss = [0.2968950569629669, 0.004756023641675711, 0.2964194416999817]\n",
      "Step 724, loss = [0.30428171157836914, 0.006427544169127941, 0.3036389648914337]\n",
      "Step 725, loss = [0.2965398132801056, 0.004261695314198732, 0.29611364006996155]\n",
      "Step 726, loss = [0.30208373069763184, 0.004977155476808548, 0.30158600211143494]\n",
      "Step 727, loss = [0.30055829882621765, 0.0070477444678545, 0.29985353350639343]\n",
      "Step 728, loss = [0.2806568145751953, 0.007632000837475061, 0.2798936069011688]\n",
      "Step 729, loss = [0.3130945861339569, 0.002889828057959676, 0.3128055930137634]\n",
      "Step 730, loss = [0.3084654211997986, 0.003671397687867284, 0.30809828639030457]\n",
      "Step 731, loss = [0.3074539303779602, 0.0041073220781981945, 0.307043194770813]\n",
      "Step 732, loss = [0.2944197654724121, 0.0057820724323391914, 0.29384157061576843]\n",
      "Step 733, loss = [0.2995964586734772, 0.002735525369644165, 0.2993229031562805]\n",
      "Step 734, loss = [0.2795955240726471, 0.004699781537055969, 0.27912554144859314]\n",
      "Step 735, loss = [0.2951357066631317, 0.0019500115886330605, 0.294940710067749]\n",
      "Step 736, loss = [0.30407413840293884, 0.00434899004176259, 0.3036392331123352]\n",
      "Step 737, loss = [0.30005764961242676, 0.00561003340408206, 0.2994966506958008]\n",
      "Step 738, loss = [0.27429667115211487, 0.005452555604279041, 0.2737514078617096]\n",
      "Step 739, loss = [0.304392009973526, 0.0037898949813097715, 0.3040130138397217]\n",
      "Step 740, loss = [0.30309930443763733, 0.0016161962412297726, 0.30293768644332886]\n",
      "Step 741, loss = [0.302789568901062, 0.002592924050986767, 0.30253028869628906]\n",
      "Step 742, loss = [0.31037360429763794, 0.0018631364218890667, 0.31018728017807007]\n",
      "Step 743, loss = [0.30890193581581116, 0.0039252350106835365, 0.3085094094276428]\n",
      "Step 744, loss = [0.30774253606796265, 0.004314507357776165, 0.307311087846756]\n",
      "Step 745, loss = [0.2996840476989746, 0.004231492057442665, 0.2992608845233917]\n",
      "Step 746, loss = [0.2872624099254608, 0.004175372887402773, 0.2868448793888092]\n",
      "Step 747, loss = [0.30522406101226807, 0.0034912414848804474, 0.3048749268054962]\n",
      "Step 748, loss = [0.2903105318546295, 0.005238428711891174, 0.289786696434021]\n",
      "Step 749, loss = [0.3109340965747833, 0.0030111679807305336, 0.31063297390937805]\n",
      "Step 750, loss = [0.3082757592201233, 0.004350024275481701, 0.3078407645225525]\n",
      "Step 751, loss = [0.3060373365879059, 0.0035398330073803663, 0.30568334460258484]\n",
      "Step 752, loss = [0.3145206868648529, 0.004393020644783974, 0.3140813708305359]\n",
      "Step 753, loss = [0.3061893880367279, 0.0030786844436079264, 0.305881530046463]\n",
      "Step 754, loss = [0.2925678491592407, 0.005055448040366173, 0.29206231236457825]\n",
      "Step 755, loss = [0.2966831624507904, 0.0043599363416433334, 0.2962471544742584]\n",
      "Step 756, loss = [0.3101431131362915, 0.0039061368443071842, 0.309752494096756]\n",
      "Step 757, loss = [0.2821466624736786, 0.0019046561792492867, 0.28195619583129883]\n",
      "Step 758, loss = [0.30095139145851135, 0.007479121908545494, 0.30020347237586975]\n",
      "Step 759, loss = [0.30269843339920044, 0.004752666689455509, 0.30222317576408386]\n",
      "Step 760, loss = [0.30914416909217834, 0.004968821071088314, 0.3086472749710083]\n",
      "Step 761, loss = [0.300497829914093, 0.0037697351071983576, 0.30012086033821106]\n",
      "Step 762, loss = [0.28481781482696533, 0.0045781065709888935, 0.28435999155044556]\n",
      "Step 763, loss = [0.30321043729782104, 0.00553867407143116, 0.3026565611362457]\n",
      "Step 764, loss = [0.31581348180770874, 0.004803525749593973, 0.31533312797546387]\n",
      "Step 765, loss = [0.3141663074493408, 0.004509478807449341, 0.3137153685092926]\n",
      "Step 766, loss = [0.3027869164943695, 0.004354785196483135, 0.3023514449596405]\n",
      "Step 767, loss = [0.3018305003643036, 0.004249071702361107, 0.30140557885169983]\n",
      "Step 768, loss = [0.30691754817962646, 0.005653904750943184, 0.3063521683216095]\n",
      "Step 769, loss = [0.2857953906059265, 0.00541495019569993, 0.2852538824081421]\n",
      "Step 770, loss = [0.2962423264980316, 0.002088245004415512, 0.29603350162506104]\n",
      "Step 771, loss = [0.3029158115386963, 0.005773488432168961, 0.30233845114707947]\n",
      "Step 772, loss = [0.29730260372161865, 0.006203260272741318, 0.2966822683811188]\n",
      "Step 773, loss = [0.29397451877593994, 0.004147042986005545, 0.29355981945991516]\n",
      "Step 774, loss = [0.30436667799949646, 0.009691605344414711, 0.3033975064754486]\n",
      "Step 775, loss = [0.28960636258125305, 0.0018933567916974425, 0.289417028427124]\n",
      "Step 776, loss = [0.30070510506629944, 0.006710652261972427, 0.3000340461730957]\n",
      "Step 777, loss = [0.2926092743873596, 0.0031137801706790924, 0.292297899723053]\n",
      "Step 778, loss = [0.2976595461368561, 0.005103893578052521, 0.2971491515636444]\n",
      "Step 779, loss = [0.3088694214820862, 0.002907705958932638, 0.30857864022254944]\n",
      "Step 780, loss = [0.2998029887676239, 0.002664031460881233, 0.2995365858078003]\n",
      "Step 781, loss = [0.2991010248661041, 0.00333034317009151, 0.29876798391342163]\n",
      "Step 782, loss = [0.30776795744895935, 0.005442115478217602, 0.30722373723983765]\n",
      "Step 783, loss = [0.27395954728126526, 0.005089740268886089, 0.2734505832195282]\n",
      "Step 784, loss = [0.30168476700782776, 0.0007240009726956487, 0.30161237716674805]\n",
      "Step 785, loss = [0.3097609579563141, 0.003924222197383642, 0.3093685209751129]\n",
      "Step 786, loss = [0.30321359634399414, 0.0014105710433796048, 0.3030725419521332]\n",
      "Step 787, loss = [0.2983313202857971, 0.005590891465544701, 0.29777222871780396]\n",
      "Step 788, loss = [0.31299224495887756, 0.0027720557991415262, 0.3127150535583496]\n",
      "Step 789, loss = [0.28174397349357605, 0.0045435503125190735, 0.28128960728645325]\n",
      "Step 790, loss = [0.30758389830589294, 0.0038092690519988537, 0.3072029650211334]\n",
      "Step 791, loss = [0.30501899123191833, 0.004353777039796114, 0.3045836091041565]\n",
      "Step 792, loss = [0.2892082929611206, 0.00421515479683876, 0.28878676891326904]\n",
      "Step 793, loss = [0.28820720314979553, 0.0031774051021784544, 0.2878894507884979]\n",
      "Step 794, loss = [0.3036459982395172, 0.0026909837033599615, 0.3033769130706787]\n",
      "Step 795, loss = [0.3003588020801544, 0.003392927348613739, 0.3000195026397705]\n",
      "Step 796, loss = [0.3033572733402252, 0.003221285529434681, 0.3030351400375366]\n",
      "Step 797, loss = [0.2979698181152344, 0.0032379399053752422, 0.29764601588249207]\n",
      "Step 798, loss = [0.29190051555633545, 0.005254609044641256, 0.2913750410079956]\n",
      "Step 799, loss = [0.2856415808200836, 0.0037562414072453976, 0.2852659523487091]\n",
      "Step 800, loss = [0.30959299206733704, 0.005433916579931974, 0.3090496063232422]\n",
      "Step 801, loss = [0.2820846736431122, 0.0041227759793400764, 0.2816723883152008]\n",
      "Step 802, loss = [0.3060356080532074, 0.0028305775485932827, 0.30575254559516907]\n",
      "Step 803, loss = [0.2951277196407318, 0.0030487431213259697, 0.2948228418827057]\n",
      "Step 804, loss = [0.3010110855102539, 0.003752690739929676, 0.30063581466674805]\n",
      "Step 805, loss = [0.29241302609443665, 0.00464752409607172, 0.29194825887680054]\n",
      "Step 806, loss = [0.2932066023349762, 0.003591220360249281, 0.29284748435020447]\n",
      "Step 807, loss = [0.2960934340953827, 0.0037359585985541344, 0.29571983218193054]\n",
      "Step 808, loss = [0.3045710027217865, 0.004262071568518877, 0.30414479970932007]\n",
      "Step 809, loss = [0.29925674200057983, 0.004170848522335291, 0.29883965849876404]\n",
      "Step 810, loss = [0.28886154294013977, 0.004183662589639425, 0.2884431779384613]\n",
      "Step 811, loss = [0.30112916231155396, 0.003010496497154236, 0.30082809925079346]\n",
      "Step 812, loss = [0.28379327058792114, 0.005488406401127577, 0.28324443101882935]\n",
      "Step 813, loss = [0.2982037663459778, 0.003223156090825796, 0.29788145422935486]\n",
      "Step 814, loss = [0.30865219235420227, 0.002853675279766321, 0.3083668351173401]\n",
      "Step 815, loss = [0.3090341091156006, 0.004683248698711395, 0.30856579542160034]\n",
      "Step 816, loss = [0.29805293679237366, 0.005593758542090654, 0.2974935472011566]\n",
      "Step 817, loss = [0.30311906337738037, 0.005462293513119221, 0.3025728464126587]\n",
      "Step 818, loss = [0.2956969141960144, 0.0048304349184036255, 0.29521387815475464]\n",
      "Step 819, loss = [0.3052453398704529, 0.002510189078748226, 0.3049943149089813]\n",
      "Step 820, loss = [0.30552157759666443, 0.005780275911092758, 0.3049435615539551]\n",
      "Step 821, loss = [0.2921927571296692, 0.005927611142396927, 0.29159998893737793]\n",
      "Step 822, loss = [0.2882225513458252, 0.005874749273061752, 0.28763508796691895]\n",
      "Step 823, loss = [0.2895951271057129, 0.003945458680391312, 0.2892005741596222]\n",
      "Step 824, loss = [0.3026028573513031, 0.00660964660346508, 0.3019419014453888]\n",
      "Step 825, loss = [0.29334092140197754, 0.0038491857703775167, 0.29295599460601807]\n",
      "Step 826, loss = [0.30336934328079224, 0.003162786364555359, 0.3030530512332916]\n",
      "Step 827, loss = [0.31477048993110657, 0.008994895964860916, 0.31387099623680115]\n",
      "Step 828, loss = [0.3033781349658966, 0.0025880569592118263, 0.30311933159828186]\n",
      "Step 829, loss = [0.29942741990089417, 0.004911811091005802, 0.29893624782562256]\n",
      "Step 830, loss = [0.2989961802959442, 0.003534747287631035, 0.29864269495010376]\n",
      "Step 831, loss = [0.3063463568687439, 0.005673978943377733, 0.30577895045280457]\n",
      "Step 832, loss = [0.3116513192653656, 0.0008873624028638005, 0.31156259775161743]\n",
      "Step 833, loss = [0.2924032509326935, 0.0037446122150868177, 0.2920287847518921]\n",
      "Step 834, loss = [0.3075760304927826, 0.006118433550000191, 0.3069641888141632]\n",
      "Step 835, loss = [0.2980545461177826, 0.0034162935335189104, 0.29771292209625244]\n",
      "Step 836, loss = [0.30999621748924255, 0.005358172580599785, 0.3094604015350342]\n",
      "Step 837, loss = [0.2999626696109772, 0.003147735958918929, 0.29964789748191833]\n",
      "Step 838, loss = [0.2982569634914398, 0.0037583219818770885, 0.2978811264038086]\n",
      "Step 839, loss = [0.3040803074836731, 0.007903976365923882, 0.30328992009162903]\n",
      "Step 840, loss = [0.2959795594215393, 0.00341714802198112, 0.295637845993042]\n",
      "Step 841, loss = [0.3098752796649933, 0.0032046318519860506, 0.3095548152923584]\n",
      "Step 842, loss = [0.2859092354774475, 0.005370765924453735, 0.28537216782569885]\n",
      "Step 843, loss = [0.28636643290519714, 0.002343915868550539, 0.2861320376396179]\n",
      "Step 844, loss = [0.30103087425231934, 0.003919630777090788, 0.30063891410827637]\n",
      "Step 845, loss = [0.3049982488155365, 0.005025675054639578, 0.3044956922531128]\n",
      "Step 846, loss = [0.30422794818878174, 0.004695831332355738, 0.3037583529949188]\n",
      "Step 847, loss = [0.3030753433704376, 0.006466765888035297, 0.302428662776947]\n",
      "Step 848, loss = [0.3072018027305603, 0.0038743545301258564, 0.30681437253952026]\n",
      "Step 849, loss = [0.29973652958869934, 0.005852396599948406, 0.29915130138397217]\n",
      "Step 850, loss = [0.3131425678730011, 0.004782028496265411, 0.31266435980796814]\n",
      "Step 851, loss = [0.29495590925216675, 0.0024446232710033655, 0.2947114408016205]\n",
      "Step 852, loss = [0.28198134899139404, 0.003639406058937311, 0.2816174030303955]\n",
      "Step 853, loss = [0.2926997244358063, 0.002602193970233202, 0.2924394905567169]\n",
      "Step 854, loss = [0.2909359633922577, 0.001932139159180224, 0.29074275493621826]\n",
      "Step 855, loss = [0.30399665236473083, 0.0037285350263118744, 0.3036237955093384]\n",
      "Step 856, loss = [0.29927557706832886, 0.004373146221041679, 0.2988382577896118]\n",
      "Step 857, loss = [0.31318458914756775, 0.0045136637054383755, 0.3127332329750061]\n",
      "Step 858, loss = [0.3039303421974182, 0.0027527669444680214, 0.30365505814552307]\n",
      "Step 859, loss = [0.30738571286201477, 0.004170478321611881, 0.30696865916252136]\n",
      "Step 860, loss = [0.2901482582092285, 0.0044186897575855255, 0.28970637917518616]\n",
      "Step 861, loss = [0.30574578046798706, 0.006894225254654884, 0.3050563633441925]\n",
      "Step 862, loss = [0.30563631653785706, 0.003646294353529811, 0.3052716851234436]\n",
      "Step 863, loss = [0.2942761778831482, 0.002626600209623575, 0.29401353001594543]\n",
      "Step 864, loss = [0.30667027831077576, 0.005106578581035137, 0.3061596155166626]\n",
      "Step 865, loss = [0.30409520864486694, 0.0026715733110904694, 0.30382806062698364]\n",
      "Step 866, loss = [0.2761978209018707, 0.0057733445428311825, 0.2756204903125763]\n",
      "Step 867, loss = [0.2957042157649994, 0.0037808683700859547, 0.2953261435031891]\n",
      "Step 868, loss = [0.29865044355392456, 0.004685597028583288, 0.2981818914413452]\n",
      "Step 869, loss = [0.31053563952445984, 0.003691825084388256, 0.31016644835472107]\n",
      "Step 870, loss = [0.28666675090789795, 0.0030645825900137424, 0.2863602936267853]\n",
      "Step 871, loss = [0.29230448603630066, 0.0036857405211776495, 0.29193592071533203]\n",
      "Step 872, loss = [0.2956469655036926, 0.005904777906835079, 0.2950564920902252]\n",
      "Step 873, loss = [0.3037967085838318, 0.00497468514367938, 0.3032992482185364]\n",
      "Step 874, loss = [0.31416407227516174, 0.007618088275194168, 0.3134022653102875]\n",
      "Step 875, loss = [0.29691898822784424, 0.0032287337817251682, 0.29659610986709595]\n",
      "Step 876, loss = [0.3038511276245117, 0.006312112789601088, 0.30321991443634033]\n",
      "Step 877, loss = [0.3065244257450104, 0.008477197028696537, 0.3056766986846924]\n",
      "Step 878, loss = [0.2947658598423004, 0.004586526658385992, 0.2943072021007538]\n",
      "Step 879, loss = [0.28978803753852844, 0.0035192989744246006, 0.28943610191345215]\n",
      "Step 880, loss = [0.30020976066589355, 0.0018765459535643458, 0.30002209544181824]\n",
      "Step 881, loss = [0.28984272480010986, 0.005114981904625893, 0.28933122754096985]\n",
      "Step 882, loss = [0.28912901878356934, 0.002977394964545965, 0.28883129358291626]\n",
      "Step 883, loss = [0.31152719259262085, 0.0025098829064518213, 0.3112761974334717]\n",
      "Step 884, loss = [0.297162264585495, 0.00452585332095623, 0.29670968651771545]\n",
      "Step 885, loss = [0.2927763760089874, 0.002383790910243988, 0.29253798723220825]\n",
      "Step 886, loss = [0.3110543191432953, 0.003315609646961093, 0.3107227683067322]\n",
      "Step 887, loss = [0.29559430480003357, 0.002654113806784153, 0.29532888531684875]\n",
      "Step 888, loss = [0.30280524492263794, 0.0056655751541256905, 0.30223867297172546]\n",
      "Step 889, loss = [0.28211545944213867, 0.0041463421657681465, 0.28170081973075867]\n",
      "Step 890, loss = [0.30977901816368103, 0.004741152282804251, 0.3093048930168152]\n",
      "Step 891, loss = [0.3017575740814209, 0.004300589673221111, 0.30132752656936646]\n",
      "Step 892, loss = [0.3011190891265869, 0.006362686865031719, 0.3004828095436096]\n",
      "Step 893, loss = [0.3014042377471924, 0.0047453478910028934, 0.3009296953678131]\n",
      "Step 894, loss = [0.2986401617527008, 0.003292654175311327, 0.29831090569496155]\n",
      "Step 895, loss = [0.302876740694046, 0.005625184625387192, 0.30231422185897827]\n",
      "Step 896, loss = [0.29759591817855835, 0.00788724236190319, 0.296807199716568]\n",
      "Step 897, loss = [0.3090345561504364, 0.004176615737378597, 0.30861690640449524]\n",
      "Step 898, loss = [0.3084980547428131, 0.0036793213803321123, 0.3081301152706146]\n",
      "Step 899, loss = [0.28294625878334045, 0.004502713214606047, 0.28249597549438477]\n",
      "Step 900, loss = [0.2950434684753418, 0.005136346444487572, 0.29452982544898987]\n",
      "Step 901, loss = [0.28913992643356323, 0.006759196519851685, 0.2884640097618103]\n",
      "Step 902, loss = [0.3107183575630188, 0.006372146308422089, 0.3100811541080475]\n",
      "Step 903, loss = [0.29984796047210693, 0.002323274500668049, 0.29961562156677246]\n",
      "Step 904, loss = [0.29815271496772766, 0.0031586564145982265, 0.2978368401527405]\n",
      "Step 905, loss = [0.30163535475730896, 0.00532657140865922, 0.3011026978492737]\n",
      "Step 906, loss = [0.2986363172531128, 0.002421643817797303, 0.2983941435813904]\n",
      "Step 907, loss = [0.3077094852924347, 0.004323866683989763, 0.307277113199234]\n",
      "Step 908, loss = [0.31757864356040955, 0.005049399100244045, 0.3170737028121948]\n",
      "Step 909, loss = [0.30472296476364136, 0.005643720272928476, 0.30415859818458557]\n",
      "Step 910, loss = [0.3057541847229004, 0.0038822959177196026, 0.3053659498691559]\n",
      "Step 911, loss = [0.30643734335899353, 0.003933011554181576, 0.3060440421104431]\n",
      "Step 912, loss = [0.31160202622413635, 0.0056914761662483215, 0.31103289127349854]\n",
      "Step 913, loss = [0.3147466480731964, 0.004060146398842335, 0.31434062123298645]\n",
      "Step 914, loss = [0.29894310235977173, 0.001505343709141016, 0.2987925708293915]\n",
      "Step 915, loss = [0.3012758791446686, 0.0051888530142605305, 0.3007569909095764]\n",
      "Step 916, loss = [0.2878716289997101, 0.0036558997817337513, 0.2875060439109802]\n",
      "Step 917, loss = [0.29689130187034607, 0.005531590897589922, 0.29633814096450806]\n",
      "Step 918, loss = [0.2971581816673279, 0.00200330326333642, 0.2969578504562378]\n",
      "Step 919, loss = [0.30202171206474304, 0.006205112207680941, 0.30140119791030884]\n",
      "Step 920, loss = [0.299141526222229, 0.00455595925450325, 0.2986859381198883]\n",
      "Step 921, loss = [0.28577104210853577, 0.0020196521654725075, 0.28556907176971436]\n",
      "Step 922, loss = [0.3021014630794525, 0.0033722128719091415, 0.30176424980163574]\n",
      "Step 923, loss = [0.29967406392097473, 0.007236424833536148, 0.2989504337310791]\n",
      "Step 924, loss = [0.2973947823047638, 0.0019144178368151188, 0.29720333218574524]\n",
      "Step 925, loss = [0.2731380760669708, 0.009257342666387558, 0.27221235632896423]\n",
      "Step 926, loss = [0.3128323256969452, 0.009167310781776905, 0.3119156062602997]\n",
      "Step 927, loss = [0.2844398021697998, 0.0036816319916397333, 0.2840716242790222]\n",
      "Step 928, loss = [0.3013780415058136, 0.005586831830441952, 0.30081936717033386]\n",
      "Step 929, loss = [0.30051833391189575, 0.002952813170850277, 0.30022305250167847]\n",
      "Step 930, loss = [0.2944733202457428, 0.006042643450200558, 0.2938690483570099]\n",
      "Step 931, loss = [0.2956719398498535, 0.0020258398726582527, 0.29546934366226196]\n",
      "Step 932, loss = [0.2852441966533661, 0.002466819016262889, 0.28499752283096313]\n",
      "Step 933, loss = [0.30974113941192627, 0.0010797020513564348, 0.30963316559791565]\n",
      "Step 934, loss = [0.31073546409606934, 0.0027952638920396566, 0.31045594811439514]\n",
      "Step 935, loss = [0.29029908776283264, 0.0045616100542247295, 0.2898429334163666]\n",
      "Step 936, loss = [0.30522263050079346, 0.0041800606995821, 0.30480462312698364]\n",
      "Step 937, loss = [0.30393603444099426, 0.004721358418464661, 0.3034639060497284]\n",
      "Step 938, loss = [0.31037670373916626, 0.003611127845942974, 0.31001558899879456]\n",
      "Step 939, loss = [0.2980564534664154, 0.006645113229751587, 0.29739195108413696]\n",
      "Step 940, loss = [0.2933867573738098, 0.004690614528954029, 0.2929176986217499]\n",
      "Step 941, loss = [0.3069632649421692, 0.004280434921383858, 0.3065352141857147]\n",
      "Step 942, loss = [0.3022940158843994, 0.003759448416531086, 0.30191805958747864]\n",
      "Step 943, loss = [0.2965834140777588, 0.007826335728168488, 0.2958007752895355]\n",
      "Step 944, loss = [0.30589956045150757, 0.004562061280012131, 0.30544334650039673]\n",
      "Step 945, loss = [0.28277063369750977, 0.006636005826294422, 0.28210702538490295]\n",
      "Step 946, loss = [0.3140466809272766, 0.0037386389449238777, 0.313672810792923]\n",
      "Step 947, loss = [0.3169333338737488, 0.006354993674904108, 0.31629782915115356]\n",
      "Step 948, loss = [0.30076587200164795, 0.005155655555427074, 0.3002502918243408]\n",
      "Step 949, loss = [0.2961561381816864, 0.005639770999550819, 0.29559215903282166]\n",
      "Step 950, loss = [0.2958570122718811, 0.002028299029916525, 0.29565417766571045]\n",
      "Step 951, loss = [0.30339616537094116, 0.0021066321060061455, 0.30318549275398254]\n",
      "Step 952, loss = [0.3093147873878479, 0.004068807233124971, 0.3089078962802887]\n",
      "Step 953, loss = [0.307199627161026, 0.005913802422583103, 0.30660825967788696]\n",
      "Step 954, loss = [0.2992546558380127, 0.0051427362486720085, 0.2987403869628906]\n",
      "Step 955, loss = [0.29296091198921204, 0.004233303479850292, 0.2925375699996948]\n",
      "Step 956, loss = [0.29610928893089294, 0.002800930757075548, 0.2958292067050934]\n",
      "Step 957, loss = [0.28668156266212463, 0.0032664951868355274, 0.2863548994064331]\n",
      "Step 958, loss = [0.2989360988140106, 0.003144799266010523, 0.29862162470817566]\n",
      "Step 959, loss = [0.2963734269142151, 0.005420753266662359, 0.2958313524723053]\n",
      "Step 960, loss = [0.29231807589530945, 0.004745888523757458, 0.2918434739112854]\n",
      "Step 961, loss = [0.3043367564678192, 0.005551820155233145, 0.30378156900405884]\n",
      "Step 962, loss = [0.2988448441028595, 0.005085017532110214, 0.29833635687828064]\n",
      "Step 963, loss = [0.2894451320171356, 0.005206290632486343, 0.28892451524734497]\n",
      "Step 964, loss = [0.3112143278121948, 0.005018915515393019, 0.31071242690086365]\n",
      "Step 965, loss = [0.31430089473724365, 0.004551664926111698, 0.3138457238674164]\n",
      "Step 966, loss = [0.3011530041694641, 0.00421186164021492, 0.3007318079471588]\n",
      "Step 967, loss = [0.30314210057258606, 0.0016280973795801401, 0.3029792904853821]\n",
      "Step 968, loss = [0.2929728627204895, 0.0058753956109285355, 0.2923853099346161]\n",
      "Step 969, loss = [0.2879027724266052, 0.005056033842265606, 0.287397176027298]\n",
      "Step 970, loss = [0.3022250831127167, 0.0047489795833826065, 0.30175018310546875]\n",
      "Step 971, loss = [0.2954355478286743, 0.0013835245044901967, 0.29529720544815063]\n",
      "Step 972, loss = [0.3045940697193146, 0.003664447460323572, 0.3042276203632355]\n",
      "Step 973, loss = [0.3039281666278839, 0.004051012918353081, 0.30352306365966797]\n",
      "Step 974, loss = [0.29613643884658813, 0.0019504192750900984, 0.29594138264656067]\n",
      "Step 975, loss = [0.29481950402259827, 0.004983997903764248, 0.29432108998298645]\n",
      "Step 976, loss = [0.27797597646713257, 0.006130760535597801, 0.2773629128932953]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11754 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 977, loss = [0.2754249572753906, 0.004829030483961105, 0.2749420404434204]\n",
      "Update target distribution epoch 1 step 978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11754/11754 [1:50:55<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 978, loss = [0.2942192554473877, 0.005992645397782326, 0.2936199903488159]\n",
      "Step 979, loss = [0.2819516360759735, 0.006045473739504814, 0.2813470959663391]\n",
      "Step 980, loss = [0.2938497066497803, 0.006667150184512138, 0.29318299889564514]\n",
      "Step 981, loss = [0.30570659041404724, 0.004970535635948181, 0.30520954728126526]\n",
      "Step 982, loss = [0.28577059507369995, 0.0009125305223278701, 0.28567934036254883]\n",
      "Step 983, loss = [0.3003879189491272, 0.00584600493311882, 0.29980331659317017]\n",
      "Step 984, loss = [0.3017997145652771, 0.005348269362002611, 0.3012648820877075]\n",
      "Step 985, loss = [0.3108314573764801, 0.0033531461376696825, 0.31049615144729614]\n",
      "Step 986, loss = [0.29579055309295654, 0.004670096095651388, 0.29532355070114136]\n",
      "Step 987, loss = [0.29591643810272217, 0.003651042003184557, 0.2955513298511505]\n",
      "Step 988, loss = [0.3166084289550781, 0.004952377639710903, 0.3161132037639618]\n",
      "Step 989, loss = [0.2970319390296936, 0.001837313873693347, 0.29684820771217346]\n",
      "Step 990, loss = [0.30294373631477356, 0.0042664543725550175, 0.3025170862674713]\n",
      "Step 991, loss = [0.30502569675445557, 0.003941293340176344, 0.3046315610408783]\n",
      "Step 992, loss = [0.2990323007106781, 0.005899013020098209, 0.29844239354133606]\n",
      "Step 993, loss = [0.27243533730506897, 0.004884514957666397, 0.27194687724113464]\n",
      "Step 994, loss = [0.3109860420227051, 0.0013556204503402114, 0.31085047125816345]\n",
      "Step 995, loss = [0.30908241868019104, 0.002509678015485406, 0.30883145332336426]\n",
      "Step 996, loss = [0.3040635585784912, 0.003590709064155817, 0.30370450019836426]\n",
      "Step 997, loss = [0.3085613250732422, 0.0014697469305247068, 0.3084143400192261]\n",
      "Step 998, loss = [0.3064073920249939, 0.0033324151299893856, 0.3060741424560547]\n",
      "Step 999, loss = [0.2888721823692322, 0.0029256860725581646, 0.2885796129703522]\n",
      "Step 1000, loss = [0.29929062724113464, 0.0031636131461709738, 0.29897427558898926]\n",
      "Step 1001, loss = [0.30054837465286255, 0.007447920739650726, 0.29980358481407166]\n",
      "Step 1002, loss = [0.2925923764705658, 0.005249675363302231, 0.29206740856170654]\n",
      "Step 1003, loss = [0.2994072437286377, 0.0031245348509401083, 0.2990947961807251]\n",
      "Step 1004, loss = [0.2970477342605591, 0.0036653554998338223, 0.2966811954975128]\n",
      "Step 1005, loss = [0.29724597930908203, 0.0020611919462680817, 0.29703986644744873]\n",
      "Step 1006, loss = [0.2997048795223236, 0.004777902737259865, 0.2992270886898041]\n",
      "Step 1007, loss = [0.2861984968185425, 0.002182312309741974, 0.2859802544116974]\n",
      "Step 1008, loss = [0.30326157808303833, 0.004965933505445719, 0.30276498198509216]\n",
      "Step 1009, loss = [0.3013972342014313, 0.005540958605706692, 0.30084314942359924]\n",
      "Step 1010, loss = [0.2841916084289551, 0.0048097874969244, 0.28371062874794006]\n",
      "Step 1011, loss = [0.3046855330467224, 0.005201795604079962, 0.3041653633117676]\n",
      "Step 1012, loss = [0.2889799177646637, 0.0059598106890916824, 0.28838393092155457]\n",
      "Step 1013, loss = [0.2996426224708557, 0.0032932518515735865, 0.2993133068084717]\n",
      "Step 1014, loss = [0.29454126954078674, 0.003588515566661954, 0.2941824197769165]\n",
      "Step 1015, loss = [0.3073123097419739, 0.00450355838984251, 0.3068619668483734]\n",
      "Step 1016, loss = [0.3042062520980835, 0.00355291785672307, 0.3038509488105774]\n",
      "Step 1017, loss = [0.30034691095352173, 0.004148261621594429, 0.2999320924282074]\n",
      "Step 1018, loss = [0.300822377204895, 0.004322275519371033, 0.3003901541233063]\n",
      "Step 1019, loss = [0.2916404604911804, 0.006844285875558853, 0.2909560203552246]\n",
      "Step 1020, loss = [0.31326669454574585, 0.004810068756341934, 0.31278568506240845]\n",
      "Step 1021, loss = [0.29062336683273315, 0.004430246539413929, 0.29018035531044006]\n",
      "Step 1022, loss = [0.3039810359477997, 0.003964981064200401, 0.3035845458507538]\n",
      "Step 1023, loss = [0.2936933934688568, 0.0032996186055243015, 0.29336342215538025]\n",
      "Step 1024, loss = [0.3018134534358978, 0.0076814256608486176, 0.301045298576355]\n",
      "Step 1025, loss = [0.27681055665016174, 0.008182428777217865, 0.2759923040866852]\n",
      "Step 1026, loss = [0.30303043127059937, 0.002983294427394867, 0.30273211002349854]\n",
      "Step 1027, loss = [0.2970220744609833, 0.0024218682665377855, 0.29677990078926086]\n",
      "Step 1028, loss = [0.30143317580223083, 0.006383588537573814, 0.3007948100566864]\n",
      "Step 1029, loss = [0.29914069175720215, 0.00633736839517951, 0.2985069453716278]\n",
      "Step 1030, loss = [0.31110215187072754, 0.0038147279992699623, 0.31072068214416504]\n",
      "Step 1031, loss = [0.3075493276119232, 0.004789111204445362, 0.30707040429115295]\n",
      "Step 1032, loss = [0.2964131534099579, 0.0048162806779146194, 0.29593151807785034]\n",
      "Step 1033, loss = [0.2975357472896576, 0.002779785543680191, 0.29725778102874756]\n",
      "Step 1034, loss = [0.2981199026107788, 0.0067429873161017895, 0.2974455952644348]\n",
      "Step 1035, loss = [0.2847880721092224, 0.003624226897954941, 0.28442564606666565]\n",
      "Step 1036, loss = [0.3005230128765106, 0.00833891797810793, 0.2996891140937805]\n",
      "Step 1037, loss = [0.29505324363708496, 0.004103481769561768, 0.2946428954601288]\n",
      "Step 1038, loss = [0.2921694219112396, 0.005508318077772856, 0.29161858558654785]\n",
      "Step 1039, loss = [0.30620306730270386, 0.0052360352128744125, 0.30567947030067444]\n",
      "Step 1040, loss = [0.30692219734191895, 0.005355407949537039, 0.30638664960861206]\n",
      "Step 1041, loss = [0.29550305008888245, 0.010585442185401917, 0.2944445013999939]\n",
      "Step 1042, loss = [0.28821608424186707, 0.005052077118307352, 0.28771087527275085]\n",
      "Step 1043, loss = [0.28311988711357117, 0.0016706022433936596, 0.28295281529426575]\n",
      "Step 1044, loss = [0.28584781289100647, 0.004351167939603329, 0.2854126989841461]\n",
      "Step 1045, loss = [0.3074958920478821, 0.004172661807388067, 0.30707862973213196]\n",
      "Step 1046, loss = [0.29735955595970154, 0.001381510286591947, 0.2972213923931122]\n",
      "Step 1047, loss = [0.3134728670120239, 0.0028562801890075207, 0.31318724155426025]\n",
      "Step 1048, loss = [0.2906923294067383, 0.0019425093196332455, 0.2904980778694153]\n",
      "Step 1049, loss = [0.30569636821746826, 0.0037259371019899845, 0.3053237795829773]\n",
      "Step 1050, loss = [0.2845260798931122, 0.005206340923905373, 0.28400543332099915]\n",
      "Step 1051, loss = [0.30166441202163696, 0.006394367665052414, 0.3010249733924866]\n",
      "Step 1052, loss = [0.3077012002468109, 0.0036512825172394514, 0.30733606219291687]\n",
      "Step 1053, loss = [0.2899306118488312, 0.006027190946042538, 0.2893278896808624]\n",
      "Step 1054, loss = [0.30500108003616333, 0.004186801612377167, 0.3045823872089386]\n",
      "Step 1055, loss = [0.30778539180755615, 0.0028331857174634933, 0.30750206112861633]\n",
      "Step 1056, loss = [0.3039073348045349, 0.006157515570521355, 0.30329158902168274]\n",
      "Step 1057, loss = [0.2906959354877472, 0.0037255208007991314, 0.2903233766555786]\n",
      "Step 1058, loss = [0.30449458956718445, 0.002428557490929961, 0.3042517304420471]\n",
      "Step 1059, loss = [0.3010079264640808, 0.006272658705711365, 0.3003806471824646]\n",
      "Step 1060, loss = [0.2911948263645172, 0.004125111736357212, 0.29078230261802673]\n",
      "Step 1061, loss = [0.30520421266555786, 0.006902182474732399, 0.30451399087905884]\n",
      "Step 1062, loss = [0.31099405884742737, 0.00521097332239151, 0.3104729652404785]\n",
      "Step 1063, loss = [0.3046400547027588, 0.0035274657420814037, 0.304287314414978]\n",
      "Step 1064, loss = [0.2832161784172058, 0.0020562850404530764, 0.2830105423927307]\n",
      "Step 1065, loss = [0.2929914891719818, 0.004329292103648186, 0.29255855083465576]\n",
      "Step 1066, loss = [0.27657046914100647, 0.0029667457565665245, 0.27627378702163696]\n",
      "Step 1067, loss = [0.3073354661464691, 0.004465878009796143, 0.3068888783454895]\n",
      "Step 1068, loss = [0.29364636540412903, 0.004809512756764889, 0.2931654155254364]\n",
      "Step 1069, loss = [0.30174320936203003, 0.004302579909563065, 0.30131295323371887]\n",
      "Step 1070, loss = [0.30711910128593445, 0.004076486453413963, 0.30671146512031555]\n",
      "Step 1071, loss = [0.2972540855407715, 0.002564858878031373, 0.296997606754303]\n",
      "Step 1072, loss = [0.31332436203956604, 0.006464011501520872, 0.31267794966697693]\n",
      "Step 1073, loss = [0.31248021125793457, 0.00485893152654171, 0.3119943141937256]\n",
      "Step 1074, loss = [0.2996141016483307, 0.005132455378770828, 0.2991008460521698]\n",
      "Step 1075, loss = [0.28967201709747314, 0.004771915730088949, 0.28919482231140137]\n",
      "Step 1076, loss = [0.3110824227333069, 0.004358281381428242, 0.31064659357070923]\n",
      "Step 1077, loss = [0.3117448389530182, 0.00843566469848156, 0.3109012842178345]\n",
      "Step 1078, loss = [0.302062451839447, 0.004082404542714357, 0.30165421962738037]\n",
      "Step 1079, loss = [0.2933748662471771, 0.003311976557597518, 0.29304367303848267]\n",
      "Step 1080, loss = [0.2970835566520691, 0.0032551412004977465, 0.2967580556869507]\n",
      "Step 1081, loss = [0.3010314404964447, 0.006823284551501274, 0.3003491163253784]\n",
      "Step 1082, loss = [0.28115740418434143, 0.0035854103043675423, 0.28079885244369507]\n",
      "Step 1083, loss = [0.3144194185733795, 0.004639314487576485, 0.31395548582077026]\n",
      "Step 1084, loss = [0.29363274574279785, 0.0029496699571609497, 0.29333779215812683]\n",
      "Step 1085, loss = [0.2993957996368408, 0.004092120565474033, 0.2989865839481354]\n",
      "Step 1086, loss = [0.30010637640953064, 0.003020459320396185, 0.29980432987213135]\n",
      "Step 1087, loss = [0.2946409285068512, 0.004230015445500612, 0.29421791434288025]\n",
      "Step 1088, loss = [0.29429975152015686, 0.005661021918058395, 0.2937336564064026]\n",
      "Step 1089, loss = [0.30728793144226074, 0.002046876819804311, 0.30708324909210205]\n",
      "Step 1090, loss = [0.30340221524238586, 0.004703743848949671, 0.30293184518814087]\n",
      "Step 1091, loss = [0.2947773039340973, 0.008664408698678017, 0.2939108610153198]\n",
      "Step 1092, loss = [0.30981674790382385, 0.007541200611740351, 0.3090626299381256]\n",
      "Step 1093, loss = [0.30550822615623474, 0.003720541251823306, 0.30513617396354675]\n",
      "Step 1094, loss = [0.30664366483688354, 0.004595712758600712, 0.3061840832233429]\n",
      "Step 1095, loss = [0.2852713167667389, 0.0035817334428429604, 0.28491315245628357]\n",
      "Step 1096, loss = [0.29687032103538513, 0.00442415289580822, 0.2964279055595398]\n",
      "Step 1097, loss = [0.29520183801651, 0.0034464015625417233, 0.2948572039604187]\n",
      "Step 1098, loss = [0.296890527009964, 0.006330202333629131, 0.29625749588012695]\n",
      "Step 1099, loss = [0.295611172914505, 0.0035584052093327045, 0.2952553331851959]\n",
      "Step 1100, loss = [0.29452818632125854, 0.003338402835652232, 0.2941943407058716]\n",
      "Step 1101, loss = [0.30613595247268677, 0.005010715685784817, 0.30563488602638245]\n",
      "Step 1102, loss = [0.2992934584617615, 0.0028841570019721985, 0.29900503158569336]\n",
      "Step 1103, loss = [0.2871089279651642, 0.006701210513710976, 0.28643879294395447]\n",
      "Step 1104, loss = [0.3076136112213135, 0.0034032175317406654, 0.3072732985019684]\n",
      "Step 1105, loss = [0.29399535059928894, 0.0025604735128581524, 0.29373928904533386]\n",
      "Step 1106, loss = [0.27924472093582153, 0.002645618747919798, 0.27898016571998596]\n",
      "Step 1107, loss = [0.2943924367427826, 0.0063460529781877995, 0.293757826089859]\n",
      "Step 1108, loss = [0.29007959365844727, 0.0038383174687623978, 0.28969576954841614]\n",
      "Step 1109, loss = [0.29636162519454956, 0.005395182408392429, 0.2958221137523651]\n",
      "Step 1110, loss = [0.29906126856803894, 0.003966601565480232, 0.2986645996570587]\n",
      "Step 1111, loss = [0.3009065091609955, 0.00608390336856246, 0.30029812455177307]\n",
      "Step 1112, loss = [0.2883300483226776, 0.0034280987456440926, 0.28798723220825195]\n",
      "Step 1113, loss = [0.2885685861110687, 0.0034872579853981733, 0.2882198691368103]\n",
      "Step 1114, loss = [0.3125084936618805, 0.005751867778599262, 0.311933308839798]\n",
      "Step 1115, loss = [0.29753798246383667, 0.006157988682389259, 0.2969221770763397]\n",
      "Step 1116, loss = [0.2970923185348511, 0.004537692293524742, 0.296638548374176]\n",
      "Step 1117, loss = [0.30851247906684875, 0.0041344668716192245, 0.30809903144836426]\n",
      "Step 1118, loss = [0.3107767105102539, 0.006068017333745956, 0.31016990542411804]\n",
      "Step 1119, loss = [0.30235201120376587, 0.0025664332788437605, 0.30209535360336304]\n",
      "Step 1120, loss = [0.3061271011829376, 0.004252624697983265, 0.3057018518447876]\n",
      "Step 1121, loss = [0.3136301338672638, 0.006439821794629097, 0.3129861652851105]\n",
      "Step 1122, loss = [0.31008461117744446, 0.005105087533593178, 0.30957409739494324]\n",
      "Step 1123, loss = [0.30516862869262695, 0.00324252899736166, 0.30484437942504883]\n",
      "Step 1124, loss = [0.3091161549091339, 0.0020430749282240868, 0.30891185998916626]\n",
      "Step 1125, loss = [0.3021151125431061, 0.0070851873606443405, 0.301406592130661]\n",
      "Step 1126, loss = [0.29979968070983887, 0.001938543515279889, 0.2996058166027069]\n",
      "Step 1127, loss = [0.2830434739589691, 0.003379142377525568, 0.28270554542541504]\n",
      "Step 1128, loss = [0.2941746711730957, 0.0023173599038273096, 0.293942928314209]\n",
      "Step 1129, loss = [0.29783862829208374, 0.004028841853141785, 0.2974357306957245]\n",
      "Step 1130, loss = [0.2999594211578369, 0.00474888738244772, 0.299484521150589]\n",
      "Step 1131, loss = [0.3096984326839447, 0.006628683768212795, 0.3090355694293976]\n",
      "Step 1132, loss = [0.2973238527774811, 0.0032969899475574493, 0.296994149684906]\n",
      "Step 1133, loss = [0.30051547288894653, 0.0039470000192523, 0.3001207709312439]\n",
      "Step 1134, loss = [0.3085838854312897, 0.006331147626042366, 0.3079507648944855]\n",
      "Step 1135, loss = [0.2937925457954407, 0.0018897122936323285, 0.2936035692691803]\n",
      "Step 1136, loss = [0.2985483705997467, 0.0043443795293569565, 0.29811394214630127]\n",
      "Step 1137, loss = [0.2905106842517853, 0.002401983365416527, 0.29027047753334045]\n",
      "Step 1138, loss = [0.29891252517700195, 0.003945900592952967, 0.29851794242858887]\n",
      "Step 1139, loss = [0.30362436175346375, 0.0012212207075208426, 0.30350223183631897]\n",
      "Step 1140, loss = [0.29259324073791504, 0.0055108871310949326, 0.29204216599464417]\n",
      "Step 1141, loss = [0.28821033239364624, 0.003113599494099617, 0.2878989577293396]\n",
      "Step 1142, loss = [0.30615559220314026, 0.004291355609893799, 0.30572646856307983]\n",
      "Step 1143, loss = [0.29874929785728455, 0.0031965619418770075, 0.2984296381473541]\n",
      "Step 1144, loss = [0.29714253544807434, 0.002534635365009308, 0.296889066696167]\n",
      "Step 1145, loss = [0.29982537031173706, 0.00453819427639246, 0.29937154054641724]\n",
      "Step 1146, loss = [0.2992628514766693, 0.0029774336144328117, 0.29896509647369385]\n",
      "Step 1147, loss = [0.3079648017883301, 0.003827918553724885, 0.3075820207595825]\n",
      "Step 1148, loss = [0.3122914135456085, 0.003923606127500534, 0.3118990659713745]\n",
      "Step 1149, loss = [0.3090710937976837, 0.006587457377463579, 0.3084123432636261]\n",
      "Step 1150, loss = [0.294847309589386, 0.005854165181517601, 0.2942619025707245]\n",
      "Step 1151, loss = [0.3006497621536255, 0.0036923708394169807, 0.30028051137924194]\n",
      "Step 1152, loss = [0.31288662552833557, 0.004670687485486269, 0.3124195635318756]\n",
      "Step 1153, loss = [0.308980256319046, 0.0032644043676555157, 0.3086538016796112]\n",
      "Step 1154, loss = [0.30051371455192566, 0.00301283854059875, 0.30021244287490845]\n",
      "Step 1155, loss = [0.2989388704299927, 0.0033860518597066402, 0.2986002564430237]\n",
      "Step 1156, loss = [0.2869100868701935, 0.005747504532337189, 0.2863353490829468]\n",
      "Step 1157, loss = [0.2996431887149811, 0.004148814361542463, 0.299228310585022]\n",
      "Step 1158, loss = [0.28890863060951233, 0.007259417325258255, 0.28818267583847046]\n",
      "Step 1159, loss = [0.2967551648616791, 0.004787792451679707, 0.29627639055252075]\n",
      "Step 1160, loss = [0.3098565340042114, 0.004037123639136553, 0.3094528317451477]\n",
      "Step 1161, loss = [0.2950894832611084, 0.005027442239224911, 0.29458674788475037]\n",
      "Step 1162, loss = [0.30846384167671204, 0.0029089986346662045, 0.30817294120788574]\n",
      "Step 1163, loss = [0.2877710163593292, 0.003948593512177467, 0.28737616539001465]\n",
      "Step 1164, loss = [0.3069785237312317, 0.004678851459175348, 0.3065106272697449]\n",
      "Step 1165, loss = [0.30131009221076965, 0.003596802009269595, 0.30095040798187256]\n",
      "Step 1166, loss = [0.29412373900413513, 0.004088191315531731, 0.2937149107456207]\n",
      "Step 1167, loss = [0.3030320703983307, 0.005744833964854479, 0.3024576008319855]\n",
      "Step 1168, loss = [0.300625741481781, 0.002371931681409478, 0.30038854479789734]\n",
      "Step 1169, loss = [0.28695616126060486, 0.005529598332941532, 0.28640320897102356]\n",
      "Step 1170, loss = [0.28889068961143494, 0.006516584195196629, 0.2882390320301056]\n",
      "Step 1171, loss = [0.3003745675086975, 0.0018096185522153974, 0.3001936078071594]\n",
      "Step 1172, loss = [0.30452224612236023, 0.0028137555345892906, 0.304240882396698]\n",
      "Step 1173, loss = [0.2917700707912445, 0.004300741478800774, 0.2913399934768677]\n",
      "Step 1174, loss = [0.30963385105133057, 0.006134744733572006, 0.30902037024497986]\n",
      "Step 1175, loss = [0.295026957988739, 0.0046801017597317696, 0.29455894231796265]\n",
      "Step 1176, loss = [0.28728193044662476, 0.00399460643529892, 0.2868824601173401]\n",
      "Step 1177, loss = [0.3117017149925232, 0.004019696731120348, 0.31129974126815796]\n",
      "Step 1178, loss = [0.2866381108760834, 0.005951352417469025, 0.2860429883003235]\n",
      "Step 1179, loss = [0.30610206723213196, 0.006170600652694702, 0.3054850101470947]\n",
      "Step 1180, loss = [0.3058824837207794, 0.004419142380356789, 0.3054405748844147]\n",
      "Step 1181, loss = [0.30488425493240356, 0.002621551975607872, 0.3046221137046814]\n",
      "Step 1182, loss = [0.28814950585365295, 0.009135700762271881, 0.28723594546318054]\n",
      "Step 1183, loss = [0.28538694977760315, 0.00375111261382699, 0.28501182794570923]\n",
      "Step 1184, loss = [0.30903348326683044, 0.0016191513277590275, 0.3088715672492981]\n",
      "Step 1185, loss = [0.3012697994709015, 0.0035031577572226524, 0.30091947317123413]\n",
      "Step 1186, loss = [0.3033362627029419, 0.002986634150147438, 0.3030376136302948]\n",
      "Step 1187, loss = [0.28723788261413574, 0.0031074052676558495, 0.28692713379859924]\n",
      "Step 1188, loss = [0.3133362829685211, 0.003193357028067112, 0.31301695108413696]\n",
      "Step 1189, loss = [0.306995689868927, 0.0037880265153944492, 0.306616872549057]\n",
      "Step 1190, loss = [0.3079143166542053, 0.007431528065353632, 0.30717116594314575]\n",
      "Step 1191, loss = [0.2954416871070862, 0.007789346389472485, 0.294662743806839]\n",
      "Step 1192, loss = [0.2792753577232361, 0.005182372406125069, 0.27875712513923645]\n",
      "Step 1193, loss = [0.3037826716899872, 0.0016283262521028519, 0.3036198318004608]\n",
      "Step 1194, loss = [0.2991221249103546, 0.0020314513240009546, 0.2989189922809601]\n",
      "Step 1195, loss = [0.30108121037483215, 0.004003341309726238, 0.30068087577819824]\n",
      "Step 1196, loss = [0.3061271011829376, 0.003553105751052499, 0.3057717978954315]\n",
      "Step 1197, loss = [0.29336440563201904, 0.005880843847990036, 0.29277631640434265]\n",
      "Step 1198, loss = [0.3017348647117615, 0.006010711193084717, 0.30113378167152405]\n",
      "Step 1199, loss = [0.30513909459114075, 0.0055542318150401115, 0.30458366870880127]\n",
      "Step 1200, loss = [0.31313759088516235, 0.004747459664940834, 0.31266283988952637]\n",
      "Step 1201, loss = [0.28559327125549316, 0.004490642808377743, 0.28514420986175537]\n",
      "Step 1202, loss = [0.3101120889186859, 0.00582620594650507, 0.30952945351600647]\n",
      "Step 1203, loss = [0.3011808395385742, 0.005696876440197229, 0.30061113834381104]\n",
      "Step 1204, loss = [0.30995801091194153, 0.002901117317378521, 0.3096678853034973]\n",
      "Step 1205, loss = [0.3109096884727478, 0.0044130850583314896, 0.3104683756828308]\n",
      "Step 1206, loss = [0.29011592268943787, 0.004161075688898563, 0.28969982266426086]\n",
      "Step 1207, loss = [0.3043860197067261, 0.0023245695047080517, 0.30415356159210205]\n",
      "Step 1208, loss = [0.302462100982666, 0.0025143721140921116, 0.30221065878868103]\n",
      "Step 1209, loss = [0.2949877083301544, 0.005098653957247734, 0.2944778501987457]\n",
      "Step 1210, loss = [0.3030030131340027, 0.0033185312058776617, 0.3026711642742157]\n",
      "Step 1211, loss = [0.29110729694366455, 0.0064628152176737785, 0.290461003780365]\n",
      "Step 1212, loss = [0.30555835366249084, 0.0033072505611926317, 0.3052276372909546]\n",
      "Step 1213, loss = [0.2876411974430084, 0.004651919938623905, 0.2871760129928589]\n",
      "Step 1214, loss = [0.2848391532897949, 0.0024272371083498, 0.28459644317626953]\n",
      "Step 1215, loss = [0.307134747505188, 0.0035986006259918213, 0.30677488446235657]\n",
      "Step 1216, loss = [0.27707618474960327, 0.0037003327161073685, 0.27670615911483765]\n",
      "Step 1217, loss = [0.3067505359649658, 0.002499483060091734, 0.3065005838871002]\n",
      "Step 1218, loss = [0.2892857491970062, 0.002810033969581127, 0.28900474309921265]\n",
      "Step 1219, loss = [0.3034495711326599, 0.006559187080711126, 0.3027936518192291]\n",
      "Step 1220, loss = [0.3161318898200989, 0.00481783552095294, 0.3156501054763794]\n",
      "Step 1221, loss = [0.28555360436439514, 0.004876359365880489, 0.28506597876548767]\n",
      "Step 1222, loss = [0.2960869073867798, 0.004816319327801466, 0.29560527205467224]\n",
      "Step 1223, loss = [0.2831854224205017, 0.005084339529275894, 0.2826769948005676]\n",
      "Step 1224, loss = [0.30948808789253235, 0.0019388822838664055, 0.309294193983078]\n",
      "Step 1225, loss = [0.307320237159729, 0.004593327641487122, 0.30686089396476746]\n",
      "Step 1226, loss = [0.29916873574256897, 0.0030041446443647146, 0.298868328332901]\n",
      "Step 1227, loss = [0.2942534387111664, 0.003013188950717449, 0.2939521074295044]\n",
      "Step 1228, loss = [0.3046450912952423, 0.003741889027878642, 0.3042708933353424]\n",
      "Step 1229, loss = [0.29589250683784485, 0.004991794005036354, 0.29539331793785095]\n",
      "Step 1230, loss = [0.2966064214706421, 0.003586574923247099, 0.2962477505207062]\n",
      "Step 1231, loss = [0.2814635932445526, 0.005481778644025326, 0.28091540932655334]\n",
      "Step 1232, loss = [0.31205877661705017, 0.002393945585936308, 0.3118193745613098]\n",
      "Step 1233, loss = [0.30652496218681335, 0.0027421903796494007, 0.30625075101852417]\n",
      "Step 1234, loss = [0.3064118027687073, 0.007307396270334721, 0.305681049823761]\n",
      "Step 1235, loss = [0.3093738257884979, 0.006155600771307945, 0.3087582588195801]\n",
      "Step 1236, loss = [0.29441624879837036, 0.004710319451987743, 0.29394522309303284]\n",
      "Step 1237, loss = [0.29833248257637024, 0.005272946320474148, 0.29780519008636475]\n",
      "Step 1238, loss = [0.3007119596004486, 0.003933910746127367, 0.30031856894493103]\n",
      "Step 1239, loss = [0.3099590241909027, 0.00501431617885828, 0.30945760011672974]\n",
      "Step 1240, loss = [0.30604907870292664, 0.0037954538129270077, 0.30566954612731934]\n",
      "Step 1241, loss = [0.2963048815727234, 0.005431857891380787, 0.29576170444488525]\n",
      "Step 1242, loss = [0.305326908826828, 0.0026670913212001324, 0.3050602078437805]\n",
      "Step 1243, loss = [0.30932286381721497, 0.004485339857637882, 0.30887433886528015]\n",
      "Step 1244, loss = [0.29863929748535156, 0.00486663356423378, 0.2981526255607605]\n",
      "Step 1245, loss = [0.297404408454895, 0.0061349449679255486, 0.2967909276485443]\n",
      "Step 1246, loss = [0.31224218010902405, 0.001988731324672699, 0.31204330921173096]\n",
      "Step 1247, loss = [0.3069583773612976, 0.006147843785583973, 0.30634358525276184]\n",
      "Step 1248, loss = [0.29703664779663086, 0.0025256802327930927, 0.29678407311439514]\n",
      "Step 1249, loss = [0.3023183345794678, 0.0059590209275484085, 0.3017224371433258]\n",
      "Step 1250, loss = [0.3035293221473694, 0.004750764928758144, 0.30305424332618713]\n",
      "Step 1251, loss = [0.2975073754787445, 0.005971182603389025, 0.29691025614738464]\n",
      "Step 1252, loss = [0.31692102551460266, 0.0059249019250273705, 0.3163285255432129]\n",
      "Step 1253, loss = [0.29234763979911804, 0.0019107013940811157, 0.2921565771102905]\n",
      "Step 1254, loss = [0.2974727749824524, 0.0024415089283138514, 0.2972286343574524]\n",
      "Step 1255, loss = [0.29750701785087585, 0.005995331797748804, 0.2969074845314026]\n",
      "Step 1256, loss = [0.2892039120197296, 0.007288313936442137, 0.28847506642341614]\n",
      "Step 1257, loss = [0.2959251403808594, 0.0024022143334150314, 0.29568493366241455]\n",
      "Step 1258, loss = [0.2949567139148712, 0.0072779348120093346, 0.2942289113998413]\n",
      "Step 1259, loss = [0.29773157835006714, 0.005313870497047901, 0.29720020294189453]\n",
      "Step 1260, loss = [0.28545188903808594, 0.005031281150877476, 0.28494876623153687]\n",
      "Step 1261, loss = [0.2998298108577728, 0.0035514661576598883, 0.29947465658187866]\n",
      "Step 1262, loss = [0.28984394669532776, 0.002981324214488268, 0.28954580426216125]\n",
      "Step 1263, loss = [0.3039912283420563, 0.00567278265953064, 0.3034239411354065]\n",
      "Step 1264, loss = [0.28510725498199463, 0.0037302281707525253, 0.28473421931266785]\n",
      "Step 1265, loss = [0.2929060459136963, 0.0047347997315227985, 0.292432576417923]\n",
      "Step 1266, loss = [0.2963542640209198, 0.0035566254518926144, 0.29599860310554504]\n",
      "Step 1267, loss = [0.29877275228500366, 0.004317226354032755, 0.2983410358428955]\n",
      "Step 1268, loss = [0.30218055844306946, 0.004199403338134289, 0.30176061391830444]\n",
      "Step 1269, loss = [0.3019227981567383, 0.0018088161014020443, 0.30174192786216736]\n",
      "Step 1270, loss = [0.3027447760105133, 0.004604461137205362, 0.3022843301296234]\n",
      "Step 1271, loss = [0.3005322813987732, 0.001489354413934052, 0.3003833591938019]\n",
      "Step 1272, loss = [0.2922423183917999, 0.007760755252093077, 0.29146623611450195]\n",
      "Step 1273, loss = [0.29966357350349426, 0.009418448433279991, 0.2987217307090759]\n",
      "Step 1274, loss = [0.307583212852478, 0.004625250585377216, 0.307120680809021]\n",
      "Step 1275, loss = [0.3051398992538452, 0.004489643964916468, 0.3046909272670746]\n",
      "Step 1276, loss = [0.30059653520584106, 0.005414596758782864, 0.3000550866127014]\n",
      "Step 1277, loss = [0.29600992798805237, 0.0019521056674420834, 0.29581472277641296]\n",
      "Step 1278, loss = [0.30692732334136963, 0.005663108080625534, 0.30636101961135864]\n",
      "Step 1279, loss = [0.2921006679534912, 0.006986585911363363, 0.29140201210975647]\n",
      "Step 1280, loss = [0.3112632632255554, 0.005336337722837925, 0.31072962284088135]\n",
      "Step 1281, loss = [0.30241116881370544, 0.0068246908485889435, 0.3017286956310272]\n",
      "Step 1282, loss = [0.30725768208503723, 0.004588023293763399, 0.30679887533187866]\n",
      "Step 1283, loss = [0.30389291048049927, 0.006671093869954348, 0.3032258152961731]\n",
      "Step 1284, loss = [0.2886003255844116, 0.0033521405421197414, 0.2882651090621948]\n",
      "Step 1285, loss = [0.303681880235672, 0.0040398407727479935, 0.3032779097557068]\n",
      "Step 1286, loss = [0.3084683120250702, 0.0038247518241405487, 0.3080858290195465]\n",
      "Step 1287, loss = [0.3032709062099457, 0.0020634077955037355, 0.3030645549297333]\n",
      "Step 1288, loss = [0.2965104281902313, 0.007222490850836039, 0.2957881689071655]\n",
      "Step 1289, loss = [0.29453322291374207, 0.0035991729237139225, 0.29417330026626587]\n",
      "Step 1290, loss = [0.30103257298469543, 0.004157920368015766, 0.3006167709827423]\n",
      "Step 1291, loss = [0.2988507151603699, 0.002327132038772106, 0.29861798882484436]\n",
      "Step 1292, loss = [0.3015497624874115, 0.004122053273022175, 0.3011375665664673]\n",
      "Step 1293, loss = [0.29223424196243286, 0.0029915175400674343, 0.2919350862503052]\n",
      "Step 1294, loss = [0.30734536051750183, 0.0034378557465970516, 0.3070015609264374]\n",
      "Step 1295, loss = [0.3039519488811493, 0.003599904477596283, 0.3035919666290283]\n",
      "Step 1296, loss = [0.30837568640708923, 0.003983858972787857, 0.3079772889614105]\n",
      "Step 1297, loss = [0.2747623026371002, 0.005593531299382448, 0.27420294284820557]\n",
      "Step 1298, loss = [0.2944691777229309, 0.003603555727750063, 0.2941088080406189]\n",
      "Step 1299, loss = [0.2984922528266907, 0.005572330206632614, 0.29793500900268555]\n",
      "Step 1300, loss = [0.30922332406044006, 0.001898316666483879, 0.30903348326683044]\n",
      "Step 1301, loss = [0.30933770537376404, 0.004068755079060793, 0.3089308440685272]\n",
      "Step 1302, loss = [0.301331102848053, 0.006912908982485533, 0.300639808177948]\n",
      "Step 1303, loss = [0.3174527585506439, 0.004171255510300398, 0.31703564524650574]\n",
      "Step 1304, loss = [0.2907091975212097, 0.003720741719007492, 0.29033711552619934]\n",
      "Step 1305, loss = [0.29578739404678345, 0.0025132806040346622, 0.295536071062088]\n",
      "Step 1306, loss = [0.30965670943260193, 0.006833343766629696, 0.30897337198257446]\n",
      "Step 1307, loss = [0.30358821153640747, 0.004335995297878981, 0.3031546175479889]\n",
      "Step 1308, loss = [0.31271398067474365, 0.004522101022303104, 0.31226176023483276]\n",
      "Step 1309, loss = [0.3050563633441925, 0.004428412299603224, 0.30461353063583374]\n",
      "Step 1310, loss = [0.30402249097824097, 0.006130035035312176, 0.30340948700904846]\n",
      "Step 1311, loss = [0.30091044306755066, 0.002996225608512759, 0.30061081051826477]\n",
      "Step 1312, loss = [0.30395379662513733, 0.004366644658148289, 0.3035171329975128]\n",
      "Step 1313, loss = [0.30363354086875916, 0.0034943572245538235, 0.3032841086387634]\n",
      "Step 1314, loss = [0.3021944463253021, 0.007305535487830639, 0.30146390199661255]\n",
      "Step 1315, loss = [0.3128200173377991, 0.0023321793414652348, 0.31258681416511536]\n",
      "Step 1316, loss = [0.2828504741191864, 0.00464160181581974, 0.28238630294799805]\n",
      "Step 1317, loss = [0.3047625720500946, 0.00592817272990942, 0.30416974425315857]\n",
      "Step 1318, loss = [0.30919158458709717, 0.005176542326807976, 0.3086739182472229]\n",
      "Step 1319, loss = [0.30236688256263733, 0.0026212730444967747, 0.30210474133491516]\n",
      "Step 1320, loss = [0.3124573826789856, 0.005259212572127581, 0.31193146109580994]\n",
      "Step 1321, loss = [0.2967313826084137, 0.00953573640435934, 0.2957777976989746]\n",
      "Step 1322, loss = [0.30167075991630554, 0.0015796319348737597, 0.30151280760765076]\n",
      "Step 1323, loss = [0.30185797810554504, 0.004454284906387329, 0.30141255259513855]\n",
      "Step 1324, loss = [0.2915394604206085, 0.002303041284903884, 0.2913091480731964]\n",
      "Step 1325, loss = [0.3151894807815552, 0.0039039377588778734, 0.31479910016059875]\n",
      "Step 1326, loss = [0.30496299266815186, 0.004544527269899845, 0.3045085370540619]\n",
      "Step 1327, loss = [0.31008172035217285, 0.00393905583769083, 0.3096878230571747]\n",
      "Step 1328, loss = [0.286819726228714, 0.0029101348482072353, 0.28652870655059814]\n",
      "Step 1329, loss = [0.3020192086696625, 0.001890745246782899, 0.30183014273643494]\n",
      "Step 1330, loss = [0.3142108619213104, 0.006085407920181751, 0.3136023283004761]\n",
      "Step 1331, loss = [0.2974308729171753, 0.004866092465817928, 0.296944260597229]\n",
      "Step 1332, loss = [0.3147434890270233, 0.00306497560814023, 0.31443700194358826]\n",
      "Step 1333, loss = [0.3035038113594055, 0.005143906455487013, 0.3029894232749939]\n",
      "Step 1334, loss = [0.30239957571029663, 0.0011375423055142164, 0.3022858202457428]\n",
      "Step 1335, loss = [0.3062940537929535, 0.006084078457206488, 0.3056856393814087]\n",
      "Step 1336, loss = [0.3138403594493866, 0.005312234628945589, 0.3133091330528259]\n",
      "Step 1337, loss = [0.30747902393341064, 0.007029443513602018, 0.3067760765552521]\n",
      "Step 1338, loss = [0.29265594482421875, 0.005979685112833977, 0.29205799102783203]\n",
      "Step 1339, loss = [0.3014117181301117, 0.0036623128689825535, 0.3010454773902893]\n",
      "Step 1340, loss = [0.3062373995780945, 0.003469818504527211, 0.30589041113853455]\n",
      "Step 1341, loss = [0.31029659509658813, 0.005438300780951977, 0.30975276231765747]\n",
      "Step 1342, loss = [0.3066447973251343, 0.005487246438860893, 0.30609607696533203]\n",
      "Step 1343, loss = [0.2801942229270935, 0.0061659133061766624, 0.2795776426792145]\n",
      "Step 1344, loss = [0.2981969118118286, 0.0022610733285546303, 0.29797080159187317]\n",
      "Step 1345, loss = [0.30884069204330444, 0.002872332464903593, 0.30855345726013184]\n",
      "Step 1346, loss = [0.302644282579422, 0.005690748803317547, 0.30207520723342896]\n",
      "Step 1347, loss = [0.3050202429294586, 0.004827024880796671, 0.3045375347137451]\n",
      "Step 1348, loss = [0.2844253480434418, 0.0038674776442348957, 0.28403860330581665]\n",
      "Step 1349, loss = [0.3103717863559723, 0.005299890413880348, 0.3098418116569519]\n",
      "Step 1350, loss = [0.2921273410320282, 0.0047592842020094395, 0.2916513979434967]\n",
      "Step 1351, loss = [0.30836808681488037, 0.003677232190966606, 0.3080003559589386]\n",
      "Step 1352, loss = [0.27401474118232727, 0.004372480791062117, 0.273577481508255]\n",
      "Step 1353, loss = [0.28676578402519226, 0.006893962621688843, 0.2860763967037201]\n",
      "Step 1354, loss = [0.31499558687210083, 0.004034345969557762, 0.3145921528339386]\n",
      "Step 1355, loss = [0.2971973717212677, 0.0024934415705502033, 0.29694801568984985]\n",
      "Step 1356, loss = [0.2802344560623169, 0.004664587788283825, 0.2797679901123047]\n",
      "Step 1357, loss = [0.3097040355205536, 0.006227638106793165, 0.3090812861919403]\n",
      "Step 1358, loss = [0.3026050925254822, 0.004240312613546848, 0.30218106508255005]\n",
      "Step 1359, loss = [0.30820924043655396, 0.0029999040998518467, 0.3079092502593994]\n",
      "Step 1360, loss = [0.2687569260597229, 0.0029153325594961643, 0.26846539974212646]\n",
      "Step 1361, loss = [0.31081637740135193, 0.0039601693861186504, 0.31042036414146423]\n",
      "Step 1362, loss = [0.3072543442249298, 0.006123426370322704, 0.30664199590682983]\n",
      "Step 1363, loss = [0.2798507511615753, 0.0038610633928328753, 0.27946463227272034]\n",
      "Step 1364, loss = [0.3041553199291229, 0.0036723886150866747, 0.30378809571266174]\n",
      "Step 1365, loss = [0.29965782165527344, 0.0053038508631289005, 0.2991274297237396]\n",
      "Step 1366, loss = [0.29317930340766907, 0.006666370667517185, 0.2925126552581787]\n",
      "Step 1367, loss = [0.3068402111530304, 0.004382221028208733, 0.3064019978046417]\n",
      "Step 1368, loss = [0.29971805214881897, 0.00349945155903697, 0.29936811327934265]\n",
      "Step 1369, loss = [0.3016584515571594, 0.004726774990558624, 0.3011857867240906]\n",
      "Step 1370, loss = [0.29276570677757263, 0.0028888899832963943, 0.2924768030643463]\n",
      "Step 1371, loss = [0.31043580174446106, 0.0074093881994485855, 0.3096948564052582]\n",
      "Step 1372, loss = [0.30450618267059326, 0.0020026296842843294, 0.30430591106414795]\n",
      "Step 1373, loss = [0.3065396547317505, 0.0010681399144232273, 0.306432843208313]\n",
      "Step 1374, loss = [0.3044414222240448, 0.002786540426313877, 0.30416277050971985]\n",
      "Step 1375, loss = [0.3023097813129425, 0.004867982119321823, 0.3018229901790619]\n",
      "Step 1376, loss = [0.3097599148750305, 0.005197328515350819, 0.3092401921749115]\n",
      "Step 1377, loss = [0.29766085743904114, 0.006613145116716623, 0.2969995439052582]\n",
      "Step 1378, loss = [0.30817803740501404, 0.0038029202260077, 0.30779775977134705]\n",
      "Step 1379, loss = [0.30398765206336975, 0.005738643929362297, 0.3034137785434723]\n",
      "Step 1380, loss = [0.30444666743278503, 0.003322494449093938, 0.304114431142807]\n",
      "Step 1381, loss = [0.30389294028282166, 0.005113980732858181, 0.3033815324306488]\n",
      "Step 1382, loss = [0.3055524230003357, 0.002223449293524027, 0.3053300678730011]\n",
      "Step 1383, loss = [0.28842177987098694, 0.005590110085904598, 0.28786277770996094]\n",
      "Step 1384, loss = [0.2995128035545349, 0.0016503439983353019, 0.29934775829315186]\n",
      "Step 1385, loss = [0.29182907938957214, 0.0038981037214398384, 0.2914392650127411]\n",
      "Step 1386, loss = [0.30112123489379883, 0.004971019923686981, 0.30062413215637207]\n",
      "Step 1387, loss = [0.30034130811691284, 0.004102161154150963, 0.2999310791492462]\n",
      "Step 1388, loss = [0.2868380546569824, 0.006256660912185907, 0.28621238470077515]\n",
      "Step 1389, loss = [0.2787098288536072, 0.006981964688748121, 0.27801162004470825]\n",
      "Step 1390, loss = [0.31243059039115906, 0.00716981990262866, 0.3117136061191559]\n",
      "Step 1391, loss = [0.30177971720695496, 0.006059776060283184, 0.30117374658584595]\n",
      "Step 1392, loss = [0.3132578134536743, 0.005802925210446119, 0.3126775324344635]\n",
      "Step 1393, loss = [0.3037310242652893, 0.0037979637272655964, 0.3033512234687805]\n",
      "Step 1394, loss = [0.3046753704547882, 0.004905609413981438, 0.30418482422828674]\n",
      "Step 1395, loss = [0.2921519875526428, 0.002985855797305703, 0.2918533980846405]\n",
      "Step 1396, loss = [0.2902071177959442, 0.004763225093483925, 0.2897307872772217]\n",
      "Step 1397, loss = [0.30022603273391724, 0.0018236266914755106, 0.30004367232322693]\n",
      "Step 1398, loss = [0.3056991994380951, 0.0020776442252099514, 0.30549144744873047]\n",
      "Step 1399, loss = [0.30552735924720764, 0.0049260701052844524, 0.3050347566604614]\n",
      "Step 1400, loss = [0.2931772470474243, 0.0022275950759649277, 0.2929544746875763]\n",
      "Step 1401, loss = [0.31471139192581177, 0.0046786954626441, 0.31424352526664734]\n",
      "Step 1402, loss = [0.30555909872055054, 0.0020585814490914345, 0.3053532540798187]\n",
      "Step 1403, loss = [0.27998775243759155, 0.006652283947914839, 0.2793225347995758]\n",
      "Step 1404, loss = [0.29866138100624084, 0.00440823333337903, 0.29822054505348206]\n",
      "Step 1405, loss = [0.30125218629837036, 0.002538028173148632, 0.30099838972091675]\n",
      "Step 1406, loss = [0.31240394711494446, 0.004596225917339325, 0.3119443356990814]\n",
      "Step 1407, loss = [0.3064512312412262, 0.0037637767381966114, 0.306074857711792]\n",
      "Step 1408, loss = [0.3051726520061493, 0.002947296015918255, 0.3048779368400574]\n",
      "Step 1409, loss = [0.28879499435424805, 0.004368226043879986, 0.2883581817150116]\n",
      "Step 1410, loss = [0.29090556502342224, 0.007336904294788837, 0.29017186164855957]\n",
      "Step 1411, loss = [0.3078501522541046, 0.0027197119779884815, 0.3075781762599945]\n",
      "Step 1412, loss = [0.29634666442871094, 0.007922040298581123, 0.2955544590950012]\n",
      "Step 1413, loss = [0.29778793454170227, 0.005267726257443428, 0.29726114869117737]\n",
      "Step 1414, loss = [0.3047173023223877, 0.007752864621579647, 0.3039420247077942]\n",
      "Step 1415, loss = [0.2985170781612396, 0.006561674177646637, 0.29786092042922974]\n",
      "Step 1416, loss = [0.3126610517501831, 0.004338536411523819, 0.31222718954086304]\n",
      "Step 1417, loss = [0.30863550305366516, 0.003802688093855977, 0.30825522541999817]\n",
      "Step 1418, loss = [0.30493906140327454, 0.006186574697494507, 0.30432039499282837]\n",
      "Step 1419, loss = [0.30595269799232483, 0.004663949366658926, 0.3054862916469574]\n",
      "Step 1420, loss = [0.29928016662597656, 0.0024166698567569256, 0.29903849959373474]\n",
      "Step 1421, loss = [0.3062993884086609, 0.003302327124401927, 0.30596914887428284]\n",
      "Step 1422, loss = [0.2969282865524292, 0.005197035148739815, 0.29640859365463257]\n",
      "Step 1423, loss = [0.2955209016799927, 0.001587109174579382, 0.2953622043132782]\n",
      "Step 1424, loss = [0.2977173924446106, 0.004903355147689581, 0.29722705483436584]\n",
      "Step 1425, loss = [0.3056713938713074, 0.002498920075595379, 0.30542150139808655]\n",
      "Step 1426, loss = [0.2934064269065857, 0.0029083637055009604, 0.2931155860424042]\n",
      "Step 1427, loss = [0.29531627893447876, 0.00380252068862319, 0.29493603110313416]\n",
      "Step 1428, loss = [0.30731046199798584, 0.004542602226138115, 0.3068562150001526]\n",
      "Step 1429, loss = [0.3075202405452728, 0.00868644192814827, 0.30665159225463867]\n",
      "Step 1430, loss = [0.27312248945236206, 0.0035685612820088863, 0.2727656364440918]\n",
      "Step 1431, loss = [0.3064994513988495, 0.0022061611525714397, 0.3062788248062134]\n",
      "Step 1432, loss = [0.29230496287345886, 0.0038931139279156923, 0.2919156551361084]\n",
      "Step 1433, loss = [0.299956351518631, 0.0028573537711054087, 0.29967060685157776]\n",
      "Step 1434, loss = [0.30791419744491577, 0.00530823040753603, 0.30738338828086853]\n",
      "Step 1435, loss = [0.3081457316875458, 0.008255626074969769, 0.30732017755508423]\n",
      "Step 1436, loss = [0.3039540648460388, 0.004478554241359234, 0.30350619554519653]\n",
      "Step 1437, loss = [0.31333473324775696, 0.0035317009314894676, 0.31298157572746277]\n",
      "Step 1438, loss = [0.2899734377861023, 0.006649020593613386, 0.2893085479736328]\n",
      "Step 1439, loss = [0.28891274333000183, 0.0018823345890268683, 0.28872451186180115]\n",
      "Step 1440, loss = [0.30724582076072693, 0.004869784694164991, 0.306758850812912]\n",
      "Step 1441, loss = [0.2947623133659363, 0.005236667115241289, 0.2942386567592621]\n",
      "Step 1442, loss = [0.2980448007583618, 0.0038694259710609913, 0.29765784740448]\n",
      "Step 1443, loss = [0.3056166172027588, 0.00359942764043808, 0.3052566647529602]\n",
      "Step 1444, loss = [0.2996729016304016, 0.0024583288468420506, 0.2994270622730255]\n",
      "Step 1445, loss = [0.29527366161346436, 0.0065491776913404465, 0.29461875557899475]\n",
      "Step 1446, loss = [0.2930905520915985, 0.005946905817836523, 0.29249584674835205]\n",
      "Step 1447, loss = [0.30248746275901794, 0.004333311691880226, 0.30205413699150085]\n",
      "Step 1448, loss = [0.3002406358718872, 0.0045904191210865974, 0.29978159070014954]\n",
      "Step 1449, loss = [0.30192074179649353, 0.0037701025139540434, 0.3015437424182892]\n",
      "Step 1450, loss = [0.30753985047340393, 0.0024706667754799128, 0.30729278922080994]\n",
      "Step 1451, loss = [0.291621595621109, 0.004210374318063259, 0.29120054841041565]\n",
      "Step 1452, loss = [0.3101367950439453, 0.0023672913666814566, 0.30990007519721985]\n",
      "Step 1453, loss = [0.29174908995628357, 0.004002443980425596, 0.2913488447666168]\n",
      "Step 1454, loss = [0.3121401071548462, 0.00414397194981575, 0.3117257058620453]\n",
      "Step 1455, loss = [0.30971527099609375, 0.004911122843623161, 0.3092241585254669]\n",
      "Step 1456, loss = [0.29557058215141296, 0.005574114620685577, 0.2950131595134735]\n",
      "Step 1457, loss = [0.29529112577438354, 0.004865220282226801, 0.2948046028614044]\n",
      "Step 1458, loss = [0.30740392208099365, 0.006124251056462526, 0.3067914843559265]\n",
      "Step 1459, loss = [0.28905370831489563, 0.005083981901407242, 0.28854531049728394]\n",
      "Step 1460, loss = [0.2891390323638916, 0.007482781540602446, 0.28839075565338135]\n",
      "Step 1461, loss = [0.303497314453125, 0.004265768453478813, 0.30307072401046753]\n",
      "Step 1462, loss = [0.3060789704322815, 0.003906876314431429, 0.3056882917881012]\n",
      "Step 1463, loss = [0.2776753902435303, 0.005808470770716667, 0.2770945429801941]\n",
      "Step 1464, loss = [0.2990739643573761, 0.00577882956713438, 0.2984960675239563]\n",
      "Step 1465, loss = [0.2948203682899475, 0.0034978084731847048, 0.29447057843208313]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11754 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1466, loss = [0.29956409335136414, 0.005631490144878626, 0.29900094866752625]\n",
      "Update target distribution epoch 1 step 1467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11754/11754 [1:50:38<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1467, loss = [0.3025108575820923, 0.006499174982309341, 0.3018609285354614]\n",
      "Step 1468, loss = [0.2898796796798706, 0.0034544419031590223, 0.28953424096107483]\n",
      "Step 1469, loss = [0.31747201085090637, 0.01060156524181366, 0.3164118528366089]\n",
      "Epoch 1, loss = [0.29965752 0.00431334 0.29922619]\n",
      "\n",
      "Start of epoch 2\n",
      "Step 0, loss = [0.3017106354236603, 0.005007724277675152, 0.30120986700057983]\n",
      "Step 1, loss = [0.3099864423274994, 0.003872957080602646, 0.3095991611480713]\n",
      "Step 2, loss = [0.308196097612381, 0.005364443175494671, 0.30765965580940247]\n",
      "Step 3, loss = [0.28014394640922546, 0.006477866321802139, 0.2794961631298065]\n",
      "Step 4, loss = [0.30990177392959595, 0.006407734006643295, 0.3092609941959381]\n",
      "Step 5, loss = [0.28914594650268555, 0.0027820297982543707, 0.2888677418231964]\n",
      "Step 6, loss = [0.3069169819355011, 0.0032074637711048126, 0.3065962493419647]\n",
      "Step 7, loss = [0.3086921274662018, 0.0020409722346812487, 0.30848804116249084]\n",
      "Step 8, loss = [0.28498175740242004, 0.0028010753449052572, 0.2847016453742981]\n",
      "Step 9, loss = [0.3048553168773651, 0.002231435850262642, 0.30463218688964844]\n",
      "Step 10, loss = [0.28871437907218933, 0.003358769929036498, 0.28837850689888]\n",
      "Step 11, loss = [0.306723415851593, 0.001673289923928678, 0.3065560758113861]\n",
      "Step 12, loss = [0.2972164750099182, 0.003497065044939518, 0.296866774559021]\n",
      "Step 13, loss = [0.30107349157333374, 0.0022761677391827106, 0.3008458614349365]\n",
      "Step 14, loss = [0.30189457535743713, 0.005218418315052986, 0.3013727366924286]\n",
      "Step 15, loss = [0.30401864647865295, 0.004003260284662247, 0.30361831188201904]\n",
      "Step 16, loss = [0.30992478132247925, 0.004722791723906994, 0.30945250391960144]\n",
      "Step 17, loss = [0.2996639609336853, 0.0029889065772295, 0.2993650734424591]\n",
      "Step 18, loss = [0.3083558976650238, 0.008825261145830154, 0.307473361492157]\n",
      "Step 19, loss = [0.30627110600471497, 0.005697124637663364, 0.3057014048099518]\n",
      "Step 20, loss = [0.283871591091156, 0.0020484193228185177, 0.2836667597293854]\n",
      "Step 21, loss = [0.29400548338890076, 0.004555276595056057, 0.29354995489120483]\n",
      "Step 22, loss = [0.31364646553993225, 0.009223094210028648, 0.31272414326667786]\n",
      "Step 23, loss = [0.2953035235404968, 0.0033232138957828283, 0.29497119784355164]\n",
      "Step 24, loss = [0.2915380895137787, 0.0049361418932676315, 0.2910444736480713]\n",
      "Step 25, loss = [0.2861153185367584, 0.0026845871470868587, 0.28584685921669006]\n",
      "Step 26, loss = [0.294620156288147, 0.0054252976551651955, 0.29407763481140137]\n",
      "Step 27, loss = [0.30613598227500916, 0.004370460752397776, 0.3056989312171936]\n",
      "Step 28, loss = [0.30306103825569153, 0.0040689799934625626, 0.3026541471481323]\n",
      "Step 29, loss = [0.29658243060112, 0.002036366844549775, 0.2963787913322449]\n",
      "Step 30, loss = [0.29332149028778076, 0.005791381001472473, 0.2927423417568207]\n",
      "Step 31, loss = [0.3169373571872711, 0.0042800153605639935, 0.3165093660354614]\n",
      "Step 32, loss = [0.29984429478645325, 0.006354026962071657, 0.2992088794708252]\n",
      "Step 33, loss = [0.30523037910461426, 0.0029402689542621374, 0.30493634939193726]\n",
      "Step 34, loss = [0.2968203127384186, 0.004754230380058289, 0.2963448762893677]\n",
      "Step 35, loss = [0.303699791431427, 0.004576115868985653, 0.30324217677116394]\n",
      "Step 36, loss = [0.29956376552581787, 0.004310225136578083, 0.29913273453712463]\n",
      "Step 37, loss = [0.30287694931030273, 0.00323671312071383, 0.30255326628685]\n",
      "Step 38, loss = [0.28932368755340576, 0.004654451739042997, 0.28885823488235474]\n",
      "Step 39, loss = [0.2908969223499298, 0.004585203714668751, 0.2904384136199951]\n",
      "Step 40, loss = [0.2967037558555603, 0.005055895075201988, 0.29619815945625305]\n",
      "Step 41, loss = [0.29205936193466187, 0.004873690195381641, 0.2915720045566559]\n",
      "Step 42, loss = [0.3134727478027344, 0.004106659442186356, 0.31306207180023193]\n",
      "Step 43, loss = [0.3041449785232544, 0.0017944861901924014, 0.3039655387401581]\n",
      "Step 44, loss = [0.28764182329177856, 0.005474856123328209, 0.2870943248271942]\n",
      "Step 45, loss = [0.30828598141670227, 0.004406468011438847, 0.3078453242778778]\n",
      "Step 46, loss = [0.2854554355144501, 0.004079231061041355, 0.2850475013256073]\n",
      "Step 47, loss = [0.29912832379341125, 0.0062309554778039455, 0.2985052168369293]\n",
      "Step 48, loss = [0.29748937487602234, 0.00481832679361105, 0.2970075309276581]\n",
      "Step 49, loss = [0.3089306652545929, 0.003027375554665923, 0.3086279332637787]\n",
      "Step 50, loss = [0.31151121854782104, 0.0020187715999782085, 0.3113093376159668]\n",
      "Step 51, loss = [0.2961479425430298, 0.0025836341083049774, 0.29588958621025085]\n",
      "Step 52, loss = [0.28182029724121094, 0.005517199635505676, 0.28126856684684753]\n",
      "Step 53, loss = [0.3136117160320282, 0.00547727569937706, 0.31306397914886475]\n",
      "Step 54, loss = [0.2984282076358795, 0.007615787908434868, 0.29766663908958435]\n",
      "Step 55, loss = [0.30514445900917053, 0.00367962708696723, 0.30477648973464966]\n",
      "Step 56, loss = [0.2977643311023712, 0.004605889320373535, 0.2973037362098694]\n",
      "Step 57, loss = [0.29615920782089233, 0.004070973489433527, 0.2957521080970764]\n",
      "Step 58, loss = [0.30638495087623596, 0.0036795036867260933, 0.3060170114040375]\n",
      "Step 59, loss = [0.305804044008255, 0.004000212997198105, 0.30540403723716736]\n",
      "Step 60, loss = [0.2965867221355438, 0.004353791009634733, 0.296151340007782]\n",
      "Step 61, loss = [0.2914164662361145, 0.004635539837181568, 0.2909529209136963]\n",
      "Step 62, loss = [0.30697938799858093, 0.0037038030568510294, 0.30660900473594666]\n",
      "Step 63, loss = [0.30811676383018494, 0.0072565944865345955, 0.30739110708236694]\n",
      "Step 64, loss = [0.31146788597106934, 0.0039355517365038395, 0.31107431650161743]\n",
      "Step 65, loss = [0.30656397342681885, 0.005199940875172615, 0.30604398250579834]\n",
      "Step 66, loss = [0.2842056453227997, 0.004322068300098181, 0.2837734520435333]\n",
      "Step 67, loss = [0.3041507303714752, 0.0031264766585081816, 0.3038380742073059]\n",
      "Step 68, loss = [0.3086843192577362, 0.0036520538851618767, 0.3083191215991974]\n",
      "Step 69, loss = [0.30211958289146423, 0.004092054441571236, 0.3017103672027588]\n",
      "Step 70, loss = [0.3035500645637512, 0.0029854774475097656, 0.3032515048980713]\n",
      "Step 71, loss = [0.2860100269317627, 0.0038256943225860596, 0.28562745451927185]\n",
      "Step 72, loss = [0.2955905497074127, 0.0038156588561832905, 0.29520899057388306]\n",
      "Step 73, loss = [0.2998872995376587, 0.006582442205399275, 0.29922905564308167]\n",
      "Step 74, loss = [0.29532840847969055, 0.006055740639567375, 0.2947228252887726]\n",
      "Step 75, loss = [0.29784122109413147, 0.004671722650527954, 0.29737403988838196]\n",
      "Step 76, loss = [0.3036282956600189, 0.0034883618354797363, 0.30327945947647095]\n",
      "Step 77, loss = [0.308393657207489, 0.0032111527398228645, 0.3080725371837616]\n",
      "Step 78, loss = [0.30060145258903503, 0.006480660289525986, 0.2999534010887146]\n",
      "Step 79, loss = [0.3128242790699005, 0.003776674857363105, 0.31244662404060364]\n",
      "Step 80, loss = [0.30093351006507874, 0.005189583171159029, 0.3004145622253418]\n",
      "Step 81, loss = [0.3072535991668701, 0.003974216990172863, 0.3068561851978302]\n",
      "Step 82, loss = [0.30083855986595154, 0.004115317948162556, 0.30042701959609985]\n",
      "Step 83, loss = [0.30271658301353455, 0.004698825068771839, 0.30224668979644775]\n",
      "Step 84, loss = [0.2825094163417816, 0.004531980957835913, 0.28205621242523193]\n",
      "Step 85, loss = [0.3014334440231323, 0.004657138604670763, 0.3009677231311798]\n",
      "Step 86, loss = [0.30431634187698364, 0.006094553507864475, 0.3037068843841553]\n",
      "Step 87, loss = [0.2884499728679657, 0.0024065382312983274, 0.28820931911468506]\n",
      "Step 88, loss = [0.30270394682884216, 0.0039739408530294895, 0.30230656266212463]\n",
      "Step 89, loss = [0.2872128486633301, 0.003482441883534193, 0.28686460852622986]\n",
      "Step 90, loss = [0.29741719365119934, 0.0021761872339993715, 0.2971995770931244]\n",
      "Step 91, loss = [0.2960820198059082, 0.005856840871274471, 0.2954963445663452]\n",
      "Step 92, loss = [0.3029974400997162, 0.0017918585799634457, 0.30281826853752136]\n",
      "Step 93, loss = [0.30208009481430054, 0.003082332666963339, 0.3017718493938446]\n",
      "Step 94, loss = [0.30300024151802063, 0.004521985072642565, 0.30254805088043213]\n",
      "Step 95, loss = [0.3025679886341095, 0.0025120535865426064, 0.3023167848587036]\n",
      "Step 96, loss = [0.2934340834617615, 0.0023406080435961485, 0.2932000160217285]\n",
      "Step 97, loss = [0.3067459464073181, 0.005996524356305599, 0.3061462938785553]\n",
      "Step 98, loss = [0.294994056224823, 0.007014410104602575, 0.2942926287651062]\n",
      "Step 99, loss = [0.2853597402572632, 0.005199494305998087, 0.28483977913856506]\n",
      "Step 100, loss = [0.30847442150115967, 0.0023517818190157413, 0.30823925137519836]\n",
      "Step 101, loss = [0.31000229716300964, 0.0035162100102752447, 0.3096506893634796]\n",
      "Step 102, loss = [0.29831618070602417, 0.006027447525411844, 0.29771342873573303]\n",
      "Step 103, loss = [0.29899150133132935, 0.0026577329263091087, 0.2987257242202759]\n",
      "Step 104, loss = [0.30590641498565674, 0.005805307999253273, 0.3053258955478668]\n",
      "Step 105, loss = [0.3113957345485687, 0.005063334479928017, 0.3108893930912018]\n",
      "Step 106, loss = [0.3139382600784302, 0.004005867056548595, 0.31353768706321716]\n",
      "Step 107, loss = [0.3092223107814789, 0.003654167288914323, 0.30885690450668335]\n",
      "Step 108, loss = [0.3040921688079834, 0.0025690868496894836, 0.30383527278900146]\n",
      "Step 109, loss = [0.29641997814178467, 0.00460684671998024, 0.2959592938423157]\n",
      "Step 110, loss = [0.3090086877346039, 0.0037307031452655792, 0.3086356222629547]\n",
      "Step 111, loss = [0.2910797894001007, 0.004719933494925499, 0.2906078100204468]\n",
      "Step 112, loss = [0.30487731099128723, 0.00473446398973465, 0.3044038712978363]\n",
      "Step 113, loss = [0.30315491557121277, 0.005846395157277584, 0.30257028341293335]\n",
      "Step 114, loss = [0.30732980370521545, 0.005113608203828335, 0.3068184554576874]\n",
      "Step 115, loss = [0.2910781502723694, 0.005434554070234299, 0.29053470492362976]\n",
      "Step 116, loss = [0.3008630573749542, 0.004697962664067745, 0.3003932535648346]\n",
      "Step 117, loss = [0.30116620659828186, 0.0027459985576570034, 0.30089160799980164]\n",
      "Step 118, loss = [0.29784271121025085, 0.004307516850531101, 0.2974119484424591]\n",
      "Step 119, loss = [0.2937631905078888, 0.003968280274420977, 0.29336637258529663]\n",
      "Step 120, loss = [0.2923011779785156, 0.0016630657482892275, 0.2921348810195923]\n",
      "Step 121, loss = [0.3091762959957123, 0.002581103704869747, 0.30891817808151245]\n",
      "Step 122, loss = [0.305834025144577, 0.005106142722070217, 0.30532342195510864]\n",
      "Step 123, loss = [0.3028879463672638, 0.0037621124647557735, 0.30251172184944153]\n",
      "Step 124, loss = [0.31721827387809753, 0.00556087214499712, 0.3166621923446655]\n",
      "Step 125, loss = [0.30311089754104614, 0.0042357356287539005, 0.30268731713294983]\n",
      "Step 126, loss = [0.2997010052204132, 0.0036530662328004837, 0.29933568835258484]\n",
      "Step 127, loss = [0.3053823411464691, 0.005920496303588152, 0.30479028820991516]\n",
      "Step 128, loss = [0.2980870306491852, 0.0037200869992375374, 0.2977150082588196]\n",
      "Step 129, loss = [0.2889101207256317, 0.003946259617805481, 0.28851550817489624]\n",
      "Step 130, loss = [0.3011097311973572, 0.0051425304263830185, 0.3005954921245575]\n",
      "Step 131, loss = [0.29781627655029297, 0.0057512251660227776, 0.2972411513328552]\n",
      "Step 132, loss = [0.3078497052192688, 0.0055612837895751, 0.307293564081192]\n",
      "Step 133, loss = [0.29363349080085754, 0.0030791694298386574, 0.2933255732059479]\n",
      "Step 134, loss = [0.299772709608078, 0.0029952689073979855, 0.29947319626808167]\n",
      "Step 135, loss = [0.28414449095726013, 0.004118942655622959, 0.2837325930595398]\n",
      "Step 136, loss = [0.30489757657051086, 0.0055154417641460896, 0.3043460249900818]\n",
      "Step 137, loss = [0.2981705069541931, 0.0020332809071987867, 0.29796716570854187]\n",
      "Step 138, loss = [0.30258825421333313, 0.00658706808462739, 0.3019295334815979]\n",
      "Step 139, loss = [0.2981601357460022, 0.004009980242699385, 0.29775914549827576]\n",
      "Step 140, loss = [0.29653462767601013, 0.0017632078379392624, 0.2963583171367645]\n",
      "Step 141, loss = [0.30202749371528625, 0.007406956981867552, 0.3012867867946625]\n",
      "Step 142, loss = [0.29611119627952576, 0.005302418954670429, 0.2955809533596039]\n",
      "Step 143, loss = [0.3135170042514801, 0.004803911782801151, 0.31303662061691284]\n",
      "Step 144, loss = [0.29635339975357056, 0.006672735325992107, 0.29568612575531006]\n",
      "Step 145, loss = [0.29591819643974304, 0.004124731756746769, 0.29550573229789734]\n",
      "Step 146, loss = [0.2825499475002289, 0.006851566955447197, 0.28186479210853577]\n",
      "Step 147, loss = [0.2951388359069824, 0.005606897175312042, 0.2945781350135803]\n",
      "Step 148, loss = [0.31048083305358887, 0.00210375408641994, 0.3102704584598541]\n",
      "Step 149, loss = [0.28893807530403137, 0.006071394309401512, 0.28833094239234924]\n",
      "Step 150, loss = [0.29025083780288696, 0.002179915551096201, 0.29003283381462097]\n",
      "Step 151, loss = [0.2885024845600128, 0.005880691576749086, 0.2879144251346588]\n",
      "Step 152, loss = [0.31381553411483765, 0.0025054276920855045, 0.3135649859905243]\n",
      "Step 153, loss = [0.3025505840778351, 0.0029824315570294857, 0.3022523522377014]\n",
      "Step 154, loss = [0.2960972189903259, 0.005237414501607418, 0.29557347297668457]\n",
      "Step 155, loss = [0.29542917013168335, 0.002952934941276908, 0.29513388872146606]\n",
      "Step 156, loss = [0.2857559323310852, 0.006153216119855642, 0.28514060378074646]\n",
      "Step 157, loss = [0.3097408711910248, 0.004082453437149525, 0.30933263897895813]\n",
      "Step 158, loss = [0.3024710416793823, 0.004087287001311779, 0.3020623028278351]\n",
      "Step 159, loss = [0.31144604086875916, 0.0037947562523186207, 0.31106656789779663]\n",
      "Step 160, loss = [0.29870712757110596, 0.0071802097372710705, 0.2979891002178192]\n",
      "Step 161, loss = [0.3028854727745056, 0.0036190433893352747, 0.30252358317375183]\n",
      "Step 162, loss = [0.307730108499527, 0.0060442965477705, 0.30712568759918213]\n",
      "Step 163, loss = [0.3008464276790619, 0.005395662039518356, 0.30030685663223267]\n",
      "Step 164, loss = [0.29151979088783264, 0.0018014225643128157, 0.291339635848999]\n",
      "Step 165, loss = [0.28726187348365784, 0.0028778323903679848, 0.28697410225868225]\n",
      "Step 166, loss = [0.28865936398506165, 0.004547422751784325, 0.2882046103477478]\n",
      "Step 167, loss = [0.3033391833305359, 0.005622202530503273, 0.302776962518692]\n",
      "Step 168, loss = [0.3071598708629608, 0.004148992709815502, 0.3067449629306793]\n",
      "Step 169, loss = [0.307812362909317, 0.004084109328687191, 0.30740395188331604]\n",
      "Step 170, loss = [0.3014526069164276, 0.0036273531150072813, 0.301089882850647]\n",
      "Step 171, loss = [0.2748309075832367, 0.0019078898476436734, 0.27464011311531067]\n",
      "Step 172, loss = [0.2882615625858307, 0.004079581703990698, 0.2878535985946655]\n",
      "Step 173, loss = [0.29759928584098816, 0.004505859687924385, 0.2971487045288086]\n",
      "Step 174, loss = [0.29641544818878174, 0.004774810746312141, 0.2959379553794861]\n",
      "Step 175, loss = [0.312289297580719, 0.0015501612797379494, 0.3121342957019806]\n",
      "Step 176, loss = [0.2899424135684967, 0.0033465346787124872, 0.2896077632904053]\n",
      "Step 177, loss = [0.29335644841194153, 0.004330325871706009, 0.2929234206676483]\n",
      "Step 178, loss = [0.3050190210342407, 0.003257026895880699, 0.3046933114528656]\n",
      "Step 179, loss = [0.2932213246822357, 0.0031976669561117887, 0.29290154576301575]\n",
      "Step 180, loss = [0.2803305685520172, 0.00501926289871335, 0.27982863783836365]\n",
      "Step 181, loss = [0.3014458119869232, 0.002836903091520071, 0.30116212368011475]\n",
      "Step 182, loss = [0.2898365259170532, 0.001395626226440072, 0.28969696164131165]\n",
      "Step 183, loss = [0.2975241243839264, 0.002678150311112404, 0.29725632071495056]\n",
      "Step 184, loss = [0.3013887107372284, 0.002872062148526311, 0.3011015057563782]\n",
      "Step 185, loss = [0.29128754138946533, 0.002166316146031022, 0.2910709083080292]\n",
      "Step 186, loss = [0.3018140494823456, 0.002230602316558361, 0.3015909790992737]\n",
      "Step 187, loss = [0.30291423201560974, 0.00392311904579401, 0.3025219142436981]\n",
      "Step 188, loss = [0.3081900179386139, 0.003932652994990349, 0.30779674649238586]\n",
      "Step 189, loss = [0.29583433270454407, 0.0044994838535785675, 0.29538437724113464]\n",
      "Step 190, loss = [0.2963241636753082, 0.006850435398519039, 0.29563912749290466]\n",
      "Step 191, loss = [0.3082868456840515, 0.0020580878481268883, 0.3080810308456421]\n",
      "Step 192, loss = [0.29528331756591797, 0.003822366241365671, 0.2949010729789734]\n",
      "Step 193, loss = [0.29504233598709106, 0.004052257165312767, 0.29463711380958557]\n",
      "Step 194, loss = [0.3072125315666199, 0.0036375843919813633, 0.30684876441955566]\n",
      "Step 195, loss = [0.2850908041000366, 0.0052670505829155445, 0.2845641076564789]\n",
      "Step 196, loss = [0.30066293478012085, 0.005355070345103741, 0.30012741684913635]\n",
      "Step 197, loss = [0.29866769909858704, 0.0019521652720868587, 0.29847249388694763]\n",
      "Step 198, loss = [0.30621466040611267, 0.004427887499332428, 0.3057718575000763]\n",
      "Step 199, loss = [0.2986750304698944, 0.002954194787889719, 0.2983796000480652]\n",
      "Step 200, loss = [0.28946495056152344, 0.0030853156931698322, 0.28915640711784363]\n",
      "Step 201, loss = [0.29953500628471375, 0.0060987407341599464, 0.29892513155937195]\n",
      "Step 202, loss = [0.29047730565071106, 0.005829173140227795, 0.2898944020271301]\n",
      "Step 203, loss = [0.30429768562316895, 0.004785411991178989, 0.3038191497325897]\n",
      "Step 204, loss = [0.29425671696662903, 0.0034166760742664337, 0.2939150631427765]\n",
      "Step 205, loss = [0.29795143008232117, 0.0030163084156811237, 0.2976498007774353]\n",
      "Step 206, loss = [0.29853007197380066, 0.0015267847338691354, 0.2983773946762085]\n",
      "Step 207, loss = [0.3022088408470154, 0.004119371995329857, 0.30179691314697266]\n",
      "Step 208, loss = [0.2969789505004883, 0.003083521733060479, 0.2966705858707428]\n",
      "Step 209, loss = [0.2865338623523712, 0.0029475444462150335, 0.2862391173839569]\n",
      "Step 210, loss = [0.2773091793060303, 0.003744532587006688, 0.2769347131252289]\n",
      "Step 211, loss = [0.29266631603240967, 0.0031383950263261795, 0.29235246777534485]\n",
      "Step 212, loss = [0.31542307138442993, 0.004861641675233841, 0.31493690609931946]\n",
      "Step 213, loss = [0.29959380626678467, 0.0020277907606214285, 0.2993910312652588]\n",
      "Step 214, loss = [0.2969360649585724, 0.004535044077783823, 0.29648256301879883]\n",
      "Step 215, loss = [0.2682086229324341, 0.011318745091557503, 0.2670767605304718]\n",
      "Step 216, loss = [0.28750064969062805, 0.001384984701871872, 0.28736215829849243]\n",
      "Step 217, loss = [0.3021794557571411, 0.006077226717025042, 0.30157172679901123]\n",
      "Step 218, loss = [0.29075610637664795, 0.003047254402190447, 0.29045137763023376]\n",
      "Step 219, loss = [0.3045077919960022, 0.005190792493522167, 0.3039887249469757]\n",
      "Step 220, loss = [0.29701700806617737, 0.004900595173239708, 0.2965269386768341]\n",
      "Step 221, loss = [0.308338463306427, 0.0043840231373906136, 0.307900071144104]\n",
      "Step 222, loss = [0.2923601269721985, 0.0033783609978854656, 0.2920222878456116]\n",
      "Step 223, loss = [0.28776299953460693, 0.008231289684772491, 0.2869398593902588]\n",
      "Step 224, loss = [0.2943759858608246, 0.0035432009026408195, 0.2940216660499573]\n",
      "Step 225, loss = [0.2935353219509125, 0.005900730844587088, 0.2929452359676361]\n",
      "Step 226, loss = [0.28902411460876465, 0.00484705064445734, 0.28853940963745117]\n",
      "Step 227, loss = [0.29271477460861206, 0.0027529983781278133, 0.29243946075439453]\n",
      "Step 228, loss = [0.29425036907196045, 0.003581227036193013, 0.2938922345638275]\n",
      "Step 229, loss = [0.30668967962265015, 0.004848984070122242, 0.30620479583740234]\n",
      "Step 230, loss = [0.2976030707359314, 0.003790719900280237, 0.2972239851951599]\n",
      "Step 231, loss = [0.29136866331100464, 0.0023813892621546984, 0.29113051295280457]\n",
      "Step 232, loss = [0.31040433049201965, 0.004964861087501049, 0.30990785360336304]\n",
      "Step 233, loss = [0.30736422538757324, 0.0016579926013946533, 0.3071984350681305]\n",
      "Step 234, loss = [0.30561086535453796, 0.0020412616431713104, 0.30540674924850464]\n",
      "Step 235, loss = [0.27656322717666626, 0.00504952110350132, 0.27605828642845154]\n",
      "Step 236, loss = [0.2967641353607178, 0.002569853328168392, 0.2965071499347687]\n",
      "Step 237, loss = [0.3129819631576538, 0.004625081084668636, 0.31251946091651917]\n",
      "Step 238, loss = [0.2986955940723419, 0.005405555013567209, 0.2981550395488739]\n",
      "Step 239, loss = [0.3046020269393921, 0.002010857220739126, 0.3044009506702423]\n",
      "Step 240, loss = [0.30705222487449646, 0.003934822510927916, 0.3066587448120117]\n",
      "Step 241, loss = [0.3076184391975403, 0.0038156944792717695, 0.3072368800640106]\n",
      "Step 242, loss = [0.29287198185920715, 0.0033820699900388718, 0.2925337851047516]\n",
      "Step 243, loss = [0.31257885694503784, 0.004720599390566349, 0.31210678815841675]\n",
      "Step 244, loss = [0.29971179366111755, 0.006404407788068056, 0.299071341753006]\n",
      "Step 245, loss = [0.2945508360862732, 0.0045812176540493965, 0.29409271478652954]\n",
      "Step 246, loss = [0.3125625550746918, 0.0038129426538944244, 0.3121812641620636]\n",
      "Step 247, loss = [0.30554458498954773, 0.0047644563019275665, 0.30506813526153564]\n",
      "Step 248, loss = [0.3065102994441986, 0.003470453666523099, 0.3061632513999939]\n",
      "Step 249, loss = [0.2974912226200104, 0.0035117901861667633, 0.2971400320529938]\n",
      "Step 250, loss = [0.30393826961517334, 0.005130338948220015, 0.30342522263526917]\n",
      "Step 251, loss = [0.30922064185142517, 0.004520742688328028, 0.3087685704231262]\n",
      "Step 252, loss = [0.29730114340782166, 0.004312905482947826, 0.29686984419822693]\n",
      "Step 253, loss = [0.29734310507774353, 0.0016240169061347842, 0.297180712223053]\n",
      "Step 254, loss = [0.3010960817337036, 0.0033351671881973743, 0.3007625639438629]\n",
      "Step 255, loss = [0.299580454826355, 0.005298239644616842, 0.29905062913894653]\n",
      "Step 256, loss = [0.2928273677825928, 0.0034956499002873898, 0.2924778163433075]\n",
      "Step 257, loss = [0.3105529546737671, 0.004388412460684776, 0.3101141154766083]\n",
      "Step 258, loss = [0.3146539032459259, 0.0060755908489227295, 0.31404635310173035]\n",
      "Step 259, loss = [0.2954182028770447, 0.005630923435091972, 0.29485511779785156]\n",
      "Step 260, loss = [0.2705165147781372, 0.002953193848952651, 0.27022120356559753]\n",
      "Step 261, loss = [0.3079849183559418, 0.005614784080535173, 0.3074234426021576]\n",
      "Step 262, loss = [0.29768040776252747, 0.0029631746001541615, 0.2973840832710266]\n",
      "Step 263, loss = [0.30238306522369385, 0.00457377266138792, 0.3019256889820099]\n",
      "Step 264, loss = [0.30004575848579407, 0.0017765549710020423, 0.299868106842041]\n",
      "Step 265, loss = [0.29729458689689636, 0.0026922214310616255, 0.2970253527164459]\n",
      "Step 266, loss = [0.30604931712150574, 0.004436975810676813, 0.30560562014579773]\n",
      "Step 267, loss = [0.3022066056728363, 0.0033120112493634224, 0.30187541246414185]\n",
      "Step 268, loss = [0.30836865305900574, 0.0027862973511219025, 0.3080900311470032]\n",
      "Step 269, loss = [0.3088285028934479, 0.0037520569749176502, 0.3084532916545868]\n",
      "Step 270, loss = [0.3016969561576843, 0.005610755644738674, 0.3011358678340912]\n",
      "Step 271, loss = [0.2983582019805908, 0.0065095992758870125, 0.2977072298526764]\n",
      "Step 272, loss = [0.31007179617881775, 0.003142046742141247, 0.3097575902938843]\n",
      "Step 273, loss = [0.3033318519592285, 0.0033432829659432173, 0.30299752950668335]\n",
      "Step 274, loss = [0.3122783899307251, 0.003399498760700226, 0.31193843483924866]\n",
      "Step 275, loss = [0.2974141538143158, 0.005565384402871132, 0.296857625246048]\n",
      "Step 276, loss = [0.2888263165950775, 0.0035213178489357233, 0.2884741723537445]\n",
      "Step 277, loss = [0.302462100982666, 0.002633356023579836, 0.30219876766204834]\n",
      "Step 278, loss = [0.30572906136512756, 0.004719769582152367, 0.30525708198547363]\n",
      "Step 279, loss = [0.3040405511856079, 0.0026908963918685913, 0.3037714660167694]\n",
      "Step 280, loss = [0.2941346764564514, 0.004004139453172684, 0.29373425245285034]\n",
      "Step 281, loss = [0.2838066816329956, 0.004584179725497961, 0.2833482623100281]\n",
      "Step 282, loss = [0.2978605031967163, 0.004490797407925129, 0.29741141200065613]\n",
      "Step 283, loss = [0.3082053065299988, 0.00428509246557951, 0.3077768087387085]\n",
      "Step 284, loss = [0.29631707072257996, 0.004677911754697561, 0.2958492934703827]\n",
      "Step 285, loss = [0.3135643005371094, 0.005714696832001209, 0.3129928410053253]\n",
      "Step 286, loss = [0.3016027510166168, 0.0011403340613469481, 0.3014887273311615]\n",
      "Step 287, loss = [0.2989391088485718, 0.0028465548530220985, 0.2986544668674469]\n",
      "Step 288, loss = [0.29576197266578674, 0.0041746594943106174, 0.2953445017337799]\n",
      "Step 289, loss = [0.30632367730140686, 0.0036138882860541344, 0.30596229434013367]\n",
      "Step 290, loss = [0.3043910562992096, 0.005819590762257576, 0.30380910634994507]\n",
      "Step 291, loss = [0.3028821647167206, 0.002971408888697624, 0.30258503556251526]\n",
      "Step 292, loss = [0.29815828800201416, 0.004387186374515295, 0.2977195680141449]\n",
      "Step 293, loss = [0.2943355441093445, 0.0035040839575231075, 0.29398512840270996]\n",
      "Step 294, loss = [0.30220577120780945, 0.0023853788152337074, 0.30196723341941833]\n",
      "Step 295, loss = [0.2915983200073242, 0.005533457733690739, 0.2910449802875519]\n",
      "Step 296, loss = [0.2990493178367615, 0.0027991561219096184, 0.29876941442489624]\n",
      "Step 297, loss = [0.29183605313301086, 0.004904836416244507, 0.2913455665111542]\n",
      "Step 298, loss = [0.30172863602638245, 0.0017416978953406215, 0.30155447125434875]\n",
      "Step 299, loss = [0.28660160303115845, 0.005033103749155998, 0.28609830141067505]\n",
      "Step 300, loss = [0.293348491191864, 0.005152386613190174, 0.29283323884010315]\n",
      "Step 301, loss = [0.30687233805656433, 0.004806282930076122, 0.30639171600341797]\n",
      "Step 302, loss = [0.30300503969192505, 0.006341812666505575, 0.3023708462715149]\n",
      "Step 303, loss = [0.2903333604335785, 0.005564277060329914, 0.28977692127227783]\n",
      "Step 304, loss = [0.29772984981536865, 0.003431465942412615, 0.29738670587539673]\n",
      "Step 305, loss = [0.2946367859840393, 0.004454455338418484, 0.2941913306713104]\n",
      "Step 306, loss = [0.2992697060108185, 0.004700331948697567, 0.29879966378211975]\n",
      "Step 307, loss = [0.2847784757614136, 0.003956103231757879, 0.2843828797340393]\n",
      "Step 308, loss = [0.3042828142642975, 0.0022567494306713343, 0.30405715107917786]\n",
      "Step 309, loss = [0.294594407081604, 0.004567275755107403, 0.2941376864910126]\n",
      "Step 310, loss = [0.3035440742969513, 0.0044161053374409676, 0.3031024634838104]\n",
      "Step 311, loss = [0.3042944371700287, 0.002918301383033395, 0.3040026128292084]\n",
      "Step 312, loss = [0.29534512758255005, 0.005662142299115658, 0.2947789132595062]\n",
      "Step 313, loss = [0.3122657239437103, 0.006376299075782299, 0.3116281032562256]\n",
      "Step 314, loss = [0.3047809600830078, 0.005124061368405819, 0.30426856875419617]\n",
      "Step 315, loss = [0.3065773546695709, 0.0024628136307001114, 0.306331068277359]\n",
      "Step 316, loss = [0.29742375016212463, 0.0023650764487683773, 0.2971872389316559]\n",
      "Step 317, loss = [0.30290940403938293, 0.0030876370146870613, 0.3026006519794464]\n",
      "Step 318, loss = [0.3019736111164093, 0.00336671550758183, 0.3016369342803955]\n",
      "Step 319, loss = [0.29612934589385986, 0.006193452049046755, 0.2955099940299988]\n",
      "Step 320, loss = [0.3006162643432617, 0.0059290360659360886, 0.3000233471393585]\n",
      "Step 321, loss = [0.29492098093032837, 0.005410787183791399, 0.2943798899650574]\n",
      "Step 322, loss = [0.2931287884712219, 0.0033507714979350567, 0.29279372096061707]\n",
      "Step 323, loss = [0.30234232544898987, 0.0016546333208680153, 0.3021768629550934]\n",
      "Step 324, loss = [0.2938803434371948, 0.002236987929791212, 0.2936566472053528]\n",
      "Step 325, loss = [0.2961565852165222, 0.00292616942897439, 0.29586395621299744]\n",
      "Step 326, loss = [0.3074798583984375, 0.004185215570032597, 0.3070613443851471]\n",
      "Step 327, loss = [0.3013664782047272, 0.002220609225332737, 0.30114442110061646]\n",
      "Step 328, loss = [0.30038464069366455, 0.006825409363955259, 0.29970210790634155]\n",
      "Step 329, loss = [0.2948106825351715, 0.0029001333750784397, 0.29452067613601685]\n",
      "Step 330, loss = [0.3036341667175293, 0.0025164978578686714, 0.3033825159072876]\n",
      "Step 331, loss = [0.31017476320266724, 0.0009102710755541921, 0.3100837469100952]\n",
      "Step 332, loss = [0.30303359031677246, 0.0029735970310866833, 0.30273622274398804]\n",
      "Step 333, loss = [0.3074168264865875, 0.004072698764503002, 0.3070095479488373]\n",
      "Step 334, loss = [0.3035619556903839, 0.004438561853021383, 0.30311810970306396]\n",
      "Step 335, loss = [0.2878742218017578, 0.0034377598203718662, 0.28753045201301575]\n",
      "Step 336, loss = [0.31316161155700684, 0.0026426168624311686, 0.31289735436439514]\n",
      "Step 337, loss = [0.29384830594062805, 0.007735933177173138, 0.29307472705841064]\n",
      "Step 338, loss = [0.3117164671421051, 0.005546648055315018, 0.3111618161201477]\n",
      "Step 339, loss = [0.3088861405849457, 0.00798853486776352, 0.3080872893333435]\n",
      "Step 340, loss = [0.30347198247909546, 0.0032221521250903606, 0.3031497597694397]\n",
      "Step 341, loss = [0.2983918786048889, 0.006936280056834221, 0.2976982593536377]\n",
      "Step 342, loss = [0.2991248071193695, 0.003948529250919819, 0.29872995615005493]\n",
      "Step 343, loss = [0.28756454586982727, 0.0005613382090814412, 0.28750839829444885]\n",
      "Step 344, loss = [0.30232271552085876, 0.005987655837088823, 0.3017239570617676]\n",
      "Step 345, loss = [0.2892434000968933, 0.004399179946631193, 0.28880348801612854]\n",
      "Step 346, loss = [0.3097330331802368, 0.0038456323090940714, 0.309348464012146]\n",
      "Step 347, loss = [0.3006307780742645, 0.0036016497761011124, 0.30027061700820923]\n",
      "Step 348, loss = [0.3018101751804352, 0.0029800136107951403, 0.3015121817588806]\n",
      "Step 349, loss = [0.3005812168121338, 0.0050017982721328735, 0.3000810444355011]\n",
      "Step 350, loss = [0.30184876918792725, 0.005444916896522045, 0.30130428075790405]\n",
      "Step 351, loss = [0.29075944423675537, 0.0027151936665177345, 0.2904879152774811]\n",
      "Step 352, loss = [0.2880406677722931, 0.003075794316828251, 0.2877330780029297]\n",
      "Step 353, loss = [0.2976885139942169, 0.0029907976277172565, 0.2973894476890564]\n",
      "Step 354, loss = [0.30499157309532166, 0.0038171233609318733, 0.30460986495018005]\n",
      "Step 355, loss = [0.305855929851532, 0.007446895353496075, 0.30511122941970825]\n",
      "Step 356, loss = [0.3123794198036194, 0.0029657811392098665, 0.31208282709121704]\n",
      "Step 357, loss = [0.3044150471687317, 0.004190218634903431, 0.3039960265159607]\n",
      "Step 358, loss = [0.3015797734260559, 0.004752726759761572, 0.30110448598861694]\n",
      "Step 359, loss = [0.29801055788993835, 0.005385790951550007, 0.2974719703197479]\n",
      "Step 360, loss = [0.3003791868686676, 0.004044709727168083, 0.2999747097492218]\n",
      "Step 361, loss = [0.29410815238952637, 0.005927453748881817, 0.2935154139995575]\n",
      "Step 362, loss = [0.2930329442024231, 0.003310753731057048, 0.2927018702030182]\n",
      "Step 363, loss = [0.3015925884246826, 0.0035089338198304176, 0.3012416958808899]\n",
      "Step 364, loss = [0.3004974126815796, 0.0034744974691420794, 0.30014997720718384]\n",
      "Step 365, loss = [0.3034261465072632, 0.004629762843251228, 0.30296316742897034]\n",
      "Step 366, loss = [0.29513585567474365, 0.0020826286636292934, 0.29492759704589844]\n",
      "Step 367, loss = [0.30244937539100647, 0.004909627139568329, 0.3019584119319916]\n",
      "Step 368, loss = [0.29371505975723267, 0.0018160422332584858, 0.29353344440460205]\n",
      "Step 369, loss = [0.28670138120651245, 0.0009128280798904598, 0.28661009669303894]\n",
      "Step 370, loss = [0.31256231665611267, 0.0029273696709424257, 0.31226956844329834]\n",
      "Step 371, loss = [0.2931064963340759, 0.0063130673952400684, 0.2924751937389374]\n",
      "Step 372, loss = [0.31738853454589844, 0.0037715525832027197, 0.31701138615608215]\n",
      "Step 373, loss = [0.30112770199775696, 0.005708644166588783, 0.30055683851242065]\n",
      "Step 374, loss = [0.27921733260154724, 0.004396574106067419, 0.27877768874168396]\n",
      "Step 375, loss = [0.28781914710998535, 0.0044792876578867435, 0.2873712182044983]\n",
      "Step 376, loss = [0.297764390707016, 0.003666788339614868, 0.2973977029323578]\n",
      "Step 377, loss = [0.2951551377773285, 0.002525375224649906, 0.29490259289741516]\n",
      "Step 378, loss = [0.30536848306655884, 0.002329861046746373, 0.30513548851013184]\n",
      "Step 379, loss = [0.30175837874412537, 0.002290917094796896, 0.30152928829193115]\n",
      "Step 380, loss = [0.29376479983329773, 0.003583770478144288, 0.2934064269065857]\n",
      "Step 381, loss = [0.27278056740760803, 0.003791830502450466, 0.2724013924598694]\n",
      "Step 382, loss = [0.30293625593185425, 0.004677142947912216, 0.30246853828430176]\n",
      "Step 383, loss = [0.30171525478363037, 0.005147078074514866, 0.3012005388736725]\n",
      "Step 384, loss = [0.3010137379169464, 0.004929651506245136, 0.30052077770233154]\n",
      "Step 385, loss = [0.28447654843330383, 0.001855136128142476, 0.28429102897644043]\n",
      "Step 386, loss = [0.29867419600486755, 0.004094382748007774, 0.2982647716999054]\n",
      "Step 387, loss = [0.30121561884880066, 0.005500982515513897, 0.3006655275821686]\n",
      "Step 388, loss = [0.28547102212905884, 0.0025876681320369244, 0.2852122485637665]\n",
      "Step 389, loss = [0.29571014642715454, 0.00324314646422863, 0.29538583755493164]\n",
      "Step 390, loss = [0.287123441696167, 0.0012459090212360024, 0.28699883818626404]\n",
      "Step 391, loss = [0.2918620705604553, 0.0042436267249286175, 0.29143771529197693]\n",
      "Step 392, loss = [0.3030455708503723, 0.0035039526410400867, 0.3026951849460602]\n",
      "Step 393, loss = [0.2967933714389801, 0.004247643984854221, 0.2963685989379883]\n",
      "Step 394, loss = [0.29414933919906616, 0.007062823045998812, 0.29344305396080017]\n",
      "Step 395, loss = [0.30693456530570984, 0.002060960279777646, 0.3067284822463989]\n",
      "Step 396, loss = [0.30706536769866943, 0.005790482275187969, 0.3064863085746765]\n",
      "Step 397, loss = [0.3020716905593872, 0.0013496726751327515, 0.30193671584129333]\n",
      "Step 398, loss = [0.3015025854110718, 0.004494894295930862, 0.30105310678482056]\n",
      "Step 399, loss = [0.2689414620399475, 0.008405333384871483, 0.26810091733932495]\n",
      "Step 400, loss = [0.29991212487220764, 0.001800726866349578, 0.2997320592403412]\n",
      "Step 401, loss = [0.3013817071914673, 0.0018651370191946626, 0.3011952042579651]\n",
      "Step 402, loss = [0.2888929545879364, 0.005569826811552048, 0.28833597898483276]\n",
      "Step 403, loss = [0.30090996623039246, 0.003042896743863821, 0.3006056845188141]\n",
      "Step 404, loss = [0.3068271279335022, 0.004395494237542152, 0.3063875734806061]\n",
      "Step 405, loss = [0.29682210087776184, 0.0048078857362270355, 0.29634130001068115]\n",
      "Step 406, loss = [0.29933294653892517, 0.004920776933431625, 0.29884088039398193]\n",
      "Step 407, loss = [0.31166622042655945, 0.003866602201014757, 0.3112795650959015]\n",
      "Step 408, loss = [0.30881068110466003, 0.004357213154435158, 0.30837497115135193]\n",
      "Step 409, loss = [0.28863316774368286, 0.004337996244430542, 0.28819936513900757]\n",
      "Step 410, loss = [0.3057812750339508, 0.0045023709535598755, 0.3053310513496399]\n",
      "Step 411, loss = [0.29701563715934753, 0.0034547457471489906, 0.29667016863822937]\n",
      "Step 412, loss = [0.2817267179489136, 0.005318312905728817, 0.28119489550590515]\n",
      "Step 413, loss = [0.30346962809562683, 0.0030789112206548452, 0.30316174030303955]\n",
      "Step 414, loss = [0.28054672479629517, 0.002124327700585127, 0.2803342938423157]\n",
      "Step 415, loss = [0.30259740352630615, 0.003814408555626869, 0.30221596360206604]\n",
      "Step 416, loss = [0.30713391304016113, 0.0030861792620271444, 0.30682530999183655]\n",
      "Step 417, loss = [0.30167388916015625, 0.0021777665242552757, 0.30145612359046936]\n",
      "Step 418, loss = [0.290178120136261, 0.003295216476544738, 0.28984859585762024]\n",
      "Step 419, loss = [0.3117588460445404, 0.0053987205028533936, 0.3112189769744873]\n",
      "Step 420, loss = [0.28175103664398193, 0.00783331785351038, 0.28096771240234375]\n",
      "Step 421, loss = [0.3006370961666107, 0.0037803715094923973, 0.3002590537071228]\n",
      "Step 422, loss = [0.3062931299209595, 0.001176312449388206, 0.30617550015449524]\n",
      "Step 423, loss = [0.2974083423614502, 0.004487119615077972, 0.29695963859558105]\n",
      "Step 424, loss = [0.30938470363616943, 0.004427303560078144, 0.30894196033477783]\n",
      "Step 425, loss = [0.30294013023376465, 0.003449558513239026, 0.3025951683521271]\n",
      "Step 426, loss = [0.31242021918296814, 0.0032103373669087887, 0.3120991885662079]\n",
      "Step 427, loss = [0.2957614064216614, 0.005906524136662483, 0.29517075419425964]\n",
      "Step 428, loss = [0.30523428320884705, 0.004873066674917936, 0.30474698543548584]\n",
      "Step 429, loss = [0.27999207377433777, 0.004734475631266832, 0.27951863408088684]\n",
      "Step 430, loss = [0.31690528988838196, 0.0036202874034643173, 0.31654325127601624]\n",
      "Step 431, loss = [0.3134363889694214, 0.004185857716947794, 0.3130178153514862]\n",
      "Step 432, loss = [0.29272788763046265, 0.0066930619068443775, 0.2920585870742798]\n",
      "Step 433, loss = [0.30127274990081787, 0.005217982456088066, 0.3007509410381317]\n",
      "Step 434, loss = [0.31284743547439575, 0.0033589014783501625, 0.31251153349876404]\n",
      "Step 435, loss = [0.30522945523262024, 0.005201011896133423, 0.3047093451023102]\n",
      "Step 436, loss = [0.30839231610298157, 0.004274040460586548, 0.30796492099761963]\n",
      "Step 437, loss = [0.3016745448112488, 0.0020247665233910084, 0.3014720678329468]\n",
      "Step 438, loss = [0.30652856826782227, 0.003415831131860614, 0.3061869740486145]\n",
      "Step 439, loss = [0.293716698884964, 0.003081448609009385, 0.2934085428714752]\n",
      "Step 440, loss = [0.29738306999206543, 0.0031164821702986956, 0.2970714271068573]\n",
      "Step 441, loss = [0.3043059706687927, 0.001979994121938944, 0.3041079640388489]\n",
      "Step 442, loss = [0.3043157458305359, 0.0027946732006967068, 0.30403628945350647]\n",
      "Step 443, loss = [0.30129894614219666, 0.0016538894269615412, 0.30113354325294495]\n",
      "Step 444, loss = [0.3033920228481293, 0.0035890550352633, 0.30303311347961426]\n",
      "Step 445, loss = [0.2974931001663208, 0.00279805576428771, 0.29721328616142273]\n",
      "Step 446, loss = [0.2794095277786255, 0.00557567598298192, 0.2788519561290741]\n",
      "Step 447, loss = [0.3029131293296814, 0.0016959611093625426, 0.302743524312973]\n",
      "Step 448, loss = [0.29016223549842834, 0.004549196921288967, 0.2897073030471802]\n",
      "Step 449, loss = [0.3007565140724182, 0.004567970521748066, 0.3002997040748596]\n",
      "Step 450, loss = [0.30023932456970215, 0.0038217066321521997, 0.29985713958740234]\n",
      "Step 451, loss = [0.2887631356716156, 0.005512441508471966, 0.2882118821144104]\n",
      "Step 452, loss = [0.2874152362346649, 0.002094373805448413, 0.2872057855129242]\n",
      "Step 453, loss = [0.29123279452323914, 0.003514843061566353, 0.29088130593299866]\n",
      "Step 454, loss = [0.29752686619758606, 0.0012004788732156157, 0.2974068224430084]\n",
      "Step 455, loss = [0.295597106218338, 0.0034987404942512512, 0.29524722695350647]\n",
      "Step 456, loss = [0.31384971737861633, 0.002843620255589485, 0.31356534361839294]\n",
      "Step 457, loss = [0.30281862616539, 0.002534919185563922, 0.3025651276111603]\n",
      "Step 458, loss = [0.3048419654369354, 0.005830579437315464, 0.30425891280174255]\n",
      "Step 459, loss = [0.2978613078594208, 0.0020271483808755875, 0.2976585924625397]\n",
      "Step 460, loss = [0.2978542149066925, 0.005425456911325455, 0.2973116636276245]\n",
      "Step 461, loss = [0.3035293519496918, 0.0036797262728214264, 0.3031613826751709]\n",
      "Step 462, loss = [0.3106219470500946, 0.00628729909658432, 0.3099932074546814]\n",
      "Step 463, loss = [0.2894570827484131, 0.001780605991370976, 0.2892790138721466]\n",
      "Step 464, loss = [0.29178065061569214, 0.004976795054972172, 0.29128298163414]\n",
      "Step 465, loss = [0.3026696741580963, 0.0038309928495436907, 0.3022865653038025]\n",
      "Step 466, loss = [0.29348528385162354, 0.003099527908489108, 0.2931753396987915]\n",
      "Step 467, loss = [0.2913711369037628, 0.005838998127728701, 0.2907872498035431]\n",
      "Step 468, loss = [0.3095952570438385, 0.0022556737530976534, 0.30936968326568604]\n",
      "Step 469, loss = [0.3028636574745178, 0.004144387319684029, 0.30244922637939453]\n",
      "Step 470, loss = [0.3053659498691559, 0.004642747342586517, 0.30490168929100037]\n",
      "Step 471, loss = [0.31487902998924255, 0.008234298788011074, 0.31405559182167053]\n",
      "Step 472, loss = [0.2947257161140442, 0.0033883543219417334, 0.2943868935108185]\n",
      "Step 473, loss = [0.30531927943229675, 0.002998810727149248, 0.30501940846443176]\n",
      "Step 474, loss = [0.2889261245727539, 0.0023017884232103825, 0.28869593143463135]\n",
      "Step 475, loss = [0.29903414845466614, 0.004597221501171589, 0.29857441782951355]\n",
      "Step 476, loss = [0.30430084466934204, 0.0024052925873547792, 0.30406031012535095]\n",
      "Step 477, loss = [0.29710912704467773, 0.002991863526403904, 0.29680994153022766]\n",
      "Step 478, loss = [0.30167698860168457, 0.0041550397872924805, 0.3012614846229553]\n",
      "Step 479, loss = [0.3005027174949646, 0.006458030082285404, 0.29985690116882324]\n",
      "Step 480, loss = [0.30116400122642517, 0.002531657926738262, 0.3009108304977417]\n",
      "Step 481, loss = [0.30673179030418396, 0.005425579380244017, 0.30618923902511597]\n",
      "Step 482, loss = [0.2857697904109955, 0.0020368837285786867, 0.2855660915374756]\n",
      "Step 483, loss = [0.30893516540527344, 0.0031789690256118774, 0.3086172640323639]\n",
      "Step 484, loss = [0.3060694634914398, 0.002324832836166024, 0.3058369755744934]\n",
      "Step 485, loss = [0.3024340271949768, 0.0025909091345965862, 0.3021749258041382]\n",
      "Step 486, loss = [0.29309892654418945, 0.006302284076809883, 0.29246869683265686]\n",
      "Step 487, loss = [0.30748799443244934, 0.0058069429360330105, 0.3069072961807251]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11754 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 488, loss = [0.2983928918838501, 0.003954599611461163, 0.29799744486808777]\n",
      "Update target distribution epoch 2 step 489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11754/11754 [1:50:51<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 489, loss = [0.29700160026550293, 0.00588879594579339, 0.29641270637512207]\n",
      "Step 490, loss = [0.3108060956001282, 0.0025652276817709208, 0.3105495870113373]\n",
      "Step 491, loss = [0.3023543655872345, 0.0035225169267505407, 0.30200210213661194]\n",
      "Step 492, loss = [0.28777042031288147, 0.0022648214362561703, 0.2875439524650574]\n",
      "Step 493, loss = [0.30496469140052795, 0.005298135336488485, 0.3044348657131195]\n",
      "Step 494, loss = [0.3020792603492737, 0.008172165602445602, 0.3012620508670807]\n",
      "Step 495, loss = [0.2950313687324524, 0.002997703617438674, 0.29473158717155457]\n",
      "Step 496, loss = [0.29001179337501526, 0.0015188232064247131, 0.28985992074012756]\n",
      "Step 497, loss = [0.30225878953933716, 0.0030516409315168858, 0.30195361375808716]\n",
      "Step 498, loss = [0.2988731265068054, 0.0038702243473380804, 0.2984861135482788]\n",
      "Step 499, loss = [0.3064618706703186, 0.0032407045364379883, 0.3061378002166748]\n",
      "Step 500, loss = [0.28736528754234314, 0.004712671507149935, 0.2868940234184265]\n",
      "Step 501, loss = [0.29937493801116943, 0.006096025463193655, 0.2987653315067291]\n",
      "Step 502, loss = [0.30544018745422363, 0.0036915373057127, 0.30507102608680725]\n",
      "Step 503, loss = [0.3000418245792389, 0.005684218369424343, 0.2994734048843384]\n",
      "Step 504, loss = [0.2962386906147003, 0.0033237531315535307, 0.29590630531311035]\n",
      "Step 505, loss = [0.3050861656665802, 0.00335945887491107, 0.3047502338886261]\n",
      "Step 506, loss = [0.30265679955482483, 0.003002310637384653, 0.3023565709590912]\n",
      "Step 507, loss = [0.31309568881988525, 0.004967208951711655, 0.31259897351264954]\n",
      "Step 508, loss = [0.30183032155036926, 0.006107392720878124, 0.3012195825576782]\n",
      "Step 509, loss = [0.30469679832458496, 0.003754877718165517, 0.3043213188648224]\n",
      "Step 510, loss = [0.2897239029407501, 0.0010219982359558344, 0.2896217107772827]\n",
      "Step 511, loss = [0.2990572154521942, 0.004918703809380531, 0.2985653579235077]\n",
      "Step 512, loss = [0.30490317940711975, 0.005358744878321886, 0.3043673038482666]\n",
      "Step 513, loss = [0.3021225929260254, 0.0030444529838860035, 0.3018181622028351]\n",
      "Step 514, loss = [0.30485033988952637, 0.007311579771339893, 0.30411916971206665]\n",
      "Step 515, loss = [0.305789977312088, 0.004300612024962902, 0.30535992980003357]\n",
      "Step 516, loss = [0.2920476794242859, 0.001994361402466893, 0.29184824228286743]\n",
      "Step 517, loss = [0.3019311726093292, 0.004319517873227596, 0.30149921774864197]\n",
      "Step 518, loss = [0.2954217493534088, 0.005134603939950466, 0.2949082851409912]\n",
      "Step 519, loss = [0.2935027778148651, 0.0017228941433131695, 0.29333049058914185]\n",
      "Step 520, loss = [0.3040778636932373, 0.003960138652473688, 0.3036818504333496]\n",
      "Step 521, loss = [0.30200549960136414, 0.006576464977115393, 0.30134785175323486]\n",
      "Step 522, loss = [0.312401682138443, 0.005532127805054188, 0.3118484616279602]\n",
      "Step 523, loss = [0.28693801164627075, 0.0017197472043335438, 0.28676602244377136]\n",
      "Step 524, loss = [0.31310147047042847, 0.0038562901318073273, 0.3127158284187317]\n",
      "Step 525, loss = [0.303886741399765, 0.004801684059202671, 0.30340656638145447]\n",
      "Step 526, loss = [0.2785930335521698, 0.006191933527588844, 0.27797383069992065]\n",
      "Step 527, loss = [0.29745325446128845, 0.006330234929919243, 0.2968202233314514]\n",
      "Step 528, loss = [0.30620723962783813, 0.005430563353002071, 0.30566418170928955]\n",
      "Step 529, loss = [0.2992939352989197, 0.0041351476684212685, 0.2988804280757904]\n",
      "Step 530, loss = [0.2973140478134155, 0.005106313154101372, 0.29680341482162476]\n",
      "Step 531, loss = [0.30057501792907715, 0.004277981352061033, 0.3001472055912018]\n",
      "Step 532, loss = [0.29480114579200745, 0.0038816670421510935, 0.2944129705429077]\n",
      "Step 533, loss = [0.29602476954460144, 0.007098194677382708, 0.2953149378299713]\n",
      "Step 534, loss = [0.3103283643722534, 0.0057024965062737465, 0.30975812673568726]\n",
      "Step 535, loss = [0.3069370687007904, 0.0050833215937018394, 0.3064287304878235]\n",
      "Step 536, loss = [0.3087123930454254, 0.0037979965563863516, 0.3083325922489166]\n",
      "Step 537, loss = [0.3051007390022278, 0.007278450299054384, 0.3043729066848755]\n",
      "Step 538, loss = [0.30712994933128357, 0.0021215607412159443, 0.30691778659820557]\n",
      "Step 539, loss = [0.2970178723335266, 0.0048843384720385075, 0.2965294420719147]\n",
      "Step 540, loss = [0.30509504675865173, 0.0027381812687963247, 0.3048212230205536]\n",
      "Step 541, loss = [0.29534754157066345, 0.006207527592778206, 0.29472678899765015]\n",
      "Step 542, loss = [0.3032186031341553, 0.004757174290716648, 0.3027428984642029]\n",
      "Step 543, loss = [0.3020060956478119, 0.004171834327280521, 0.30158892273902893]\n",
      "Step 544, loss = [0.2963043451309204, 0.0053222267888486385, 0.29577213525772095]\n",
      "Step 545, loss = [0.29452264308929443, 0.005905117839574814, 0.29393213987350464]\n",
      "Step 546, loss = [0.3159876763820648, 0.003587129060178995, 0.3156289756298065]\n",
      "Step 547, loss = [0.30074334144592285, 0.0038305018097162247, 0.3003602921962738]\n",
      "Step 548, loss = [0.3105663061141968, 0.006662934552878141, 0.3099000155925751]\n",
      "Step 549, loss = [0.3025226891040802, 0.0028225616551935673, 0.30224043130874634]\n",
      "Step 550, loss = [0.2902984619140625, 0.004602889064699411, 0.28983816504478455]\n",
      "Step 551, loss = [0.3056739866733551, 0.0057953838258981705, 0.305094450712204]\n",
      "Step 552, loss = [0.29886215925216675, 0.0031490724068135023, 0.29854723811149597]\n",
      "Step 553, loss = [0.2994065582752228, 0.0042691645212471485, 0.29897964000701904]\n",
      "Step 554, loss = [0.2991297245025635, 0.0022174150217324495, 0.298907995223999]\n",
      "Step 555, loss = [0.3026006817817688, 0.002627295907586813, 0.3023379445075989]\n",
      "Step 556, loss = [0.3071618676185608, 0.00462573766708374, 0.30669930577278137]\n",
      "Step 557, loss = [0.2867892384529114, 0.0037260910030454397, 0.286416620016098]\n",
      "Step 558, loss = [0.2914259135723114, 0.0026076119393110275, 0.29116514325141907]\n",
      "Step 559, loss = [0.3007836639881134, 0.007867979817092419, 0.29999685287475586]\n",
      "Step 560, loss = [0.29468587040901184, 0.002147386781871319, 0.2944711446762085]\n",
      "Step 561, loss = [0.30143865942955017, 0.003224354237318039, 0.3011162281036377]\n",
      "Step 562, loss = [0.3037042021751404, 0.004778094124048948, 0.30322638154029846]\n",
      "Step 563, loss = [0.2984463572502136, 0.006244990509003401, 0.29782184958457947]\n",
      "Step 564, loss = [0.2849845588207245, 0.003532464848831296, 0.28463131189346313]\n",
      "Step 565, loss = [0.2952350080013275, 0.004858029540628195, 0.2947492003440857]\n",
      "Step 566, loss = [0.31062161922454834, 0.002022650558501482, 0.31041935086250305]\n",
      "Step 567, loss = [0.2941382825374603, 0.004064504988491535, 0.29373183846473694]\n",
      "Step 568, loss = [0.3027168810367584, 0.004922178573906422, 0.30222466588020325]\n",
      "Step 569, loss = [0.3056895136833191, 0.004076468292623758, 0.3052818775177002]\n",
      "Step 570, loss = [0.305400550365448, 0.007019823882728815, 0.30469855666160583]\n",
      "Step 571, loss = [0.29607588052749634, 0.005573241971433163, 0.29551854729652405]\n",
      "Step 572, loss = [0.30965086817741394, 0.007734533399343491, 0.3088774085044861]\n",
      "Step 573, loss = [0.307245135307312, 0.006178329233080149, 0.3066273033618927]\n",
      "Step 574, loss = [0.31171557307243347, 0.0028180377557873726, 0.3114337623119354]\n",
      "Step 575, loss = [0.29060685634613037, 0.007133609149605036, 0.2898935079574585]\n",
      "Step 576, loss = [0.27878859639167786, 0.00422362145036459, 0.27836623787879944]\n",
      "Step 577, loss = [0.2991695702075958, 0.0053161922842264175, 0.2986379563808441]\n",
      "Step 578, loss = [0.2917451858520508, 0.003883692901581526, 0.29135680198669434]\n",
      "Step 579, loss = [0.29061001539230347, 0.0040885815396904945, 0.2902011573314667]\n",
      "Step 580, loss = [0.3084772229194641, 0.004527897574007511, 0.30802443623542786]\n",
      "Step 581, loss = [0.29498907923698425, 0.006618766114115715, 0.2943271994590759]\n",
      "Step 582, loss = [0.28508102893829346, 0.003863633843138814, 0.2846946716308594]\n",
      "Step 583, loss = [0.2978413999080658, 0.006457231473177671, 0.2971956729888916]\n",
      "Step 584, loss = [0.290835976600647, 0.003304576501250267, 0.2905055284500122]\n",
      "Step 585, loss = [0.29811832308769226, 0.0029546276200562716, 0.29782286286354065]\n",
      "Step 586, loss = [0.29960909485816956, 0.004600337240844965, 0.2991490662097931]\n",
      "Step 587, loss = [0.30622565746307373, 0.004210898652672768, 0.305804580450058]\n",
      "Step 588, loss = [0.3007589280605316, 0.005187308415770531, 0.3002401888370514]\n",
      "Step 589, loss = [0.2953142821788788, 0.005505997221916914, 0.2947636842727661]\n",
      "Step 590, loss = [0.29409193992614746, 0.0028267884626984596, 0.29380926489830017]\n",
      "Step 591, loss = [0.2984892725944519, 0.0037612111773341894, 0.2981131374835968]\n",
      "Step 592, loss = [0.29176199436187744, 0.004772147163748741, 0.2912847697734833]\n",
      "Step 593, loss = [0.3129194378852844, 0.00206775707192719, 0.3127126693725586]\n",
      "Step 594, loss = [0.3063543736934662, 0.005958515219390392, 0.305758535861969]\n",
      "Step 595, loss = [0.3022758364677429, 0.0033230746630579233, 0.3019435405731201]\n",
      "Step 596, loss = [0.3095262348651886, 0.006872240453958511, 0.30883902311325073]\n",
      "Step 597, loss = [0.2885921895503998, 0.0013370368396863341, 0.2884584963321686]\n",
      "Step 598, loss = [0.31025025248527527, 0.0022591515444219112, 0.31002435088157654]\n",
      "Step 599, loss = [0.297296404838562, 0.0026816946920007467, 0.29702824354171753]\n",
      "Step 600, loss = [0.29572203755378723, 0.0016433058772236109, 0.2955577075481415]\n",
      "Step 601, loss = [0.3014319837093353, 0.002448340877890587, 0.3011871576309204]\n",
      "Step 602, loss = [0.3079524636268616, 0.0037813459057360888, 0.3075743317604065]\n",
      "Step 603, loss = [0.3029569983482361, 0.003435098100453615, 0.3026134967803955]\n",
      "Step 604, loss = [0.3068476915359497, 0.005317034665495157, 0.30631598830223083]\n",
      "Step 605, loss = [0.29840362071990967, 0.0032437965273857117, 0.298079252243042]\n",
      "Step 606, loss = [0.2908741533756256, 0.0028964418452233076, 0.2905845046043396]\n",
      "Step 607, loss = [0.3066509962081909, 0.005797594785690308, 0.3060712218284607]\n",
      "Step 608, loss = [0.2919115722179413, 0.004880386870354414, 0.2914235293865204]\n",
      "Step 609, loss = [0.30125099420547485, 0.004989634733647108, 0.30075204372406006]\n",
      "Step 610, loss = [0.3037009835243225, 0.0077982936054468155, 0.3029211461544037]\n",
      "Step 611, loss = [0.3105226159095764, 0.0052377162501215935, 0.30999884009361267]\n",
      "Step 612, loss = [0.3029065430164337, 0.0036224834620952606, 0.3025442957878113]\n",
      "Step 613, loss = [0.29303762316703796, 0.003563679289072752, 0.2926812469959259]\n",
      "Step 614, loss = [0.30627939105033875, 0.00524730421602726, 0.3057546615600586]\n",
      "Step 615, loss = [0.2824089527130127, 0.0030180036555975676, 0.2821071445941925]\n",
      "Step 616, loss = [0.29634931683540344, 0.003721412271261215, 0.2959771752357483]\n",
      "Step 617, loss = [0.3097284436225891, 0.006200361996889114, 0.3091084063053131]\n",
      "Step 618, loss = [0.3028390109539032, 0.004933392163366079, 0.3023456633090973]\n",
      "Step 619, loss = [0.30540168285369873, 0.005866392515599728, 0.30481505393981934]\n",
      "Step 620, loss = [0.2813934087753296, 0.0028190002776682377, 0.2811115086078644]\n",
      "Step 621, loss = [0.2965163290500641, 0.004920434672385454, 0.29602429270744324]\n",
      "Step 622, loss = [0.2886499762535095, 0.004569347016513348, 0.2881930470466614]\n",
      "Step 623, loss = [0.29962384700775146, 0.003857501782476902, 0.29923808574676514]\n",
      "Step 624, loss = [0.30665916204452515, 0.0029679075814783573, 0.3063623607158661]\n",
      "Step 625, loss = [0.2790870666503906, 0.0030708753038197756, 0.2787799835205078]\n",
      "Step 626, loss = [0.2984478175640106, 0.003503592684864998, 0.2980974614620209]\n",
      "Step 627, loss = [0.3053572475910187, 0.015554413199424744, 0.3038018047809601]\n",
      "Step 628, loss = [0.30094224214553833, 0.006891118362545967, 0.30025312304496765]\n",
      "Step 629, loss = [0.2821404039859772, 0.005812415853142738, 0.28155916929244995]\n",
      "Step 630, loss = [0.3050245940685272, 0.0016027481760829687, 0.3048643171787262]\n",
      "Step 631, loss = [0.31182190775871277, 0.006608005613088608, 0.3111611008644104]\n",
      "Step 632, loss = [0.3103865385055542, 0.00663765100762248, 0.30972278118133545]\n",
      "Step 633, loss = [0.3116731345653534, 0.0044890278950333595, 0.31122422218322754]\n",
      "Step 634, loss = [0.2950684130191803, 0.009313253685832024, 0.2941370904445648]\n",
      "Step 635, loss = [0.3017140030860901, 0.003229364287108183, 0.301391065120697]\n",
      "Step 636, loss = [0.2881455719470978, 0.009234382770955563, 0.28722214698791504]\n",
      "Step 637, loss = [0.29749763011932373, 0.007067038677632809, 0.2967909276485443]\n",
      "Step 638, loss = [0.292644739151001, 0.0032044262625277042, 0.2923243045806885]\n",
      "Step 639, loss = [0.3067333400249481, 0.0036055922973901033, 0.3063727915287018]\n",
      "Step 640, loss = [0.29721957445144653, 0.001421549590304494, 0.2970774173736572]\n",
      "Step 641, loss = [0.3076048493385315, 0.0042387256398797035, 0.3071809709072113]\n",
      "Step 642, loss = [0.296959787607193, 0.0040483782067894936, 0.29655495285987854]\n",
      "Step 643, loss = [0.2968181371688843, 0.004888696130365133, 0.2963292598724365]\n",
      "Step 644, loss = [0.30127644538879395, 0.002959609031677246, 0.30098047852516174]\n",
      "Step 645, loss = [0.31171706318855286, 0.00810086727142334, 0.3109069764614105]\n",
      "Step 646, loss = [0.30094027519226074, 0.004059756174683571, 0.30053430795669556]\n",
      "Step 647, loss = [0.30606809258461, 0.005084964446723461, 0.30555960536003113]\n",
      "Step 648, loss = [0.30227649211883545, 0.0036008269526064396, 0.3019164204597473]\n",
      "Step 649, loss = [0.29541465640068054, 0.0020969149190932512, 0.2952049672603607]\n",
      "Step 650, loss = [0.31120872497558594, 0.0056600915268063545, 0.31064271926879883]\n",
      "Step 651, loss = [0.2955799996852875, 0.0027981745079159737, 0.2953001856803894]\n",
      "Step 652, loss = [0.2775474786758423, 0.004346095025539398, 0.2771128714084625]\n",
      "Step 653, loss = [0.3043701648712158, 0.004919611848890781, 0.30387821793556213]\n",
      "Step 654, loss = [0.31358975172042847, 0.003381451591849327, 0.3132516145706177]\n",
      "Step 655, loss = [0.30319929122924805, 0.003678489476442337, 0.3028314411640167]\n",
      "Step 656, loss = [0.2999019920825958, 0.006573939695954323, 0.29924461245536804]\n",
      "Step 657, loss = [0.29205048084259033, 0.006346231792122126, 0.29141587018966675]\n",
      "Step 658, loss = [0.306041419506073, 0.005024257581681013, 0.30553898215293884]\n",
      "Step 659, loss = [0.27978774905204773, 0.00649167038500309, 0.27913859486579895]\n",
      "Step 660, loss = [0.3040940463542938, 0.004577161278575659, 0.3036363422870636]\n",
      "Step 661, loss = [0.29829174280166626, 0.00442853569984436, 0.2978488802909851]\n",
      "Step 662, loss = [0.29692721366882324, 0.007404436822980642, 0.29618677496910095]\n",
      "Step 663, loss = [0.29035359621047974, 0.003349769627675414, 0.29001861810684204]\n",
      "Step 664, loss = [0.3006128966808319, 0.0035888662096112967, 0.3002540171146393]\n",
      "Step 665, loss = [0.3032011091709137, 0.00284059951081872, 0.3029170632362366]\n",
      "Step 666, loss = [0.29357102513313293, 0.0022689616307616234, 0.2933441400527954]\n",
      "Step 667, loss = [0.3071553409099579, 0.002910402836278081, 0.30686429142951965]\n",
      "Step 668, loss = [0.30696946382522583, 0.006454920396208763, 0.30632397532463074]\n",
      "Step 669, loss = [0.3083585202693939, 0.003720248816534877, 0.3079864978790283]\n",
      "Step 670, loss = [0.3059310019016266, 0.003743498120456934, 0.30555665493011475]\n",
      "Step 671, loss = [0.28882965445518494, 0.0031815830152481794, 0.2885114848613739]\n",
      "Step 672, loss = [0.2904370129108429, 0.0037502353079617023, 0.29006198048591614]\n",
      "Step 673, loss = [0.30405905842781067, 0.007518453057855368, 0.3033072054386139]\n",
      "Step 674, loss = [0.28840646147727966, 0.003369120880961418, 0.28806954622268677]\n",
      "Step 675, loss = [0.3134652376174927, 0.004764779936522245, 0.3129887580871582]\n",
      "Step 676, loss = [0.3030065894126892, 0.0049593690782785416, 0.30251064896583557]\n",
      "Step 677, loss = [0.3010677099227905, 0.003443629713729024, 0.3007233440876007]\n",
      "Step 678, loss = [0.3135550022125244, 0.0031180635560303926, 0.31324321031570435]\n",
      "Step 679, loss = [0.3020462095737457, 0.006842352915555239, 0.30136197805404663]\n",
      "Step 680, loss = [0.29873672127723694, 0.003409068565815687, 0.2983958125114441]\n",
      "Step 681, loss = [0.30896684527397156, 0.00392662500962615, 0.3085741698741913]\n",
      "Step 682, loss = [0.30140936374664307, 0.005663941614329815, 0.3008429706096649]\n",
      "Step 683, loss = [0.3002597689628601, 0.004576847888529301, 0.29980209469795227]\n",
      "Step 684, loss = [0.29308703541755676, 0.0053342110477387905, 0.2925536036491394]\n",
      "Step 685, loss = [0.2863973081111908, 0.005268879234790802, 0.28587043285369873]\n",
      "Step 686, loss = [0.287515252828598, 0.002706144005060196, 0.28724464774131775]\n",
      "Step 687, loss = [0.29373899102211, 0.004406281281262636, 0.2932983636856079]\n",
      "Step 688, loss = [0.3018619418144226, 0.003866353537887335, 0.30147531628608704]\n",
      "Step 689, loss = [0.29638829827308655, 0.006189875304698944, 0.2957693040370941]\n",
      "Step 690, loss = [0.29183444380760193, 0.0030942033044993877, 0.2915250360965729]\n",
      "Step 691, loss = [0.2935642898082733, 0.005844204220920801, 0.2929798662662506]\n",
      "Step 692, loss = [0.294743150472641, 0.0019428430823609233, 0.2945488691329956]\n",
      "Step 693, loss = [0.299224317073822, 0.001083124428987503, 0.29911601543426514]\n",
      "Step 694, loss = [0.2869751751422882, 0.004176495596766472, 0.28655752539634705]\n",
      "Step 695, loss = [0.27657243609428406, 0.006213690619915724, 0.2759510576725006]\n",
      "Step 696, loss = [0.29274678230285645, 0.004169903229922056, 0.2923297882080078]\n",
      "Step 697, loss = [0.30323633551597595, 0.003549255896359682, 0.3028814196586609]\n",
      "Step 698, loss = [0.30713093280792236, 0.005350535735487938, 0.30659589171409607]\n",
      "Step 699, loss = [0.3027805685997009, 0.0031695994548499584, 0.3024636209011078]\n",
      "Step 700, loss = [0.2943801283836365, 0.0037453602999448776, 0.2940056025981903]\n",
      "Step 701, loss = [0.2901027202606201, 0.0027580438181757927, 0.2898269295692444]\n",
      "Step 702, loss = [0.27864548563957214, 0.00399965513497591, 0.2782455086708069]\n",
      "Step 703, loss = [0.29427579045295715, 0.002452664077281952, 0.2940305173397064]\n",
      "Step 704, loss = [0.2935813069343567, 0.005402263253927231, 0.29304108023643494]\n",
      "Step 705, loss = [0.2896347939968109, 0.006055064965039492, 0.2890293002128601]\n",
      "Step 706, loss = [0.3122822046279907, 0.008154158480465412, 0.311466783285141]\n",
      "Step 707, loss = [0.2906794548034668, 0.0015651339199393988, 0.2905229330062866]\n",
      "Step 708, loss = [0.3024772107601166, 0.00512740621343255, 0.3019644618034363]\n",
      "Step 709, loss = [0.3056943714618683, 0.0055161816999316216, 0.30514276027679443]\n",
      "Step 710, loss = [0.30337709188461304, 0.003286231542006135, 0.3030484616756439]\n",
      "Step 711, loss = [0.2966151237487793, 0.003460492240265012, 0.29626908898353577]\n",
      "Step 712, loss = [0.3001498878002167, 0.005581761710345745, 0.29959172010421753]\n",
      "Step 713, loss = [0.2914378345012665, 0.004373155999928713, 0.29100051522254944]\n",
      "Step 714, loss = [0.29871493577957153, 0.006254215259104967, 0.29808950424194336]\n",
      "Step 715, loss = [0.30651843547821045, 0.002224475145339966, 0.3062959909439087]\n",
      "Step 716, loss = [0.3046931028366089, 0.005709927994757891, 0.30412212014198303]\n",
      "Step 717, loss = [0.30055296421051025, 0.0018173402640968561, 0.3003712296485901]\n",
      "Step 718, loss = [0.29403409361839294, 0.006036809645593166, 0.2934304177761078]\n",
      "Step 719, loss = [0.30904698371887207, 0.005044751800596714, 0.30854251980781555]\n",
      "Step 720, loss = [0.3072524964809418, 0.004281168803572655, 0.30682438611984253]\n",
      "Step 721, loss = [0.2999840974807739, 0.00527444202452898, 0.2994566559791565]\n",
      "Step 722, loss = [0.296098917722702, 0.0032321694307029247, 0.29577571153640747]\n",
      "Step 723, loss = [0.29830726981163025, 0.00751532893627882, 0.29755574464797974]\n",
      "Step 724, loss = [0.3067624270915985, 0.005614303983747959, 0.3062010109424591]\n",
      "Step 725, loss = [0.29830679297447205, 0.007527093403041363, 0.297554075717926]\n",
      "Step 726, loss = [0.3027014136314392, 0.004685051739215851, 0.30223292112350464]\n",
      "Step 727, loss = [0.29843780398368835, 0.006187348160892725, 0.2978190779685974]\n",
      "Step 728, loss = [0.29617366194725037, 0.004561379086226225, 0.2957175374031067]\n",
      "Step 729, loss = [0.3024347126483917, 0.00586874270811677, 0.3018478453159332]\n",
      "Step 730, loss = [0.2838709056377411, 0.004402288235723972, 0.28343066573143005]\n",
      "Step 731, loss = [0.3094792664051056, 0.005393476225435734, 0.3089399039745331]\n",
      "Step 732, loss = [0.2982906699180603, 0.005805640947073698, 0.297710120677948]\n",
      "Step 733, loss = [0.29537370800971985, 0.0035370327532291412, 0.2950200140476227]\n",
      "Step 734, loss = [0.2950507402420044, 0.007490228395909071, 0.29430171847343445]\n",
      "Step 735, loss = [0.3072580099105835, 0.0034132383298128843, 0.3069166839122772]\n",
      "Step 736, loss = [0.302430123090744, 0.006357104517519474, 0.3017944097518921]\n",
      "Step 737, loss = [0.30956798791885376, 0.004181868862360716, 0.3091498017311096]\n",
      "Step 738, loss = [0.28857696056365967, 0.004472132306545973, 0.2881297469139099]\n",
      "Step 739, loss = [0.2960718274116516, 0.0036512422375380993, 0.29570668935775757]\n",
      "Step 740, loss = [0.29918932914733887, 0.002577739069238305, 0.2989315688610077]\n",
      "Step 741, loss = [0.304044246673584, 0.003645090851932764, 0.3036797344684601]\n",
      "Step 742, loss = [0.30579960346221924, 0.00848078541457653, 0.3049515187740326]\n",
      "Step 743, loss = [0.3024628162384033, 0.007495874539017677, 0.301713228225708]\n",
      "Step 744, loss = [0.31190887093544006, 0.008326931856572628, 0.31107616424560547]\n",
      "Step 745, loss = [0.3060944974422455, 0.0026955213397741318, 0.3058249354362488]\n",
      "Step 746, loss = [0.30927157402038574, 0.004202625714242458, 0.30885130167007446]\n",
      "Step 747, loss = [0.3137999176979065, 0.006662722676992416, 0.3131336569786072]\n",
      "Step 748, loss = [0.29813387989997864, 0.001673939754255116, 0.29796648025512695]\n",
      "Step 749, loss = [0.30101075768470764, 0.0033844609279185534, 0.300672322511673]\n",
      "Step 750, loss = [0.3018004596233368, 0.0045610396191477776, 0.3013443648815155]\n",
      "Step 751, loss = [0.2895338535308838, 0.003411970566958189, 0.28919264674186707]\n",
      "Step 752, loss = [0.29935702681541443, 0.004216963425278664, 0.29893532395362854]\n",
      "Step 753, loss = [0.295861154794693, 0.006328602787107229, 0.2952283024787903]\n",
      "Step 754, loss = [0.3040507733821869, 0.002883756533265114, 0.30376240611076355]\n",
      "Step 755, loss = [0.2989782392978668, 0.00323494547046721, 0.2986547350883484]\n",
      "Step 756, loss = [0.3002278804779053, 0.00608496880158782, 0.2996193766593933]\n",
      "Step 757, loss = [0.3038557469844818, 0.006516634486615658, 0.30320408940315247]\n",
      "Step 758, loss = [0.30536821484565735, 0.006424927618354559, 0.3047257363796234]\n",
      "Step 759, loss = [0.3061034083366394, 0.0027919493149966, 0.3058242201805115]\n",
      "Step 760, loss = [0.3061911463737488, 0.004761498421430588, 0.30571499466896057]\n",
      "Step 761, loss = [0.2825305759906769, 0.006285548210144043, 0.281902015209198]\n",
      "Step 762, loss = [0.28923770785331726, 0.005171735771000385, 0.2887205481529236]\n",
      "Step 763, loss = [0.3112717270851135, 0.005560291465371847, 0.3107157051563263]\n",
      "Step 764, loss = [0.28975024819374084, 0.0030764020048081875, 0.28944259881973267]\n",
      "Step 765, loss = [0.3001425266265869, 0.007206522393971682, 0.29942187666893005]\n",
      "Step 766, loss = [0.2997090518474579, 0.0049115861766040325, 0.2992178797721863]\n",
      "Step 767, loss = [0.28974467515945435, 0.0029008775018155575, 0.2894545793533325]\n",
      "Step 768, loss = [0.27321064472198486, 0.0036337978672236204, 0.2728472650051117]\n",
      "Step 769, loss = [0.2859514653682709, 0.00461492920294404, 0.2854899764060974]\n",
      "Step 770, loss = [0.2816372513771057, 0.002996839117258787, 0.28133755922317505]\n",
      "Step 771, loss = [0.2896125614643097, 0.005379586014896631, 0.2890745997428894]\n",
      "Step 772, loss = [0.30414077639579773, 0.0031891819089651108, 0.303821861743927]\n",
      "Step 773, loss = [0.30234262347221375, 0.010314526036381721, 0.3013111650943756]\n",
      "Step 774, loss = [0.30305686593055725, 0.0034899248275905848, 0.30270788073539734]\n",
      "Step 775, loss = [0.3015685975551605, 0.0038885409012436867, 0.3011797368526459]\n",
      "Step 776, loss = [0.2981353998184204, 0.0032920544035732746, 0.2978062033653259]\n",
      "Step 777, loss = [0.3090228736400604, 0.004400999750941992, 0.30858278274536133]\n",
      "Step 778, loss = [0.30101117491722107, 0.0058321175165474415, 0.30042797327041626]\n",
      "Step 779, loss = [0.2997525632381439, 0.005026983562856913, 0.2992498576641083]\n",
      "Step 780, loss = [0.293269544839859, 0.003260320518165827, 0.2929435074329376]\n",
      "Step 781, loss = [0.29672473669052124, 0.005171424709260464, 0.29620760679244995]\n",
      "Step 782, loss = [0.2984279990196228, 0.004278914071619511, 0.2980000972747803]\n",
      "Step 783, loss = [0.2948370575904846, 0.005767226219177246, 0.29426032304763794]\n",
      "Step 784, loss = [0.3154541552066803, 0.007014499511569738, 0.3147526979446411]\n",
      "Step 785, loss = [0.306445449590683, 0.0050744605250656605, 0.3059380054473877]\n",
      "Step 786, loss = [0.3015993535518646, 0.005879819858819246, 0.3010113835334778]\n",
      "Step 787, loss = [0.30460068583488464, 0.004769984632730484, 0.3041236996650696]\n",
      "Step 788, loss = [0.30667808651924133, 0.003058934584259987, 0.30637219548225403]\n",
      "Step 789, loss = [0.3039783835411072, 0.003991276025772095, 0.3035792410373688]\n",
      "Step 790, loss = [0.3046445846557617, 0.0022734287194907665, 0.3044172525405884]\n",
      "Step 791, loss = [0.3014447093009949, 0.0056542893871665, 0.30087926983833313]\n",
      "Step 792, loss = [0.30344730615615845, 0.0074958098120987415, 0.30269771814346313]\n",
      "Step 793, loss = [0.29395946860313416, 0.005072195082902908, 0.29345226287841797]\n",
      "Step 794, loss = [0.3135237395763397, 0.0043918676674366, 0.31308454275131226]\n",
      "Step 795, loss = [0.29348334670066833, 0.0016677739331498742, 0.2933165729045868]\n",
      "Step 796, loss = [0.310657262802124, 0.00340825947932899, 0.31031644344329834]\n",
      "Step 797, loss = [0.299226850271225, 0.008222955279052258, 0.2984045445919037]\n",
      "Step 798, loss = [0.2923189103603363, 0.005291171371936798, 0.29178979992866516]\n",
      "Step 799, loss = [0.2912851572036743, 0.0073860036209225655, 0.29054656624794006]\n",
      "Step 800, loss = [0.30760765075683594, 0.005404896102845669, 0.3070671558380127]\n",
      "Step 801, loss = [0.3047432601451874, 0.006141947116702795, 0.30412906408309937]\n",
      "Step 802, loss = [0.30774128437042236, 0.005703520029783249, 0.30717092752456665]\n",
      "Step 803, loss = [0.29702478647232056, 0.004939038772135973, 0.2965308725833893]\n",
      "Step 804, loss = [0.302773118019104, 0.004688599146902561, 0.3023042678833008]\n",
      "Step 805, loss = [0.3096645474433899, 0.00552357267588377, 0.30911219120025635]\n",
      "Step 806, loss = [0.3041629493236542, 0.005646649748086929, 0.3035982847213745]\n",
      "Step 807, loss = [0.30190354585647583, 0.008672461844980717, 0.3010362982749939]\n",
      "Step 808, loss = [0.29905566573143005, 0.004013780038803816, 0.2986542880535126]\n",
      "Step 809, loss = [0.2906106412410736, 0.0036119408905506134, 0.29024943709373474]\n",
      "Step 810, loss = [0.3081878423690796, 0.006991958245635033, 0.30748865008354187]\n",
      "Step 811, loss = [0.30270349979400635, 0.0049368091858923435, 0.3022098243236542]\n",
      "Step 812, loss = [0.2891327440738678, 0.00866090040653944, 0.288266658782959]\n",
      "Step 813, loss = [0.30478817224502563, 0.006181703880429268, 0.30417001247406006]\n",
      "Step 814, loss = [0.2919493317604065, 0.0043718512170016766, 0.2915121614933014]\n",
      "Step 815, loss = [0.3032713830471039, 0.007109079975634813, 0.3025604784488678]\n",
      "Step 816, loss = [0.2996429204940796, 0.002515042433515191, 0.29939141869544983]\n",
      "Step 817, loss = [0.3101525902748108, 0.0026596831157803535, 0.309886634349823]\n",
      "Step 818, loss = [0.2904813587665558, 0.0019676117226481438, 0.2902846038341522]\n",
      "Step 819, loss = [0.3085196018218994, 0.004374428652226925, 0.3080821633338928]\n",
      "Step 820, loss = [0.2958119809627533, 0.005342664197087288, 0.2952777147293091]\n",
      "Step 821, loss = [0.2957824468612671, 0.003751856042072177, 0.2954072654247284]\n",
      "Step 822, loss = [0.3023284077644348, 0.0047698779962956905, 0.30185142159461975]\n",
      "Step 823, loss = [0.3133261501789093, 0.0044922917149960995, 0.3128769099712372]\n",
      "Step 824, loss = [0.3034542202949524, 0.00536138704046607, 0.30291807651519775]\n",
      "Step 825, loss = [0.28797364234924316, 0.005129307508468628, 0.28746071457862854]\n",
      "Step 826, loss = [0.2989063858985901, 0.004142290912568569, 0.2984921634197235]\n",
      "Step 827, loss = [0.29790735244750977, 0.002526756376028061, 0.2976546883583069]\n",
      "Step 828, loss = [0.29915085434913635, 0.004404852166771889, 0.2987103760242462]\n",
      "Step 829, loss = [0.2926425337791443, 0.0029236353002488613, 0.292350172996521]\n",
      "Step 830, loss = [0.30370235443115234, 0.0034666634164750576, 0.30335569381713867]\n",
      "Step 831, loss = [0.27764075994491577, 0.002401369158178568, 0.2774006128311157]\n",
      "Step 832, loss = [0.28883740305900574, 0.003754527075216174, 0.28846195340156555]\n",
      "Step 833, loss = [0.30341729521751404, 0.00390821136534214, 0.3030264675617218]\n",
      "Step 834, loss = [0.30171388387680054, 0.0015603727661073208, 0.30155783891677856]\n",
      "Step 835, loss = [0.30567923188209534, 0.005518320947885513, 0.30512741208076477]\n",
      "Step 836, loss = [0.3021719455718994, 0.005106464494019747, 0.30166131258010864]\n",
      "Step 837, loss = [0.29529452323913574, 0.0019872081466019154, 0.2950958013534546]\n",
      "Step 838, loss = [0.3004619777202606, 0.005236953962594271, 0.29993829131126404]\n",
      "Step 839, loss = [0.2993791103363037, 0.006328478455543518, 0.298746258020401]\n",
      "Step 840, loss = [0.3020341694355011, 0.0032359077595174313, 0.3017105758190155]\n",
      "Step 841, loss = [0.275707483291626, 0.00559374550357461, 0.2751481235027313]\n",
      "Step 842, loss = [0.29580003023147583, 0.0026201806031167507, 0.2955380082130432]\n",
      "Step 843, loss = [0.3130897581577301, 0.008900061249732971, 0.31219974160194397]\n",
      "Step 844, loss = [0.28357842564582825, 0.006344836205244064, 0.2829439342021942]\n",
      "Step 845, loss = [0.30294978618621826, 0.006028365809470415, 0.30234694480895996]\n",
      "Step 846, loss = [0.30176448822021484, 0.006564854644238949, 0.3011080026626587]\n",
      "Step 847, loss = [0.30933666229248047, 0.003169965697452426, 0.30901965498924255]\n",
      "Step 848, loss = [0.2954566478729248, 0.0033713465090841055, 0.2951195240020752]\n",
      "Step 849, loss = [0.29683375358581543, 0.0024525399785488844, 0.2965885102748871]\n",
      "Step 850, loss = [0.29643160104751587, 0.004416009411215782, 0.295989990234375]\n",
      "Step 851, loss = [0.2909232974052429, 0.0031868310179561377, 0.2906046211719513]\n",
      "Step 852, loss = [0.3034067153930664, 0.0017502496484667063, 0.30323168635368347]\n",
      "Step 853, loss = [0.30782076716423035, 0.004334656056016684, 0.3073872923851013]\n",
      "Step 854, loss = [0.29504868388175964, 0.001334860920906067, 0.29491519927978516]\n",
      "Step 855, loss = [0.30273643136024475, 0.0049459850415587425, 0.30224183201789856]\n",
      "Step 856, loss = [0.30455437302589417, 0.00479415338486433, 0.3040749430656433]\n",
      "Step 857, loss = [0.30300283432006836, 0.005620784126222134, 0.3024407625198364]\n",
      "Step 858, loss = [0.30420440435409546, 0.0045469654724001884, 0.3037497103214264]\n",
      "Step 859, loss = [0.3107192814350128, 0.00761291291564703, 0.30995798110961914]\n",
      "Step 860, loss = [0.29501378536224365, 0.005141756497323513, 0.29449960589408875]\n",
      "Step 861, loss = [0.3083595335483551, 0.0035566450096666813, 0.30800387263298035]\n",
      "Step 862, loss = [0.29928502440452576, 0.002791906241327524, 0.2990058362483978]\n",
      "Step 863, loss = [0.3002481162548065, 0.0008511179476045072, 0.30016300082206726]\n",
      "Step 864, loss = [0.2984427511692047, 0.0028963761869817972, 0.2981531023979187]\n",
      "Step 865, loss = [0.30190789699554443, 0.005815837997943163, 0.30132630467414856]\n",
      "Step 866, loss = [0.3016132712364197, 0.004496913868933916, 0.30116358399391174]\n",
      "Step 867, loss = [0.3042212426662445, 0.0049802884459495544, 0.30372321605682373]\n",
      "Step 868, loss = [0.2906106114387512, 0.0019173750188201666, 0.2904188632965088]\n",
      "Step 869, loss = [0.2978019714355469, 0.00256803329102695, 0.2975451648235321]\n",
      "Step 870, loss = [0.29847782850265503, 0.0038146553561091423, 0.29809635877609253]\n",
      "Step 871, loss = [0.3109631836414337, 0.004719526506960392, 0.3104912340641022]\n",
      "Step 872, loss = [0.30112704634666443, 0.005339883267879486, 0.3005930483341217]\n",
      "Step 873, loss = [0.2884155213832855, 0.005100924521684647, 0.28790542483329773]\n",
      "Step 874, loss = [0.2916557788848877, 0.002832661150023341, 0.29137250781059265]\n",
      "Step 875, loss = [0.3046439588069916, 0.003383353818207979, 0.3043056130409241]\n",
      "Step 876, loss = [0.2980745732784271, 0.0019132823217660189, 0.2978832423686981]\n",
      "Step 877, loss = [0.2933475375175476, 0.002275526989251375, 0.29311999678611755]\n",
      "Step 878, loss = [0.30685603618621826, 0.0056568048894405365, 0.3062903583049774]\n",
      "Step 879, loss = [0.2785370349884033, 0.004791892133653164, 0.27805784344673157]\n",
      "Step 880, loss = [0.3014546036720276, 0.005447965115308762, 0.3009098172187805]\n",
      "Step 881, loss = [0.28748995065689087, 0.005368706304579973, 0.2869530916213989]\n",
      "Step 882, loss = [0.30291494727134705, 0.003750454168766737, 0.3025399148464203]\n",
      "Step 883, loss = [0.293875515460968, 0.006160680204629898, 0.2932594418525696]\n",
      "Step 884, loss = [0.30824407935142517, 0.006560806185007095, 0.30758801102638245]\n",
      "Step 885, loss = [0.3031547963619232, 0.007313128560781479, 0.30242347717285156]\n",
      "Step 886, loss = [0.280489444732666, 0.004401640500873327, 0.28004929423332214]\n",
      "Step 887, loss = [0.2933686673641205, 0.002915311371907592, 0.29307714104652405]\n",
      "Step 888, loss = [0.3019590675830841, 0.00319663155823946, 0.3016394078731537]\n",
      "Step 889, loss = [0.2929002642631531, 0.0020229395013302565, 0.2926979660987854]\n",
      "Step 890, loss = [0.2867877185344696, 0.004394107963889837, 0.2863483130931854]\n",
      "Step 891, loss = [0.30246809124946594, 0.0028606532141566277, 0.30218201875686646]\n",
      "Step 892, loss = [0.29640644788742065, 0.00563054159283638, 0.2958433926105499]\n",
      "Step 893, loss = [0.29115381836891174, 0.009756111539900303, 0.29017820954322815]\n",
      "Step 894, loss = [0.30073973536491394, 0.005171649158000946, 0.30022257566452026]\n",
      "Step 895, loss = [0.303126722574234, 0.003730031196027994, 0.3027537167072296]\n",
      "Step 896, loss = [0.3006843626499176, 0.004794437903910875, 0.30020493268966675]\n",
      "Step 897, loss = [0.2984711527824402, 0.004974170122295618, 0.29797372221946716]\n",
      "Step 898, loss = [0.31486231088638306, 0.00582082336768508, 0.314280241727829]\n",
      "Step 899, loss = [0.3106163740158081, 0.0019806642085313797, 0.3104183077812195]\n",
      "Step 900, loss = [0.2864624559879303, 0.0029812525026500225, 0.2861643433570862]\n",
      "Step 901, loss = [0.29976335167884827, 0.0023505454882979393, 0.2995283007621765]\n",
      "Step 902, loss = [0.29671329259872437, 0.004448250867426395, 0.2962684631347656]\n",
      "Step 903, loss = [0.299602746963501, 0.004346441477537155, 0.29916810989379883]\n",
      "Step 904, loss = [0.287795752286911, 0.006114148534834385, 0.28718432784080505]\n",
      "Step 905, loss = [0.2988450825214386, 0.004789834842085838, 0.29836609959602356]\n",
      "Step 906, loss = [0.30435892939567566, 0.003932835999876261, 0.30396565794944763]\n",
      "Step 907, loss = [0.30866774916648865, 0.002732975408434868, 0.3083944618701935]\n",
      "Step 908, loss = [0.29590657353401184, 0.005040323361754417, 0.29540252685546875]\n",
      "Step 909, loss = [0.30508217215538025, 0.006664984859526157, 0.30441567301750183]\n",
      "Step 910, loss = [0.30167266726493835, 0.005092657171189785, 0.3011634051799774]\n",
      "Step 911, loss = [0.3090161979198456, 0.004453623201698065, 0.30857083201408386]\n",
      "Step 912, loss = [0.2849762737751007, 0.005222616251558065, 0.28445401787757874]\n",
      "Step 913, loss = [0.29640236496925354, 0.0033018728718161583, 0.29607218503952026]\n",
      "Step 914, loss = [0.288403183221817, 0.006078075617551804, 0.28779536485671997]\n",
      "Step 915, loss = [0.3058128356933594, 0.0038641151040792465, 0.3054264187812805]\n",
      "Step 916, loss = [0.3034319579601288, 0.003358815098181367, 0.30309608578681946]\n",
      "Step 917, loss = [0.2946268320083618, 0.005042762029916048, 0.2941225469112396]\n",
      "Step 918, loss = [0.2991989552974701, 0.0048651075921952724, 0.29871243238449097]\n",
      "Step 919, loss = [0.29837992787361145, 0.006201073061674833, 0.2977598309516907]\n",
      "Step 920, loss = [0.30578213930130005, 0.0069944607093930244, 0.30508267879486084]\n",
      "Step 921, loss = [0.31290164589881897, 0.004670202732086182, 0.3124346137046814]\n",
      "Step 922, loss = [0.2898372411727905, 0.003534083254635334, 0.28948384523391724]\n",
      "Step 923, loss = [0.3004571795463562, 0.004776222165673971, 0.299979567527771]\n",
      "Step 924, loss = [0.2908652424812317, 0.00403555715456605, 0.2904616892337799]\n",
      "Step 925, loss = [0.3059990704059601, 0.004165823105722666, 0.3055824935436249]\n",
      "Step 926, loss = [0.29411226511001587, 0.0043082828633487225, 0.29368144273757935]\n",
      "Step 927, loss = [0.2818513512611389, 0.0059158094227313995, 0.28125977516174316]\n",
      "Step 928, loss = [0.2984112501144409, 0.0027973558753728867, 0.29813152551651]\n",
      "Step 929, loss = [0.3084864914417267, 0.004410122521221638, 0.30804547667503357]\n",
      "Step 930, loss = [0.3041546642780304, 0.005202346481382847, 0.3036344349384308]\n",
      "Step 931, loss = [0.3034003973007202, 0.003193489508703351, 0.30308103561401367]\n",
      "Step 932, loss = [0.29841870069503784, 0.004701847210526466, 0.2979485094547272]\n",
      "Step 933, loss = [0.30634796619415283, 0.001478513702750206, 0.3062001168727875]\n",
      "Step 934, loss = [0.2967442274093628, 0.0034538800828158855, 0.2963988482952118]\n",
      "Step 935, loss = [0.30758434534072876, 0.0015424457378685474, 0.30743008852005005]\n",
      "Step 936, loss = [0.297102153301239, 0.006269557401537895, 0.29647520184516907]\n",
      "Step 937, loss = [0.3125133514404297, 0.003499054815620184, 0.31216344237327576]\n",
      "Step 938, loss = [0.2905183434486389, 0.003036233363673091, 0.2902147173881531]\n",
      "Step 939, loss = [0.2949303984642029, 0.005319193936884403, 0.2943984866142273]\n",
      "Step 940, loss = [0.3034287095069885, 0.005492346361279488, 0.3028794825077057]\n",
      "Step 941, loss = [0.29666122794151306, 0.004116767086088657, 0.29624953866004944]\n",
      "Step 942, loss = [0.30005139112472534, 0.005264099687337875, 0.2995249927043915]\n",
      "Step 943, loss = [0.2956946790218353, 0.003583707846701145, 0.2953363060951233]\n",
      "Step 944, loss = [0.3083891272544861, 0.005984056740999222, 0.30779072642326355]\n",
      "Step 945, loss = [0.30802470445632935, 0.004190325736999512, 0.30760568380355835]\n",
      "Step 946, loss = [0.30604079365730286, 0.005295213777571917, 0.3055112659931183]\n",
      "Step 947, loss = [0.3128712773323059, 0.0036422782577574253, 0.3125070631504059]\n",
      "Step 948, loss = [0.30901816487312317, 0.00370758306235075, 0.30864739418029785]\n",
      "Step 949, loss = [0.3031070828437805, 0.002136623952537775, 0.30289342999458313]\n",
      "Step 950, loss = [0.3028229773044586, 0.002540333429351449, 0.3025689423084259]\n",
      "Step 951, loss = [0.30879202485084534, 0.003281111130490899, 0.3084639012813568]\n",
      "Step 952, loss = [0.3005959391593933, 0.002285403199493885, 0.3003673851490021]\n",
      "Step 953, loss = [0.2892112135887146, 0.002970863599330187, 0.28891411423683167]\n",
      "Step 954, loss = [0.30700695514678955, 0.005913842003792524, 0.3064155578613281]\n",
      "Step 955, loss = [0.30143263936042786, 0.0033018796239048243, 0.3011024594306946]\n",
      "Step 956, loss = [0.30299869179725647, 0.004870948381721973, 0.302511602640152]\n",
      "Step 957, loss = [0.300345778465271, 0.004182617180049419, 0.2999275028705597]\n",
      "Step 958, loss = [0.30043622851371765, 0.00464244931936264, 0.2999719977378845]\n",
      "Step 959, loss = [0.31173932552337646, 0.004631525836884975, 0.3112761676311493]\n",
      "Step 960, loss = [0.2906103730201721, 0.002003083936870098, 0.2904100716114044]\n",
      "Step 961, loss = [0.2859005630016327, 0.0025458275340497494, 0.285645991563797]\n",
      "Step 962, loss = [0.2924293577671051, 0.0018696769839152694, 0.2922423779964447]\n",
      "Step 963, loss = [0.2955824136734009, 0.0054629286751151085, 0.29503610730171204]\n",
      "Step 964, loss = [0.2965342402458191, 0.005267664790153503, 0.2960074841976166]\n",
      "Step 965, loss = [0.30011165142059326, 0.004921135026961565, 0.29961952567100525]\n",
      "Step 966, loss = [0.30368274450302124, 0.003946993034332991, 0.3032880425453186]\n",
      "Step 967, loss = [0.30591005086898804, 0.004447518847882748, 0.30546531081199646]\n",
      "Step 968, loss = [0.3101041316986084, 0.004721077159047127, 0.3096320331096649]\n",
      "Step 969, loss = [0.3064427077770233, 0.006409107241779566, 0.3058018088340759]\n",
      "Step 970, loss = [0.31225258111953735, 0.006930963136255741, 0.3115594983100891]\n",
      "Step 971, loss = [0.2915467917919159, 0.0012783657293766737, 0.29141896963119507]\n",
      "Step 972, loss = [0.2963068187236786, 0.009605360217392445, 0.29534628987312317]\n",
      "Step 973, loss = [0.3013608753681183, 0.004856929648667574, 0.300875186920166]\n",
      "Step 974, loss = [0.2992191016674042, 0.005171570926904678, 0.2987019419670105]\n",
      "Step 975, loss = [0.2880248725414276, 0.008242855779826641, 0.28720059990882874]\n",
      "Step 976, loss = [0.30331185460090637, 0.004465772770345211, 0.30286526679992676]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11754 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 977, loss = [0.31010451912879944, 0.005159587599337101, 0.30958855152130127]\n",
      "Update target distribution epoch 2 step 978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11754/11754 [1:50:46<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 978, loss = [0.28978100419044495, 0.0025488054379820824, 0.2895261347293854]\n",
      "Step 979, loss = [0.28881093859672546, 0.005757520906627178, 0.2882351875305176]\n",
      "Step 980, loss = [0.30692142248153687, 0.004872568883001804, 0.30643415451049805]\n",
      "Step 981, loss = [0.3118194341659546, 0.00585118867456913, 0.31123432517051697]\n",
      "Step 982, loss = [0.3109918236732483, 0.004852106794714928, 0.3105066120624542]\n",
      "Step 983, loss = [0.30727723240852356, 0.0018370944308117032, 0.3070935308933258]\n",
      "Step 984, loss = [0.306101530790329, 0.005940512754023075, 0.30550748109817505]\n",
      "Step 985, loss = [0.3104613125324249, 0.004506835248321295, 0.3100106418132782]\n",
      "Step 986, loss = [0.3032020032405853, 0.006859363988041878, 0.30251607298851013]\n",
      "Step 987, loss = [0.29870784282684326, 0.0020588277839124203, 0.29850196838378906]\n",
      "Step 988, loss = [0.2937013506889343, 0.004473836161196232, 0.29325395822525024]\n",
      "Step 989, loss = [0.3032580018043518, 0.004580964799970388, 0.30279991030693054]\n",
      "Step 990, loss = [0.30208197236061096, 0.00518399104475975, 0.301563560962677]\n",
      "Step 991, loss = [0.3106541037559509, 0.004429453983902931, 0.3102111518383026]\n",
      "Step 992, loss = [0.30022767186164856, 0.00501379556953907, 0.299726277589798]\n",
      "Step 993, loss = [0.30494457483291626, 0.007278137374669313, 0.30421677231788635]\n",
      "Step 994, loss = [0.30078572034835815, 0.005628729239106178, 0.30022284388542175]\n",
      "Step 995, loss = [0.3074164092540741, 0.003322329604998231, 0.30708417296409607]\n",
      "Step 996, loss = [0.3033996522426605, 0.00708293030038476, 0.30269137024879456]\n",
      "Step 997, loss = [0.307370662689209, 0.003767101326957345, 0.3069939613342285]\n",
      "Step 998, loss = [0.30514270067214966, 0.004481946118175983, 0.3046945035457611]\n",
      "Step 999, loss = [0.29569852352142334, 0.002652654889971018, 0.29543325304985046]\n",
      "Step 1000, loss = [0.2876889705657959, 0.004553657025098801, 0.2872335910797119]\n",
      "Step 1001, loss = [0.29675406217575073, 0.005334366578608751, 0.2962206304073334]\n",
      "Step 1002, loss = [0.2970341742038727, 0.007365913596004248, 0.2962975800037384]\n",
      "Step 1003, loss = [0.3042127788066864, 0.002490387298166752, 0.3039637506008148]\n",
      "Step 1004, loss = [0.2987314760684967, 0.002916559111326933, 0.2984398305416107]\n",
      "Step 1005, loss = [0.2961465120315552, 0.0047956593334674835, 0.2956669330596924]\n",
      "Step 1006, loss = [0.29584118723869324, 0.002371002919971943, 0.29560407996177673]\n",
      "Step 1007, loss = [0.302254855632782, 0.007404940668493509, 0.3015143573284149]\n",
      "Step 1008, loss = [0.2859925925731659, 0.004488334059715271, 0.2855437695980072]\n",
      "Step 1009, loss = [0.30510401725769043, 0.0038275588303804398, 0.30472126603126526]\n",
      "Step 1010, loss = [0.28253698348999023, 0.0035623004660010338, 0.2821807563304901]\n",
      "Step 1011, loss = [0.3022187650203705, 0.0034331288188695908, 0.30187544226646423]\n",
      "Step 1012, loss = [0.2935601472854614, 0.003435867838561535, 0.2932165563106537]\n",
      "Step 1013, loss = [0.29492902755737305, 0.004065429791808128, 0.2945224940776825]\n",
      "Step 1014, loss = [0.3054972290992737, 0.005422338843345642, 0.30495500564575195]\n",
      "Step 1015, loss = [0.30526983737945557, 0.006361597217619419, 0.3046336770057678]\n",
      "Step 1016, loss = [0.293456107378006, 0.004331246949732304, 0.2930229902267456]\n",
      "Step 1017, loss = [0.31259363889694214, 0.004793555475771427, 0.31211426854133606]\n",
      "Step 1018, loss = [0.2850417196750641, 0.006073878146708012, 0.28443431854248047]\n",
      "Step 1019, loss = [0.29713091254234314, 0.005771061405539513, 0.2965538203716278]\n",
      "Step 1020, loss = [0.304517537355423, 0.004245024640113115, 0.30409303307533264]\n",
      "Step 1021, loss = [0.30498549342155457, 0.00640945415943861, 0.3043445348739624]\n",
      "Step 1022, loss = [0.3048672676086426, 0.004740164149552584, 0.3043932616710663]\n",
      "Step 1023, loss = [0.298997163772583, 0.006610187701880932, 0.2983361482620239]\n",
      "Step 1024, loss = [0.30063650012016296, 0.005609431769698858, 0.30007556080818176]\n",
      "Step 1025, loss = [0.2914458215236664, 0.0036273645237088203, 0.29108309745788574]\n",
      "Step 1026, loss = [0.3065643608570099, 0.005899057723581791, 0.30597445368766785]\n",
      "Step 1027, loss = [0.29095742106437683, 0.007254611235111952, 0.29023197293281555]\n",
      "Step 1028, loss = [0.3063862919807434, 0.0064322929829359055, 0.3057430684566498]\n",
      "Step 1029, loss = [0.30535322427749634, 0.004402630962431431, 0.3049129545688629]\n",
      "Step 1030, loss = [0.3136858344078064, 0.005018772557377815, 0.3131839632987976]\n",
      "Step 1031, loss = [0.30232539772987366, 0.005100509151816368, 0.30181536078453064]\n",
      "Step 1032, loss = [0.30544060468673706, 0.004705721512436867, 0.30497002601623535]\n",
      "Step 1033, loss = [0.297502338886261, 0.005491679534316063, 0.2969531714916229]\n",
      "Step 1034, loss = [0.2889307141304016, 0.006713102571666241, 0.2882594168186188]\n",
      "Step 1035, loss = [0.29853036999702454, 0.003915358334779739, 0.298138827085495]\n",
      "Step 1036, loss = [0.3009060025215149, 0.003235528012737632, 0.3005824387073517]\n",
      "Step 1037, loss = [0.3035760521888733, 0.005860523786395788, 0.30298998951911926]\n",
      "Step 1038, loss = [0.3036455810070038, 0.004354869481176138, 0.3032100796699524]\n",
      "Step 1039, loss = [0.2973284423351288, 0.0037719032261520624, 0.2969512641429901]\n",
      "Step 1040, loss = [0.297943651676178, 0.004013789817690849, 0.2975422739982605]\n",
      "Step 1041, loss = [0.30392566323280334, 0.004687944892793894, 0.3034568727016449]\n",
      "Step 1042, loss = [0.2894498109817505, 0.002749457722529769, 0.2891748547554016]\n",
      "Step 1043, loss = [0.3043816387653351, 0.004077380523085594, 0.303973913192749]\n",
      "Step 1044, loss = [0.3111205995082855, 0.005214011296629906, 0.3105992078781128]\n",
      "Step 1045, loss = [0.28538385033607483, 0.005611817352473736, 0.2848226726055145]\n",
      "Step 1046, loss = [0.29601889848709106, 0.002276394283398986, 0.29579126834869385]\n",
      "Step 1047, loss = [0.291610985994339, 0.005395563319325447, 0.29107141494750977]\n",
      "Step 1048, loss = [0.2954002618789673, 0.0049465312622487545, 0.2949056029319763]\n",
      "Step 1049, loss = [0.3055547773838043, 0.006109337322413921, 0.30494382977485657]\n",
      "Step 1050, loss = [0.29899242520332336, 0.0043526943773031235, 0.2985571622848511]\n",
      "Step 1051, loss = [0.2903496026992798, 0.006560314912348986, 0.28969356417655945]\n",
      "Step 1052, loss = [0.29093536734580994, 0.005349241197109222, 0.2904004454612732]\n",
      "Step 1053, loss = [0.30716055631637573, 0.0062802862375974655, 0.30653253197669983]\n",
      "Step 1054, loss = [0.2995532751083374, 0.0031112730503082275, 0.29924213886260986]\n",
      "Step 1055, loss = [0.31282567977905273, 0.005012350156903267, 0.3123244345188141]\n",
      "Step 1056, loss = [0.3064124584197998, 0.005134990904480219, 0.3058989644050598]\n",
      "Step 1057, loss = [0.2966346740722656, 0.0009480106527917087, 0.29653987288475037]\n",
      "Step 1058, loss = [0.30390268564224243, 0.005463547073304653, 0.3033563196659088]\n",
      "Step 1059, loss = [0.3024095594882965, 0.004280693829059601, 0.30198147892951965]\n",
      "Step 1060, loss = [0.3061780631542206, 0.00475488742813468, 0.3057025671005249]\n",
      "Step 1061, loss = [0.300285279750824, 0.004497786518186331, 0.2998355031013489]\n",
      "Step 1062, loss = [0.3088058531284332, 0.005365800112485886, 0.3082692623138428]\n",
      "Step 1063, loss = [0.30187487602233887, 0.006580089218914509, 0.30121687054634094]\n",
      "Step 1064, loss = [0.31438300013542175, 0.0032788249664008617, 0.31405511498451233]\n",
      "Step 1065, loss = [0.3078329563140869, 0.006660842336714268, 0.3071668744087219]\n",
      "Step 1066, loss = [0.3116857409477234, 0.0028084400109946728, 0.31140488386154175]\n",
      "Step 1067, loss = [0.3008787930011749, 0.0045971954241395, 0.30041906237602234]\n",
      "Step 1068, loss = [0.2959219515323639, 0.006205568090081215, 0.2953014075756073]\n",
      "Step 1069, loss = [0.31166762113571167, 0.00572698749601841, 0.31109490990638733]\n",
      "Step 1070, loss = [0.315676748752594, 0.0034241299144923687, 0.31533434987068176]\n",
      "Step 1071, loss = [0.3122057616710663, 0.003950654529035091, 0.311810702085495]\n",
      "Step 1072, loss = [0.2983511686325073, 0.00548997288569808, 0.2978021800518036]\n",
      "Step 1073, loss = [0.2988300323486328, 0.0033641692716628313, 0.2984936237335205]\n",
      "Step 1074, loss = [0.3078770339488983, 0.0024618543684482574, 0.30763083696365356]\n",
      "Step 1075, loss = [0.3091667890548706, 0.00477512925863266, 0.30868926644325256]\n",
      "Step 1076, loss = [0.29734572768211365, 0.004457686096429825, 0.2968999445438385]\n",
      "Step 1077, loss = [0.29642269015312195, 0.004091490991413593, 0.2960135340690613]\n",
      "Step 1078, loss = [0.2928480803966522, 0.0032238687854260206, 0.29252567887306213]\n",
      "Step 1079, loss = [0.3071931302547455, 0.005587292835116386, 0.30663439631462097]\n",
      "Step 1080, loss = [0.29690393805503845, 0.003672549035400152, 0.2965366840362549]\n",
      "Step 1081, loss = [0.3008957505226135, 0.004344528075307608, 0.3004612922668457]\n",
      "Step 1082, loss = [0.2919279634952545, 0.0042950669303536415, 0.29149845242500305]\n",
      "Step 1083, loss = [0.29986509680747986, 0.00532718887552619, 0.2993323802947998]\n",
      "Step 1084, loss = [0.29394954442977905, 0.0036488063633441925, 0.2935846745967865]\n",
      "Step 1085, loss = [0.3067888915538788, 0.005149549338966608, 0.3062739372253418]\n",
      "Step 1086, loss = [0.3004433512687683, 0.0027816826477646828, 0.30016517639160156]\n",
      "Step 1087, loss = [0.29116711020469666, 0.004875979386270046, 0.2906795144081116]\n",
      "Step 1088, loss = [0.30460822582244873, 0.005155498627573252, 0.304092675447464]\n",
      "Step 1089, loss = [0.30725350975990295, 0.0034744665026664734, 0.3069060742855072]\n",
      "Step 1090, loss = [0.2806398272514343, 0.007374281529337168, 0.2799023985862732]\n",
      "Step 1091, loss = [0.29293766617774963, 0.001299987779930234, 0.2928076684474945]\n",
      "Step 1092, loss = [0.28757423162460327, 0.003786733839660883, 0.2871955633163452]\n",
      "Step 1093, loss = [0.30531826615333557, 0.0033411020413041115, 0.3049841523170471]\n",
      "Step 1094, loss = [0.3000025749206543, 0.00438263826072216, 0.29956430196762085]\n",
      "Step 1095, loss = [0.30733436346054077, 0.0015784299466758966, 0.30717653036117554]\n",
      "Step 1096, loss = [0.29683807492256165, 0.004565060138702393, 0.29638156294822693]\n",
      "Step 1097, loss = [0.3071443438529968, 0.002299507614225149, 0.30691438913345337]\n",
      "Step 1098, loss = [0.2869020998477936, 0.003730210941284895, 0.2865290641784668]\n",
      "Step 1099, loss = [0.3041706085205078, 0.004602621775120497, 0.30371034145355225]\n",
      "Step 1100, loss = [0.29759493470191956, 0.005782765336334705, 0.2970166504383087]\n",
      "Step 1101, loss = [0.3098321855068207, 0.00366390822455287, 0.30946579575538635]\n",
      "Step 1102, loss = [0.31262707710266113, 0.005513906478881836, 0.312075674533844]\n",
      "Step 1103, loss = [0.3034789264202118, 0.003066345816478133, 0.3031722903251648]\n",
      "Step 1104, loss = [0.3077508509159088, 0.006387125700712204, 0.3071121275424957]\n",
      "Step 1105, loss = [0.29433849453926086, 0.005246131680905819, 0.29381388425827026]\n",
      "Step 1106, loss = [0.3014828860759735, 0.004333983175456524, 0.30104950070381165]\n",
      "Step 1107, loss = [0.3044554889202118, 0.0065576862543821335, 0.30379971861839294]\n",
      "Step 1108, loss = [0.30469536781311035, 0.003771358635276556, 0.30431821942329407]\n",
      "Step 1109, loss = [0.308700829744339, 0.002302695531398058, 0.30847054719924927]\n",
      "Step 1110, loss = [0.3099225163459778, 0.004956165794283152, 0.3094269037246704]\n",
      "Step 1111, loss = [0.28984737396240234, 0.004114752635359764, 0.28943589329719543]\n",
      "Step 1112, loss = [0.30862757563591003, 0.0029089455492794514, 0.30833667516708374]\n",
      "Step 1113, loss = [0.30472856760025024, 0.0033413413912057877, 0.3043944239616394]\n",
      "Step 1114, loss = [0.29904916882514954, 0.010995359160006046, 0.2979496419429779]\n",
      "Step 1115, loss = [0.3054574429988861, 0.002372846007347107, 0.3052201569080353]\n",
      "Step 1116, loss = [0.3049946427345276, 0.007182971108704805, 0.30427634716033936]\n",
      "Step 1117, loss = [0.3021593689918518, 0.004574004560709, 0.30170196294784546]\n",
      "Step 1118, loss = [0.30887219309806824, 0.005655675195157528, 0.30830663442611694]\n",
      "Step 1119, loss = [0.30497440695762634, 0.000974615104496479, 0.3048769533634186]\n",
      "Step 1120, loss = [0.29372453689575195, 0.002876174170523882, 0.2934369146823883]\n",
      "Step 1121, loss = [0.2932558059692383, 0.006331608630716801, 0.2926226556301117]\n",
      "Step 1122, loss = [0.29530349373817444, 0.002449146704748273, 0.29505857825279236]\n",
      "Step 1123, loss = [0.28916746377944946, 0.004552136175334454, 0.2887122631072998]\n",
      "Step 1124, loss = [0.28800132870674133, 0.0030351215973496437, 0.28769782185554504]\n",
      "Step 1125, loss = [0.3068605661392212, 0.004901637323200703, 0.30637040734291077]\n",
      "Step 1126, loss = [0.292373925447464, 0.007778972387313843, 0.29159602522850037]\n",
      "Step 1127, loss = [0.2970329523086548, 0.0032657296396791935, 0.2967063784599304]\n",
      "Step 1128, loss = [0.29632675647735596, 0.003780969185754657, 0.29594865441322327]\n",
      "Step 1129, loss = [0.30510395765304565, 0.0050531914457678795, 0.3045986294746399]\n",
      "Step 1130, loss = [0.30568456649780273, 0.003907260950654745, 0.30529382824897766]\n",
      "Step 1131, loss = [0.30269762873649597, 0.0061556557193398476, 0.3020820617675781]\n",
      "Step 1132, loss = [0.30032098293304443, 0.003865462727844715, 0.299934446811676]\n",
      "Step 1133, loss = [0.3001479506492615, 0.0024915975518524647, 0.29989880323410034]\n",
      "Step 1134, loss = [0.30996865034103394, 0.006515028886497021, 0.30931714177131653]\n",
      "Step 1135, loss = [0.31325528025627136, 0.004415089264512062, 0.31281375885009766]\n",
      "Step 1136, loss = [0.2883615791797638, 0.005837615579366684, 0.2877778112888336]\n",
      "Step 1137, loss = [0.2924618124961853, 0.00395665830001235, 0.29206615686416626]\n",
      "Step 1138, loss = [0.28456345200538635, 0.0030868016183376312, 0.2842547595500946]\n",
      "Step 1139, loss = [0.30842429399490356, 0.005406753160059452, 0.307883620262146]\n",
      "Step 1140, loss = [0.31021615862846375, 0.004994639195501804, 0.30971670150756836]\n",
      "Step 1141, loss = [0.3040551543235779, 0.0060327970422804356, 0.30345186591148376]\n",
      "Step 1142, loss = [0.3119746446609497, 0.007162163965404034, 0.3112584352493286]\n",
      "Step 1143, loss = [0.30991706252098083, 0.004241877235472202, 0.30949288606643677]\n",
      "Step 1144, loss = [0.2980802655220032, 0.004857824184000492, 0.29759448766708374]\n",
      "Step 1145, loss = [0.2857013940811157, 0.0035391245037317276, 0.28534749150276184]\n",
      "Step 1146, loss = [0.30369889736175537, 0.0016094200545921922, 0.3035379648208618]\n",
      "Step 1147, loss = [0.30391985177993774, 0.0027667395770549774, 0.3036431670188904]\n",
      "Step 1148, loss = [0.30764421820640564, 0.003574059344828129, 0.30728679895401]\n",
      "Step 1149, loss = [0.2695635259151459, 0.005714951083064079, 0.26899203658103943]\n",
      "Step 1150, loss = [0.3014560341835022, 0.005441309418529272, 0.30091190338134766]\n",
      "Step 1151, loss = [0.2977488338947296, 0.0024716253392398357, 0.29750168323516846]\n",
      "Step 1152, loss = [0.2865947186946869, 0.0029787886887788773, 0.2862968444824219]\n",
      "Step 1153, loss = [0.3096431493759155, 0.004415983334183693, 0.30920153856277466]\n",
      "Step 1154, loss = [0.30250099301338196, 0.003917650319635868, 0.3021092414855957]\n",
      "Step 1155, loss = [0.30111363530158997, 0.007272695191204548, 0.30038636922836304]\n",
      "Step 1156, loss = [0.29393115639686584, 0.0049518803134560585, 0.2934359610080719]\n",
      "Step 1157, loss = [0.2985442578792572, 0.004579454660415649, 0.2980863153934479]\n",
      "Step 1158, loss = [0.30816879868507385, 0.00560679379850626, 0.30760812759399414]\n",
      "Step 1159, loss = [0.2775038480758667, 0.003820260986685753, 0.27712181210517883]\n",
      "Step 1160, loss = [0.30298808217048645, 0.003782436018809676, 0.3026098310947418]\n",
      "Step 1161, loss = [0.3016098737716675, 0.006411931477487087, 0.3009686768054962]\n",
      "Step 1162, loss = [0.2968582510948181, 0.003990804310888052, 0.2964591681957245]\n",
      "Step 1163, loss = [0.3022420406341553, 0.003079051850363612, 0.3019341230392456]\n",
      "Step 1164, loss = [0.2943829298019409, 0.0033574493136256933, 0.29404717683792114]\n",
      "Step 1165, loss = [0.30127689242362976, 0.005724187009036541, 0.3007044792175293]\n",
      "Step 1166, loss = [0.29830530285835266, 0.005598181858658791, 0.2977454960346222]\n",
      "Step 1167, loss = [0.29921674728393555, 0.0024298911448568106, 0.29897376894950867]\n",
      "Step 1168, loss = [0.29781705141067505, 0.006357118487358093, 0.2971813380718231]\n",
      "Step 1169, loss = [0.30445215106010437, 0.006717832759022713, 0.30378037691116333]\n",
      "Step 1170, loss = [0.296204537153244, 0.004609693773090839, 0.29574355483055115]\n",
      "Step 1171, loss = [0.29246923327445984, 0.007379063870757818, 0.2917313277721405]\n",
      "Step 1172, loss = [0.28677603602409363, 0.004962877370417118, 0.28627973794937134]\n",
      "Step 1173, loss = [0.28792911767959595, 0.0049663688987493515, 0.2874324917793274]\n",
      "Step 1174, loss = [0.3024156093597412, 0.008016757667064667, 0.3016139268875122]\n",
      "Step 1175, loss = [0.2950817048549652, 0.0028487746603786945, 0.29479682445526123]\n",
      "Step 1176, loss = [0.313009649515152, 0.002803993411362171, 0.31272923946380615]\n",
      "Step 1177, loss = [0.2934360206127167, 0.00489475391805172, 0.29294654726982117]\n",
      "Step 1178, loss = [0.3068716824054718, 0.004384668543934822, 0.30643320083618164]\n",
      "Step 1179, loss = [0.31077149510383606, 0.007935035042464733, 0.3099779784679413]\n",
      "Step 1180, loss = [0.3065449595451355, 0.0028208252042531967, 0.30626288056373596]\n",
      "Step 1181, loss = [0.3026708662509918, 0.003706688992679119, 0.30230018496513367]\n",
      "Step 1182, loss = [0.31339579820632935, 0.0040525635704398155, 0.31299054622650146]\n",
      "Step 1183, loss = [0.29449349641799927, 0.0015036846743896604, 0.29434311389923096]\n",
      "Step 1184, loss = [0.30338457226753235, 0.006401793099939823, 0.30274438858032227]\n",
      "Step 1185, loss = [0.3040185570716858, 0.004238155670464039, 0.3035947382450104]\n",
      "Step 1186, loss = [0.3120551109313965, 0.001954121747985482, 0.31185969710350037]\n",
      "Step 1187, loss = [0.300816148519516, 0.0033954032696783543, 0.300476610660553]\n",
      "Step 1188, loss = [0.2998029887676239, 0.00430777482688427, 0.29937222599983215]\n",
      "Step 1189, loss = [0.3051386773586273, 0.0013034665025770664, 0.30500832200050354]\n",
      "Step 1190, loss = [0.30166566371917725, 0.006916260812431574, 0.300974041223526]\n",
      "Step 1191, loss = [0.3039449155330658, 0.004497609566897154, 0.3034951686859131]\n",
      "Step 1192, loss = [0.3089204430580139, 0.0029799528419971466, 0.30862244963645935]\n",
      "Step 1193, loss = [0.30854058265686035, 0.004961877129971981, 0.3080444037914276]\n",
      "Step 1194, loss = [0.30330729484558105, 0.0014592010993510485, 0.3031613826751709]\n",
      "Step 1195, loss = [0.3052918016910553, 0.004305290058255196, 0.30486127734184265]\n",
      "Step 1196, loss = [0.29719141125679016, 0.0035547292791306973, 0.29683592915534973]\n",
      "Step 1197, loss = [0.29912784695625305, 0.004370209760963917, 0.2986908257007599]\n",
      "Step 1198, loss = [0.30891671776771545, 0.0027525881305336952, 0.3086414635181427]\n",
      "Step 1199, loss = [0.2845630943775177, 0.003597793634980917, 0.28420332074165344]\n",
      "Step 1200, loss = [0.29553869366645813, 0.004942078143358231, 0.295044481754303]\n",
      "Step 1201, loss = [0.3041008412837982, 0.002774410182610154, 0.30382341146469116]\n",
      "Step 1202, loss = [0.2844288647174835, 0.0026786255184561014, 0.2841610014438629]\n",
      "Step 1203, loss = [0.3003920018672943, 0.0035675466060638428, 0.3000352382659912]\n",
      "Step 1204, loss = [0.2938545346260071, 0.007859614677727222, 0.2930685877799988]\n",
      "Step 1205, loss = [0.30774378776550293, 0.005949699319899082, 0.307148814201355]\n",
      "Step 1206, loss = [0.313930481672287, 0.002726381178945303, 0.31365785002708435]\n",
      "Step 1207, loss = [0.301596075296402, 0.003851907793432474, 0.301210880279541]\n",
      "Step 1208, loss = [0.29211151599884033, 0.004219366237521172, 0.29168957471847534]\n",
      "Step 1209, loss = [0.29325392842292786, 0.0032110780011862516, 0.29293280839920044]\n",
      "Step 1210, loss = [0.29988259077072144, 0.0019782474264502525, 0.2996847629547119]\n",
      "Step 1211, loss = [0.29435959458351135, 0.005131165497004986, 0.2938464879989624]\n",
      "Step 1212, loss = [0.3011375069618225, 0.0018744836561381817, 0.3009500503540039]\n",
      "Step 1213, loss = [0.30221879482269287, 0.006971548777073622, 0.3015216290950775]\n",
      "Step 1214, loss = [0.3053741157054901, 0.0027542589232325554, 0.30509868264198303]\n",
      "Step 1215, loss = [0.29708898067474365, 0.006316625513136387, 0.29645732045173645]\n",
      "Step 1216, loss = [0.28879374265670776, 0.0055549005046486855, 0.2882382571697235]\n",
      "Step 1217, loss = [0.2875291109085083, 0.00436768401414156, 0.28709232807159424]\n",
      "Step 1218, loss = [0.2981501519680023, 0.0027572661638259888, 0.29787442088127136]\n",
      "Step 1219, loss = [0.2796674072742462, 0.00483635812997818, 0.2791837751865387]\n",
      "Step 1220, loss = [0.29571813344955444, 0.0030839242972433567, 0.2954097390174866]\n",
      "Step 1221, loss = [0.30186569690704346, 0.0037378515116870403, 0.301491916179657]\n",
      "Step 1222, loss = [0.29795190691947937, 0.0033953036181628704, 0.29761236906051636]\n",
      "Step 1223, loss = [0.29280972480773926, 0.006593843922019005, 0.2921503484249115]\n",
      "Step 1224, loss = [0.2999469041824341, 0.0014934256905689836, 0.29979756474494934]\n",
      "Step 1225, loss = [0.2918585538864136, 0.007287333719432354, 0.29112982749938965]\n",
      "Step 1226, loss = [0.30378544330596924, 0.004090874455869198, 0.30337634682655334]\n",
      "Step 1227, loss = [0.2908938527107239, 0.006113233510404825, 0.2902825176715851]\n",
      "Step 1228, loss = [0.2886570990085602, 0.0034930878318846226, 0.288307785987854]\n",
      "Step 1229, loss = [0.2858401834964752, 0.006288336124271154, 0.28521135449409485]\n",
      "Step 1230, loss = [0.313040554523468, 0.005329218693077564, 0.31250762939453125]\n",
      "Step 1231, loss = [0.29489776492118835, 0.0036137106362730265, 0.29453638195991516]\n",
      "Step 1232, loss = [0.30796363949775696, 0.0016681253910064697, 0.307796835899353]\n",
      "Step 1233, loss = [0.2999213635921478, 0.0029516657814383507, 0.2996262013912201]\n",
      "Step 1234, loss = [0.3000951409339905, 0.005161846522241831, 0.2995789647102356]\n",
      "Step 1235, loss = [0.2938980460166931, 0.006340567022562027, 0.2932640016078949]\n",
      "Step 1236, loss = [0.3046068847179413, 0.005528266541659832, 0.30405405163764954]\n",
      "Step 1237, loss = [0.30483517050743103, 0.004950498230755329, 0.304340124130249]\n",
      "Step 1238, loss = [0.3110518157482147, 0.003921014256775379, 0.3106597065925598]\n",
      "Step 1239, loss = [0.2995433509349823, 0.003790271934121847, 0.2991643249988556]\n",
      "Step 1240, loss = [0.28855445981025696, 0.004268982447683811, 0.2881275713443756]\n",
      "Step 1241, loss = [0.30065709352493286, 0.0054362379014492035, 0.3001134693622589]\n",
      "Step 1242, loss = [0.31214627623558044, 0.005504980683326721, 0.31159576773643494]\n",
      "Step 1243, loss = [0.304078072309494, 0.005772780627012253, 0.30350080132484436]\n",
      "Step 1244, loss = [0.30101463198661804, 0.0044566793367266655, 0.30056896805763245]\n",
      "Step 1245, loss = [0.2915239930152893, 0.0023088655434548855, 0.29129311442375183]\n",
      "Step 1246, loss = [0.29616597294807434, 0.003634431166574359, 0.2958025336265564]\n",
      "Step 1247, loss = [0.2855054438114166, 0.0012202225625514984, 0.2853834331035614]\n",
      "Step 1248, loss = [0.30333462357521057, 0.0029318383894860744, 0.3030414283275604]\n",
      "Step 1249, loss = [0.302173912525177, 0.0026068766601383686, 0.30191323161125183]\n",
      "Step 1250, loss = [0.30488768219947815, 0.0030804029665887356, 0.30457964539527893]\n",
      "Step 1251, loss = [0.3040924072265625, 0.0037422948516905308, 0.3037181794643402]\n",
      "Step 1252, loss = [0.2975114583969116, 0.007293637841939926, 0.29678210616111755]\n",
      "Step 1253, loss = [0.3071804642677307, 0.002648078603670001, 0.30691567063331604]\n",
      "Step 1254, loss = [0.29630330204963684, 0.002339423168450594, 0.29606935381889343]\n",
      "Step 1255, loss = [0.30693307518959045, 0.0024807890877127647, 0.3066850006580353]\n",
      "Step 1256, loss = [0.277247816324234, 0.0041818274185061455, 0.27682963013648987]\n",
      "Step 1257, loss = [0.3126806318759918, 0.00473084207624197, 0.31220754981040955]\n",
      "Step 1258, loss = [0.3030438721179962, 0.0017105217557400465, 0.30287280678749084]\n",
      "Step 1259, loss = [0.3005961775779724, 0.0037702363915741444, 0.3002191483974457]\n",
      "Step 1260, loss = [0.29354435205459595, 0.006309191696345806, 0.29291343688964844]\n",
      "Step 1261, loss = [0.29231342673301697, 0.003989970311522484, 0.2919144332408905]\n",
      "Step 1262, loss = [0.2960611879825592, 0.005242508836090565, 0.29553693532943726]\n",
      "Step 1263, loss = [0.28768402338027954, 0.0067066615447402, 0.28701335191726685]\n",
      "Step 1264, loss = [0.2954167127609253, 0.0038108082953840494, 0.29503563046455383]\n",
      "Step 1265, loss = [0.3051598370075226, 0.0037983304355293512, 0.3047800064086914]\n",
      "Step 1266, loss = [0.2904266119003296, 0.0032120668329298496, 0.290105402469635]\n",
      "Step 1267, loss = [0.2941349148750305, 0.00452703982591629, 0.2936822175979614]\n",
      "Step 1268, loss = [0.2954443097114563, 0.0025776848196983337, 0.2951865494251251]\n",
      "Step 1269, loss = [0.3046344518661499, 0.0063686915673315525, 0.30399757623672485]\n",
      "Step 1270, loss = [0.3062615990638733, 0.003974332939833403, 0.305864155292511]\n",
      "Step 1271, loss = [0.31019508838653564, 0.0056807235814630985, 0.3096270263195038]\n",
      "Step 1272, loss = [0.3034959137439728, 0.0028472081758081913, 0.30321118235588074]\n",
      "Step 1273, loss = [0.2913391888141632, 0.004682183265686035, 0.2908709645271301]\n",
      "Step 1274, loss = [0.2974395453929901, 0.00223237881436944, 0.2972162961959839]\n",
      "Step 1275, loss = [0.28561413288116455, 0.006781613454222679, 0.28493598103523254]\n",
      "Step 1276, loss = [0.30737611651420593, 0.0025366144254803658, 0.30712246894836426]\n",
      "Step 1277, loss = [0.2995951473712921, 0.003336631925776601, 0.2992614805698395]\n",
      "Step 1278, loss = [0.3028278648853302, 0.006175863556563854, 0.30221027135849]\n",
      "Step 1279, loss = [0.3004697263240814, 0.005896417889744043, 0.29988008737564087]\n",
      "Step 1280, loss = [0.29646822810173035, 0.006133905611932278, 0.2958548367023468]\n",
      "Step 1281, loss = [0.29330092668533325, 0.0020312718115746975, 0.2930977940559387]\n",
      "Step 1282, loss = [0.2877557575702667, 0.0049546887166798115, 0.2872602939605713]\n",
      "Step 1283, loss = [0.29993388056755066, 0.004816308617591858, 0.2994522452354431]\n",
      "Step 1284, loss = [0.30545520782470703, 0.006681538186967373, 0.3047870397567749]\n",
      "Step 1285, loss = [0.2855669856071472, 0.0019686396699398756, 0.2853701114654541]\n",
      "Step 1286, loss = [0.29838645458221436, 0.006103916559368372, 0.29777607321739197]\n",
      "Step 1287, loss = [0.303219735622406, 0.0042456090450286865, 0.3027951717376709]\n",
      "Step 1288, loss = [0.30379602313041687, 0.0043948134407401085, 0.3033565282821655]\n",
      "Step 1289, loss = [0.2611103355884552, 0.0034051795955747366, 0.2607698142528534]\n",
      "Step 1290, loss = [0.28205904364585876, 0.0034706767182797194, 0.28171196579933167]\n",
      "Step 1291, loss = [0.29089319705963135, 0.003978479187935591, 0.2904953360557556]\n",
      "Step 1292, loss = [0.3055154085159302, 0.0060111526399850845, 0.30491429567337036]\n",
      "Step 1293, loss = [0.29318055510520935, 0.004850620869547129, 0.2926954925060272]\n",
      "Step 1294, loss = [0.29256653785705566, 0.007593867368996143, 0.2918071448802948]\n",
      "Step 1295, loss = [0.2968834340572357, 0.002892590593546629, 0.29659417271614075]\n",
      "Step 1296, loss = [0.3029271066188812, 0.007479586638510227, 0.30217915773391724]\n",
      "Step 1297, loss = [0.30926981568336487, 0.005579985212534666, 0.30871182680130005]\n",
      "Step 1298, loss = [0.29745668172836304, 0.0047906506806612015, 0.29697760939598083]\n",
      "Step 1299, loss = [0.28931376338005066, 0.006739442236721516, 0.2886398136615753]\n",
      "Step 1300, loss = [0.3069151043891907, 0.003205329179763794, 0.306594580411911]\n",
      "Step 1301, loss = [0.30530887842178345, 0.004322883207350969, 0.3048765957355499]\n",
      "Step 1302, loss = [0.30370402336120605, 0.006467192899435759, 0.30305731296539307]\n",
      "Step 1303, loss = [0.293987512588501, 0.007458706386387348, 0.2932416498661041]\n",
      "Step 1304, loss = [0.3040925860404968, 0.0050768558867275715, 0.30358490347862244]\n",
      "Step 1305, loss = [0.30097612738609314, 0.004665306769311428, 0.30050960183143616]\n",
      "Step 1306, loss = [0.3132687509059906, 0.002464734483510256, 0.31302228569984436]\n",
      "Step 1307, loss = [0.2970922589302063, 0.003468106733635068, 0.2967454493045807]\n",
      "Step 1308, loss = [0.29870355129241943, 0.00560138002038002, 0.2981434166431427]\n",
      "Step 1309, loss = [0.3026622533798218, 0.004166741855442524, 0.3022455871105194]\n",
      "Step 1310, loss = [0.3130127191543579, 0.006269033066928387, 0.31238582730293274]\n",
      "Step 1311, loss = [0.27054205536842346, 0.0024695629253983498, 0.270295113325119]\n",
      "Step 1312, loss = [0.3034667670726776, 0.005213749594986439, 0.30294540524482727]\n",
      "Step 1313, loss = [0.2938409745693207, 0.004876701161265373, 0.2933533191680908]\n",
      "Step 1314, loss = [0.29679617285728455, 0.0036486347671598196, 0.296431303024292]\n",
      "Step 1315, loss = [0.28742408752441406, 0.004301132634282112, 0.28699398040771484]\n",
      "Step 1316, loss = [0.29859042167663574, 0.0031421182211488485, 0.29827621579170227]\n",
      "Step 1317, loss = [0.30487045645713806, 0.006108909845352173, 0.3042595684528351]\n",
      "Step 1318, loss = [0.30993571877479553, 0.002642001723870635, 0.3096715211868286]\n",
      "Step 1319, loss = [0.26386895775794983, 0.005450813099741936, 0.2633238732814789]\n",
      "Step 1320, loss = [0.28970372676849365, 0.0043472424149513245, 0.28926900029182434]\n",
      "Step 1321, loss = [0.3089510500431061, 0.004130072891712189, 0.3085380494594574]\n",
      "Step 1322, loss = [0.2883684039115906, 0.003353217151015997, 0.28803306818008423]\n",
      "Step 1323, loss = [0.29724177718162537, 0.002310458105057478, 0.29701071977615356]\n",
      "Step 1324, loss = [0.30445608496665955, 0.0038019968196749687, 0.3040758967399597]\n",
      "Step 1325, loss = [0.30463334918022156, 0.004363562911748886, 0.3041969835758209]\n",
      "Step 1326, loss = [0.3043838441371918, 0.005042895674705505, 0.3038795590400696]\n",
      "Step 1327, loss = [0.29439452290534973, 0.0037601199001073837, 0.2940185070037842]\n",
      "Step 1328, loss = [0.3000883162021637, 0.0020909032318741083, 0.2998792231082916]\n",
      "Step 1329, loss = [0.3048318922519684, 0.002947636879980564, 0.3045371174812317]\n",
      "Step 1330, loss = [0.29822614789009094, 0.006119434256106615, 0.2976142168045044]\n",
      "Step 1331, loss = [0.3094492554664612, 0.005899523850530386, 0.30885928869247437]\n",
      "Step 1332, loss = [0.2975817918777466, 0.0038094446063041687, 0.29720085859298706]\n",
      "Step 1333, loss = [0.3019635081291199, 0.004704252351075411, 0.3014930784702301]\n",
      "Step 1334, loss = [0.3031296133995056, 0.006381205283105373, 0.3024914860725403]\n",
      "Step 1335, loss = [0.3013269901275635, 0.004099758341908455, 0.30091699957847595]\n",
      "Step 1336, loss = [0.30192244052886963, 0.0037665555719286203, 0.30154579877853394]\n",
      "Step 1337, loss = [0.2907922565937042, 0.004874907433986664, 0.2903047800064087]\n",
      "Step 1338, loss = [0.28537896275520325, 0.00372479110956192, 0.28500649333000183]\n",
      "Step 1339, loss = [0.3013773262500763, 0.00436174962669611, 0.30094113945961]\n",
      "Step 1340, loss = [0.2942429780960083, 0.00390056474134326, 0.29385292530059814]\n",
      "Step 1341, loss = [0.29533854126930237, 0.003725993912667036, 0.2949659526348114]\n",
      "Step 1342, loss = [0.307541161775589, 0.00494918879121542, 0.30704623460769653]\n",
      "Step 1343, loss = [0.3163028359413147, 0.004205008968710899, 0.3158823251724243]\n",
      "Step 1344, loss = [0.29945623874664307, 0.004485421348363161, 0.29900768399238586]\n",
      "Step 1345, loss = [0.2926423251628876, 0.0033100368455052376, 0.29231131076812744]\n",
      "Step 1346, loss = [0.3014482855796814, 0.003203649539500475, 0.30112791061401367]\n",
      "Step 1347, loss = [0.2941231429576874, 0.0063693942502141, 0.29348620772361755]\n",
      "Step 1348, loss = [0.292324423789978, 0.007746447809040546, 0.29154977202415466]\n",
      "Step 1349, loss = [0.3076055943965912, 0.005075151566416025, 0.3070980906486511]\n",
      "Step 1350, loss = [0.2762855887413025, 0.005193868651986122, 0.27576619386672974]\n",
      "Step 1351, loss = [0.3162606358528137, 0.0024150817189365625, 0.31601911783218384]\n",
      "Step 1352, loss = [0.29804807901382446, 0.004434880334883928, 0.29760459065437317]\n",
      "Step 1353, loss = [0.29708579182624817, 0.004914314951747656, 0.2965943515300751]\n",
      "Step 1354, loss = [0.3024214506149292, 0.003698628395795822, 0.3020515739917755]\n",
      "Step 1355, loss = [0.2917112708091736, 0.005784099921584129, 0.2911328673362732]\n",
      "Step 1356, loss = [0.3106814920902252, 0.006605694070458412, 0.31002092361450195]\n",
      "Step 1357, loss = [0.29631760716438293, 0.0028970828279852867, 0.29602789878845215]\n",
      "Step 1358, loss = [0.2940656840801239, 0.0047094314359128475, 0.29359474778175354]\n",
      "Step 1359, loss = [0.2974235713481903, 0.0058852918446063995, 0.2968350350856781]\n",
      "Step 1360, loss = [0.3023067116737366, 0.004419426433742046, 0.30186477303504944]\n",
      "Step 1361, loss = [0.28367751836776733, 0.004751570988446474, 0.2832023501396179]\n",
      "Step 1362, loss = [0.281713604927063, 0.0032808163668960333, 0.28138551115989685]\n",
      "Step 1363, loss = [0.2847260534763336, 0.005147479474544525, 0.28421130776405334]\n",
      "Step 1364, loss = [0.308845192193985, 0.0017363699153065681, 0.3086715638637543]\n",
      "Step 1365, loss = [0.30010610818862915, 0.003162272274494171, 0.2997898757457733]\n",
      "Step 1366, loss = [0.29705414175987244, 0.006121514365077019, 0.2964420020580292]\n",
      "Step 1367, loss = [0.2869511544704437, 0.005089985206723213, 0.2864421606063843]\n",
      "Step 1368, loss = [0.2825865149497986, 0.004506266675889492, 0.28213587403297424]\n",
      "Step 1369, loss = [0.2942769229412079, 0.008718174882233143, 0.29340511560440063]\n",
      "Step 1370, loss = [0.2942577600479126, 0.004339595790952444, 0.29382380843162537]\n",
      "Step 1371, loss = [0.3072473406791687, 0.002419915748760104, 0.3070053458213806]\n",
      "Step 1372, loss = [0.29338183999061584, 0.004214477725327015, 0.29296040534973145]\n",
      "Step 1373, loss = [0.2673683762550354, 0.004992924630641937, 0.26686909794807434]\n",
      "Step 1374, loss = [0.311926007270813, 0.005661853589117527, 0.31135982275009155]\n",
      "Step 1375, loss = [0.298174113035202, 0.005104600451886654, 0.2976636588573456]\n",
      "Step 1376, loss = [0.2884756326675415, 0.004004404880106449, 0.28807517886161804]\n",
      "Step 1377, loss = [0.3042759597301483, 0.004112186376005411, 0.3038647472858429]\n",
      "Step 1378, loss = [0.2927931547164917, 0.0039604660123586655, 0.2923971116542816]\n",
      "Step 1379, loss = [0.28098630905151367, 0.002618571277707815, 0.2807244658470154]\n",
      "Step 1380, loss = [0.28685444593429565, 0.00233061914332211, 0.2866213917732239]\n",
      "Step 1381, loss = [0.2995043694972992, 0.0013521978398784995, 0.2993691563606262]\n",
      "Step 1382, loss = [0.3086073100566864, 0.004786888137459755, 0.30812862515449524]\n",
      "Step 1383, loss = [0.3097388446331024, 0.0052496809512376785, 0.30921387672424316]\n",
      "Step 1384, loss = [0.30295491218566895, 0.004358191043138504, 0.3025190830230713]\n",
      "Step 1385, loss = [0.31052446365356445, 0.002483661752194166, 0.3102760910987854]\n",
      "Step 1386, loss = [0.3067268431186676, 0.00542862294241786, 0.30618399381637573]\n",
      "Step 1387, loss = [0.3034651577472687, 0.004354600328952074, 0.3030296862125397]\n",
      "Step 1388, loss = [0.3103543817996979, 0.007059489842504263, 0.30964842438697815]\n",
      "Step 1389, loss = [0.30822065472602844, 0.004317636601626873, 0.3077888786792755]\n",
      "Step 1390, loss = [0.3090899884700775, 0.0010442296043038368, 0.30898556113243103]\n",
      "Step 1391, loss = [0.3016386330127716, 0.0021752731408923864, 0.3014211058616638]\n",
      "Step 1392, loss = [0.2980332374572754, 0.004571553785353899, 0.29757606983184814]\n",
      "Step 1393, loss = [0.2934330105781555, 0.004847372882068157, 0.29294827580451965]\n",
      "Step 1394, loss = [0.2757582366466522, 0.0067408112809062, 0.27508416771888733]\n",
      "Step 1395, loss = [0.2977312505245209, 0.004185631405562162, 0.2973126769065857]\n",
      "Step 1396, loss = [0.29951953887939453, 0.002420109463855624, 0.29927751421928406]\n",
      "Step 1397, loss = [0.304514616727829, 0.0018914296524599195, 0.3043254613876343]\n",
      "Step 1398, loss = [0.2808549702167511, 0.003350771265104413, 0.28051990270614624]\n",
      "Step 1399, loss = [0.30893847346305847, 0.004237365908920765, 0.3085147440433502]\n",
      "Step 1400, loss = [0.30742859840393066, 0.006670928094536066, 0.3067615032196045]\n",
      "Step 1401, loss = [0.29226163029670715, 0.0036297468468546867, 0.2918986678123474]\n",
      "Step 1402, loss = [0.2897123098373413, 0.0028578941710293293, 0.2894265055656433]\n",
      "Step 1403, loss = [0.2933131754398346, 0.0019478446338325739, 0.2931183874607086]\n",
      "Step 1404, loss = [0.3069494962692261, 0.008592852391302586, 0.30609020590782166]\n",
      "Step 1405, loss = [0.3089436888694763, 0.007746193092316389, 0.30816906690597534]\n",
      "Step 1406, loss = [0.2773200273513794, 0.004911591298878193, 0.2768288552761078]\n",
      "Step 1407, loss = [0.28685295581817627, 0.005366805475205183, 0.28631627559661865]\n",
      "Step 1408, loss = [0.2925010919570923, 0.001267389627173543, 0.2923743426799774]\n",
      "Step 1409, loss = [0.30132555961608887, 0.003645027056336403, 0.30096104741096497]\n",
      "Step 1410, loss = [0.2978670597076416, 0.005159846041351557, 0.29735106229782104]\n",
      "Step 1411, loss = [0.30983036756515503, 0.007808164693415165, 0.3090495467185974]\n",
      "Step 1412, loss = [0.29124584794044495, 0.004878408275544643, 0.29075801372528076]\n",
      "Step 1413, loss = [0.3126753866672516, 0.0029708798974752426, 0.31237828731536865]\n",
      "Step 1414, loss = [0.3023364245891571, 0.003630815539509058, 0.3019733428955078]\n",
      "Step 1415, loss = [0.2980343997478485, 0.005513783544301987, 0.29748302698135376]\n",
      "Step 1416, loss = [0.2938382029533386, 0.003616178873926401, 0.29347658157348633]\n",
      "Step 1417, loss = [0.31168100237846375, 0.007074274588376284, 0.310973584651947]\n",
      "Step 1418, loss = [0.3073083162307739, 0.004367451183497906, 0.30687156319618225]\n",
      "Step 1419, loss = [0.29795509576797485, 0.006007710471749306, 0.2973543107509613]\n",
      "Step 1420, loss = [0.30955028533935547, 0.003632680978626013, 0.30918702483177185]\n",
      "Step 1421, loss = [0.29737982153892517, 0.0031864407937973738, 0.29706117510795593]\n",
      "Step 1422, loss = [0.2939991354942322, 0.002488359808921814, 0.2937502861022949]\n",
      "Step 1423, loss = [0.2872895896434784, 0.0054038213565945625, 0.2867492139339447]\n",
      "Step 1424, loss = [0.2991299629211426, 0.002913291100412607, 0.29883864521980286]\n",
      "Step 1425, loss = [0.31131911277770996, 0.003660727757960558, 0.3109530508518219]\n",
      "Step 1426, loss = [0.30529555678367615, 0.005900001153349876, 0.30470556020736694]\n",
      "Step 1427, loss = [0.3062880039215088, 0.00731024332344532, 0.305556982755661]\n",
      "Step 1428, loss = [0.2950182259082794, 0.004205894656479359, 0.2945976257324219]\n",
      "Step 1429, loss = [0.3065297603607178, 0.003918366972357035, 0.30613791942596436]\n",
      "Step 1430, loss = [0.2872770130634308, 0.006733005400747061, 0.286603718996048]\n",
      "Step 1431, loss = [0.29637813568115234, 0.006940652150660753, 0.2956840693950653]\n",
      "Step 1432, loss = [0.3009066879749298, 0.007035314105451107, 0.3002031445503235]\n",
      "Step 1433, loss = [0.2958115041255951, 0.0013774596154689789, 0.29567375779151917]\n",
      "Step 1434, loss = [0.2960014343261719, 0.0033487286418676376, 0.29566657543182373]\n",
      "Step 1435, loss = [0.3122675120830536, 0.004323561210185289, 0.3118351697921753]\n",
      "Step 1436, loss = [0.29396364092826843, 0.006127441301941872, 0.2933509051799774]\n",
      "Step 1437, loss = [0.30418863892555237, 0.006298313848674297, 0.3035587966442108]\n",
      "Step 1438, loss = [0.30271217226982117, 0.004795211832970381, 0.30223265290260315]\n",
      "Step 1439, loss = [0.30653199553489685, 0.005647640209645033, 0.30596724152565]\n",
      "Step 1440, loss = [0.287565141916275, 0.004711332730948925, 0.28709399700164795]\n",
      "Step 1441, loss = [0.3040865361690521, 0.00223010266199708, 0.303863525390625]\n",
      "Step 1442, loss = [0.30678823590278625, 0.005124465096741915, 0.30627578496932983]\n",
      "Step 1443, loss = [0.3045315444469452, 0.0025405329652130604, 0.3042774796485901]\n",
      "Step 1444, loss = [0.29582980275154114, 0.003671013517305255, 0.2954626977443695]\n",
      "Step 1445, loss = [0.30885472893714905, 0.00791160762310028, 0.3080635666847229]\n",
      "Step 1446, loss = [0.3107589781284332, 0.00225894246250391, 0.3105330765247345]\n",
      "Step 1447, loss = [0.30781200528144836, 0.0032338984310626984, 0.3074886202812195]\n",
      "Step 1448, loss = [0.28876712918281555, 0.005467508919537067, 0.2882203757762909]\n",
      "Step 1449, loss = [0.30593398213386536, 0.005070056766271591, 0.3054269850254059]\n",
      "Step 1450, loss = [0.30414092540740967, 0.007282497826963663, 0.30341267585754395]\n",
      "Step 1451, loss = [0.2919454276561737, 0.0009500223677605391, 0.29185041785240173]\n",
      "Step 1452, loss = [0.30730706453323364, 0.004568932577967644, 0.3068501651287079]\n",
      "Step 1453, loss = [0.2829105854034424, 0.006614220328629017, 0.2822491526603699]\n",
      "Step 1454, loss = [0.2890373468399048, 0.006193491630256176, 0.2884179949760437]\n",
      "Step 1455, loss = [0.29509758949279785, 0.004393447190523148, 0.29465824365615845]\n",
      "Step 1456, loss = [0.3062226474285126, 0.004252556711435318, 0.30579739809036255]\n",
      "Step 1457, loss = [0.30452242493629456, 0.004538459703326225, 0.30406856536865234]\n",
      "Step 1458, loss = [0.3104347288608551, 0.003519459394738078, 0.3100827932357788]\n",
      "Step 1459, loss = [0.30170923471450806, 0.005259283818304539, 0.3011833131313324]\n",
      "Step 1460, loss = [0.3009703755378723, 0.0057741450145840645, 0.3003929555416107]\n",
      "Step 1461, loss = [0.31092262268066406, 0.0028108181431889534, 0.3106415271759033]\n",
      "Step 1462, loss = [0.29817041754722595, 0.004106736741960049, 0.2977597415447235]\n",
      "Step 1463, loss = [0.3001960813999176, 0.002832205267623067, 0.29991286993026733]\n",
      "Step 1464, loss = [0.3070339262485504, 0.004608571529388428, 0.3065730631351471]\n",
      "Step 1465, loss = [0.29247280955314636, 0.0035741040483117104, 0.29211539030075073]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11754 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1466, loss = [0.30552926659584045, 0.0028409091755747795, 0.30524516105651855]\n",
      "Update target distribution epoch 2 step 1467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11754/11754 [1:50:51<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1467, loss = [0.3024919331073761, 0.004199749790132046, 0.3020719587802887]\n",
      "Step 1468, loss = [0.3090807795524597, 0.0061698537319898605, 0.30846378207206726]\n",
      "Step 1469, loss = [0.2939464747905731, 0.013633597642183304, 0.2925831079483032]\n",
      "Epoch 2, loss = [0.29954077 0.00437246 0.29910353]\n",
      "\n",
      "Start of epoch 3\n",
      "Step 0, loss = [0.31018340587615967, 0.004910040646791458, 0.3096924126148224]\n",
      "Step 1, loss = [0.30809521675109863, 0.0034206693526357412, 0.30775314569473267]\n",
      "Step 2, loss = [0.30391618609428406, 0.0029540599789470434, 0.3036207854747772]\n",
      "Step 3, loss = [0.3119017481803894, 0.002829545410349965, 0.3116188049316406]\n",
      "Step 4, loss = [0.2936460077762604, 0.004265031777322292, 0.29321950674057007]\n",
      "Step 5, loss = [0.31556299328804016, 0.0038650298956781626, 0.31517648696899414]\n",
      "Step 6, loss = [0.29688310623168945, 0.002177114598453045, 0.29666540026664734]\n",
      "Step 7, loss = [0.29344627261161804, 0.004592317622154951, 0.29298704862594604]\n",
      "Step 8, loss = [0.30039307475090027, 0.0017262828769162297, 0.30022045969963074]\n",
      "Step 9, loss = [0.3005618751049042, 0.003892686916515231, 0.3001725971698761]\n",
      "Step 10, loss = [0.30470719933509827, 0.004092705901712179, 0.30429792404174805]\n",
      "Step 11, loss = [0.2913714647293091, 0.001898250775411725, 0.29118165373802185]\n",
      "Step 12, loss = [0.2876332998275757, 0.005785652436316013, 0.28705474734306335]\n",
      "Step 13, loss = [0.30376559495925903, 0.002652695868164301, 0.30350032448768616]\n",
      "Step 14, loss = [0.298234760761261, 0.0029657690320163965, 0.29793819785118103]\n",
      "Step 15, loss = [0.30045971274375916, 0.0040383413434028625, 0.3000558912754059]\n",
      "Step 16, loss = [0.28952449560165405, 0.0033210301771759987, 0.2891923785209656]\n",
      "Step 17, loss = [0.2907795310020447, 0.003468958893790841, 0.2904326319694519]\n",
      "Step 18, loss = [0.2863776385784149, 0.006026890128850937, 0.28577494621276855]\n",
      "Step 19, loss = [0.30781397223472595, 0.0028424719348549843, 0.3075297176837921]\n",
      "Step 20, loss = [0.3105926513671875, 0.0031915984582155943, 0.31027349829673767]\n",
      "Step 21, loss = [0.3085014224052429, 0.004864803049713373, 0.3080149292945862]\n",
      "Step 22, loss = [0.30181658267974854, 0.004399859346449375, 0.301376610994339]\n",
      "Step 23, loss = [0.3011605739593506, 0.0033739458303898573, 0.3008231818675995]\n",
      "Step 24, loss = [0.3065107464790344, 0.006931966170668602, 0.30581754446029663]\n",
      "Step 25, loss = [0.2973065674304962, 0.0046099331229925156, 0.29684558510780334]\n",
      "Step 26, loss = [0.3012021780014038, 0.002133091213181615, 0.3009888827800751]\n",
      "Step 27, loss = [0.29206958413124084, 0.004326791036874056, 0.2916369140148163]\n",
      "Step 28, loss = [0.30979591608047485, 0.004580916836857796, 0.3093378245830536]\n",
      "Step 29, loss = [0.305463969707489, 0.0035117189399898052, 0.3051128089427948]\n",
      "Step 30, loss = [0.3063007593154907, 0.003297057468444109, 0.30597105622291565]\n",
      "Step 31, loss = [0.29113245010375977, 0.0037241801619529724, 0.2907600402832031]\n",
      "Step 32, loss = [0.2993634343147278, 0.0035363934002816677, 0.2990097999572754]\n",
      "Step 33, loss = [0.294813334941864, 0.0020420136861503124, 0.2946091294288635]\n",
      "Step 34, loss = [0.29788362979888916, 0.0047855256125330925, 0.29740506410598755]\n",
      "Step 35, loss = [0.29425737261772156, 0.004358380567282438, 0.2938215434551239]\n",
      "Step 36, loss = [0.2913648784160614, 0.0018439239356666803, 0.29118049144744873]\n",
      "Step 37, loss = [0.28596171736717224, 0.002646908164024353, 0.28569701313972473]\n",
      "Step 38, loss = [0.3043520152568817, 0.0015888824127614498, 0.3041931390762329]\n",
      "Step 39, loss = [0.29567450284957886, 0.004408435896039009, 0.29523366689682007]\n",
      "Step 40, loss = [0.3047586977481842, 0.004774792585521936, 0.30428120493888855]\n",
      "Step 41, loss = [0.30107006430625916, 0.003248716238886118, 0.3007451891899109]\n",
      "Step 42, loss = [0.30094677209854126, 0.0068480465561151505, 0.3002619743347168]\n",
      "Step 43, loss = [0.29236873984336853, 0.004018017090857029, 0.2919669449329376]\n",
      "Step 44, loss = [0.30048635601997375, 0.0022010612301528454, 0.30026623606681824]\n",
      "Step 45, loss = [0.292996883392334, 0.003950298763811588, 0.2926018536090851]\n",
      "Step 46, loss = [0.3050090968608856, 0.0030090766958892345, 0.30470818281173706]\n",
      "Step 47, loss = [0.30179762840270996, 0.006951475515961647, 0.30110248923301697]\n",
      "Step 48, loss = [0.3060905337333679, 0.0040522413328289986, 0.3056853115558624]\n",
      "Step 49, loss = [0.3118363320827484, 0.006444316357374191, 0.3111918866634369]\n",
      "Step 50, loss = [0.29835230112075806, 0.002786317840218544, 0.2980736792087555]\n",
      "Step 51, loss = [0.3041711747646332, 0.005653632804751396, 0.3036058247089386]\n",
      "Step 52, loss = [0.29426220059394836, 0.003927839454263449, 0.29386940598487854]\n",
      "Step 53, loss = [0.3052654564380646, 0.003337276866659522, 0.30493173003196716]\n",
      "Step 54, loss = [0.28945207595825195, 0.005191023461520672, 0.2889329791069031]\n",
      "Step 55, loss = [0.30284494161605835, 0.004706225357949734, 0.30237433314323425]\n",
      "Step 56, loss = [0.2941019833087921, 0.0018044220050796866, 0.2939215302467346]\n",
      "Step 57, loss = [0.28618186712265015, 0.005036302842199802, 0.2856782376766205]\n",
      "Step 58, loss = [0.292046457529068, 0.0072449008002877235, 0.2913219630718231]\n",
      "Step 59, loss = [0.3019057810306549, 0.003455639351159334, 0.3015602231025696]\n",
      "Step 60, loss = [0.2940378487110138, 0.004328629467636347, 0.2936049997806549]\n",
      "Step 61, loss = [0.29244163632392883, 0.005128870252519846, 0.2919287383556366]\n",
      "Step 62, loss = [0.2965220510959625, 0.012208187952637672, 0.295301228761673]\n",
      "Step 63, loss = [0.29223158955574036, 0.0038961058016866446, 0.291841983795166]\n",
      "Step 64, loss = [0.2940071225166321, 0.002287557814270258, 0.29377835988998413]\n",
      "Step 65, loss = [0.2960303723812103, 0.003425384173169732, 0.29568782448768616]\n",
      "Step 66, loss = [0.28851228952407837, 0.003097688779234886, 0.28820252418518066]\n",
      "Step 67, loss = [0.31018972396850586, 0.0034425044432282448, 0.3098454773426056]\n",
      "Step 68, loss = [0.2982868552207947, 0.0037238437216728926, 0.2979144752025604]\n",
      "Step 69, loss = [0.3052878677845001, 0.0027802770491689444, 0.3050098419189453]\n",
      "Step 70, loss = [0.31534543633461, 0.002817528322339058, 0.3150636851787567]\n",
      "Step 71, loss = [0.29277440905570984, 0.006563176400959492, 0.292118102312088]\n",
      "Step 72, loss = [0.3077169358730316, 0.0034185973927378654, 0.30737507343292236]\n",
      "Step 73, loss = [0.3073883056640625, 0.0048638866282999516, 0.3069019317626953]\n",
      "Step 74, loss = [0.29254066944122314, 0.003613954409956932, 0.29217928647994995]\n",
      "Step 75, loss = [0.298543781042099, 0.00450252927839756, 0.2980935275554657]\n",
      "Step 76, loss = [0.30239352583885193, 0.0024272301234304905, 0.30215081572532654]\n",
      "Step 77, loss = [0.30575722455978394, 0.007030535954982042, 0.3050541579723358]\n",
      "Step 78, loss = [0.2944481372833252, 0.00228283554315567, 0.29421985149383545]\n",
      "Step 79, loss = [0.3127260208129883, 0.004316683858633041, 0.3122943639755249]\n",
      "Step 80, loss = [0.3050631880760193, 0.0050975787453353405, 0.30455341935157776]\n",
      "Step 81, loss = [0.2982065677642822, 0.0058784885331988335, 0.29761871695518494]\n",
      "Step 82, loss = [0.3084756135940552, 0.004704105667769909, 0.3080052137374878]\n",
      "Step 83, loss = [0.3028169572353363, 0.003128306707367301, 0.30250412225723267]\n",
      "Step 84, loss = [0.30314454436302185, 0.00662641879171133, 0.30248188972473145]\n",
      "Step 85, loss = [0.28009718656539917, 0.007409149780869484, 0.2793562710285187]\n",
      "Step 86, loss = [0.3070964217185974, 0.005900433287024498, 0.30650636553764343]\n",
      "Step 87, loss = [0.29676419496536255, 0.0019071144051849842, 0.2965734899044037]\n",
      "Step 88, loss = [0.29234257340431213, 0.005548832006752491, 0.29178768396377563]\n",
      "Step 89, loss = [0.30289727449417114, 0.005097283981740475, 0.302387535572052]\n",
      "Step 90, loss = [0.29181793332099915, 0.0029924483969807625, 0.2915186882019043]\n",
      "Step 91, loss = [0.2938089370727539, 0.0037705712020397186, 0.2934318780899048]\n",
      "Step 92, loss = [0.3083561360836029, 0.003980167210102081, 0.30795812606811523]\n",
      "Step 93, loss = [0.3066714107990265, 0.003233639756217599, 0.30634805560112]\n",
      "Step 94, loss = [0.3108198940753937, 0.00612550787627697, 0.310207337141037]\n",
      "Step 95, loss = [0.3007153272628784, 0.0030188870150595903, 0.30041342973709106]\n",
      "Step 96, loss = [0.30854856967926025, 0.003073854371905327, 0.30824118852615356]\n",
      "Step 97, loss = [0.3003588318824768, 0.0017140937270596623, 0.3001874089241028]\n",
      "Step 98, loss = [0.31058046221733093, 0.005576413590461016, 0.31002283096313477]\n",
      "Step 99, loss = [0.30139729380607605, 0.003191792406141758, 0.30107811093330383]\n",
      "Step 100, loss = [0.29230833053588867, 0.0038022343069314957, 0.29192811250686646]\n",
      "Step 101, loss = [0.29310303926467896, 0.0048960912972688675, 0.2926134169101715]\n",
      "Step 102, loss = [0.30216383934020996, 0.001850035390816629, 0.30197882652282715]\n",
      "Step 103, loss = [0.3005484342575073, 0.003766646608710289, 0.30017176270484924]\n",
      "Step 104, loss = [0.3081607222557068, 0.004150307737290859, 0.30774569511413574]\n",
      "Step 105, loss = [0.29503846168518066, 0.0011404735269024968, 0.29492440819740295]\n",
      "Step 106, loss = [0.2969593405723572, 0.003601681673899293, 0.2965991795063019]\n",
      "Step 107, loss = [0.29757872223854065, 0.002653297036886215, 0.297313392162323]\n",
      "Step 108, loss = [0.295134037733078, 0.003171988297253847, 0.29481685161590576]\n",
      "Step 109, loss = [0.3050742745399475, 0.005475066602230072, 0.30452677607536316]\n",
      "Step 110, loss = [0.31240153312683105, 0.002472158055752516, 0.3121543228626251]\n",
      "Step 111, loss = [0.295146644115448, 0.002056875266134739, 0.2949409484863281]\n",
      "Step 112, loss = [0.29570549726486206, 0.0023434460163116455, 0.2954711616039276]\n",
      "Step 113, loss = [0.2964629530906677, 0.004461728967726231, 0.29601678252220154]\n",
      "Step 114, loss = [0.28820109367370605, 0.00637951772660017, 0.28756314516067505]\n",
      "Step 115, loss = [0.29394519329071045, 0.003844837425276637, 0.2935607135295868]\n",
      "Step 116, loss = [0.3006354570388794, 0.0029528860468417406, 0.3003401756286621]\n",
      "Step 117, loss = [0.29970601201057434, 0.003985758870840073, 0.2993074357509613]\n",
      "Step 118, loss = [0.3013153374195099, 0.004929132293909788, 0.3008224368095398]\n",
      "Step 119, loss = [0.30749836564064026, 0.003622810123488307, 0.30713608860969543]\n",
      "Step 120, loss = [0.30505236983299255, 0.003683140268549323, 0.304684042930603]\n",
      "Step 121, loss = [0.2988567650318146, 0.003174764337018132, 0.29853928089141846]\n",
      "Step 122, loss = [0.28601381182670593, 0.004387096501886845, 0.28557509183883667]\n",
      "Step 123, loss = [0.26423028111457825, 0.006021649111062288, 0.26362812519073486]\n",
      "Step 124, loss = [0.3093200922012329, 0.002270365133881569, 0.30909305810928345]\n",
      "Step 125, loss = [0.2994399070739746, 0.002701194491237402, 0.29916977882385254]\n",
      "Step 126, loss = [0.3093492090702057, 0.00392478471621871, 0.30895674228668213]\n",
      "Step 127, loss = [0.2927388846874237, 0.005352705717086792, 0.2922036051750183]\n",
      "Step 128, loss = [0.28368017077445984, 0.0032578567042946815, 0.28335437178611755]\n",
      "Step 129, loss = [0.28944990038871765, 0.005323776509612799, 0.28891751170158386]\n",
      "Step 130, loss = [0.3077886700630188, 0.0024105096235871315, 0.3075476288795471]\n",
      "Step 131, loss = [0.29825738072395325, 0.0036367822904139757, 0.2978937029838562]\n",
      "Step 132, loss = [0.2914288341999054, 0.002360559068620205, 0.29119277000427246]\n",
      "Step 133, loss = [0.31165558099746704, 0.004029443953186274, 0.311252623796463]\n",
      "Step 134, loss = [0.2972013056278229, 0.005657406989485025, 0.29663556814193726]\n",
      "Step 135, loss = [0.30093076825141907, 0.002676386618986726, 0.30066314339637756]\n",
      "Step 136, loss = [0.3051515221595764, 0.0034706569276750088, 0.3048044443130493]\n",
      "Step 137, loss = [0.3018670082092285, 0.0015601806808263063, 0.30171099305152893]\n",
      "Step 138, loss = [0.30033078789711, 0.003978069871664047, 0.29993298649787903]\n",
      "Step 139, loss = [0.30873703956604004, 0.004284979775547981, 0.30830854177474976]\n",
      "Step 140, loss = [0.3070894777774811, 0.0015100868185982108, 0.3069384694099426]\n",
      "Step 141, loss = [0.2954343259334564, 0.004244246520102024, 0.29500991106033325]\n",
      "Step 142, loss = [0.28765246272087097, 0.005326904822140932, 0.2871197760105133]\n",
      "Step 143, loss = [0.29566672444343567, 0.003359732683748007, 0.2953307628631592]\n",
      "Step 144, loss = [0.2961590886116028, 0.0028792505618184805, 0.29587116837501526]\n",
      "Step 145, loss = [0.3003547191619873, 0.004073130898177624, 0.2999474108219147]\n",
      "Step 146, loss = [0.30247238278388977, 0.004009398631751537, 0.3020714521408081]\n",
      "Step 147, loss = [0.3062545955181122, 0.0061222403310239315, 0.30564236640930176]\n",
      "Step 148, loss = [0.30648794770240784, 0.0024232310242950916, 0.3062456250190735]\n",
      "Step 149, loss = [0.30259740352630615, 0.0033436031080782413, 0.3022630512714386]\n",
      "Step 150, loss = [0.31053295731544495, 0.004119452554732561, 0.31012099981307983]\n",
      "Step 151, loss = [0.3136669099330902, 0.0011041060788556933, 0.3135564923286438]\n",
      "Step 152, loss = [0.30261778831481934, 0.00389627437107265, 0.3022281527519226]\n",
      "Step 153, loss = [0.28994208574295044, 0.006039778236299753, 0.2893381118774414]\n",
      "Step 154, loss = [0.3005560338497162, 0.002712947316467762, 0.300284743309021]\n",
      "Step 155, loss = [0.2764319181442261, 0.0036670388653874397, 0.2760652005672455]\n",
      "Step 156, loss = [0.2976650297641754, 0.003680598922073841, 0.2972969710826874]\n",
      "Step 157, loss = [0.3111703395843506, 0.0036885892041027546, 0.3108014762401581]\n",
      "Step 158, loss = [0.30473506450653076, 0.004072707146406174, 0.3043277859687805]\n",
      "Step 159, loss = [0.3027900457382202, 0.002565160859376192, 0.3025335371494293]\n",
      "Step 160, loss = [0.3044341802597046, 0.0034332149662077427, 0.30409085750579834]\n",
      "Step 161, loss = [0.2898268401622772, 0.002024847548455, 0.2896243631839752]\n",
      "Step 162, loss = [0.2912394404411316, 0.0025247479788959026, 0.29098695516586304]\n",
      "Step 163, loss = [0.29698047041893005, 0.004330214578658342, 0.29654744267463684]\n",
      "Step 164, loss = [0.28547993302345276, 0.0011356363538652658, 0.28536635637283325]\n",
      "Step 165, loss = [0.30135828256607056, 0.0017545446753501892, 0.3011828362941742]\n",
      "Step 166, loss = [0.2965157926082611, 0.004141971003264189, 0.2961015999317169]\n",
      "Step 167, loss = [0.29546278715133667, 0.007218548096716404, 0.2947409451007843]\n",
      "Step 168, loss = [0.28813430666923523, 0.003527313005179167, 0.28778156638145447]\n",
      "Step 169, loss = [0.2881428599357605, 0.0021775169298052788, 0.2879250943660736]\n",
      "Step 170, loss = [0.3010135591030121, 0.0029911468736827374, 0.3007144331932068]\n",
      "Step 171, loss = [0.29956337809562683, 0.006983970291912556, 0.2988649904727936]\n",
      "Step 172, loss = [0.31361904740333557, 0.006978534162044525, 0.3129211962223053]\n",
      "Step 173, loss = [0.2994123101234436, 0.0038052010349929333, 0.2990317940711975]\n",
      "Step 174, loss = [0.286196768283844, 0.004870413802564144, 0.2857097387313843]\n",
      "Step 175, loss = [0.30825117230415344, 0.003041174029931426, 0.3079470694065094]\n",
      "Step 176, loss = [0.31253641843795776, 0.003113645128905773, 0.3122250437736511]\n",
      "Step 177, loss = [0.29891282320022583, 0.00682493532076478, 0.2982303202152252]\n",
      "Step 178, loss = [0.29390034079551697, 0.0035040760412812233, 0.29354992508888245]\n",
      "Step 179, loss = [0.29967567324638367, 0.005950624588876963, 0.29908061027526855]\n",
      "Step 180, loss = [0.29040688276290894, 0.004324635956436396, 0.2899744212627411]\n",
      "Step 181, loss = [0.2902913987636566, 0.006927588488906622, 0.28959864377975464]\n",
      "Step 182, loss = [0.28641489148139954, 0.0035271551460027695, 0.28606218099594116]\n",
      "Step 183, loss = [0.2928232252597809, 0.002653049072250724, 0.2925579249858856]\n",
      "Step 184, loss = [0.31396836042404175, 0.002820973750203848, 0.3136862516403198]\n",
      "Step 185, loss = [0.2961304783821106, 0.004157443530857563, 0.29571473598480225]\n",
      "Step 186, loss = [0.29308632016181946, 0.00549066998064518, 0.29253724217414856]\n",
      "Step 187, loss = [0.2783733010292053, 0.004384391009807587, 0.27793484926223755]\n",
      "Step 188, loss = [0.30573925375938416, 0.0017199773574247956, 0.30556726455688477]\n",
      "Step 189, loss = [0.3049340844154358, 0.003674067324027419, 0.3045666813850403]\n",
      "Step 190, loss = [0.30240580439567566, 0.0033894621301442385, 0.3020668625831604]\n",
      "Step 191, loss = [0.298353374004364, 0.003834765637293458, 0.29796990752220154]\n",
      "Step 192, loss = [0.28042352199554443, 0.004076618235558271, 0.28001585602760315]\n",
      "Step 193, loss = [0.3035271465778351, 0.0027946848422288895, 0.30324769020080566]\n",
      "Step 194, loss = [0.3036350607872009, 0.002638960722833872, 0.3033711612224579]\n",
      "Step 195, loss = [0.2935431897640228, 0.002252349629998207, 0.2933179438114166]\n",
      "Step 196, loss = [0.29432186484336853, 0.004584356211125851, 0.2938634157180786]\n",
      "Step 197, loss = [0.29339054226875305, 0.005330996587872505, 0.29285743832588196]\n",
      "Step 198, loss = [0.31707078218460083, 0.0017583236331120133, 0.3168949484825134]\n",
      "Step 199, loss = [0.3057662844657898, 0.016373172402381897, 0.3041289746761322]\n",
      "Step 200, loss = [0.294514000415802, 0.006440556608140469, 0.2938699424266815]\n",
      "Step 201, loss = [0.2987087070941925, 0.005281616002321243, 0.29818055033683777]\n",
      "Step 202, loss = [0.30398041009902954, 0.0022930733393877745, 0.3037511110305786]\n",
      "Step 203, loss = [0.29085057973861694, 0.0053236475214362144, 0.29031822085380554]\n",
      "Step 204, loss = [0.3134790360927582, 0.0030279408674687147, 0.3131762444972992]\n",
      "Step 205, loss = [0.2995292842388153, 0.0058153546415269375, 0.2989477515220642]\n",
      "Step 206, loss = [0.3077082335948944, 0.001711231772787869, 0.30753710865974426]\n",
      "Step 207, loss = [0.2875390350818634, 0.004704516381025314, 0.28706857562065125]\n",
      "Step 208, loss = [0.28962036967277527, 0.004508364014327526, 0.2891695201396942]\n",
      "Step 209, loss = [0.31426143646240234, 0.005850559566169977, 0.3136763870716095]\n",
      "Step 210, loss = [0.30571839213371277, 0.003668386023491621, 0.3053515553474426]\n",
      "Step 211, loss = [0.3003160357475281, 0.002409071195870638, 0.30007511377334595]\n",
      "Step 212, loss = [0.3129937946796417, 0.004277735948562622, 0.31256601214408875]\n",
      "Step 213, loss = [0.30534249544143677, 0.005180050618946552, 0.30482450127601624]\n",
      "Step 214, loss = [0.28634312748908997, 0.0032817567698657513, 0.28601494431495667]\n",
      "Step 215, loss = [0.30496224761009216, 0.005287518724799156, 0.3044334948062897]\n",
      "Step 216, loss = [0.30039042234420776, 0.005105386022478342, 0.29987987875938416]\n",
      "Step 217, loss = [0.2989841103553772, 0.0025016912259161472, 0.2987339496612549]\n",
      "Step 218, loss = [0.3043208122253418, 0.003858855925500393, 0.3039349317550659]\n",
      "Step 219, loss = [0.309804767370224, 0.005125707946717739, 0.309292197227478]\n",
      "Step 220, loss = [0.29738664627075195, 0.003702800953760743, 0.29701635241508484]\n",
      "Step 221, loss = [0.31258702278137207, 0.004784862976521254, 0.3121085464954376]\n",
      "Step 222, loss = [0.2999032735824585, 0.005780844017863274, 0.29932519793510437]\n",
      "Step 223, loss = [0.30967018008232117, 0.004810581915080547, 0.309189110994339]\n",
      "Step 224, loss = [0.29120251536369324, 0.004050637595355511, 0.2907974421977997]\n",
      "Step 225, loss = [0.2964749336242676, 0.0033270129933953285, 0.29614222049713135]\n",
      "Step 226, loss = [0.3002674877643585, 0.004933392629027367, 0.2997741401195526]\n",
      "Step 227, loss = [0.306545615196228, 0.004215295426547527, 0.30612409114837646]\n",
      "Step 228, loss = [0.2974157929420471, 0.00515840295702219, 0.2968999445438385]\n",
      "Step 229, loss = [0.30385062098503113, 0.0032619182020425797, 0.3035244345664978]\n",
      "Step 230, loss = [0.2735298275947571, 0.005551771726459265, 0.2729746401309967]\n",
      "Step 231, loss = [0.2986132800579071, 0.006244728807359934, 0.29798880219459534]\n",
      "Step 232, loss = [0.3041580021381378, 0.005762498360127211, 0.30358174443244934]\n",
      "Step 233, loss = [0.29723289608955383, 0.0022451714612543583, 0.29700836539268494]\n",
      "Step 234, loss = [0.3124833405017853, 0.004712214693427086, 0.31201210618019104]\n",
      "Step 235, loss = [0.2978515326976776, 0.0008182289311662316, 0.297769695520401]\n",
      "Step 236, loss = [0.29956895112991333, 0.004600380081683397, 0.29910892248153687]\n",
      "Step 237, loss = [0.2975681722164154, 0.005887789651751518, 0.2969793975353241]\n",
      "Step 238, loss = [0.30052149295806885, 0.0034036708530038595, 0.300181120634079]\n",
      "Step 239, loss = [0.30493050813674927, 0.004703677725046873, 0.3044601380825043]\n",
      "Step 240, loss = [0.2968701422214508, 0.0034918608143925667, 0.2965209484100342]\n",
      "Step 241, loss = [0.30699101090431213, 0.004066923633217812, 0.30658432841300964]\n",
      "Step 242, loss = [0.3010932207107544, 0.004697543568909168, 0.30062347650527954]\n",
      "Step 243, loss = [0.2921665608882904, 0.004402474500238895, 0.29172632098197937]\n",
      "Step 244, loss = [0.31461283564567566, 0.00327616510912776, 0.3142852187156677]\n",
      "Step 245, loss = [0.2842688262462616, 0.0036606905050575733, 0.28390276432037354]\n",
      "Step 246, loss = [0.2993665039539337, 0.006201113108545542, 0.29874640703201294]\n",
      "Step 247, loss = [0.29247626662254333, 0.003822494298219681, 0.29209402203559875]\n",
      "Step 248, loss = [0.29949936270713806, 0.0029450571164488792, 0.29920485615730286]\n",
      "Step 249, loss = [0.2834923565387726, 0.002587051596492529, 0.283233642578125]\n",
      "Step 250, loss = [0.3027089536190033, 0.00542967114597559, 0.3021659851074219]\n",
      "Step 251, loss = [0.3063633143901825, 0.004547900054603815, 0.30590853095054626]\n",
      "Step 252, loss = [0.30918020009994507, 0.002625176450237632, 0.30891767144203186]\n",
      "Step 253, loss = [0.29528191685676575, 0.0033935937099158764, 0.29494255781173706]\n",
      "Step 254, loss = [0.29082125425338745, 0.0037199293728917837, 0.29044926166534424]\n",
      "Step 255, loss = [0.29589352011680603, 0.003712200094014406, 0.2955223023891449]\n",
      "Step 256, loss = [0.2991943955421448, 0.0022944482043385506, 0.2989649474620819]\n",
      "Step 257, loss = [0.30602243542671204, 0.0050499336794018745, 0.30551743507385254]\n",
      "Step 258, loss = [0.29879051446914673, 0.008290865458548069, 0.29796141386032104]\n",
      "Step 259, loss = [0.3116472363471985, 0.004112769849598408, 0.3112359642982483]\n",
      "Step 260, loss = [0.31107819080352783, 0.0023555303923785686, 0.3108426332473755]\n",
      "Step 261, loss = [0.2862878441810608, 0.00478382920846343, 0.2858094573020935]\n",
      "Step 262, loss = [0.2856740653514862, 0.0034668119624257088, 0.28532737493515015]\n",
      "Step 263, loss = [0.29792520403862, 0.0054466514848172665, 0.2973805367946625]\n",
      "Step 264, loss = [0.29968154430389404, 0.0037497435696423054, 0.29930657148361206]\n",
      "Step 265, loss = [0.3097892999649048, 0.002491635736078024, 0.30954012274742126]\n",
      "Step 266, loss = [0.29874905943870544, 0.002636710414662957, 0.2984853982925415]\n",
      "Step 267, loss = [0.28505420684814453, 0.0029994300566613674, 0.28475427627563477]\n",
      "Step 268, loss = [0.30108290910720825, 0.0040624430403113365, 0.3006766736507416]\n",
      "Step 269, loss = [0.29632800817489624, 0.0035454588942229748, 0.29597344994544983]\n",
      "Step 270, loss = [0.3026644289493561, 0.0045710839331150055, 0.3022073209285736]\n",
      "Step 271, loss = [0.2884039878845215, 0.003636333392933011, 0.2880403399467468]\n",
      "Step 272, loss = [0.2860914170742035, 0.001227883854880929, 0.2859686315059662]\n",
      "Step 273, loss = [0.3006291687488556, 0.0030327930580824614, 0.3003259003162384]\n",
      "Step 274, loss = [0.30796027183532715, 0.0052220504730939865, 0.30743807554244995]\n",
      "Step 275, loss = [0.30685675144195557, 0.003404144197702408, 0.3065163493156433]\n",
      "Step 276, loss = [0.29980891942977905, 0.0037840851582586765, 0.2994305193424225]\n",
      "Step 277, loss = [0.2927810251712799, 0.00575006939470768, 0.2922060191631317]\n",
      "Step 278, loss = [0.29520463943481445, 0.003475131932646036, 0.29485711455345154]\n",
      "Step 279, loss = [0.31015223264694214, 0.005496779922395945, 0.3096025586128235]\n",
      "Step 280, loss = [0.30228689312934875, 0.006174917332828045, 0.3016693890094757]\n",
      "Step 281, loss = [0.2996443808078766, 0.001778178964741528, 0.2994665503501892]\n",
      "Step 282, loss = [0.29939767718315125, 0.004920633044093847, 0.298905611038208]\n",
      "Step 283, loss = [0.30305543541908264, 0.003395040286704898, 0.302715927362442]\n",
      "Step 284, loss = [0.3085636794567108, 0.007016581483185291, 0.3078620135784149]\n",
      "Step 285, loss = [0.2822081744670868, 0.0037575517781078815, 0.28183242678642273]\n",
      "Step 286, loss = [0.30939480662345886, 0.006006164941936731, 0.30879420042037964]\n",
      "Step 287, loss = [0.3054209053516388, 0.003059184644371271, 0.3051149845123291]\n",
      "Step 288, loss = [0.3108089566230774, 0.004288580734282732, 0.31038010120391846]\n",
      "Step 289, loss = [0.2877736985683441, 0.004152277484536171, 0.28735846281051636]\n",
      "Step 290, loss = [0.28528279066085815, 0.003131191013380885, 0.28496965765953064]\n",
      "Step 291, loss = [0.2953602075576782, 0.003031705506145954, 0.2950570285320282]\n",
      "Step 292, loss = [0.303306519985199, 0.0029648486524820328, 0.3030100464820862]\n",
      "Step 293, loss = [0.29509982466697693, 0.002623718697577715, 0.29483744502067566]\n",
      "Step 294, loss = [0.31119075417518616, 0.003260463010519743, 0.31086471676826477]\n",
      "Step 295, loss = [0.2959784269332886, 0.00662075774744153, 0.29531633853912354]\n",
      "Step 296, loss = [0.3083514869213104, 0.005196896381676197, 0.3078317940235138]\n",
      "Step 297, loss = [0.31038379669189453, 0.002298562088981271, 0.31015393137931824]\n",
      "Step 298, loss = [0.2827623784542084, 0.007482774555683136, 0.2820141017436981]\n",
      "Step 299, loss = [0.30273282527923584, 0.0044030724093317986, 0.30229252576828003]\n",
      "Step 300, loss = [0.30755820870399475, 0.003460718784481287, 0.30721214413642883]\n",
      "Step 301, loss = [0.28707945346832275, 0.0045420927926898, 0.2866252362728119]\n",
      "Step 302, loss = [0.29960301518440247, 0.0045572612434625626, 0.29914727807044983]\n",
      "Step 303, loss = [0.2959882616996765, 0.003869977779686451, 0.2956012785434723]\n",
      "Step 304, loss = [0.3089064955711365, 0.00487571069970727, 0.3084189295768738]\n",
      "Step 305, loss = [0.30427324771881104, 0.0018778982339426875, 0.30408546328544617]\n",
      "Step 306, loss = [0.3065660893917084, 0.0039460668340325356, 0.3061714768409729]\n",
      "Step 307, loss = [0.2917316257953644, 0.004218802787363529, 0.29130974411964417]\n",
      "Step 308, loss = [0.3120051622390747, 0.003324247896671295, 0.31167274713516235]\n",
      "Step 309, loss = [0.29934462904930115, 0.004441280849277973, 0.2989005148410797]\n",
      "Step 310, loss = [0.2876380980014801, 0.0035078690852969885, 0.28728732466697693]\n",
      "Step 311, loss = [0.29237911105155945, 0.00448316615074873, 0.29193079471588135]\n",
      "Step 312, loss = [0.28107109665870667, 0.005846680141985416, 0.28048643469810486]\n",
      "Step 313, loss = [0.31157398223876953, 0.003405435476452112, 0.31123343110084534]\n",
      "Step 314, loss = [0.29703834652900696, 0.002851272001862526, 0.2967532277107239]\n",
      "Step 315, loss = [0.3040882349014282, 0.0021058705169707537, 0.30387765169143677]\n",
      "Step 316, loss = [0.28387853503227234, 0.002294219098985195, 0.28364911675453186]\n",
      "Step 317, loss = [0.2912123203277588, 0.00308678625151515, 0.29090362787246704]\n",
      "Step 318, loss = [0.29051798582077026, 0.0033964153844863176, 0.2901783585548401]\n",
      "Step 319, loss = [0.2929174602031708, 0.004377494566142559, 0.2924797236919403]\n",
      "Step 320, loss = [0.31530797481536865, 0.005508000031113625, 0.31475716829299927]\n",
      "Step 321, loss = [0.28999537229537964, 0.0048750946298241615, 0.2895078659057617]\n",
      "Step 322, loss = [0.30535387992858887, 0.004043988883495331, 0.30494949221611023]\n",
      "Step 323, loss = [0.2999892830848694, 0.004568928852677345, 0.29953238368034363]\n",
      "Step 324, loss = [0.3074767291545868, 0.005723464768379927, 0.3069043755531311]\n",
      "Step 325, loss = [0.2975110411643982, 0.0036649310495704412, 0.2971445620059967]\n",
      "Step 326, loss = [0.2972826361656189, 0.005826790817081928, 0.29669997096061707]\n",
      "Step 327, loss = [0.31084880232810974, 0.005695555824786425, 0.3102792501449585]\n",
      "Step 328, loss = [0.3022761642932892, 0.0039575304836034775, 0.301880419254303]\n",
      "Step 329, loss = [0.30277886986732483, 0.0029251405503600836, 0.3024863600730896]\n",
      "Step 330, loss = [0.29503393173217773, 0.0016290610656142235, 0.2948710322380066]\n",
      "Step 331, loss = [0.3078576326370239, 0.002794495318084955, 0.3075781762599945]\n",
      "Step 332, loss = [0.28768905997276306, 0.006621185690164566, 0.28702694177627563]\n",
      "Step 333, loss = [0.2910892963409424, 0.004977427423000336, 0.2905915677547455]\n",
      "Step 334, loss = [0.2923734486103058, 0.002615794073790312, 0.292111873626709]\n",
      "Step 335, loss = [0.2812674939632416, 0.005865550599992275, 0.28068092465400696]\n",
      "Step 336, loss = [0.3115994930267334, 0.003653968684375286, 0.31123408675193787]\n",
      "Step 337, loss = [0.2906304895877838, 0.0030650338158011436, 0.29032397270202637]\n",
      "Step 338, loss = [0.298702597618103, 0.003427813295274973, 0.29835981130599976]\n",
      "Step 339, loss = [0.27871811389923096, 0.0030029858462512493, 0.27841782569885254]\n",
      "Step 340, loss = [0.2860807180404663, 0.0040006013587117195, 0.2856806516647339]\n",
      "Step 341, loss = [0.2920003831386566, 0.002412530593574047, 0.2917591333389282]\n",
      "Step 342, loss = [0.2812763750553131, 0.0016654120991006494, 0.28110983967781067]\n",
      "Step 343, loss = [0.2909005582332611, 0.0023951514158397913, 0.2906610369682312]\n",
      "Step 344, loss = [0.3123821020126343, 0.002338157035410404, 0.3121482729911804]\n",
      "Step 345, loss = [0.2994089126586914, 0.006100897677242756, 0.2987988293170929]\n",
      "Step 346, loss = [0.2932443916797638, 0.0036840441171079874, 0.2928759753704071]\n",
      "Step 347, loss = [0.3076319098472595, 0.0031339535489678383, 0.3073185086250305]\n",
      "Step 348, loss = [0.29171332716941833, 0.003213107120245695, 0.2913920283317566]\n",
      "Step 349, loss = [0.287119597196579, 0.004936976358294487, 0.2866258919239044]\n",
      "Step 350, loss = [0.30005741119384766, 0.00290723517537117, 0.2997666895389557]\n",
      "Step 351, loss = [0.30657678842544556, 0.0030757971107959747, 0.30626919865608215]\n",
      "Step 352, loss = [0.29318109154701233, 0.006723076105117798, 0.2925087809562683]\n",
      "Step 353, loss = [0.3090878129005432, 0.003747696988284588, 0.30871304869651794]\n",
      "Step 354, loss = [0.27909064292907715, 0.005577759351581335, 0.27853286266326904]\n",
      "Step 355, loss = [0.3059976398944855, 0.004272941965609789, 0.3055703341960907]\n",
      "Step 356, loss = [0.3054114580154419, 0.0012098378501832485, 0.30529046058654785]\n",
      "Step 357, loss = [0.31312084197998047, 0.004512583836913109, 0.312669575214386]\n",
      "Step 358, loss = [0.3017563223838806, 0.003623098134994507, 0.3013940155506134]\n",
      "Step 359, loss = [0.30365413427352905, 0.003120034234598279, 0.30334213376045227]\n",
      "Step 360, loss = [0.29683858156204224, 0.0032566736917942762, 0.2965129017829895]\n",
      "Step 361, loss = [0.31409502029418945, 0.00394838023930788, 0.3137001693248749]\n",
      "Step 362, loss = [0.306063175201416, 0.0031655090861022472, 0.3057466149330139]\n",
      "Step 363, loss = [0.28441256284713745, 0.0057161846198141575, 0.28384095430374146]\n",
      "Step 364, loss = [0.29295024275779724, 0.003661632537841797, 0.292584091424942]\n",
      "Step 365, loss = [0.2987867295742035, 0.0032323282212018967, 0.29846349358558655]\n",
      "Step 366, loss = [0.3020021915435791, 0.005789448507130146, 0.30142325162887573]\n",
      "Step 367, loss = [0.29576244950294495, 0.0036116871051490307, 0.29540127515792847]\n",
      "Step 368, loss = [0.30932968854904175, 0.005417367443442345, 0.3087879419326782]\n",
      "Step 369, loss = [0.30212023854255676, 0.002122380305081606, 0.3019079864025116]\n",
      "Step 370, loss = [0.29724156856536865, 0.005806945264339447, 0.2966608703136444]\n",
      "Step 371, loss = [0.30490225553512573, 0.0019222368719056249, 0.3047100305557251]\n",
      "Step 372, loss = [0.29862719774246216, 0.0022473654244095087, 0.29840245842933655]\n",
      "Step 373, loss = [0.27674272656440735, 0.004510218743234873, 0.27629169821739197]\n",
      "Step 374, loss = [0.2988913357257843, 0.0047287470661103725, 0.29841846227645874]\n",
      "Step 375, loss = [0.2947918176651001, 0.005910580046474934, 0.29420074820518494]\n",
      "Step 376, loss = [0.30185940861701965, 0.005005193408578634, 0.3013588786125183]\n",
      "Step 377, loss = [0.30828723311424255, 0.0032277172431349754, 0.3079644739627838]\n",
      "Step 378, loss = [0.29611724615097046, 0.003056245157495141, 0.29581162333488464]\n",
      "Step 379, loss = [0.3073323369026184, 0.0024056858383119106, 0.30709177255630493]\n",
      "Step 380, loss = [0.31013521552085876, 0.0030464944429695606, 0.30983057618141174]\n",
      "Step 381, loss = [0.29677605628967285, 0.0037403665482997894, 0.2964020073413849]\n",
      "Step 382, loss = [0.2965739369392395, 0.006591679062694311, 0.29591476917266846]\n",
      "Step 383, loss = [0.308535635471344, 0.003620801493525505, 0.3081735670566559]\n",
      "Step 384, loss = [0.29809659719467163, 0.0041723307222127914, 0.2976793646812439]\n",
      "Step 385, loss = [0.297534316778183, 0.005077284760773182, 0.2970265746116638]\n",
      "Step 386, loss = [0.308435320854187, 0.004848063457757235, 0.30795052647590637]\n",
      "Step 387, loss = [0.2806433439254761, 0.0038344520144164562, 0.280259907245636]\n",
      "Step 388, loss = [0.29756468534469604, 0.005714942701160908, 0.2969931960105896]\n",
      "Step 389, loss = [0.3073809742927551, 0.0060943979769945145, 0.30677154660224915]\n",
      "Step 390, loss = [0.303080290555954, 0.0030081437435001135, 0.3027794659137726]\n",
      "Step 391, loss = [0.29716846346855164, 0.002192325424402952, 0.29694923758506775]\n",
      "Step 392, loss = [0.30220910906791687, 0.0025187695864588022, 0.30195721983909607]\n",
      "Step 393, loss = [0.2978481948375702, 0.003084421157836914, 0.29753974080085754]\n",
      "Step 394, loss = [0.2780109643936157, 0.0036635117139667273, 0.2776446044445038]\n",
      "Step 395, loss = [0.2870384156703949, 0.003346646437421441, 0.28670376539230347]\n",
      "Step 396, loss = [0.3076506555080414, 0.0037518481258302927, 0.3072754740715027]\n",
      "Step 397, loss = [0.2789916694164276, 0.007821613922715187, 0.27820950746536255]\n",
      "Step 398, loss = [0.30844077467918396, 0.005715623497962952, 0.30786922574043274]\n",
      "Step 399, loss = [0.2863107919692993, 0.0018663762602955103, 0.2861241400241852]\n",
      "Step 400, loss = [0.30053386092185974, 0.003780968487262726, 0.30015575885772705]\n",
      "Step 401, loss = [0.2845666706562042, 0.001344967749901116, 0.28443217277526855]\n",
      "Step 402, loss = [0.2950702905654907, 0.0028878524899482727, 0.29478150606155396]\n",
      "Step 403, loss = [0.29913100600242615, 0.005636502988636494, 0.29856735467910767]\n",
      "Step 404, loss = [0.28031134605407715, 0.003785716602578759, 0.27993276715278625]\n",
      "Step 405, loss = [0.2959165573120117, 0.002601500367745757, 0.2956564128398895]\n",
      "Step 406, loss = [0.3133782148361206, 0.003569167573004961, 0.31302130222320557]\n",
      "Step 407, loss = [0.28926828503608704, 0.006111545953899622, 0.28865712881088257]\n",
      "Step 408, loss = [0.30059728026390076, 0.0053480761125683784, 0.30006247758865356]\n",
      "Step 409, loss = [0.3141497075557709, 0.0036303307861089706, 0.31378668546676636]\n",
      "Step 410, loss = [0.2928510010242462, 0.004589559510350227, 0.2923920452594757]\n",
      "Step 411, loss = [0.3113356828689575, 0.003925958648324013, 0.3109430968761444]\n",
      "Step 412, loss = [0.28741687536239624, 0.002459048293530941, 0.28717097640037537]\n",
      "Step 413, loss = [0.3012690544128418, 0.004030836280435324, 0.3008659780025482]\n",
      "Step 414, loss = [0.3115587532520294, 0.004752466455101967, 0.31108349561691284]\n",
      "Step 415, loss = [0.30540305376052856, 0.004386603366583586, 0.3049643933773041]\n",
      "Step 416, loss = [0.2842785120010376, 0.004936983808875084, 0.28378480672836304]\n",
      "Step 417, loss = [0.3025701344013214, 0.003856248687952757, 0.302184522151947]\n",
      "Step 418, loss = [0.2944413125514984, 0.002683012979105115, 0.294173002243042]\n",
      "Step 419, loss = [0.298141747713089, 0.0034810276702046394, 0.2977936565876007]\n",
      "Step 420, loss = [0.30985522270202637, 0.0015847277827560902, 0.309696763753891]\n",
      "Step 421, loss = [0.29672330617904663, 0.0030722059309482574, 0.2964160740375519]\n",
      "Step 422, loss = [0.2959778606891632, 0.0038080408703535795, 0.29559704661369324]\n",
      "Step 423, loss = [0.30946987867355347, 0.0028483085334300995, 0.30918505787849426]\n",
      "Step 424, loss = [0.30514097213745117, 0.0069992756471037865, 0.30444103479385376]\n",
      "Step 425, loss = [0.29614534974098206, 0.0034411961678415537, 0.29580122232437134]\n",
      "Step 426, loss = [0.3021482825279236, 0.005268174223601818, 0.3016214668750763]\n",
      "Step 427, loss = [0.2933039367198944, 0.0032185642048716545, 0.2929820716381073]\n",
      "Step 428, loss = [0.2944031357765198, 0.005406233482062817, 0.293862521648407]\n",
      "Step 429, loss = [0.29806509613990784, 0.0028212815523147583, 0.2977829575538635]\n",
      "Step 430, loss = [0.29689404368400574, 0.007435794919729233, 0.29615047574043274]\n",
      "Step 431, loss = [0.3096246123313904, 0.006687265820801258, 0.3089558780193329]\n",
      "Step 432, loss = [0.2959161102771759, 0.003318100469186902, 0.2955842912197113]\n",
      "Step 433, loss = [0.295393168926239, 0.002263493835926056, 0.29516682028770447]\n",
      "Step 434, loss = [0.29690253734588623, 0.0023022149689495564, 0.2966723144054413]\n",
      "Step 435, loss = [0.28703832626342773, 0.003863336518406868, 0.28665199875831604]\n",
      "Step 436, loss = [0.30934908986091614, 0.003358858171850443, 0.3090132176876068]\n",
      "Step 437, loss = [0.3000863492488861, 0.005418428219854832, 0.2995445132255554]\n",
      "Step 438, loss = [0.30483612418174744, 0.003452418837696314, 0.3044908940792084]\n",
      "Step 439, loss = [0.29652994871139526, 0.00305708940140903, 0.2962242364883423]\n",
      "Step 440, loss = [0.3006601333618164, 0.0020558005198836327, 0.3004545569419861]\n",
      "Step 441, loss = [0.31247398257255554, 0.0038648995105177164, 0.3120875060558319]\n",
      "Step 442, loss = [0.29543477296829224, 0.0017206213669851422, 0.29526272416114807]\n",
      "Step 443, loss = [0.30933046340942383, 0.005080848000943661, 0.3088223934173584]\n",
      "Step 444, loss = [0.2834719121456146, 0.00640993844717741, 0.2828309237957001]\n",
      "Step 445, loss = [0.30546432733535767, 0.005638972856104374, 0.3049004375934601]\n",
      "Step 446, loss = [0.29537081718444824, 0.004703544080257416, 0.29490047693252563]\n",
      "Step 447, loss = [0.3030254542827606, 0.004024273715913296, 0.30262303352355957]\n",
      "Step 448, loss = [0.3022367060184479, 0.004210412502288818, 0.3018156588077545]\n",
      "Step 449, loss = [0.29964104294776917, 0.004914507735520601, 0.29914960265159607]\n",
      "Step 450, loss = [0.3023061752319336, 0.002606583060696721, 0.3020455241203308]\n",
      "Step 451, loss = [0.28673475980758667, 0.0032551386393606663, 0.28640925884246826]\n",
      "Step 452, loss = [0.28509488701820374, 0.001911224564537406, 0.28490376472473145]\n",
      "Step 453, loss = [0.3044686019420624, 0.005168853793293238, 0.3039517104625702]\n",
      "Step 454, loss = [0.3041488230228424, 0.003918691538274288, 0.3037569522857666]\n",
      "Step 455, loss = [0.2988073229789734, 0.001622777315787971, 0.2986450493335724]\n",
      "Step 456, loss = [0.29795411229133606, 0.0040161325596272945, 0.2975524961948395]\n",
      "Step 457, loss = [0.289048433303833, 0.009241967462003231, 0.2881242334842682]\n",
      "Step 458, loss = [0.2978140413761139, 0.0038525224663317204, 0.29742878675460815]\n",
      "Step 459, loss = [0.3022202253341675, 0.003367349039763212, 0.3018834888935089]\n",
      "Step 460, loss = [0.300965815782547, 0.004684779793024063, 0.3004973232746124]\n",
      "Step 461, loss = [0.3025319576263428, 0.003870623419061303, 0.3021448850631714]\n",
      "Step 462, loss = [0.2971147298812866, 0.0025311345234513283, 0.2968616187572479]\n",
      "Step 463, loss = [0.315582275390625, 0.0032134412322193384, 0.3152609169483185]\n",
      "Step 464, loss = [0.307181179523468, 0.0022368580102920532, 0.306957483291626]\n",
      "Step 465, loss = [0.29409170150756836, 0.005334073677659035, 0.2935582995414734]\n",
      "Step 466, loss = [0.30180761218070984, 0.004157858435064554, 0.3013918399810791]\n",
      "Step 467, loss = [0.30634981393814087, 0.0020414062310010195, 0.30614566802978516]\n",
      "Step 468, loss = [0.3055546283721924, 0.0058436403051018715, 0.30497026443481445]\n",
      "Step 469, loss = [0.3008304536342621, 0.007587808184325695, 0.30007168650627136]\n",
      "Step 470, loss = [0.2779792845249176, 0.0061241756193339825, 0.27736687660217285]\n",
      "Step 471, loss = [0.30028802156448364, 0.004428802989423275, 0.2998451292514801]\n",
      "Step 472, loss = [0.29666104912757874, 0.003120720386505127, 0.2963489890098572]\n",
      "Step 473, loss = [0.30113324522972107, 0.0030987346544861794, 0.3008233606815338]\n",
      "Step 474, loss = [0.30065613985061646, 0.0016187566798180342, 0.3004942536354065]\n",
      "Step 475, loss = [0.3012942969799042, 0.00576041080057621, 0.3007182478904724]\n",
      "Step 476, loss = [0.3087241053581238, 0.005593456327915192, 0.3081647455692291]\n",
      "Step 477, loss = [0.30260971188545227, 0.002160019939765334, 0.30239370465278625]\n",
      "Step 478, loss = [0.2967330515384674, 0.003239891491830349, 0.29640907049179077]\n",
      "Step 479, loss = [0.2919996976852417, 0.00249280477873981, 0.291750431060791]\n",
      "Step 480, loss = [0.2891176640987396, 0.002807316370308399, 0.28883692622184753]\n",
      "Step 481, loss = [0.2971053421497345, 0.003081514034420252, 0.2967971861362457]\n",
      "Step 482, loss = [0.296779990196228, 0.00439638365060091, 0.29634034633636475]\n",
      "Step 483, loss = [0.2898138761520386, 0.00543129863217473, 0.2892707586288452]\n",
      "Step 484, loss = [0.27675607800483704, 0.0015332403127104044, 0.27660274505615234]\n",
      "Step 485, loss = [0.3040228486061096, 0.003899101633578539, 0.3036329448223114]\n",
      "Step 486, loss = [0.29423996806144714, 0.0034163398668169975, 0.293898344039917]\n",
      "Step 487, loss = [0.3079638183116913, 0.0035795653238892555, 0.3076058626174927]\n",
      "Step 488, loss = [0.3086838722229004, 0.002773344051092863, 0.3084065318107605]\n",
      "Update target distribution epoch 3 step 489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 11115/11754 [1:44:50<06:05,  1.75it/s]"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    step_losses = []\n",
    "#     q_upd_step = 0\n",
    "    for step, (x_batch, idx) in enumerate(ds_train):\n",
    "        if step % update_interval == 0:\n",
    "            if step==0:\n",
    "                q = pd.read_csv('00_Data/q_distrib/q_0_0.csv', index_col=0, header=None)\n",
    "                p = pd.read_csv('00_Data/p_distrib/p_0_0.csv', index_col=0, header=None)\n",
    "            else:\n",
    "                print('Update target distribution epoch {0} step {1}'.format(epoch, step))\n",
    "                q = {}\n",
    "                with tqdm(total=len(ALL_IDS)) as pbar:\n",
    "                    for i in ALL_IDS:\n",
    "                        x, _ = get_inputs(i)\n",
    "                        x = x.reshape(1,52, 66, 56, 53)\n",
    "                        preds, _ = model.predict(x, batch_size=1, verbose=0)\n",
    "                        q[i] = preds[0]\n",
    "                        pbar.update(1)\n",
    "                q = pd.DataFrame(q).T\n",
    "                q.to_csv('00_Data/q_distrib/q_{0}_{1}.csv'.format(epoch, step), index=True, header=False)\n",
    "#                 q = pd.read_csv('00_Data/q_distrib/q_{0}_{1}.csv'.format(epoch, step), index_col=0, header=None)\n",
    "                p = target_distribution(q.values)  # update the auxiliary target distribution p\n",
    "                p = pd.DataFrame(p, index=q.index.to_list())\n",
    "                p.to_csv('00_Data/p_distrib/p_{0}_{1}.csv'.format(epoch, step), index=True, header=False)\n",
    "                p = pd.read_csv('00_Data/p_distrib/p_{0}_{1}.csv'.format(epoch, step), index_col=0, header=None)\n",
    "\n",
    "            # evaluate the clustering performance\n",
    "            y_pred = np.argmax(q.values, axis=1)\n",
    "\n",
    "            # check stop criterion\n",
    "            delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "            y_pred_last = np.copy(y_pred)\n",
    "            if step > 0 and delta_label < tol:\n",
    "                print('delta_label ', delta_label, '< tol ', tol)\n",
    "                print('Reached tolerance threshold. Stopping training.')\n",
    "                break\n",
    "#             q_uod_step+=1\n",
    "#         p = pd.read_csv('00_Data/p_distrib/p_{0}_{1}.csv'.format(epoch, step), index_col=0, header=None)\n",
    "        y_probs = p.loc[idx]\n",
    "        y_probs = y_probs.values\n",
    "#         print(x_batch)\n",
    "        loss = model.train_on_batch(x=x_batch, y=[y_probs, x_batch])\n",
    "        step_losses.append(loss)\n",
    "        print('Step {0}, loss = {1}'.format(step, loss))\n",
    "    epoch_avloss = np.mean(np.array(step_losses), axis=0)\n",
    "    print('Epoch {0}, loss = {1}'.format(epoch, epoch_avloss))\n",
    "    losses.append(epoch_avloss)\n",
    "    model.save_weights('99_Training_checkpoints/mri_clustering/run_05/model_weights_chpt_{0}.h5'.format(epoch))\n",
    "\n",
    "np.savetxt('99_Logs/mri_clustering/run_05/losses.csv', np.array(losses), delimiter=',')\n",
    "model.save_weights('99_Training_checkpoints/mri_clustering/run_05/model_weights_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 2\n",
    "# for epoch in range(epochs):\n",
    "#     print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "#     # Iterate over the batches of the dataset.\n",
    "#     for step, (x_batch_train, y_batch_train) in enumerate(ds_train):\n",
    "\n",
    "#         # Open a GradientTape to record the operations run\n",
    "#         # during the forward pass, which enables autodifferentiation.\n",
    "#         with tf.GradientTape() as tape:\n",
    "\n",
    "#             # Run the forward pass of the layer.\n",
    "#             # The operations that the layer applies\n",
    "#             # to its inputs are going to be recorded\n",
    "#             # on the GradientTape.\n",
    "#             logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "#             # Compute the loss value for this minibatch.\n",
    "#             loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "#         # Use the gradient tape to automatically retrieve\n",
    "#         # the gradients of the trainable variables with respect to the loss.\n",
    "#         grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "#         # Run one step of gradient descent by updating\n",
    "#         # the value of the variables to minimize the loss.\n",
    "#         optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "#         # Log every 200 batches.\n",
    "#         if step % 200 == 0:\n",
    "#             print(\n",
    "#                 \"Training loss (for one batch) at step %d: %.4f\"\n",
    "#                 % (step, float(loss_value))\n",
    "#             )\n",
    "#             print(\"Seen so far: %s samples\" % ((step + 1) * 64))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
